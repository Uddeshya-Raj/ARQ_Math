{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074106bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a5b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmltree = ET.parse(\"Posts.xml.txt\")\n",
    "root = xmltree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff512f",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9190111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicTags(taglist):\n",
    "    regex = re.compile(r\"(<[^<>]+>)\")\n",
    "    list = regex.findall(taglist)\n",
    "    output = [topic[1:-1] for topic in list]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6402581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeParagraphTags(body):\n",
    "    body.strip()\n",
    "    sentences = \"\"\n",
    "    splitter = re.compile(r'</p>')\n",
    "    paragraphs = re.split(splitter, body)\n",
    "    for p in range(len(paragraphs)):\n",
    "        if(len(paragraphs[p].strip()) != 0):\n",
    "            paragraphs[p] += '</p>'\n",
    "        else:\n",
    "            del paragraphs[p]\n",
    "    regex = re.compile(r'<p>(.+)</p>')\n",
    "    for p in paragraphs:\n",
    "        if(regex.search(p) is not None):\n",
    "            output = regex.search(p)\n",
    "            # print(output.group(1))\n",
    "            sentences += output.group(1).strip()+\" \"\n",
    "    # print(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb62664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAnchorTags(para):\n",
    "    #finding and removing links\n",
    "    splitter = re.compile(r'</a>')\n",
    "    lines = re.split(splitter, para)\n",
    "    line_count = len(lines)\n",
    "    for i in range(line_count-1):\n",
    "        lines[i] +='</a>'\n",
    "    regex = re.compile(r'(<a href=\")([^\"]+)(\".*>)(.*)(</a>)')\n",
    "    processed_array = []\n",
    "    processed_para = ''\n",
    "    \n",
    "    for line in lines:\n",
    "        if(regex.search(line) is not None):\n",
    "            output = regex.search(line)\n",
    "            repl = output.group(4)\n",
    "            repl = \"/\".join(repl.split('\\\\'))\n",
    "            # print(repl)\n",
    "            line = re.sub(regex, repl, line)\n",
    "            # print(\"line:\\t\", line)\n",
    "            processed_array.append(output.group(2))\n",
    "        processed_para += line\n",
    "    res_dict = {}\n",
    "    res_dict['para'] = processed_para\n",
    "    res_dict['urls'] = processed_array\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca2134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processMathFormulae(paragraph):\n",
    "    # print(paragraph)\n",
    "    splitter = re.compile(r'</span>')\n",
    "    lines = re.split(splitter, paragraph)\n",
    "    line_count = len(lines)\n",
    "    for i in range(line_count-1):\n",
    "        lines[i] +='</span>'\n",
    "    res = {}\n",
    "    res['para'] = ''\n",
    "    res['exp'] = []\n",
    "    regex = re.compile(r'(<span class=\"math-container\".*>)(.+)(</span>)')\n",
    "    for line in lines:\n",
    "        if regex.search(line) is not None:\n",
    "            output = regex.search(line)\n",
    "            line = re.sub(regex,'<math_exp>', line)\n",
    "            res['exp'].append(output.group(2))\n",
    "            \n",
    "        res['para'] += line\n",
    "    # print(res['exp'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11428d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAnyOtherTags(body):\n",
    "    # to be used at the end of cleaning\n",
    "    tag = re.compile(r'<.?\\b(?!math_exp\\b)\\w+>')\n",
    "    lines = re.split(tag, body)\n",
    "    res = \"\".join(lines)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ba7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'Id': '1', 'Type': 'question', 'Title': 'What Does it Really Mean to Have Different Kinds of Infinities?', 'Tags': ['elementary-set-theory', 'intuition', 'infinity', 'faq'], 'AcceptedAnswerId': '9', 'urls': ['http://en.wikipedia.org/wiki/The_Man_Who_Loved_Only_Numbers', 'http://en.wikipedia.org/wiki/Paul_Hoffman_(science_writer)'], 'exp': [], 'Body': 'Can someone explain to me how there can be different kinds of infinities? I was reading \"The man who loved only numbers\" by Paul Hoffman and came across the concept of countable and uncountable infinities, but they\\'re only words to me. Any help would be appreciated. '}\n",
      "3 {'Id': '3', 'Type': 'question', 'Title': 'List of interesting math podcasts?', 'Tags': ['soft-question', 'big-list', 'online-resources'], 'urls': ['http://mathfactor.uark.edu/'], 'exp': [], 'Body': 'mathfactor is one I listen to.  Does anyone else have a recommendation? '}\n",
      "4 {'Id': '4', 'Type': 'answer', 'ParentId': '3', 'urls': ['http://www.bbc.co.uk/podcasts/series/moreorless', 'http://timharford.com/', 'http://www.ft.com/home/uk'], 'exp': [], 'Body': \"More or Less is a BBC Radio 4 programme about maths and statistics in the news, and there is a free podcast. It's presented by Tim Harford, the Undercover Economist from the Financial Times. \"}\n",
      "5 {'Id': '5', 'Type': 'question', 'Title': 'How can you prove that the square root of two is irrational?', 'Tags': ['elementary-number-theory', 'proof-writing', 'radicals', 'rationality-testing'], 'AcceptedAnswerId': '7', 'urls': [], 'exp': ['\\\\sqrt{2}', '\\\\sqrt{2}'], 'Body': 'I have read a few proofs that <math_exp> is irrational. I have never, however, been able to really grasp what they were talking about. Is there a simplified proof that <math_exp> is irrational? '}\n",
      "6 {'Id': '6', 'Type': 'question', 'Title': 'What is your favorite online graphing tool?', 'Tags': ['soft-question', 'math-software'], 'AcceptedAnswerId': '281', 'urls': [], 'exp': [], 'Body': \"I'm looking for a nice, quick online graphing tool. The ability to link to, or embed the output would be handy, too. \"}\n",
      "7 {'Id': '7', 'Type': 'answer', 'ParentId': '5', 'urls': [], 'exp': ['\\\\sqrt{2}', 'p/q', '2q^2 = p^2', 'q^2', 'p^2', '2q^2', 'p^2'], 'Body': \"You use a proof by contradiction. Basically, you suppose that <math_exp> can be written as <math_exp>. Then you know that <math_exp>. However, both <math_exp> and <math_exp> have an even number of factors of two, so <math_exp> has an odd number of factors of 2, which means it can't be equal to <math_exp>. \"}\n",
      "8 {'Id': '8', 'Type': 'question', 'Title': 'How are we able to calculate specific numbers in the Fibonacci Sequence?', 'Tags': ['linear-algebra', 'combinatorics', 'generating-functions', 'fibonacci-numbers'], 'AcceptedAnswerId': '17', 'urls': [], 'exp': ['\\\\text {{1,1,2,3,5,8,13,....}}'], 'Body': \"I was reading up on the Fibonacci Sequence,  <math_exp> when I've noticed some were able to calculate specific numbers. So far I've only figured out creating an array and counting to the value, which is incredibly simple, but I reckon I can't find any formula for calculating a Fibonacci number based on it's position. Is there a way to do this? If so, how are we able to apply these formulas to arrays? \"}\n",
      "9 {'Id': '9', 'Type': 'answer', 'ParentId': '1', 'urls': ['http://en.wikipedia.org/wiki/File:Pairing_natural.svg', 'http://en.wikipedia.org/wiki/Cantor&#39;s_diagonal_argument'], 'exp': [], 'Body': 'Suppose no one ever taught you the names for ordinary numbers.  Then suppose that you and I agreed that we would trade one bushel of corn for each of my sheep.  But there\\'s a problem, we don\\'t know how to count the bushels or the sheep!  So what do we do? We form a \"bijection\" between the two sets.  That\\'s just fancy language for saying you pair things up by putting one bushel next to each of the sheep.  When we\\'re done we swap.  We\\'ve just proved that the number of sheep is the same as the number of bushels without actually counting. We can try doing the same thing with infinite sets.  So suppose you have the set of positive integers and I have the set of rational numbers and you want to trade me one positive integer for each of my rationals.  Can you do so in a way that gets all of my rational numbers? Perhaps surprisingly the answer is yes!  You make the rational numbers into a big square grid with the numerator and denominators as the two coordinates.  Then you start placing your \"bushels\" along diagonals of increasing size, see wikipedia. This says that the rational numbers are \"countable\" that is you can find a clever way to count them off in the above fashion. The remarkable fact is that for the real numbers there\\'s no way at all to count them off in this way.  No matter how clever you are you won\\'t be able to scam me out of all of my real numbers by placing a natural number next to each of them.  The proof of that is Cantor\\'s clever \"diagonal argument.\" '}\n",
      "11 {'Id': '11', 'Type': 'question', 'Title': 'Is it true that <span class=\"math-container\" id=\"360\">0.999999999\\\\dots=1</span>?', 'Tags': ['real-analysis', 'algebra-precalculus', 'real-numbers', 'decimal-expansion'], 'AcceptedAnswerId': '49', 'urls': [], 'exp': ['0.999999999\\\\dots=1'], 'Body': \"I'm told by smart people that <math_exp> and I believe them, but is there a proof that explains why this is? \"}\n",
      "13 {'Id': '13', 'Type': 'answer', 'ParentId': '1', 'urls': [], 'exp': [], 'Body': \"The basic concept is thus: The key to seeing this is using the 'diagonal slash' argument as originally put forward by Cantor. With a countable infinity, you can create a list of all the items in the set and assign each one a different natural number. This can be done with the naturals (obviously) and the complete range of integers (including negative numbers) and even the rational numbers (so including fractions). It cannot be done with the reals due to the diagonal slash argument: This shows a difference between two obviously infinite sets and leads to the somewhat scary conclusion that there are (at least) 2 different forms of infinity. \"}\n",
      "14 {'Id': '14', 'Type': 'answer', 'ParentId': '1', 'urls': ['http://en.wikipedia.org/wiki/Countable_set', 'http://en.wikipedia.org/wiki/Uncountable_set', 'http://en.wikipedia.org/wiki/Cardinality'], 'exp': [], 'Body': \"Just simple intuitive explanation. How many Natural (Integers) Numbers could you count? There are infinitely many, yet you can count them. It's called Countable Set. How many Real Numbers are there? Infinitely as well (Since at least every Natural Number is a Real Number). Yet you won't be able to count them (Intuitively, Let's say you name a number the first, then find the second, I can, for sure, find a number in between, their average which is Real Number as well).  It's called Uncountable Set. What you are after is how we define how big is a given set. Then you should look for Cardinality. \"}\n",
      "15 {'Id': '15', 'Type': 'answer', 'ParentId': '6', 'urls': ['http://www.wolframalpha.com/input/?i=sin%28x%29'], 'exp': [], 'Body': 'Well, I am not sure where you want to embed the graphs, but Wolfram Alpha is pretty handy for graphing. It has most of the features of Mathematica, can handle 3D functions, and fancy scaling and such. I highly recommend it. '}\n",
      "16 {'Id': '16', 'Type': 'answer', 'ParentId': '5', 'urls': [], 'exp': ['\\\\sqrt{2}', 'R=\\\\sqrt{2}=\\\\frac{Q}{D}', 'Q', 'D', 'R', 'R^2 = 2 = \\\\frac{Q^2}{D^2}', 'Q', 'D', 'Q^2', '2', 'Q^2 = 2^1 x', 'x', 'Q^2', '\\\\sqrt{2}'], 'Body': 'Consider this proof by contradiction: Assume that <math_exp> is rational. Then there exists some rational <math_exp>, where <math_exp> and <math_exp> are positive integers and relatively prime (since <math_exp> can be expressed in simplified form). Now consider <math_exp>. Since <math_exp> and <math_exp> are relatively prime, this means that only <math_exp> can have <math_exp> in its prime decomposition, and the exponent must be one. Thus, <math_exp>, for some odd integer <math_exp>. But <math_exp> is a square, and thus the exponents for all of its prime factors must be even. Here we have a contradiction. Thus, <math_exp> must be irrational. '}\n",
      "17 {'Id': '17', 'Type': 'answer', 'ParentId': '8', 'urls': ['http://en.wikipedia.org/wiki/Fibonacci_number#Relation_to_the_golden_ratio'], 'exp': ['F\\\\left(n\\\\right) = {{\\\\varphi^n-(1-\\\\varphi)^n} \\\\over {\\\\sqrt 5}}'], 'Body': 'Wikipedia has a closed-form function called \"Binet\\'s formula\". http://en.wikipedia.org/wiki/Fibonacci_number#Relation_to_the_golden_ratio <math_exp> This is based on the Golden Ratio. '}\n",
      "18 {'Id': '18', 'Type': 'question', 'Title': 'How do you calculate the semi-minor axis of an ellipsoid?', 'Tags': ['geometry'], 'AcceptedAnswerId': '93', 'urls': [], 'exp': [], 'Body': 'Given the semi-major axis and a flattening factor, is it possible to calculate the semi-minor axis? '}\n",
      "20 {'Id': '20', 'Type': 'question', 'Title': 'What is a real number (also rational, decimal, integer, natural, cardinal, ordinal...)?', 'Tags': ['terminology', 'definition', 'number-systems'], 'AcceptedAnswerId': '40', 'urls': [], 'exp': [], 'Body': \"In mathematics, there seem to be a lot of different types of numbers. What exactly are: And as workmad3 points out, some more advanced types of numbers (I'd never heard of) Are there any other types of classifications of a number I missed? \"}\n",
      "22 {'Id': '22', 'Type': 'question', 'Title': 'Why is the matrix-defined Cross Product of two 3D vectors always orthogonal?', 'Tags': ['linear-algebra', 'matrices', 'inner-product-space', 'orthogonality', 'cross-product'], 'AcceptedAnswerId': '53', 'urls': [], 'exp': ['\\\\left&lt;a,b,c\\\\right&gt;\\\\times\\\\left&lt;d,e,f\\\\right&gt; = \\\\left|     \\\\begin{array}{ccc}  i &amp; j &amp; k\\\\\\\\  a &amp; b &amp; c\\\\\\\\  d &amp; e &amp; f  \\\\end{array}    \\\\right|', 'k', 'k'], 'Body': 'By matrix-defined, I mean <math_exp> ...instead of the definition of the product of the magnitudes multiplied by the sign of their angle, in the direction orthogonal) If I try cross producting two vectors with no <math_exp> component, I get one with only <math_exp>, which is expected. But why? As has been pointed out, I am asking why the algebraic definition lines up with the geometric definition. '}\n",
      "24 {'Id': '24', 'Type': 'answer', 'ParentId': '8', 'urls': ['http://mathworld.wolfram.com/BinetsFibonacciNumberFormula.html'], 'exp': [], 'Body': \"The closed form calculation for Fibonacci sequences is known as Binet's Formula. \"}\n",
      "26 {'Id': '26', 'Type': 'answer', 'ParentId': '8', 'urls': ['http://mathworld.wolfram.com/BinetsFibonacciNumberFormula.html', 'http://en.wikipedia.org/wiki/Binet_formula#Closed_form_expression'], 'exp': [], 'Body': \"You can use Binet's formula, described at http://mathworld.wolfram.com/BinetsFibonacciNumberFormula.html (see also Wikipedia for a proof: http://en.wikipedia.org/wiki/Binet_formula#Closed_form_expression ) \"}\n",
      "29 {'Id': '29', 'Type': 'question', 'Title': 'Can you recommend a decent online or software calculator?', 'Tags': ['soft-question', 'big-list', 'math-software', 'computer-algebra-systems'], 'AcceptedAnswerId': '1572', 'urls': [], 'exp': [], 'Body': \"I'm looking for an online or software calculator that can show me the history of items I typed in, much like an expensive Ti calculator. Can you recommend any? \"}\n",
      "30 {'Id': '30', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['\\\\frac{1}{3} = 0.\\\\bar{3}', '3\\\\times \\\\left( \\\\frac{1}{3} \\\\right) = \\\\left( 0.\\\\bar{3} \\\\right) \\\\times 3', '\\\\frac{3}{3} = 0.\\\\bar{9}'], 'Body': 'Given (by long division): <math_exp> Multiply by 3: <math_exp> Therefore: <math_exp> QED. '}\n",
      "31 {'Id': '31', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['\\\\begin{align} \\\\frac13&amp;=0.333\\\\ldots&amp;\\\\hbox{(by long division)}\\\\\\\\ \\\\implies0.333\\\\ldots\\\\times3&amp;=0.999\\\\ldots&amp;\\\\hbox{(multiplying each digit by $3$)} \\\\end{align}', '0.333\\\\ldots\\\\times3=1', '0.999\\\\ldots=1'], 'Body': \"Indeed this is true. The underlying reason is that decimal numbers are not unique representations of the reals. (Technically, there does not exist a bijection between the set of all decimal numbers and the reals.) Here's a very simple proof: <math_exp> Then we already know <math_exp> therefore <math_exp>. \"}\n",
      "35 {'Id': '35', 'Type': 'answer', 'ParentId': '1', 'urls': ['http://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel'], 'exp': [], 'Body': \"Hilbert's Hotel is a classic demonstration. \"}\n",
      "38 {'Id': '38', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://instacalc.com/'], 'exp': [], 'Body': 'Lots of people like to use Instacalc which lets you do unit conversions and store intermediate calculations in variables. '}\n",
      "40 {'Id': '40', 'Type': 'answer', 'ParentId': '20', 'urls': [], 'exp': [], 'Body': 'Natural numbers The \"counting\" numbers.  (That is, all integers, that are one or greater). Whole numbers The Natural numbers, and zero. Integers The Whole numbers, and the negatives of the Natural numbers. Rational numbers Any number that may be expressed by any integer A divided by any integer B, where B is not zero. Irrational numbers Any number that cannot be expressed as a rational number, but is not imaginary.  All irrational numbers have an infinite decimal representation. Real numbers All of the Rational and Irrational numbers. Imaginary numbers All Real numbers, multiplied by the square root of negative one.  Imaginary numbers are signified by the letter i. Complex numbers Numbers composed of the sum of a Real and an Imaginary number.  This includes all Real and all Imaginary numbers. '}\n",
      "41 {'Id': '41', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': [], 'Body': '.999...  = 1 because .999... is a concise symbolic representation of \"the limit of some variable as it approaches one.\" Therefore, .999... = 1 for the same reason the limit of x as x approaches 1 equals 1. '}\n",
      "42 {'Id': '42', 'Type': 'answer', 'ParentId': '22', 'urls': [], 'exp': ['\\\\mathbf{i}', '\\\\mathbf{j}', '\\\\mathbf{k}', '(x, y)', '\\\\mathbf{i}', '\\\\mathbf{j}', 'z', '\\\\mathbf{k}'], 'Body': 'The obvious but slightly trite answer is \"because that\\'s just how the cross-product works as an operation\". If you\\'re looking for an intuitive reason, the cross-product by definition produces a vector that is orthogonal to the two operand (input) vectors. You know that the base vectors <math_exp>, <math_exp>, and <math_exp> are all orthogonal, thus if your two input vectors lie on the <math_exp> plane (i.e. only <math_exp> and <math_exp> components), you know that any orthogonal vector must have only a component in the <math_exp> direction (multiple of <math_exp>). '}\n",
      "43 {'Id': '43', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': [\"9's\", '0.999', '1', \"9's\", '1'], 'Body': \"You can visualise it by thinking about it in infinitesimals. The more <math_exp> you have on the end of <math_exp>, the closer you get to <math_exp>. When you add an infinite number of <math_exp> to the decimal expansion, you are infinitely close to <math_exp> (or an infinitesimal distance away). And this isn't a rigorous proof, just an aid to visualisation of the result. \"}\n",
      "44 {'Id': '44', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['1/3=0.3333\\\\ldots', '\\\\begin{align}     0.9999\\\\ldots × 10 &amp;= 9.9999\\\\ldots\\\\\\\\     0.9999\\\\ldots × (9+1) &amp;= 9.9999\\\\ldots\\\\\\\\ \\\\text{by distribution rule: }\\\\Space{15ex}{0ex}{0ex} \\\\\\\\     0.9999\\\\ldots × 9 + 0.9999\\\\ldots × 1 &amp;= 9.9999\\\\ldots\\\\\\\\     0.9999\\\\ldots × 9 &amp;= 9.9999\\\\dots-0.9999\\\\ldots\\\\\\\\     0.9999\\\\ldots × 9 &amp;= 9\\\\\\\\     0.9999\\\\ldots &amp;= 1 \\\\end{align}', '9.999\\\\ldots - 0.999\\\\ldots = 9', '0.999\\\\ldots × 10 = 9.999\\\\ldots'], 'Body': \"What I really don't like about all the above answers, is the underlying assumption that <math_exp> How do you know that? It seems to me like assuming the something which is already known. A proof I really like is: <math_exp> The only things I need to assume is, that <math_exp> and that <math_exp> These seems to me intuitive enough to take for granted. The proof is from an old high school level math book of the Open University in Israel. \"}\n",
      "47 {'Id': '47', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://www.googleguide.com/help/calculator.html'], 'exp': [], 'Body': \"Google's calculator is very powerful:  http://www.googleguide.com/help/calculator.html and your use history will be stored in your browser history. \"}\n",
      "48 {'Id': '48', 'Type': 'answer', 'ParentId': '20', 'urls': [], 'exp': [], 'Body': \"You might as well start at the 'lowest': Real numbers are all the Rational numbers and all the others. Cardinality is the number of elements in a set. \"}\n",
      "49 {'Id': '49', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['.99999\\\\ldots', '.9', '.99', '.999', '.9999', '1', 'x', 'x', '1', '10^{-k}', 'k', 'k', '.99999\\\\ldots = 1', '1.0000\\\\ldots -.99999\\\\ldots = .00000\\\\ldots = 0', '1.0 -.9 = .1', '1.00-.99 = .01', '1.000-.999=.001', '\\\\ldots', '1.000\\\\ldots -.99999\\\\ldots = .000\\\\ldots = 0'], 'Body': \"What does it mean when you refer to <math_exp>?  Symbols don't mean anything in particular until you've defined what you mean by them. In this case the definition is that you are taking the limit of <math_exp>, <math_exp>, <math_exp>, <math_exp>, etc.  What does it mean to say that limit is <math_exp>?  Well, it means that no matter how small a number <math_exp> you pick, I can show you a point in that sequence such that all further numbers in the sequence are within distance <math_exp> of <math_exp>.  But certainly whatever number you choose your number is bigger than <math_exp> for some <math_exp>.  So I can just pick my point to be the <math_exp>th spot in the sequence. A more intuitive way of explaining the above argument is that the reason <math_exp> is that their difference is zero.  So let's subtract <math_exp>.  That is, <math_exp> <math_exp> <math_exp>, <math_exp> <math_exp> \"}\n",
      "51 {'Id': '51', 'Type': 'question', 'Title': 'The cow in the field problem (intersecting circular areas)', 'Tags': ['geometry'], 'AcceptedAnswerId': '109', 'urls': [], 'exp': [], 'Body': 'What length of rope should be used to tie a cow to an exterior fence post of a circular field so that the cow can only graze half of the grass within that field? updated: To be clear: the cow should be tied to a post on the exterior of the field, not a post at the center of the field. '}\n",
      "52 {'Id': '52', 'Type': 'answer', 'ParentId': '1', 'urls': [], 'exp': ['a_1,a_2,a_3,...', '0,1,-1,2,-2,3,-3,...', '\\\\frac{1}{1},\\\\frac{1}{2},\\\\frac{2}{1},\\\\frac{1}{3},\\\\frac{2}{2},\\\\frac{3}{1},\\\\frac{1}{4},\\\\frac{2}{3},\\\\frac{3}{2},\\\\frac{4}{1},\\\\frac{1}{5},\\\\frac{2}{4},...', '\\\\frac{1}{1}, \\\\frac{2}{2}', '\\\\frac{1}{2}, \\\\frac{2}{4}', '\\\\frac{1}{1},\\\\frac{1}{2},\\\\frac{2}{1},\\\\frac{1}{3},\\\\frac{3}{1},\\\\frac{1}{4},\\\\frac{2}{3},\\\\frac{3}{2},\\\\frac{4}{1},\\\\frac{1}{5},...', 'q_n', 'n', '0,q_1,-q_1,q_2,-q_2,q_3,-q_3,...'], 'Body': \"A countably infinite set is a set for which you can list the elements <math_exp> For example, the set of all integers is countably infinite since I can list its elements as follows: <math_exp> So is the set of rational numbers, but this is more difficult to see. Let's start with the positive rationals. Can you see the pattern in this listing? <math_exp> (Hint: Add the numerator and denominator to see a different pattern.) This listing has lots of repeats, e.g. <math_exp> and <math_exp>. That's ok since I can condense the listing by skipping over any repeats. <math_exp> Let's write <math_exp> for the <math_exp>-th element of this list. Then <math_exp> is a listing of all rational numbers. A countable set is a set which is either finite or countably infinite; an uncountable set is a set which is not countable. Thus, an uncountable set is an infinite set which has no listing of all of its elements (as in the definition of countably infinite set). An example of an uncountable set is the set of all real numbers. To see this, you can use the diagonal method. Ask another question to see how this works... \"}\n",
      "53 {'Id': '53', 'Type': 'answer', 'ParentId': '22', 'urls': [], 'exp': ['a\\\\cdot b=0', '(a \\\\times b)\\\\cdot a = a_1(a_2b_3-a_3b_2)-a_2(a_1b_3-a_3b_1)-a_3(a_1b_2-a_2b_1)=0', '(a \\\\times b)\\\\cdot b-0'], 'Body': 'Assuming you know the definition of orthogonal as \"a is orthogonal to b iff <math_exp> then we could calculate <math_exp> and  <math_exp>, so the cross product is orthogonal to both. As Nold mentioned, if the two vectors a and b lie in the x,y plane, then the orthogonal vectors must be purely in the z direction. '}\n",
      "54 {'Id': '54', 'Type': 'answer', 'ParentId': '20', 'urls': ['http://en.wikipedia.org/wiki/Base_10'], 'exp': [], 'Body': 'Real numbers are any numbers you can locate (even approximately) on an infinite number line.  This is a theoretical number line with infinite \"resolution\" that extends infinitely in both positive and negative directions. <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/09/Number-line.gif\" alt=\"\"> One neat property about real numbers is that they are orderable -- that is, given any two real numbers, you can tell which one is \"higher\" and which one is \"lower\" than the other. Real numbers are closed under multiplication, addition, and subtraction.  That is, if you perform any of these operations on two real numbers, their result will always be real as well.  They are almost closed under division, except for the whole divide-by-zero issue. Not real numbers: infinity the square root of -1 1/0 There isn\\'t a rigorous definition of \"decimals\", because depending on where you use it, you\\'ll get different definitions. In the elementary sense, it means any number that has a \"decimal part\"; or a part after the radix (decimal point, etc.). In a more advance sense, it means any number written in Base 10. Natural numbers are often also called \"counting numbers\", because they are the numbers you count with.  (0,) 1, 2, 3, 4, etc. There is some disagreement in the mathematics community over whether or not 0 is a natural number. In linguistics, this means the natural \"numbers\" themselves (1, 2, 3, etc.)  But you probably don\\'t want to know about linguistics. In Set Theory, two sets have the same cardinality if each element could be paired up with an element of the other set. {1,2,3} and {4,5,6} share the same cardinality because you can pair up 1&amp;4, 2&amp;5, 3&amp;6. In linguistics, this means 1st, 2nd, 3rd, etc.  But you probably don\\'t want to know about linguistics. In Set Theory, an ordinal is a well-ordered set. '}\n",
      "55 {'Id': '55', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://www.wolframalpha.com'], 'exp': [], 'Body': 'Essentially the most helpful is WolframAlpha, as Ami said, you can use your browser history here too. WolframAlpha can carry out complex equations can comparisons much like a TI Calculator. Additionally they have some areas where you can see the simplification of an equation paired with charts and graphs where possible. '}\n",
      "56 {'Id': '56', 'Type': 'question', 'Title': 'What Is An Inner Product Space?', 'Tags': ['linear-algebra', 'vector-spaces', 'inner-product-space'], 'urls': [], 'exp': [], 'Body': \"As I've understood it, what I've learned is that the dot product is just one of many possible inner product spaces. Can someone explain this concept? When is it useful to define it as something other than the dot product? \"}\n",
      "57 {'Id': '57', 'Type': 'question', 'Title': 'How do the Properties of Relations work?', 'Tags': ['elementary-set-theory', 'relations'], 'AcceptedAnswerId': '134', 'urls': [], 'exp': ['R \\\\rightarrow  R', 'R \\\\rightarrow S', 'S \\\\rightarrow  R', 'R \\\\rightarrow S', 'R \\\\rightarrow  R', 'S \\\\rightarrow  S', 'R \\\\rightarrow S', 'R \\\\rightarrow  R', 'S \\\\rightarrow  S', 'R \\\\rightarrow S', 'S \\\\rightarrow T', 'R \\\\rightarrow T'], 'Body': 'This is simply not clicking for me. I\\'m currently learning math during the summer vacation and I\\'m on the chapter for relations and functions. There are five properties for a relation: Reflexive - <math_exp> Symmetrical - <math_exp> ; <math_exp> Antisymmetrical - <math_exp> &amp;&amp; (<math_exp>|| <math_exp>) Asymmetrical -<math_exp> &amp;&amp; !(<math_exp>|| <math_exp>) Transitive - if <math_exp> &amp;&amp; <math_exp>, then <math_exp> If that\\'s not what you call the properties in English, then please let me know- I have to study it in German, unfortunately, and these are my rough translations. Continuing on, I just don\\'t know what to do with this information practically. The examples of the book are horrible: 1) \"Is the same age as\" is apparently reflexive, symmetrical and transitive.  2) \"Is related to\" is also apparently reflexive, symmetrical and transitive. 3) \"Is older than\" is asymmetric, antisymmetric and transitive. There are more useless examples like this. I have no idea how it comes to these conclusions because we\\'re talking about a literal statement. I was hoping perhaps for some real mathematical examples, but the book falls short on those. I would greatly appreciate it if somebody could explain the above example and perhaps give me a better use for Relations other than... that. Also, how can a relation be a- and antisymmetrical at the same time? Don\\'t they cancel each other out? '}\n",
      "58 {'Id': '58', 'Type': 'question', 'Title': \"Real life usage of Benford's Law\", 'Tags': ['soft-question', 'big-list', 'statistics', 'applications'], 'AcceptedAnswerId': '70', 'urls': [], 'exp': [], 'Body': \"I recently discovered Benford's Law. I find it very fascinating. I'm wondering what are some of the real life uses of Benford's law. Specific examples would be great. \"}\n",
      "59 {'Id': '59', 'Type': 'question', 'Title': 'Calculating an Angle from <span class=\"math-container\" id=\"911\">2</span> points in space', 'Tags': ['linear-algebra', 'geometry'], 'AcceptedAnswerId': '65', 'urls': [], 'exp': ['$p_1$', '$p_2$', '(0,0)', '2D', 'p_1', 'p_2', '3D'], 'Body': 'Given two points <math_exp> , <math_exp> around the origin <math_exp> in <math_exp> space, how would you calculate the angle from <math_exp> to <math_exp>? How would this change in <math_exp> space? '}\n",
      "60 {'Id': '60', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['0.9999... \\\\neq 1', '0.9999... &lt; 1', 'x', '0.9999... &lt; x &lt; 1', 'x', '9', 'x = 0.9999...', 'x &lt; 0.9999...', 'x', '0.9999...', '1'], 'Body': \"Suppose this was not the case, i.e. <math_exp>. Then <math_exp> (I hope we agree on that). But between two distinct real numbers, there's always another one (say <math_exp>) in between, hence <math_exp>. The decimal representation of <math_exp> must have a digit somewhere that is not <math_exp> (otherwise <math_exp>). But that means it's actually smaller, <math_exp>, contradicting the definition of <math_exp>. Thus, the assumption that there's a number between <math_exp> and <math_exp> is false, hence they're equal. \"}\n",
      "62 {'Id': '62', 'Type': 'answer', 'ParentId': '51', 'urls': [], 'exp': ['\\\\pi r^2', 'r_1', 'r_2', '\\\\frac{(\\\\pi r_1^2)}{2} = \\\\pi r_2^2', '\\\\frac{r_1^2 }{2} =r_2^2', 'r_2 =\\\\frac{ r_1 }{\\\\sqrt{2}}', '2'], 'Body': 'So, the area of the field is <math_exp> and you want the cow to be able to graze an area equal to half of that. All you need to do is set up the equation (<math_exp> is the radius of the field, <math_exp> is the length of the rope desired): <math_exp> You can then simplify it down: <math_exp> and then taking roots: <math_exp> So you need a rope that is equal to the radius divided by the square root of <math_exp>, and the post can be no closer than this distance to the edge of the field. '}\n",
      "63 {'Id': '63', 'Type': 'answer', 'ParentId': '58', 'urls': [], 'exp': [], 'Body': \"I've heard of it being as a rough check to see if accounting numbers were being made up \"}\n",
      "64 {'Id': '64', 'Type': 'answer', 'ParentId': '51', 'urls': [], 'exp': ['A', 'A = \\\\pi R^2', 'R', 'r', '\\\\pi r^2 = A / 2', '\\\\pi r^2 = \\\\pi R^2 / 2', 'r = R / \\\\sqrt(2)'], 'Body': \"Let the total area of the field = <math_exp>. We know <math_exp> where <math_exp> = the radius of the field. We want the cow to be able to graze half the area, so we solve for a length of rope <math_exp> such that <math_exp>. This gives: <math_exp>, hence <math_exp>. In words, the length of the cow's rope should be the radius of the field divided by sqrt(2). \"}\n",
      "65 {'Id': '65', 'Type': 'answer', 'ParentId': '59', 'urls': [], 'exp': ['\\\\vec p_1', '\\\\vec p_2', '\\\\vec p_1\\\\cdot \\\\vec p_2 = |\\\\vec p_1| \\\\cdot |\\\\vec p_2| \\\\cdot \\\\cos \\\\theta', '\\\\theta', '\\\\theta = \\\\arccos\\\\left(\\\\frac{\\\\vec p_1 \\\\cdot \\\\vec p_2}{|\\\\vec p_1| \\\\cdot |\\\\vec p_2|}\\\\right)', 'v = \\\\arccos\\\\left(\\\\frac{x_1x_2 + y_1y_2}{\\\\sqrt{(x_1^2+y_1^2) \\\\cdot (x_2^2+y_2^2)}}\\\\right)', 'v = \\\\arccos\\\\left(\\\\frac{x_1x_2 + y_1y_2 + z_1z_2}{\\\\sqrt{(x_1^2+y_1^2+z_1^2) \\\\cdot (x_2^2+y_2^2+z_2^2)}}\\\\right)'], 'Body': 'Assuming this is relative to the origin (as John pointed out): Given two position vectors <math_exp> and <math_exp>, their dot product is: <math_exp> Solving for <math_exp>, we get: <math_exp> In a 2D space this equals: <math_exp> And extended for 3D space: <math_exp> '}\n",
      "66 {'Id': '66', 'Type': 'answer', 'ParentId': '57', 'urls': [], 'exp': [], 'Body': 'There are several important kinds of relations, each of which satisfy a different collection of properties: Equivalence relations: These are reflexive, symmetric, and transitive.  Essentially they\\'re relations that \"behave like equality.\"  The most important elementary one is \"equivalence modulo m,\" where say 1 = 6 = 11 modulo 5. Partial orderings: These are reflexive, transitive, and (anti-symmetric or maybe asymmetric, I\\'m having trouble parsing your logical statements).  Essentially they\\'re relations that \"behave like less than or equal to.\"  An important elementary example is \"divides\" where we say a|b if the ratio b/a is an integer.  Note that 2|6 but 6 does not divide 2.  However if 2|4 and 4|8 certainly 2|8. '}\n",
      "67 {'Id': '67', 'Type': 'question', 'Title': 'What is an elliptic curve, and how are they used in cryptography?', 'Tags': ['cryptography', 'elliptic-curves'], 'AcceptedAnswerId': '72', 'urls': [], 'exp': [], 'Body': \"I hear a lot about Elliptic Curve Cryptography these days, but I'm still not quite sure what they are or how they relate to crypto... \"}\n",
      "68 {'Id': '68', 'Type': 'answer', 'ParentId': '59', 'urls': [], 'exp': [], 'Body': 'I will assume that you mean the angle of the line from p1 to p2 with respect to the x-axis This is the best I can do given the information you have provided. In any case, the official mathsy way would be to find the dot product between the two, and divide by the magnitude of p1-p2 and take the arccossine. You can normalize a vector by dividing every term by the magnitude (length) of the entire vector. For 3D, the same thing applies: You could also possibly mean the angle between the line from the origin to p1 and the line from the origin to p2. You can do this with dot products, as well; but both vectors must be normalized. where a is the normalized vector from the origin to p1 and b is the normalized vector from the origin to p2. '}\n",
      "69 {'Id': '69', 'Type': 'answer', 'ParentId': '1', 'urls': ['http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument'], 'exp': [], 'Body': 'You can see that there are infinitely many natural numbers 1, 2, 3, ..., and infinitely many real numbers, such as 0, 1, pi, etc. But are these two infinities the same? Well, suppose you have two sets of objects, e.g. people and horses, and you want to know if the number of objects in one set is the same as in the other. The simplest way is to find a way of corresponding the objects one-to-one. For instance, if you see a parade of people riding horses, you will know that there are as many people as there are horses, because there is such a one-to-one correspondence. We say that an set with infinitely many things is \"countable,\" if we can find a one-to-one correspondence between the things in this set and the natural numbers. E.g., the integers are countable: 1 &lt;-> 0, 2 &lt;-> -1, 3 &lt;-> 1, 4 &lt;-> -2, 5 &lt;-> 2, etc, gives such a correspondence. However, the set of real numbers is NOT countable! This was proven for the first time by Georg Cantor. Here is a proof using the so-called diagonal argument. '}\n",
      "70 {'Id': '70', 'Type': 'answer', 'ParentId': '58', 'urls': [], 'exp': [], 'Body': 'Forensic accountancy is a popular use, and is actually admissible as evidence in the USA. '}\n",
      "71 {'Id': '71', 'Type': 'question', 'Title': 'Real world uses of Quaternions?', 'Tags': ['soft-question', 'big-list', 'linear-algebra', 'applications', 'quaternions'], 'AcceptedAnswerId': '78', 'urls': [], 'exp': [], 'Body': \"I've recently started reading about Quaternions, and I keep reading that for example they're used in computer graphics and mechanics calculations to calculate movement and rotation, but without real explanations of the benefits of using them. I'm wondering what exactly can be done with Quaternions that can't be done as easily (or easier) using more tradition approaches, such as with Vectors? \"}\n",
      "72 {'Id': '72', 'Type': 'answer', 'ParentId': '67', 'urls': ['http://www.math.brown.edu/~jhs/Presentations/WyomingEllipticCurve.pdf'], 'exp': [], 'Body': 'Here is a super nice powerpoint on the subject! http://www.math.brown.edu/~jhs/Presentations/WyomingEllipticCurve.pdf '}\n",
      "73 {'Id': '73', 'Type': 'answer', 'ParentId': '56', 'urls': ['http://en.wikipedia.org/wiki/Inner_product_space#Definition'], 'exp': [], 'Body': \"An inner product space is a vector space for which the inner product is defined. The inner product is also known as the 'dot product' for 2D or 3D Euclidean space. An arbitrary number of inner products can be defined according to three rules, though most are a lot less intuitive/practical than the Euclidean (dot) product. Side note: It may seem slightly esoteric, but as a physicist the obvious application of inner product spaces are Hilbert spaces used in quantum mechanics. The inner product of an eigenfunction with a wavefunction in Hilbert space gives the corresponding eigenvalue. \"}\n",
      "74 {'Id': '74', 'Type': 'question', 'Title': 'Sum of Gaussian Variables', 'Tags': ['probability-theory'], 'AcceptedAnswerId': '1273', 'urls': [], 'exp': ['X', 'Y', 'Y=X+Z', 'X', 'Z', 'Y', 'Z', 'X', 'Z'], 'Body': \"Let's say I know <math_exp> is a Gaussian Variable. Moreover, I know <math_exp> is a Gaussian Variable and <math_exp>. Let's <math_exp> and <math_exp> are Independent. How can I prove <math_exp> is a Gaussian Random Variable if and only if <math_exp> is a Gaussian R.V.? It's easy to show the other way around (<math_exp>, <math_exp> Orthogonal and Normal hence create a Gaussian Vector hence any Linear Combination of the two is a Gaussian Variable). Thanks \"}\n",
      "75 {'Id': '75', 'Type': 'question', 'Title': 'What are the differences between rings, groups, and fields?', 'Tags': ['terminology', 'abstract-algebra'], 'AcceptedAnswerId': '80', 'urls': [], 'exp': [], 'Body': 'Rings, groups, and fields all feel similar. What are the differences between them, both in definition and in how they are used? '}\n",
      "76 {'Id': '76', 'Type': 'answer', 'ParentId': '57', 'urls': [], 'exp': [], 'Body': 'Asymmetric means simply \"not symmetric\". So in the binary case, it is NOT the case that if a is related to b, b is related to a. Antisymmetric means that if a is related to b, and b is related to a, a = b. To explain your third example: \"is older than\" is asymmetric because if Alice is older than Bob, Bob is NOT older than Alice. \"is older than\" is antisymmetric since if Alice is older than Bob, and Bob is older than Alice, Alice must be Bob because someone must be older (and if this is not the case, Alice simply has two names..). \"is older than\" is transitive since if Alice is older than Bob, and Bob is older than Charlie, Alice is also older than Charlie. So asymmetric and antisymmetric don\\'t cancel out because the first means it\\'s sort of a one-way relation, whereas the second means, loosely, that if it you reverse the operands and both statements are true, the operands must be the same. '}\n",
      "77 {'Id': '77', 'Type': 'question', 'Title': 'Understanding Dot and Cross Product', 'Tags': ['linear-algebra', 'cross-product'], 'AcceptedAnswerId': '85', 'urls': [], 'exp': [], 'Body': 'What purposes do the Dot and Cross products serve? Do you have any clear examples of when you would use them? '}\n",
      "78 {'Id': '78', 'Type': 'answer', 'ParentId': '71', 'urls': [], 'exp': [], 'Body': \"I believe they are used in quantum physics as well, because rotation with quaternions models Spinors extremely well (due to the lovely property that you need to rotate a point in quaternionic space around 2 full revolutions to get back to your 'original', which is exactly what happens with spin-1/2 particles). They are also, as you said, used in computer graphics a lot for several reasons: I think there are other uses, but a lot of them have been superseded by more general Vectors. \"}\n",
      "79 {'Id': '79', 'Type': 'answer', 'ParentId': '20', 'urls': ['http://mathworld.wolfram.com/PeanosAxioms.html', 'http://mathworld.wolfram.com/CauchySequence.html'], 'exp': [], 'Body': 'The natural numbers can be defined by Peano\\'s Axioms (sometimes called the Peano Postulates): (This definition includes 0 in the natural numbers; altering rules 1, 3, and 5 to refer to one instead of zero excludes 0 from the natural numbers.  Whether or not 0 is a natural number varies in various texts.) The whole numbers are the natural numbers with the additive identity element called 0. The integers are the whole numbers and their additive inverses. The rational numbers are numbers that can be expressed as a ratio of an integer to a non-zero integer. The real numbers are the set of numbers that are limits of Cauchy sequences of rational numbers. The irrational numbers are the real numbers that are not rational numbers. The complex numbers are the numbers that can be expressed as a + b * i where a and b are real numbers and i behaves like a real number under addition/multiplication/distribution/etc., with the added rule that i2 = -1. The imaginary numbers are sometimes defined to be the \"pure imaginary\" numbers--complex numbers for which the \"real part\" a = 0, sometimes with the added restriction that b is not zero--and are sometimes defined to be the non-real complex numbers. The algebraic numbers are numbers that are solutions to polynomial equations with integer coefficients. The transcendental numbers are complex numbers (sometimes limited to real numbers) that are not algebraic. '}\n",
      "80 {'Id': '80', 'Type': 'answer', 'ParentId': '75', 'urls': [], 'exp': [], 'Body': 'They should feel similar! In fact, every ring is a group, and every field is a ring. A ring is a group with an additional operation, where the second operation is associative and the distributive properties make the two operations \"compatible\". A field is a ring such that the second operation also satisfies all the group properties (after throwing out the additive identity); i.e. it has multiplicative inverses, multiplicative identity, and is commutative. '}\n",
      "81 {'Id': '81', 'Type': 'question', 'Title': 'List of Interesting Math Blogs', 'Tags': ['soft-question', 'big-list', 'online-resources'], 'urls': [], 'exp': [], 'Body': \"I have the one or other interesting Math blog in my feedreader that I follow. It would be interesting to compile a list of Math blogs that are interesting to read, and do not require research-level math skills. I'll start with my entries: \"}\n",
      "82 {'Id': '82', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://gilkalai.wordpress.com/'], 'exp': [], 'Body': \"Gil Kalai's blog is pretty awesome. Mostly covers combinatorics. \"}\n",
      "83 {'Id': '83', 'Type': 'question', 'Title': 'What are some good ways to get children excited about math?', 'Tags': ['big-list', 'education'], 'AcceptedAnswerId': '88', 'urls': [], 'exp': [], 'Body': \"I'm talking in the range of 10-12 years old, but this question isn't limited to only that range. Do you have any advice on cool things to show kids that might spark their interest in spending more time with math? The difficulty for some to learn math can be pretty overwhelming. Do you have any teaching techniques that you find valuable? \"}\n",
      "84 {'Id': '84', 'Type': 'answer', 'ParentId': '77', 'urls': [], 'exp': [], 'Body': 'The dot product can be used to find the length of a vector or the angle between two vectors. The cross product is used to find a vector which is perpendicular to the plane spanned by two vectors. '}\n",
      "85 {'Id': '85', 'Type': 'answer', 'ParentId': '77', 'urls': [], 'exp': [], 'Body': 'When you deal with vectors, sometimes you say to yourself, \"Darn I wish there was a function that...\" was zero when two vectors are perpendicular, letting me test perpendicularness.\" Dot Product would let me find the angle between two vectors.\" Dot Product (actually gives the cosine of the angle between two normalized vectors) would let me \\'project\\' one vector onto another, or give the length of one vector in the direction of another.\" Dot Product could tell me how much force is actually helping the object move, when pushing at an angle.\" Dot Product could tell me how much a vector field is \\'spreading out\\'.\" Cross Product could give me a vector that is perpendicular to two other vectors.\" Cross Product could tell me how much torque a force was applying to a rotating system.\" Cross Product could tell me how much this vector field is \\'curling\\' up.\" Cross Product There are actually a lot more uses, but the more I study vectors, the more and more I run into a situation where I need a function to do exactly something, and then realize that the cross/dot products already did exactly what I needed! '}\n",
      "86 {'Id': '86', 'Type': 'answer', 'ParentId': '56', 'urls': [], 'exp': ['\\\\mathbb R^n', '\\\\mathbb R^n', '\\\\mathbb C^n', ' \\\\langle \\\\, f, \\\\, g \\\\, \\\\rangle = \\\\int_a^b \\\\ f(x)\\\\overline{g(x)} \\\\, dx', 'f, g', '[a,b]'], 'Body': \"As for the utility of inner product spaces: They're vector spaces where notions like the length of a vector and the angle between two vectors are available. In this way, they generalize <math_exp> but preserve some of its additional structure that comes on top of it being a vector space. Familiar friends like Cauchy-Schwarz, the parallelogram rule, and orthogonality all work in inner product spaces. (Note that there is a more general class of spaces, normed spaces, where notions of length make sense always, but an inner product cannot necessarily be defined.) The dot product is the standard inner product on <math_exp>. In general, any symmetric, positive definite matrix will give you an inner product on <math_exp>. And you can have inner products on infinite dimensional vector spaces, like <math_exp> for <math_exp> square-integrable functions on <math_exp>. This becomes useful, for example, in applications like Fourier series where you want a basis of orthonormal functions for some function space (it's not just the trigonometric functions that work). \"}\n",
      "87 {'Id': '87', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['x=0.999...', '10x=9.999...', '10x-x=9.999...-0.999...', '9x=9', 'x=1', '0.999...=1'], 'Body': '<math_exp> <math_exp> <math_exp> <math_exp> <math_exp> thus, <math_exp> '}\n",
      "88 {'Id': '88', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': \"Graph theory! It's essentially connecting the dots, but with theorems working wonders behind the scenes for when they're old enough. Simple exercises like asking how many colors you need to color the faces or vertices of a graph are often fun (so I hear). (Also, most people won't believe the 4-color theorem.) \"}\n",
      "89 {'Id': '89', 'Type': 'answer', 'ParentId': '22', 'urls': [], 'exp': ['i', 'j', 'k', 'm', 'n', 'p', '(m, n, p)', '(m, n, p) = (a, b, c)', '(m, n, p) = (d, e, f)', '(a, b, c)', '(d, e, f)', '(a, b, c)', '(d, e, f)'], 'Body': 'Note that if you replace <math_exp>, <math_exp>, and <math_exp> with <math_exp>, <math_exp>, and <math_exp>, the determinant becomes the dot-product of the vector <math_exp> with the cross-product of the two original vectors.  If <math_exp> or <math_exp>, the determinant is zero (any matrix with two identical rows has determinant zero), so the dot product of <math_exp> or <math_exp> with the cross-product is zero.  Hence, <math_exp> and <math_exp> are orthogonal to their cross-product. '}\n",
      "90 {'Id': '90', 'Type': 'question', 'Title': 'Online resources for learning Mathematics', 'Tags': ['reference-request', 'online-resources'], 'urls': ['http://ocw.mit.edu/courses/mathematics/'], 'exp': [], 'Body': \"Not sure if this is the place for it, but there are similar posts for podcasts and blogs, so I'll post this one. I'd be interested in seeing a list of online resources for mathematics learning. As someone doing a non-maths degree in college I'd be interested in finding some resources for learning more maths online, most resources I know of tend to either assume a working knowledge of maths beyond secondary school level, or only provide a brief summary of the topic at hand. I'll start off by posting MIT Open Courseware, which is a large collection of lecture notes, assignments and multimedia for the MIT mathematics courses, although in many places it's quite incomplete. \"}\n",
      "91 {'Id': '91', 'Type': 'answer', 'ParentId': '75', 'urls': [], 'exp': [], 'Body': \"You're right to think that the definitions are very similar. The main difference between groups and rings is that rings have two binary operations (usually called addition and multiplication) instead of just one binary operation. If you forget about multiplication, then a ring becomes a group with respect to addition (the identity is 0 and inverses are negatives). This group is always commutative! If you forget about addition, then a ring does not become a group with respect to multiplication. The binary operation of multiplication is associative and it does have an identity 1, but some elements like 0 do not have inverses. (This structure is called a monoid.) A commutative ring is a field when all nonzero elements have multiplicative inverses. In this case, if you forget about addition and remove 0, the remaining elements do form a group under multiplication. This group is again commutative. A division ring is a (not necessarily commutative) ring in which all nonzero elements have multiplicative inverses. Again, if you forget about addition and remove 0, the remaining elements do form a group under multiplication. This group is not necessarily commutative. An example of a division ring which is not a field are the quaternions. \"}\n",
      "92 {'Id': '92', 'Type': 'answer', 'ParentId': '90', 'urls': [], 'exp': [], 'Body': 'Two good general references: '}\n",
      "93 {'Id': '93', 'Type': 'answer', 'ParentId': '18', 'urls': [], 'exp': ['j', 'n', 'e', 'n = \\\\sqrt{(j\\\\sqrt{1 - e^{2}}) \\\\times (j(1 - e^{2}))}'], 'Body': \"Possibly something like this. Correct me if I'm wrong. <math_exp> = semi-major <math_exp> = semi-minor <math_exp> = eccentricity <math_exp> \"}\n",
      "94 {'Id': '94', 'Type': 'answer', 'ParentId': '90', 'urls': [], 'exp': [], 'Body': ''}\n",
      "95 {'Id': '95', 'Type': 'answer', 'ParentId': '90', 'urls': ['http://www.mathscentre.ac.uk/'], 'exp': [], 'Body': 'A useful one for undergraduate level maths is Mathcentre.  It has useful background material for people studying maths, or who need some maths background for other courses. '}\n",
      "96 {'Id': '96', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://math.ucr.edu/home/baez/this.week.html', 'http://terrytao.wordpress.com/'], 'exp': [], 'Body': \"Not always pure math, but I think John Baez' This Week in Mathematical Physics contains a lot of really interesting math reads. I should add Terry Tao's What's new. It's a very active math blog (both in posts and comments) and definitely covers some cutting edge math, even if it can be way over my head. \"}\n",
      "97 {'Id': '97', 'Type': 'question', 'Title': 'How to accurately calculate the error function <span class=\"math-container\" id=\"954\">\\\\operatorname{erf}(x)</span> with a computer?', 'Tags': ['statistics', 'algorithms', 'numerical-methods', 'special-functions'], 'AcceptedAnswerId': '1224', 'urls': ['http://en.wikipedia.org/wiki/Error_function', 'https://rads.stackoverflow.com/amzn/click/0486612724', 'http://people.math.sfu.ca/~cbm/aands/page_299.htm'], 'exp': ['\\\\operatorname{erf}(x)=\\\\frac{2}{\\\\sqrt{\\\\pi}}\\\\int_0^x e^{-t^2}\\\\ dt'], 'Body': 'I am looking for an accurate algorithm to calculate the error function <math_exp> I have tried using the following formula, <img src=\"https://i.stack.imgur.com/MY9Mv.gif\" alt=\"math97 second question example\"> (Handbook of Mathematical Functions, formula 7.1.26), but the results are not accurate enough for the application. '}\n",
      "98 {'Id': '98', 'Type': 'question', 'Title': 'Chased By a Lion and other Pursuit Problems', 'Tags': ['reference-request'], 'AcceptedAnswerId': '106', 'urls': [], 'exp': [], 'Body': \"I am looking for a reference (book or article) that poses a problem that seems to be a classic, in that I've heard it posed many times, but that I've never seen written anywhere: that of the possibility of a man in a circular pen with a lion, each with some maximum speed, avoiding capture by that lion. References to pursuit problems in general would also be appreciated, and the original source of this problem. \"}\n",
      "99 {'Id': '99', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://www.futilitycloset.com/'], 'exp': [], 'Body': \"Not a pure math blog, but it's one of the most fascinating blogs in my RSS. Futility Closet \"}\n",
      "100 {'Id': '100', 'Type': 'answer', 'ParentId': '83', 'urls': ['http://nrich.maths.org/public/'], 'exp': [], 'Body': \"If they're fairly mathematically inclined anyway, then try to get them solving interesting problems with an obvious mathematical content, if they're less mathematically inclined try to find problems where the usage of maths isn't as explicit. Problems with a very mathematical bent can be found at places like NRich, they update their problems monthly, the Stage 2 and 3 problems cover that age range. Other sources for problems could be video games (resource management based games require mathematical thinking), code breaking or programming a game (a simple driving game in Flash requires a lot of maths based problem solving). If they've convinced themselves that they can't do Maths then you basically need to smuggle the maths into your chosen activity without them realising that they're doing maths to solve the problem. \"}\n",
      "101 {'Id': '101', 'Type': 'question', 'Title': 'Faulty logic when summing large integers?', 'Tags': ['arithmetic', 'project-euler'], 'AcceptedAnswerId': '105', 'urls': ['http://www.ProjectEuler.net'], 'exp': ['13', '50', '37107287533902102798797998220837590246510135740250', '0135740250', '64', '10'], 'Body': \"This  is in relation to the Euler Problem <math_exp> from http://www.ProjectEuler.net. Work out the first ten digits of the sum of the following one-hundred <math_exp>-digit numbers. <math_exp> Now, this was my thinking: I can freely discard the last fourty digits and leave the last ten. <math_exp> And then simply sum those. This would be large enough to be stored in a <math_exp>-bit data-type and a lot easier to compute. However, my answer isn't being accepted, so I'm forced to question my logic. However, I don't see a problem. The last fourty digits will never make a difference because they are at least a magnitude of <math_exp> larger than the preceding values and therefore never carry backwards into smaller areas. Is this not correct? \"}\n",
      "102 {'Id': '102', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': 'This really depend on how smart the kid is. I lean toward discrete math, elementary number theory related topics when talking to non-math people about math. They requires little background knowledge. There are some fun problems in discrete math, especially combinatorics. Simple probability is also nice. So are logic problem. Both topics can be used to formulate some simple puzzles. A simple number theory puzzle I assume a bright 10 year old can solve it. '}\n",
      "103 {'Id': '103', 'Type': 'answer', 'ParentId': '71', 'urls': ['http://en.wikipedia.org/wiki/Slerp'], 'exp': [], 'Body': 'One place they are frequently used is in computer games when you want to smoothly transition from one rotation to another. An artist might have said \"at this time I want the head oriented like this and at that time I want it like this\". The computer needs to work out what happens in-between these poses. It\\'s quite easy to find in-between poses using quaternions. If the two poses are reasonably similar, then you can get a half-way orientation simply by taking the average of the quaternions. You can find out more here. '}\n",
      "104 {'Id': '104', 'Type': 'answer', 'ParentId': '101', 'urls': [], 'exp': [], 'Body': 'First you are doing it in the wrong end, second, the statement in general is still not correct. for example: Say if you want the first 2 digits, you will get 10 if you discard the last 2 digit and do the sum. The right answer is 11 '}\n",
      "105 {'Id': '105', 'Type': 'answer', 'ParentId': '101', 'urls': [], 'exp': [], 'Body': \"If you were supposed to find the last ten digits, you could just ignore the first 40 digits of each number. However you're supposed to find the first ten digits, so that doesn't work. And you can't just ignore the last digits of each number either because those can carry over. \"}\n",
      "106 {'Id': '106', 'Type': 'answer', 'ParentId': '98', 'urls': ['http://mathworld.wolfram.com/LionandManProblem.html'], 'exp': [], 'Body': \"Since you're asking for a reference, perhaps this will do? Wolfram Mathworld says the problem was listed by Rado in 1925. The reference is on the problem description page, here. \"}\n",
      "107 {'Id': '107', 'Type': 'question', 'Title': 'Paradox: increasing sequence that goes to <span class=\"math-container\" id=\"974\">0</span>?', 'Tags': ['soft-question', 'paradoxes'], 'AcceptedAnswerId': '1159', 'urls': [], 'exp': ['10', '1', '10', '30', '1', '2', '3', '10', '45', '2', '3', '4', '4', '5', '6', '7', '7.5', '11', '4', '5', '6', '8', '8', '9', '10', '11', '12', '13', '14', '15', '11', '11', '11', '0'], 'Body': \"It is <math_exp> o'clock, and I have a box. Inside the box is a ball marked <math_exp>. At <math_exp>:<math_exp>, I will remove the ball marked <math_exp>, and add two balls, labeled <math_exp> and <math_exp>. At <math_exp>:<math_exp>, I will remove the balls labeled <math_exp> and <math_exp>, and add <math_exp> balls, marked <math_exp>, <math_exp>, <math_exp>, and <math_exp>. <math_exp> minutes before <math_exp>, I will remove the balls labeled <math_exp>, <math_exp>, and <math_exp>, and add <math_exp> balls, labeled <math_exp>, <math_exp>, <math_exp>, <math_exp>, <math_exp>, <math_exp>, <math_exp>, and <math_exp>. This pattern continues. Each time I reach the halfway point between my previous action and <math_exp> o'clock, I add some balls, and remove some other balls. Each time I remove one more ball than I removed last time, but add twice as many balls as I added last time. The result is that as it gets closer and closer to <math_exp>, the number of balls in the box continues to increase. Yet every ball that I put in was eventually removed. So just how many balls will be in the box when the clock strikes <math_exp>? <math_exp>, or infinitely many? What's going on here? \"}\n",
      "108 {'Id': '108', 'Type': 'answer', 'ParentId': '71', 'urls': [], 'exp': [], 'Body': 'You can view a real-world example of quaternions in computer graphics with the open source program known as NASA WorldWind (http://worldwind.arc.nasa.gov/java/). It uses a Quaternion object to represent rotation of various geometries. The class definition itself is located in the src/gov/nasa/worldwind/geom/Quaternion.java file. '}\n",
      "109 {'Id': '109', 'Type': 'answer', 'ParentId': '51', 'urls': [], 'exp': ['R', 't', '\\\\frac{1}{2}R^2(t-\\\\sin(t))', 't', 'R', '\\\\theta', '2\\\\theta', '\\\\beta', '\\\\beta = 4\\\\theta', '\\\\theta + \\\\pi/2 +\\\\alpha/2=\\\\pi', '\\\\alpha =\\\\pi-2\\\\theta', '\\\\frac{1}{2}L^2(\\\\alpha-\\\\sin\\\\alpha)+\\\\frac{1}{2}R^2(\\\\beta-\\\\sin\\\\beta)=R^2(\\\\frac{1}{2}(L/R)^2((\\\\pi-2\\\\theta)-\\\\sin(\\\\pi-2\\\\theta))+\\\\frac{1}{2}(4\\\\theta-\\\\sin(4\\\\theta)))', 'a = CE = L/R=2\\\\sin(\\\\theta)', '\\\\frac{1}{2}\\\\pi R^2', 'R^2(2(\\\\sin(\\\\theta))^2((\\\\pi-2\\\\theta)-\\\\sin(\\\\pi-2\\\\theta))+\\\\frac{1}{2}(4\\\\theta-\\\\sin(4\\\\theta)))=R^2\\\\frac{\\\\pi}{2}', 'R^2(\\\\pi+(2\\\\theta-\\\\pi)\\\\cos(2\\\\theta)-\\\\sin(2\\\\theta)=\\\\frac{\\\\pi}{2})', '\\\\pi+\\\\alpha\\\\cos\\\\alpha-\\\\sin\\\\alpha', '\\\\theta', '\\\\theta = \\\\text{ca. } 0.618', 'L=\\\\text{ca. }1.159 R'], 'Body': '<img src=\"https://imgur.com/9TSIy.png\" alt=\"diagram\"> The field is the smaller/left circle, centered at A.  The cow is tied to the post at E.  The larger/right circle is the grazing radius.  Let the radius of the field be R and the length of the rope be L. The grazable area is the union of a segment of the circular field and a segment of the circle defined by the rope length.  (A segment of a circle is a sector of a circle less the triangle defined by the center of the circle and the endpoints of the arc.)  The area of a segment of a circle of radius <math_exp> with central angle <math_exp> is <math_exp>, where <math_exp> is measured in radians. In order to express the grazable area in terms of <math_exp> and one angle, we consider the angles ∠CED and ∠CAD (which define the segments of the circles; call these α and β for convenience) and the triangle CEF. Let <math_exp> be ∠EFC. <math_exp> is an inscribed angle for the central angle <math_exp> over the same arc, making <math_exp>. The sum of angles in triangle CEF is <math_exp> or <math_exp>. The grazable area is <math_exp>, where <math_exp>.  We want that to be equal to half the area of the field, <math_exp>. That is, the equality of areas is <math_exp> Simplifying: <math_exp> (The grazable area seems to be <math_exp>; can this be seen easily?) <img src=\"https://imgur.com/E7ep6.jpg\" alt=\"Grazable area depending on <math_exp>\"> The desired equality of areas is obtained for <math_exp> or  <math_exp> . '}\n",
      "110 {'Id': '110', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': [], 'Body': 'One argument against this is that 0.99999999... is \"somewhat\" less than 1. How much exactly? If the above is true, the following also must be true: Let\\'s calculate: Thus: But: Indeed: Now let\\'s see what we can deduce from (0), (1) and (2). Thus: Quod erat demonstrandum. Pardon my unicode. '}\n",
      "112 {'Id': '112', 'Type': 'question', 'Title': 'What are gradients and how would I use them?', 'Tags': ['calculus', 'terminology'], 'AcceptedAnswerId': '115', 'urls': [], 'exp': ['\\\\nabla'], 'Body': 'I keep seeing this symbol <math_exp> around and I know enough to understand that it represents the term \"gradient.\" But what is a gradient? When would I want to use one mathematically? '}\n",
      "113 {'Id': '113', 'Type': 'question', 'Title': 'What are some classic fallacious proofs?', 'Tags': ['soft-question', 'big-list', 'fake-proofs'], 'AcceptedAnswerId': '130', 'urls': [], 'exp': ['a = b', 'a-b = 0', '(a-b)'], 'Body': \"If you know it, also try to include the precise reason why the proof is fallacious. To start this off, let me post the one that most people know already: As @jan-gorzny pointed out, in this case, line 5 is wrong since <math_exp> implies <math_exp>, and so you can't divide out <math_exp>. \"}\n",
      "114 {'Id': '114', 'Type': 'answer', 'ParentId': '107', 'urls': [], 'exp': [], 'Body': \"During one math class I had the same problem. the professor told us the answer can be a arbitrary integer, depend on how you remove the balls. Since you said every ball you put in was eventually removed, I will make it simple and assume you first number the balls to natural numbers, and then add and remove the balls with the smallest numbers first. In this case, the bin will be empty--every ball put in have been removed. If you define the sequence in the way such that a_n = amount of balls when you do the nth move, then this sequence never get to zero, because there is no n correspond to 11 o'clock. The increasing sequence doesn't define how many balls in the bin at 11. Therefore there is no paradox. \"}\n",
      "115 {'Id': '115', 'Type': 'answer', 'ParentId': '112', 'urls': [], 'exp': [], 'Body': 'The ∇ (pronounced \"del\") is an operator, more technically.  In 3D, it (more or less) means the vector So, if f(x,y,z) = x^2 + y^3*z + sin(z), ∇f = &lt; 2x, 3y^2*z, y^3 + cos(z) &gt; It\\'s actually a bit more subtle than that; technically it means And when you do ∇f, it\\'s sort of like a \"multiplication\" of ∇ and f; Only, not multiplication, but operation. There are some neat properties about the del operator.  Here are a couple: The most famous is that ∇f yields the gradient of f.  That is, at any point (x,y,z), ∇f(x,y,z) is the vector pointing in the direction where it is most increasing.  The magnitude of it is the magnitude of the increase. This is easier to understand with, say, a 2D f(x,y).  If f(x,y) represents the height of a point at (x,y), then ∇f(x,y) represents the steepest incline from that point.  Or rather, if you placed a ball on that point, it would start rolling in the opposite direction of the gradient vector. Normally, for multi-dimensional functions, it is easiest to find the derivative along an axis (x, y, z, etc.).  With ∇, you can find the derivative along any arbitrary direction by using ∇f * u, where * is the dot product and u is the unit vector along the direction you are calculating. ∇ is also used to calculate divergence (amount that vectors are \"spreading out\") and curl (amount that vectors are \"curling up\") of a vector field. Divergence is ∇ * f (dot product), and curl is ∇ x f (cross product) They aren\\'t truly \"products\" in the sense.  Rather, when you are calculating divergence and curl and you must do d/dx * (something), you are actually doing d/dx (something) or d(something)/dx. '}\n",
      "116 {'Id': '116', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': ['0.99999\\\\ldots = \\\\frac{9}{10} + \\\\frac{9}{10^2} + \\\\frac{9}{10^3} + \\\\cdots', 'a = \\\\frac{9}{10}', 'r = \\\\frac{1}{10}', '\\\\frac{\\\\frac{9}{10}}{1 - \\\\frac{1}{10}} = \\\\frac{\\\\frac{9}{10}}{\\\\frac{9}{10}} = 1.'], 'Body': 'Assuming: <math_exp> This is the infinite geometric series with first term <math_exp> and common ratio <math_exp>, so it has sum <math_exp> '}\n",
      "117 {'Id': '117', 'Type': 'answer', 'ParentId': '98', 'urls': ['http://press.princeton.edu/titles/8371.html'], 'exp': [], 'Body': 'Here is a book on this type of problem http://press.princeton.edu/titles/8371.html it is also briefly mentioned in his other book \"Euler\\'s Fabulous Formula\". '}\n",
      "118 {'Id': '118', 'Type': 'question', 'Title': 'How would you describe calculus in simple terms?', 'Tags': ['soft-question', 'calculus'], 'AcceptedAnswerId': '125', 'urls': [], 'exp': [], 'Body': 'I keep hearing about this weird type of math called calculus. I only have experience with geometry and algebra. Can you try to explain what it is to me? '}\n",
      "119 {'Id': '119', 'Type': 'answer', 'ParentId': '90', 'urls': [], 'exp': [], 'Body': \"The following reddit post has a decent list of math resources: One site I did not see it their list that I've found very helpful: \"}\n",
      "120 {'Id': '120', 'Type': 'question', 'Title': 'Why is 1 not a prime number?', 'Tags': ['elementary-number-theory', 'prime-numbers', 'terminology'], 'AcceptedAnswerId': '122', 'urls': [], 'exp': ['1', '1'], 'Body': 'Why is <math_exp> not considered a prime number? Or, why is the definition of prime numbers given for integers greater than <math_exp>? '}\n",
      "122 {'Id': '122', 'Type': 'answer', 'ParentId': '120', 'urls': [], 'exp': [], 'Body': 'One of the whole \"points\" of defining primes is to be able to uniquely and finitely prime factorize every natural number. If 1 was prime, then this would be more or less impossible. '}\n",
      "123 {'Id': '123', 'Type': 'question', 'Title': 'Real world uses of hyperbolic trigonometric functions', 'Tags': ['soft-question', 'big-list', 'applications', 'hyperbolic-functions'], 'urls': [], 'exp': [], 'Body': 'I covered hyperbolic trigonometric functions in a recent maths course. However I was never presented with any reasons as to why (or even if) they are useful. Is there any good examples of their uses outside academia? '}\n",
      "124 {'Id': '124', 'Type': 'answer', 'ParentId': '123', 'urls': [], 'exp': [], 'Body': 'If you take a rope, fix the two ends, and let it hang under the force of gravity, it will naturally form a hyperbolic cosine curve. '}\n",
      "125 {'Id': '125', 'Type': 'answer', 'ParentId': '118', 'urls': [], 'exp': [], 'Body': 'There came a time in mathematics when people encountered situations where they had to deal with really, really, really small things. Not just small like 0.01; but small as in infinitesimally small.  Think of \"the smallest positive number that is still greater than zero\" and you\\'ll realize what sort of problems mathematicians began encountering. Soon, this problem became more than just theoretical or abstract.  It became very, very real. For example, velocity.  We know that average velocity is the change in position per change in time (i.e., 5 miles per hour).  But what about velocity at a point in time?  What does it mean to be going 5 mph at this moment? One solution someone came up with was to say \"it\\'s the change in position divided by the change in time, where the change in time is an infinitesimally small amount of time\".  But how would you handle/calculate that? Another problem came about trying to find the area under a curve.  The current accepted solution was to divide the curve into rectangles, and add together the area of the rectangles.  However, in order to find the exact area under the curve, you\\'d need to divide it into rectangles that were infinitesimally tiny, and, therefore, add up an infinite amount of tiny rectangles -- to something that was finite (area). Calculus came about as the system of math dedicated to studying these infinitesimally small changes.  In fact, I do believe some people describe calculus as \"the study of continuous changes\". '}\n",
      "126 {'Id': '126', 'Type': 'answer', 'ParentId': '123', 'urls': [], 'exp': [], 'Body': 'An equation for a catenary curve can be given in terms of hyperbolic cosine.  Catenary curves appear in many places, such as the Gateway Arch in St. Louis, MO. '}\n",
      "127 {'Id': '127', 'Type': 'answer', 'ParentId': '118', 'urls': [], 'exp': [], 'Body': \"Calculus is a field which deals with two seemingly unrelated things. (1) the area beneath a graph and the x-axis. (2) the slope (or gradient) of a curve at different points. Part (1) is also called 'integration' and 'anti-differentiation', and part (2) is called 'differentiation'. \"}\n",
      "128 {'Id': '128', 'Type': 'question', 'Title': 'How do you prove that <span class=\"math-container\" id=\"1294\">p(n \\\\xi)</span> for <span class=\"math-container\" id=\"1295\">\\\\xi</span> irrational and <span class=\"math-container\" id=\"1296\">p</span> a polynomial is uniformly distributed modulo 1?', 'Tags': ['number-theory'], 'AcceptedAnswerId': '1163', 'urls': [], 'exp': ['{n \\\\xi}', 'n = 0, 1, 2, \\\\dots', '\\\\xi', 'e^{2\\\\pi i k x}', 'k', 'f(x)', 't', '\\\\int_0^1 f(x) dx = \\\\lim \\\\frac{1}{N} \\\\sum_{i=0}^{N-1} f(t+i \\\\xi)', 'p', '{p(n \\\\xi)}', '\\\\xi'], 'Body': \"The Weyl equidistribution theorem states that the sequence of fractional parts <math_exp>, <math_exp> is uniformly distributed for <math_exp> irrational. This can be proved using a bit of ergodic theory, specifically the fact that an irrational rotation is uniquely ergodic with respect to Lebesgue measure. It can also be proved by simply playing with trigonometric polynomials (i.e., polynomials in <math_exp> for <math_exp> an integer) and using the fact they are dense in the space of all continuous functions with period 1.  In particular, one shows that if <math_exp> is a continuous function with period 1, then for any <math_exp>,  <math_exp>. One shows this by checking this (directly) for trigonometric polynomials via the geometric series.  This is a very elementary and nice proof. The general form of Weyl's theorem states that if <math_exp> is a monic integer-valued polynomial, then the sequence <math_exp> for <math_exp> irrational is uniformly distributed modulo 1.  I believe this can be proved using extensions of these ergodic theory techniques -- it's an exercise in Katok and Hasselblatt.  I'd like to see an elementary proof. Can the general form of Weyl's theorem be proved using the same elementary techniques as in the basic version? \"}\n",
      "129 {'Id': '129', 'Type': 'answer', 'ParentId': '118', 'urls': ['http://en.wikipedia.org/wiki/Infinitesimal', 'http://en.wikipedia.org/wiki/Calculus'], 'exp': [], 'Body': \"To be very brief and succinct: Calculus is the study of how quantities change Slightly more technically, it a subject based on infinitesimals. It may be pointing out the obvious, but the Wikipedia article does actually provide a pretty decent beginners introduction to the subject. You'll generally want to start with differential calculus and move on quickly to integral calculus, followed by linking up the two (fundamental theorem of calculus) and moving on from there. \"}\n",
      "130 {'Id': '130', 'Type': 'answer', 'ParentId': '113', 'urls': ['http://en.wikipedia.org/wiki/Mathematical_fallacy'], 'exp': [], 'Body': 'Wikipedia has a long list of these: http://en.wikipedia.org/wiki/Mathematical_fallacy '}\n",
      "132 {'Id': '132', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': \"I recently taught a once-a-week geometry class to grades 7-10 where we did some graph theory, some surfaces, some spherical geometry, a lot of complex numbers, and basic ideas of homeomorphism and homotopy. Most couldn't work with complex numbers at the beginning: showing their use in proofs of analytic geometry results had the benefit of not boring the kids who had already seen and worked with complex numbers, and giving the kids who hadn't a reason to learn the technicalities (getting under the hood of these pretty pictures). Euler's formula and the relation of planar graphs to polyhedra showed them the basics of a connection between geometric and algebraic intuition, outside the coordinate geometry many were used to, and got them thinking about ways to define familiar things mathematically (space), and what we can learn about familiar things through mathematics (surprising things like orientability through the Möbius strip, or sphere eversion). In general I've found geometry to be a very good place to start with people with a professed fear or disinterest in mathematics. For me, it's the quickest way to show the distance between what mathematicians play with when they're doing math and what was taught in high school, with the two column proof nonsense and the treatment of math as a branch of formal logic. Of course, I suspect that I find this path the easiest because it is one of the ones I am most excited about in mathematics, and I further imagine that in many cases, the best thing to use to get people excited about math is something you yourself are really excited about, as long as you can think to translate it well. \"}\n",
      "133 {'Id': '133', 'Type': 'answer', 'ParentId': '3', 'urls': ['http://www.aracnet.com/~eseligma/mm/'], 'exp': [], 'Body': 'I listen to Math Mutation Podcast. The topics are interesting and understandable by a layman. '}\n",
      "134 {'Id': '134', 'Type': 'answer', 'ParentId': '57', 'urls': [], 'exp': ['R', 'S', 'T', 'x, y', 'z', 'x: xRx', 'xRy', 'x', 'y', 'x,y', 'xRy', 'yRx', 'xRy', 'x', 'y', '2', 'x,y', 'xRy', 'yRx', 'x = y', 'xRy', 'x', 'y', 'x,y', 'xRy', 'yRx', 'xRy', 'x', 'y', 'x,y,z', 'xRy', 'yRz', 'xRz', 'xRy', 'x', 'y'], 'Body': \"I'd like to change the notation of your definitions, since <math_exp>, <math_exp> and <math_exp> would usually be used to stand for the relations themselves (and <math_exp> and <math_exp> would be more commonly chosen for the objects that might bear the relation to each other). Reflexive - For all <math_exp> Example reflexive relation: <math_exp> stands for '<math_exp> is a factor of <math_exp>' (in the set of natural numbers) Symmetric - For all <math_exp>: if <math_exp> then <math_exp> Example symmetric relation: <math_exp> stands for '<math_exp> and <math_exp> are <math_exp> metres apart' (in the set of all people in a particular room) Antisymmetric - For all <math_exp>: if <math_exp> and <math_exp> then <math_exp> Example antisymmetric relation: <math_exp> stands for '<math_exp> is a factor of <math_exp>' (in the set of natural numbers) Asymmetric - For all <math_exp>: if <math_exp> then not <math_exp> Example asymmetric relation: <math_exp> stands for '<math_exp> is taller than <math_exp>' (in the set of all people) Transitive - For all <math_exp>: if <math_exp> and <math_exp> then <math_exp> Example transitive relation: <math_exp> stands for '<math_exp> is taller than <math_exp>' (in the set of all people) \"}\n",
      "135 {'Id': '135', 'Type': 'question', 'Title': 'Why is <span class=\"math-container\" id=\"1334\">x^0 = 1</span> except when <span class=\"math-container\" id=\"1335\">x = 0</span>?', 'Tags': ['definition', 'exponentiation'], 'AcceptedAnswerId': '138', 'urls': [], 'exp': ['0^0'], 'Body': 'Why is any number (other than zero) to the power of zero equal to one? Please include in your answer an explanation of why <math_exp> should be undefined. '}\n",
      "136 {'Id': '136', 'Type': 'question', 'Title': 'Why are the differences between consecutive squares equal to the sequence of odd numbers?', 'Tags': ['elementary-number-theory'], 'AcceptedAnswerId': '183', 'urls': [], 'exp': ['0^2 = 0', '1^2 = 1', '2^2 = 4', '3^2 = 9', '4^2 = 16', '5^2 = 25', '6^2 = 36'], 'Body': 'I was playing around with the squares and saw an interesting pattern in their differences. <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> (Also, in a very related question, which major Math Research Journal should I contact to publish my groundbreaking find in?) '}\n",
      "137 {'Id': '137', 'Type': 'answer', 'ParentId': '67', 'urls': [], 'exp': ['y^2 = x^3 - Ax + B', 'm^2', 'm'], 'Body': 'The technical definition is a nonsingular projective curve of genus 1, which is an abelian variety under the group law: basially, this means that you   draw the line through two points on the curve -- which can be embedded in the projective plane -- and find where that line intersects the curve again (and call that the negative of the sum). We can always put elliptic curves in the (projectivization of the) form <math_exp>. So, the meaning of \"abelian variety\" is that you can add points on the elliptic curve, which is really useful; there isn\\'t a way to do this for most objects in algebraic geometry.  Then one can study things like the torsion points on an elliptic curve, with respect to this abelian group structure: it\\'s a theorem that there are <math_exp> torsion points of order <math_exp> for instance, if you \\'re working in an algebraically closed field. In fact, one way to think of this is that an elliptic curve is really--algebraically and topologically--a torus if you are working over the complex numbers, and the torsion points in the torus are easily determined.  (Namely, a torus is algebraically the product of two copies of the unit circle.) This also yields the theorem about torsion points for algebraically closed fields of characteristic zero via the \"Lefschetz principle.\"  (For characteristic p, you need a different argument.) Other things one can consider include the group of points with coordinates in, say, the rational numbers (assuming the curve is defined by rational coefficients). One of the central theorems is that this group is finitely generated.  The point is that the geometry of the elliptic curve leads to a rich algebraic structure. That\\'s a bit about elliptic curves; I know nothing about cryptography and can\\'t comment on that. '}\n",
      "138 {'Id': '138', 'Type': 'answer', 'ParentId': '135', 'urls': [], 'exp': [' x^a x^b = x^{a+b} ', ' 0 ', ' x^0 ', '\\\\displaystyle x^a \\\\cdot 1 = x^a\\\\cdot x^0 = x^{a+0} = x^a ', '0^0', '0^x = 0', 'x^0 = 1', 'x', '0^0', '0^0', 'f(x,y) = x^y', '(0,0)', '0^0'], 'Body': \"For non-zero bases and exponents, the relation <math_exp> holds. For this to make sense with an exponent of <math_exp>, <math_exp> needs to equal one. This gives you: <math_exp> When the base is also zero, it's not possible to define a value for <math_exp> because there is no value that is consistent with all the necessary constraints. For example, <math_exp> and <math_exp> for all positive <math_exp>, and <math_exp> can't be consistent with both of these. Another way to see that <math_exp> can't have a reasonable definition is to look at the graph of <math_exp> which is discontinuous around <math_exp>. No chosen value for <math_exp> will avoid this discontinuity. \"}\n",
      "139 {'Id': '139', 'Type': 'question', 'Title': 'How do I cut a square in half?', 'Tags': ['geometry'], 'AcceptedAnswerId': '176', 'urls': [], 'exp': ['10\\\\mathrm{m} \\\\times 10\\\\mathrm{m}', '50\\\\mathrm{m}^{2}', '7.07106781\\\\mathrm{m}'], 'Body': \"I have a square that's <math_exp>. I want to cut it in half so that I have a square with half the area. But if I cut it from top to bottom or left to right, I don't get a square, I get a rectangle! I know the area of the small square is supposed to be <math_exp>, so I can use my calculator to find out how long a side should be: it's <math_exp>. But my teacher said I should be able to do this without a calculator. How am I supposed to get that number by hand? \"}\n",
      "140 {'Id': '140', 'Type': 'answer', 'ParentId': '135', 'urls': [], 'exp': ['0^x = 0, \\\\quad x^0=1', 'x&gt;0', 'x=0', '0^0 = 1', '0^x', 'x^0'], 'Body': \"<math_exp> both are true when <math_exp>. What happens when <math_exp>? It is undefined because there is no way to chose one definition over the other. Some people define <math_exp> in their books, like Knuth, because <math_exp> is less 'useful' than <math_exp>. \"}\n",
      "141 {'Id': '141', 'Type': 'answer', 'ParentId': '135', 'urls': [], 'exp': ['x^0=1', 'x=0', 'x^a \\\\cdot x^b = x^{a+b}', 'a', 'b', 'a+b', 'a', 'b', 'x^a', 'a', 'x^0 \\\\cdot x^b = x^{0+b} = x^b', 'x', 'x^b', 'x^0 = 1', 'x=0', 'x^b', 'x^0', 'x', 'x^{-a}', '1/x^a', '(x^a)^b = x^{ab}', 'x^{1/n}', 'n'], 'Body': 'This is a question of definition, the question is \"why does it make sense to define <math_exp> except when <math_exp>?\" or \"How is this definition better than other definitions?\" The answer is that <math_exp> is an excellent formula that makes a lot of sense (multiplying <math_exp> times and then multiplying <math_exp> times is the same as multiplying <math_exp> times) and which you can prove for <math_exp> and <math_exp> positive integers.  So any sensible definition of <math_exp> for numbers <math_exp> which aren\\'t positive integers should still satisfy this identity.  In particular, <math_exp>; now if <math_exp> is not zero then you can cancel <math_exp> from both sides and get that <math_exp>.  But if <math_exp> then <math_exp> is zero and so this argument doesn\\'t tell you anything about what you should define <math_exp> to be. A similar argument should convince you that when <math_exp> is not zero then <math_exp> should be defined as <math_exp>. An argument using the related identity <math_exp> should convince you that <math_exp> is taking the <math_exp>th root. '}\n",
      "143 {'Id': '143', 'Type': 'answer', 'ParentId': '136', 'urls': [], 'exp': ['(n + 1) ^ 2 - n ^ 2 = 2n + 1'], 'Body': \"A nice thing to notice. It's basically because <math_exp> \"}\n",
      "144 {'Id': '144', 'Type': 'answer', 'ParentId': '135', 'urls': [], 'exp': ['a^0 = 1', 'a^2 / a^3 = a^{-1} = 1/a', 'a^0 = 1', 'a=0', '2^0 = 1', ' 2^{1-1} = 2^1 / 2^1 = 2/2 = 1', '0^0', '0^{1-1} = 0^1/0^1=0/0=\\\\text{UNDEFINED}'], 'Body': 'Exponents are only \"basically\" defined under the natural numbers above zero.  By this I mean, defined as \"iterated multiplication\" the same way multiplication is defined as iterated addition. The property <math_exp> only arises when we look at generalizing multiplication to the integers.  We do this by: \\\\begin{align}  a^4 / a^3 &amp;= (a\\\\cdot a\\\\cdot a\\\\cdot a)/(a\\\\cdot a\\\\cdot a) = a^1\\\\\\\\  a^4 / a^3 &amp;= a^{4-3}           = a^1 \\\\end{align} And using this, we can say: <math_exp> and also: \\\\begin{align}     a^2 / a^2                 &amp;= 1\\\\\\\\     a^2 / a^2 &amp;= a^{2-2} = a^0 = 1 \\\\end{align} So we say <math_exp>. However, notice that these proofs don\\'t have any meaning when <math_exp>, because the whole concept/idea involves fractions, and you cannot have zero be in the denominator. When we say <math_exp>, we really mean: <math_exp> But we cannot say the same for <math_exp>: <math_exp> '}\n",
      "146 {'Id': '146', 'Type': 'answer', 'ParentId': '139', 'urls': [], 'exp': [], 'Body': 'Take a pair of compasses and draw an arc between two opposite corners, centred at another corner; then draw a diagonal that bisects the arc. If you now draw two lines from the point of intersection, parallel to the sides of the square, the biggest of the resulting squares will have half the area of the original square. Here\\'s an illustration: <img src=\"https://i.stack.imgur.com/uVH7v.jpg\" alt=\"Illustration of the method\"> '}\n",
      "147 {'Id': '147', 'Type': 'answer', 'ParentId': '90', 'urls': ['http://mathonline.andreaferretti.it/'], 'exp': [], 'Body': \"Andrea Feretti's MathOnline page. \"}\n",
      "148 {'Id': '148', 'Type': 'answer', 'ParentId': '118', 'urls': [], 'exp': [], 'Body': 'One of the greatest achievements of human civilization is Newton\\'s laws of motions.  The first law says that unless a force is acting then the velocity (not the position!) of objects stay constant, while the second law says that forces act by causing an acceleration (though heavy objects require more force to accellerate). However to make sense of those laws and to apply them to real life you need to understand how to move between the following three notions: Moving down that list is called \"taking the derivative\" while moving up that list is called \"taking the integral.\"  Calculus is the study of derivatives and integerals. In particular, if you want to figure out how objects move under some force you need to be able to integrate twice.  This requires understanding a lot of calculus! In a first semester class you usually learn about derivatives and integrals of functions of one variable, that is what you need to understand physics in 1-dimension!  To understand the actual physics of the world you need to understand derivatives and integrals in 3-dimensions which requires several courses. '}\n",
      "149 {'Id': '149', 'Type': 'answer', 'ParentId': '120', 'urls': ['https://math.stackexchange.com/questions/135'], 'exp': ['1', '2', '2\\\\times1\\\\times1\\\\times1\\\\times1\\\\times1', '1', '0', '1'], 'Body': \"The main point of talking about prime numbers is Euclid's theorem that every positive integer can be written uniquely as a product of primes.  As Justin remarks, this would break horribly if <math_exp> were considered prime, for example we could factor <math_exp> as <math_exp>.  Instead we say that <math_exp> is not a prime, but it is the product of zero primes (see ? to understand why any prime multiplied by itself <math_exp> times is <math_exp>) so Euclid's theorem works out nicely! \"}\n",
      "150 {'Id': '150', 'Type': 'question', 'Title': 'Are there any functions that are (always) continuous yet not differentiable? Or vice-versa?', 'Tags': ['real-analysis', 'continuity'], 'AcceptedAnswerId': '151', 'urls': [], 'exp': [], 'Body': \"It seems like functions that are continuous always seem to be differentiable, to me.  I can't imagine one that is not.  Are there any examples of functions that are continuous, yet not differentiable? The other way around seems a bit simpler -- a differentiable function is obviously always going to be continuous.  But are there any that do not satisfy this? \"}\n",
      "151 {'Id': '151', 'Type': 'answer', 'ParentId': '150', 'urls': ['https://en.wikipedia.org/wiki/Weierstrass_function'], 'exp': ['x', '(f(x+h)-f(x))/h', 'h', 'f(x+h)', 'f(x)', 'h', 'f', 'x'], 'Body': \"It's easy to find a function which is continuous but not differentiable at a single point, e.g. f(x) = |x| is continuous but not differentiable at 0. Moreover, there are functions which are continuous but nowhere differentiable, such as the Weierstrass function. On the other hand, continuity follows from differentiability, so there are no differentiable functions which aren't also continuous. If a function is differentiable at <math_exp>, then the limit <math_exp> must exist (and be finite) as <math_exp> tends to 0, which means <math_exp> must tend to <math_exp> as <math_exp> tends to 0, which means <math_exp> is continuous at <math_exp>. \"}\n",
      "152 {'Id': '152', 'Type': 'answer', 'ParentId': '71', 'urls': [], 'exp': [], 'Body': \"To understand the benefits of using quaternions you have to consider different ways to represent rotations. Here are few ways with a summary of the pros and cons: Euler angles are the best choice if you want a user to specify an orientation in a intuitive way. They are are also space efficient (three numbers). However, it is more difficult to linear interpolate values. Consider the case where you want to interpolate between 359 and 0 degrees. Linearly interpolating would cause a large rotation, even though the two orientations are almost the same. Writing shortest path interpolation, is easy for one axis, but non-trivial when considering the three Euler angles(for instance the shortest route between (240, 57, 145) and (35, -233, -270) is not immediately clear). Rotation matrices specify a new frame of reference using three normalized and orthogonal vectors (Right, Up, Out, which when multiplied become the new x, y, z). Rotation matrices are useful for operations like strafing (side ways movement), which only requires translating along the Right vector of the camera's rotation matrix. However, there is no clear method of interpolating between them. The are also expensive to normalize which is necessary to prevent scaling from being introduced. Axis angle, as the name suggests, are a way of specifying a rotation axis and angle to rotate around that axis. You can think of Euler angles, as three axis angle rotations, where the axises are the x, y, z axis respectively. Linearly interpolating the angle in a axis angle is pretty straight forward (if you remember to take the shortest path), however linearly interpolating between different axises is not. Quaternions are a way of specifying a rotation through a axis and the cosine of half the angle. They main advantage is I can pick any two quaternions and smoothly interpolate between them. Rotors are another way to perform rotations. Rotors are basically quaternions, but instead of thinking of them as 4D complex numbers, rotors are thought of as real 3D multivectors. This makes their visualization much more understandable (compared to quaternions), but requires fluency in geometric algebra to grasp their significance. Okay with that as the background I can discuss a real world example. Say you are writing a computer game   where the characters are animated in   3ds Max. You need to export a   animation of the character to play in   your game, but cannot faithfully   represent the interpolation used by   the animation program, and thus have   to sample. The animation is going to   be represented as a list of rotations   for each joint. How should we store   the rotations? If I am going to sample every frame,   not interpolate, and space is not an   issue, I would probably store the   rotations as rotation matrices. If   space was issue, then Euler angles.   That would also let me do things like   only store one angle for joints like   the knee that have only one degree of   freedom. If I only sampled every 4 frames, and   need to interpolate it depends on   whether I am sure the frame-rate will   hold. If I am positive that the   frame-rate will hold I can use axis   angle relative rotations to perform   the interpolation. This is atypical.   In most games the frame rate can drop   past my sampling interval, which would   require skipping an element in the   list to maintain the correct playback   speed. If I am unsure of what two   orientations I need to interpolate   between, then I would use quaternions   or rotors. \"}\n",
      "154 {'Id': '154', 'Type': 'question', 'Title': 'Do complex numbers really exist?', 'Tags': ['soft-question', 'complex-numbers', 'education', 'philosophy'], 'AcceptedAnswerId': '168', 'urls': [], 'exp': [], 'Body': \"Complex numbers involve the square root of negative one, and most non-mathematicians find it hard to accept that such a number is meaningful. In contrast, they feel that real numbers have an obvious and intuitive meaning. What's the best way to explain to a non-mathematician that complex numbers are necessary and meaningful, in the same way that real numbers are? This is not a Platonic question about the reality of mathematics, or whether abstractions are as real as physical entities, but an attempt to bridge a comprehension gap that many people experience when encountering complex numbers for the first time. The wording, although provocative, is deliberately designed to match the way that many people actually ask this question. \"}\n",
      "155 {'Id': '155', 'Type': 'question', 'Title': 'How can you prove that a function has no closed form integral?', 'Tags': ['real-analysis', 'calculus', 'integration', 'faq', 'differential-algebra'], 'AcceptedAnswerId': '163', 'urls': ['https://math.stackexchange.com/questions/2328/', 'https://math.stackexchange.com/users/918/ismael'], 'exp': ['f(x)', 'f(x)', 'f(x) = x^x', '$\\\\dfrac{\\\\sin(x)}{x}$'], 'Body': 'I\\'ve come across statements in the past along the lines of \"function <math_exp> has no closed form integral\", which I assume means that there is no combination of the operations: , which when differentiated gives the function <math_exp>. I\\'ve heard this said about the function <math_exp>, for example. What sort of techniques are used to prove statements like this? What is this branch of mathematics called? Merged with \"How to prove that some functions don\\'t have a primitive\" by Ismael: Sometimes we are told that some functions like <math_exp> don\\'t have an indefinite integral, or that it can\\'t be expressed in term of other simple functions. I wonder how we can prove that kind of assertion? '}\n",
      "156 {'Id': '156', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': [], 'Body': 'Quantum mechanics, and hence physics and everything around us, fundamentally involves complex numbers. '}\n",
      "157 {'Id': '157', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': [], 'Body': \"The argument isn't worth having, as you disagree about what it means for something to 'exist'. There are many interesting mathematical objects which don't have an obvious physical counterpart. What does it mean for the Monster group to exist? \"}\n",
      "159 {'Id': '159', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': [], 'Body': \"I'll start by pointing out that a whole host of things that people think of as 'real' are on shakier ground than imaginary numbers. Given that quantum mechanics predicts a fundamental limit to how granular reality is, the whole concept of real numbers is on very shakey ground, yet people accept those as fine. I'd therefore suggest that it is merely a case of familiarity - people are less familiar with complex numbers than with some other mathematical constructs. As for an actual existence outside the realms of pure maths... your best bet is to look at quantum mechanics again. This area has some fascinating results that are only possible through the use of imaginary numbers. Incidentally, fundamental particles are the place in nature that gave a 'physicality' to negative numbers (the charge of an electron is negative) well after they were accepted as normal by most people. \"}\n",
      "160 {'Id': '160', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': [], 'Body': 'The concept of mathematical numbers and \"existing\" is a tricky one.  What actually \"exists\"? Do negative numbers exist?  Of course they do not.  You can\\'t have a negative number of apples. Yet, the beauty of negative numbers is that when we define them (rigorously), then all of a sudden we can use them to solve problems we were never ever able to solve before, or we can solve them in a much simpler way. Imagine trying to do simple physics without the idea of negative numbers! But are they \"real\"?  Do they \"exist\"?  No, they don\\'t.  But they are just tools that help us solve real life problems. To go back to your question about complex numbers, I would say that the idea that they exist or not has no bearing on whether they are actually useful in solving the problems of every day life, or making them many, many, many times more easy to solve. The math that makes your computer run involves the tool that is complex numbers, for instance. '}\n",
      "162 {'Id': '162', 'Type': 'question', 'Title': 'Why is \"the set of all sets\" a paradox, in layman\\'s terms?', 'Tags': ['paradoxes', 'logic', 'set-theory'], 'AcceptedAnswerId': '171', 'urls': [], 'exp': [], 'Body': 'I\\'ve heard of some other paradoxes involving sets (ie, \"the set of all sets that do not contain themselves\") and I understand how paradoxes arise from them.  But this one I do not understand. Why is \"the set of all sets\" a paradox?  It seems like it would be fine, to me.  There is nothing paradoxical about a set containing itself. Is it something that arises from the \"rules of sets\" that are involved in more rigorous set theory? '}\n",
      "163 {'Id': '163', 'Type': 'answer', 'ParentId': '155', 'urls': ['http://www.sci.ccny.cuny.edu/~ksda/PostedPapers/liouv06.pdf', 'http://sites.mathdoc.fr/JMPA/PDF/JMPA_1837_1_2_A7_0.pdf'], 'exp': ['f', 'g', 'g', 'f(x)\\\\exp(g(x)) \\\\, \\\\mathrm dx', 'h', \"f = h' + hg\", 'e^{x^2}'], 'Body': 'It is a theorem of Liouville, reproven later with purely algebraic methods, that for rational functions <math_exp> and <math_exp>, <math_exp> non-constant, the antiderivative <math_exp> can be expressed in terms of elementary functions if and only if there exists some rational function <math_exp> such that it is a solution to the differential equation: <math_exp> <math_exp> is another classic example of such a function with no elementary antiderivative. I don\\'t know how much math you\\'ve had, but some of this paper might be comprehensible in its broad strokes: http://www.sci.ccny.cuny.edu/~ksda/PostedPapers/liouv06.pdf Liouville\\'s original paper: Liouville, J. .\" J. Math. Pure Appl. 3, 523-546, 1838. Michael Spivak\\'s book on Calculus also has a section with a discussion of this. '}\n",
      "164 {'Id': '164', 'Type': 'question', 'Title': 'Why is the volume of a sphere <span class=\"math-container\" id=\"1887\">\\\\frac{4}{3}\\\\pi r^3</span>?', 'Tags': ['geometry', 'volume', 'solid-geometry', 'spheres'], 'AcceptedAnswerId': '181', 'urls': [], 'exp': ['\\\\frac{4}{3}\\\\pi r^3', '\\\\pi', 'r^3', '\\\\frac{4}{3}'], 'Body': \"I learned that the volume of a sphere is <math_exp>, but why? The <math_exp> kind of makes sense because its round like a circle, and the <math_exp> because it's 3-D, but <math_exp> is so random! How could somebody guess something like this for the formula? \"}\n",
      "166 {'Id': '166', 'Type': 'answer', 'ParentId': '97', 'urls': ['http://mathworld.wolfram.com/LagrangeRemainder.html'], 'exp': [], 'Body': 'You can use a Taylor polynomial of sufficient degree to guarantee the accuracy that you need.  (The Taylor series for erf(x) is given on the Wikipedia page to which you linked.)  The Lagrange Remainder term can be used to bound the error in the Taylor series approximation. '}\n",
      "167 {'Id': '167', 'Type': 'answer', 'ParentId': '154', 'urls': ['http://en.wikipedia.org/wiki/Geometric_algebra#Complex_numbers'], 'exp': [], 'Body': \"There are geometric interpretations of imaginary numbers where they are thought of as parallelograms with a front and back, or oriented parallelograms. That interpretation requires geometric algebra but only uses real numbers. Here is a link: http://en.wikipedia.org/wiki/Geometric_algebra#Complex_numbers That doesn't have any pictures so it is admittedly not intuitive, but the answer is yes. Whether you think of imaginary numbers as square root of negative 1 or as parallelogram with a front and back, they exist. \"}\n",
      "168 {'Id': '168', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': ['x - 5 = 0', 'x', 'x + 5 = 0', '2x = 1', 'x^2 = 2', 'x^2 = -1', '(0,1) * (0,1) = (-1,0)', '\\\\sqrt{-1} = i', 'i=(0,1)', '\\\\sqrt{-1}', '\\\\sqrt{-1}', '\\\\sqrt{-1}'], 'Body': 'There are a few good answers to this question, depending on the audience. I\\'ve used all of these on occasion. A way to solve polynomials We came up with equations like <math_exp>, what is <math_exp>?, and the naturals solved them (easily). Then we asked, \"wait, what about <math_exp>?\" So we invented negative numbers. Then we asked \"wait, what about <math_exp>?\" So we invented rational numbers. Then we asked \"wait, what about <math_exp>?\" so we invented irrational numbers. Finally, we asked, \"wait, what about <math_exp>?\" This is the only question that was left, so we decided to invent the \"imaginary\" numbers to solve it. All the other numbers, at some point, didn\\'t exist and didn\\'t seem \"real\", but now they\\'re fine. Now that we have imaginary numbers, we can solve every polynomial, so it makes sense that that\\'s the last place to stop. Pairs of numbers This explanation goes the route of redefinition. Tell the listener to forget everything he or she knows about imaginary numbers. You\\'re defining a new number system, only now there are always pairs of numbers. Why? For fun. Then go through explaining how addition/multiplication work. Try and find a good \"realistic\" use of pairs of numbers (many exist). Then, show that in this system,  <math_exp>, in other words, we\\'ve defined a new system, under which it makes sense to say that <math_exp>, when <math_exp>. And that\\'s really all there is to imaginary numbers: a definition of a new number system, which makes sense to use in most places. And under that system, there is an answer to <math_exp>. The historical explanation Explain the history of the imaginary numbers. Showing that mathematicians also fought against them for a long time helps people understand the mathematical process, i.e., that it\\'s all definitions in the end. I\\'m a little rusty, but I think there were certain equations that kept having parts of them which used <math_exp>, and the mathematicians kept  throwing out the equations since there is no such thing. Then, one mathematician decided to just \"roll with it\", and kept working, and found out that all those square roots cancelled each other out. Amazingly, the answer that was left was the correct answer (he was working on finding roots of polynomials, I think). Which lead him to think that there was a valid reason to use <math_exp>, even if it took a long time to understand it. '}\n",
      "169 {'Id': '169', 'Type': 'answer', 'ParentId': '164', 'urls': [], 'exp': ['4/3', '\\\\frac{4}{3} \\\\pi  r^3', 'r^3', '\\\\text{distance}^3'], 'Body': 'This was first \"guessed at\" by Archimedes, by what fraction of the volume of a cylinder a sphere is.  That is, a sphere that is contained within that cylinder.  From that, he figured out <math_exp>. You can do it these days with the tools of calculus.  One way would to use the [Disk Method], over the graph of a semicircle.  Another would be to use spherical coordinates and calculate the 3D Integral. All of these methods should give you <math_exp> Note that it\\'s not just <math_exp> because it\\'s \"3D\"; but also because it\\'s the only way the dimensional analysis would work; that is, the units would have to be in <math_exp>.  But that might be what you said, but just clarifying. '}\n",
      "170 {'Id': '170', 'Type': 'answer', 'ParentId': '120', 'urls': [], 'exp': [], 'Body': 'It\\'s important to understand that this is not something that can be proved: it\\'s a definition. We choose not to regard 1 as a prime number, simply because it makes writing lots of theorems much easier. Noah gives the best example in his answer: Euclid\\'s theorem that every positive integer can be written uniquely as a product of primes. If 1 is defined to be a prime number, then we\\'d have to change that theorem to: \"every positive integer can be written uniquely as a product of primes, except for infinite multiplications by 1\". So we choose to go with the easier path of defining 1 to not be a prime. '}\n",
      "171 {'Id': '171', 'Type': 'answer', 'ParentId': '162', 'urls': ['https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument#General_sets'], 'exp': ['|S|', 'S', '|S| &lt; |2^S|', 'S', '|S| &lt; |2^S|', '2^S', 'S', '2^S', 'S', '|2^S| \\\\leq |S|'], 'Body': \"Let <math_exp> be the cardinality of <math_exp>. We know that <math_exp>, which can be proven with generalized Cantor's diagonal argument. Theorem The set of all sets does not exist. Proof Let <math_exp> be the set of all sets, then <math_exp>, but <math_exp> is a subset of <math_exp>, because every set in <math_exp> is in <math_exp>. Therefore <math_exp>. A contradiction. Therefore the set of all sets does not exist. \"}\n",
      "173 {'Id': '173', 'Type': 'answer', 'ParentId': '154', 'urls': ['https://mathoverflow.net/questions/30156/demystifying-complex-numbers'], 'exp': [], 'Body': 'You may be interested to read the MathOverflow question \"Demystifying Complex Numbers,\" here. A teacher is asking how to motivate complex numbers to students taking complex analysis. '}\n",
      "174 {'Id': '174', 'Type': 'answer', 'ParentId': '164', 'urls': ['http://en.wikipedia.org/wiki/Pappus%27s_centroid_theorem'], 'exp': ['\\\\frac{1}{2}\\\\pi r^2', '\\\\frac{4r}{3\\\\pi}', '2\\\\pi\\\\cdot\\\\frac{4r}{3\\\\pi} = \\\\frac{8}{3}\\\\cdot r', '\\\\frac{1}{2}\\\\pi r^2\\\\cdot\\\\frac{8}{3}\\\\cdot r = \\\\frac{4}{3}\\\\pi r^3'], 'Body': \"Pappus's centroid theorem (second theorem) says that the volume of a solid formed by revolving a region about an axis is the product of the area of the region and the distance traveled by the centroid of the region when it is revolved.  A sphere can be formed by revolving a semicircle about is diameter edge. The area of the semicircle is <math_exp>.  The centroid of the semicircle can be found by intersecting two lines that both divide the area of the semicircle into two equal parts.  One such line is perpendicular to the diameter edge through the center of the semicircle (this is a line of symmetry of the semicircle).  Another such line is parallel to the diameter edge, <math_exp> away from it (verification of this is left as an exercise for the reader).  When revolved about the diameter edge of the semicircle, the centroid travels <math_exp>, so the volume of the sphere is <math_exp>. \"}\n",
      "175 {'Id': '175', 'Type': 'answer', 'ParentId': '118', 'urls': [], 'exp': [], 'Body': \"Calculus is the mathematics of change. In algebra, almost nothing ever changes. Here's a comparison of some algebra vs. calc problems: Note how the algebra problem nothing changes, where in the calc problem, the speed of the car is constantly changing. The speed of the point in relation to the ground is never the same (its zero when its at the bottom, 20fps when it's at the top. Calculus lets you figure out how fast it's going exactly at a specific moment. There are two main branches of calculus, differential and integral. These problems pertain to differential calculus as they concern how something is changing. Integral calculus deals with how much something has changed, the opposite of differential calculus. To find out how much something has changed when its rate of change isn't constant requires integral calculus. (the equation is purely hypothetical unless Jeff happens to be the size of a beluga whale). \"}\n",
      "176 {'Id': '176', 'Type': 'answer', 'ParentId': '139', 'urls': [], 'exp': [], 'Body': 'Does this give you any ideas? <img src=\"https://farm5.static.flickr.com/4119/4813793520_0d7fa53b99_o.png\" alt=\"alt text\"> '}\n",
      "177 {'Id': '177', 'Type': 'question', 'Title': 'Will this procedure generate random points uniformly distributed within a given circle? Proof?', 'Tags': ['algorithms', 'probability-theory'], 'urls': ['http://rubyquiz.strd6.com/quizzes/234-random-points-within-a-circle', 'http://gist.github.com/447554'], 'exp': ['r', 'R', '[0, 1)', 'p = (x, y)', '2r', 'R', 'x', 'y', 'x, y \\\\in [0, 1)', 'x', 'y', '[0, r)', 'r', 'p', 'x', 'p', 'y', 'p', 'p', 'p'], 'Body': \"Consider the task of generating random points uniformly distributed within a circle of a given radius <math_exp> that is centered at the origin. Assume that we are given a random number generator <math_exp> that generates a floating point number uniformly distributed in the range <math_exp>. Consider the following procedure: Generate a random point <math_exp> within a square of side <math_exp> centered at the origin. This can be easily achieved by: a. Using the random number generator <math_exp> to generate two random numbers <math_exp> and <math_exp>, where <math_exp>, and then transforming <math_exp> and <math_exp> to the range <math_exp> (by multiplying each by <math_exp>). b. Flipping a fair coin to decide whether to reflect <math_exp> around the <math_exp>-axis. c. Flipping another fair coin to decide whether to reflect <math_exp> around the <math_exp>-axis. Now, if <math_exp> happens to fall outside the given circle, discard <math_exp> and generate another point. Repeat the procedure until <math_exp> falls within the circle. Is the previous procedure correct? That is, are the random points generated by it uniformly distributed within the given circle? How can one formally [dis]prove it? Background Info The task was actually given in Ruby Quiz - Random Points within a Circle (#234). If you're interested, you can check my solution in which I've implemented the procedure described above. I would like to know whether the procedure is mathematically correct or not, but I couldn't figure out how to formally [dis]prove it. Note that the actual task was to generate random points uniformly distributed within a circle of a given radius and position, but I intentionally left that out in the question because the generated points can be easily translated to their correct positions relative to the given center. \"}\n",
      "178 {'Id': '178', 'Type': 'answer', 'ParentId': '154', 'urls': ['http://en.wikipedia.org/wiki/Complex_number', 'http://en.wikipedia.org/wiki/Electrical_impedance'], 'exp': ['i', '-1', '0', '0', '-1', '0', '-1', 'i', 'i*i', '-1'], 'Body': \"We will will first consider the most common definition of <math_exp>, as the square root of <math_exp>. When you first hear this, it sounds crazy. <math_exp> squared is <math_exp>; a positive times a positive is positive and a negative times a negative is positive too. So there doesn't actually appear to be any number that we can square to get <math_exp>. A mathematician would collectively term <math_exp>, negative numbers and positive numbers as the real numbers. They would also define the term complex numbers as a group of numbers that includes these real numbers. So while we have shown that no real number can square to get <math_exp>, we haven't even defined complex numbers at this point, so we can't rule out that one might have this property. At this point, it makes sense to ask what does a mathematician mean by a number? It certainly isn't what most people associate it with - as an abstract representation some kind of real world quantity. We need to understand that it isn't uncommon for one word to have different meanings for different groups of people - after all words mean whatever we make them mean. Most people only need to real world quantities, so they find it convenient to call those numbers. On the other hand, mathematicians explore a variety of different number systems. Indeed some, such as complex numbers, are useful for solving problems that are actually about real numbers. So mathematicians define <math_exp> as a number that obeys most of the normal algebraic laws. They also defined <math_exp> to equal <math_exp>. From this we can derive all of the standard results about complex numbers. As for whether they are real - it depends on what you want to know. Obviously, they don't correspond to quantities of physical objects. On the other hand, complex numbers can useful for representing resistance in an electric circuit. Ultimately, they are an idea and while ideas don't exist physically, saying they don't exist at all is inaccurate. \"}\n",
      "179 {'Id': '179', 'Type': 'answer', 'ParentId': '162', 'urls': ['http://en.wikipedia.org/wiki/Russel%27s_paradox'], 'exp': [], 'Body': 'An informal explanation is Russel\\'s Paradox. The wiki page is informative, here\\'s the relevant quote: Let us call a set \"abnormal\" if it is   a member of itself, and \"normal\"   otherwise. For example, take the set   of all squares. That set is not itself   a square, and therefore is not a   member of the set of all squares. So   it is \"normal\". On the other hand, if   we take the complementary set that   contains all non-squares, that set is   itself not a square and so should be   one of its own members. It is   \"abnormal\". Now we consider the set of   all normal sets, R. Attempting to   determine whether R is normal or   abnormal is impossible: If R were a   normal set, it would be contained in   the set of normal sets (itself), and   therefore be abnormal; and if it were   abnormal, it would not be contained in   the set of normal sets (itself), and   therefore be normal. This leads to the   conclusion that R is both normal and   abnormal: Russell\\'s paradox. '}\n",
      "180 {'Id': '180', 'Type': 'answer', 'ParentId': '75', 'urls': [], 'exp': ['x \\\\to x^{-1}'], 'Body': 'I won\\'t explain what a ring or a group is, because that\\'s already been done, but I\\'ll add something else.  One reason groups and rings feel similar is that they are both \"algebraic structures\" in the sense of universal algebra. So for instance, the operation of quotienting via a normal subgroup (for a group) and a two-sided ideal (for a ring) are basically instances of quotienting via an invariant equivalence relation in universal algebra.  A field, by contrast, is not really a construction of universal algebra (because the operation <math_exp> is not everywhere defined) -- which is why free fields don\\'t exist, for instance -- though they are a special case of rings. '}\n",
      "181 {'Id': '181', 'Type': 'answer', 'ParentId': '164', 'urls': ['http://en.wikipedia.org/wiki/Cavalieri%27s_principle', 'http://en.wikipedia.org/wiki/File:Sphere_cavalieri.svg'], 'exp': ['R', 'R', 'R', 'R', 'R', 'y', '\\\\pi (R^2-y^2)', '\\\\pi R^3', '\\\\frac{2}{3} \\\\pi R^3', 'R', '\\\\frac{4}{3} \\\\pi R^3'], 'Body': \"In addition to the methods of calculus, Pappus, and Archimedes already mentioned, Cavalieri's Principle can be useful for these kinds of problems. Suppose you have two solid figures lined up next to each other, each fitting between the same two parallel planes. (E.g., two stacks of pennies lying on the table, of the same height). Then, consider cutting the two solids by a plane parallel to the given two and in between them. If the cross-sectional area thus formed is the same for each of the solids for any such plane, the volumes of the solids are the same. If you're willing to accept that you know the volume of a cone is 1/3 that of the cylinder with the same base and height, you can use Cavalieri, comparing a hemisphere to a cylinder with an inscribed cone, to get the volume of the sphere. This diagram (from Wikipedia) illustrates the construction: look here Consider a cylinder of radius <math_exp> and height <math_exp>, with, inside it, an inverted cone, with base of radius <math_exp> coinciding with the top of the cylinder, and again height <math_exp>. Put next to it a hemisphere of radius <math_exp>. Now consider the cross section of each at height <math_exp> above the base. For the cylinder/cone system, the area of the cross-section is <math_exp>. It's the same for the hemisphere cross-section, as you can see by doing the Pythagorean theorem with any vector from the sphere center to a point on the sphere at height y to get the radius of the cross section (which is circular). Since the cylinder/cone and hemisphere have the same height, by Cavalieri's Principle the volumes of the two are equal. The cylinder volume is <math_exp>, the cone is a third that, so the hemisphere volume is <math_exp>. Thus the sphere of radius <math_exp> has volume <math_exp>. \"}\n",
      "183 {'Id': '183', 'Type': 'answer', 'ParentId': '136', 'urls': ['https://math.stackexchange.com/questions/136/why-are-the-differences-between-consecutive-squares-equal-to-the-sequence-of-odd/143#143'], 'exp': ['n+n+1=2n+1', '2n+1', '1+3+5+\\\\dots+(2n-1) = n^2'], 'Body': '<img src=\"https://i.imgur.com/RSieeg5.gif\" alt=\"alt text\"> Dan\\'s algebraic justification is correct, but you may get more intuition about why this is happening from the above picture. Each time you want to enlarge the square by one unit, you have to add an extra row, an extra column, and one more square to fill in the corner. These correspond directly to the <math_exp> that Dan mentioned. And of course, <math_exp> is how odd numbers look. Looking at it from the other direction, you can use the same idea to convince yourself of Noah\\'s claim that <math_exp>. Imagine the left-hand side as representing the upper sequence of green and orange squares. You add on first 1 tile, then 3, then 5, and so on to construct larger and larger squares. At each step, you are adding one row (n) and one column (another n), then removing that one tile in the top right corner where they overlap (for a total of 2n-1). '}\n",
      "184 {'Id': '184', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': \"I have found most people liked math at some point, but something happened in their learning process that made them feel so stupid, they became disenfranchised with mathematics. What tends to happen is students are presented with some mathematically result they are expected to memorize by route, which takes all the joy out of mathematics and prevents them from approaching mathematics intuitively. So I would first try to zero in on what they don't like and what parts of mathematics they have had to take on faith. You might not have to excite them if you help them learn mathematics intuitively. As per the suggestions, I would show them how mathematically equations make pretty shapes in Processing. \"}\n",
      "185 {'Id': '185', 'Type': 'question', 'Title': 'Recasting points from one vector space to another', 'Tags': ['linear-algebra', 'vector-spaces'], 'AcceptedAnswerId': '193', 'urls': [], 'exp': ['x', 'y', 'z', 'p', 'a', 'b', 'a - p', 'b - p', 'x'], 'Body': \"I have a collection of 3D points in the standard <math_exp>, <math_exp>, <math_exp> vector space. Now I pick one of the points <math_exp> as a new origin and two other points <math_exp> and <math_exp> such that <math_exp> and <math_exp> form two vectors of a new vector space. The third vector of the space I will call <math_exp> and calculate that as the cross product of the first two vectors. Now I would like to recast or reevaluate each of the points in my collection in terms of the new vector space. How do I do that? (Also, if 'recasting' not the right term here, please correct me.) \"}\n",
      "187 {'Id': '187', 'Type': 'question', 'Title': 'Euclidean Tilings that are Uniform but not Vertex-Transitive', 'Tags': ['discrete-geometry'], 'AcceptedAnswerId': '1492', 'urls': [], 'exp': [], 'Body': 'Basic definitions: a tiling of d-dimensional Euclidean space is a decomposition of that space into polyhedra such that there is no overlap between their interiors, and every point in the space is contained in some one of the polyhedra. A vertex-uniform tiling is a tiling such that each vertex figure is the same: each vertex is contained in the same number of k-faces, etc: the view of the tiling is the same from every vertex. A vertex-transitive tiling is one such that for every two vertices in the tiling, there exists an element of the symmetry group taking one to the other. Clearly all vertex-transitive tilings are vertex-uniform. For n=2, these notions coincide. However, Grunbaum, in his book on tilings, mentions but does not explain that for n >= 3, there exist vertex uniform tilings that are not vertex transitive. Can someone provide an example of such a tiling, or a reference that explains this? '}\n",
      "188 {'Id': '188', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': [], 'Body': 'No number does \"really exist\" the way trees or atoms exist. In physics people however have found use for complex numbers just as they have found use for real numbers. '}\n",
      "190 {'Id': '190', 'Type': 'answer', 'ParentId': '11', 'urls': [], 'exp': [], 'Body': \"If you take two real numbers x and y then there per definition of the real number z for which x &lt; z &lt; y or x &gt; z &gt; y is true. For x = 0.99999... and y = 1 you can't find a z and therefore 0.99999... = 1. \"}\n",
      "191 {'Id': '191', 'Type': 'answer', 'ParentId': '177', 'urls': [], 'exp': ['P(a,b)', 'P(a,b) = P[x = |a|]P[y = |b|] * 1/4', 'A \\\\cap B = B'], 'Body': \"I'm going to use <math_exp> for probability of x = a, and y = b. <math_exp> for any a, b inside the square. Therefore you are picking a coordinate uniformly inside the square. Now the problem reduced to the following: Pick a element uniformly in a set A, then discard ones that are not B (B is a subset of A). Is it the same as pick a element uniformly in the set <math_exp>? I think this is true, I don't know if the following is a formal proof(haven't done probability in a while). please point out my mistakes. P(B) is the event that a element in B is picked. P(A) is the event that a element in A is picked. Pick a x uniformly, we have b \\\\in B is P(x=b|B), it's obviously uniform. The conditional probability P(B|A) = P(A \\\\cap B)/P(A) = P(B)/P(A) = P(B) P(A) = 1, w/e you pick are in A. P(x=b| (B|A)) = P(x=b|B), which is uniform. Answer to your question: It is uniformly distributed. \"}\n",
      "193 {'Id': '193', 'Type': 'answer', 'ParentId': '185', 'urls': [], 'exp': ['i', 'j', 'k', 'i', 'a-p', 'j', 'b-p', 'k', 'i', 'j', 'k', 'a = d(a,p)*i', 'b = d(b,p)*j'], 'Body': 'You are changing your basis vectors, call your new ones <math_exp>, <math_exp>, and <math_exp> where <math_exp> is defined from <math_exp>, <math_exp> from <math_exp>, and <math_exp> the cross product. Now recall that your basis vectors should be unit, so take the length of your three vectors and divide the vectors by their length, making <math_exp>, <math_exp>, and <math_exp> unit. Now <math_exp>, <math_exp>. '}\n",
      "194 {'Id': '194', 'Type': 'question', 'Title': 'Distribution of primes?', 'Tags': ['number-theory', 'prime-numbers'], 'AcceptedAnswerId': '199', 'urls': [], 'exp': ['1', '1{,}000{,}000', '1{,}000{,}000', '2{,}000{,}000'], 'Body': 'Do primes become more or less frequent as you go further out on the number line?  That is, are there more or fewer primes between <math_exp> and <math_exp> than between <math_exp> and <math_exp>? A proof or pointer to a proof would be appreciated. '}\n",
      "195 {'Id': '195', 'Type': 'answer', 'ParentId': '185', 'urls': ['http://en.wikipedia.org/wiki/Affine_transformation'], 'exp': [], 'Body': 'What you are describing is an Affine Transformation, which is a linear transformation followed by a translation. We know this because any straight lines in your original vector space is also going to be a straight line in your transformed vector space. '}\n",
      "196 {'Id': '196', 'Type': 'question', 'Title': 'How does the wheel paradox work?', 'Tags': ['geometry'], 'AcceptedAnswerId': '229', 'urls': [], 'exp': [], 'Body': 'I keep looking at this picture and its driving me crazy. How can the smaller circle travel the same distance when its circumference is less than the entire wheel? <img src=\"https://i.stack.imgur.com/hXPBX.gif\" alt=\"demonstration image\"> '}\n",
      "197 {'Id': '197', 'Type': 'answer', 'ParentId': '196', 'urls': [], 'exp': [], 'Body': 'If the two circles are fixed, then they will be traveling the same difference, but at different velocities. In fact, the ratio of the radii is equal to the ratio of the velocities a point on either circle will be traveling. If you tried to repeat this by putting two different-sized circles on a track and making them spin to come out to be the same distance with the same angular velocity, you will notice that one of the circles will have to slide/slip along the track in order to keep them at the same pace. '}\n",
      "198 {'Id': '198', 'Type': 'question', 'Title': 'Aren\\'t constructive math proofs more \"sound\"?', 'Tags': ['proof-theory', 'constructive-mathematics'], 'urls': [], 'exp': [], 'Body': 'Since constructive mathematics allows us to avoid things like Russell\\'s Paradox, then why don\\'t they replace traditional proofs?  How do we know the \"regular\" kind of mathematics are free of paradox without a proof construction? '}\n",
      "199 {'Id': '199', 'Type': 'answer', 'ParentId': '194', 'urls': ['http://en.wikipedia.org/wiki/Prime_number_theorem'], 'exp': [], 'Body': 'From the Wikipedia article about the prime number theorem: Roughly speaking, the prime number   theorem states that if a random number   nearby some large number N is   selected, the chance of it being prime   is about 1 / ln(N), where ln(N)   denotes the natural logarithm of N.   For example, near N = 10,000, about   one in nine numbers is prime, whereas   near N = 1,000,000,000, only one in   every 21 numbers is prime. In other   words, the average gap between prime   numbers near N is roughly ln(N). '}\n",
      "200 {'Id': '200', 'Type': 'answer', 'ParentId': '162', 'urls': [], 'exp': [], 'Body': 'The \"set of all sets\" is not so much a paradox in itself as something  that inevitably leads to a contradiction, namely the well-known (and referenced in the question) Russell\\'s paradox. Given any set and a predicate applying to sets, the set of all things satisfying the predicate should be a subset of the original set.  If the \"set of all sets\" were to exist, because self-containment and non-self-containment are valid predicates, the set of all sets not containing themselves would have to exist as a set in order for our set theory to be consistent.  But this \"set of all sets\" cannot exist in a consistent set theory because of the Russel paradox. So the non-existence of the \"set of all sets\" is a consequence of the fact that presuming it\\'s existence would lead to the contradiction described by Russel\\'s paradox. This is in fact the origin of Russel\\'s paradox. In his work \"The Basic Laws of Arithmetic\", Gottlob Frege had taken as a postulate the existence of this \"set of all sets\".  In a letter to Frege, Bertrand Russell essentially blew away the basis of Frege\\'s entire work by describing the paradox and proving that this postulate could not be a part of a consistent set theory. '}\n",
      "201 {'Id': '201', 'Type': 'question', 'Title': 'Is there possibly a largest prime number?', 'Tags': ['number-theory', 'prime-numbers'], 'AcceptedAnswerId': '211', 'urls': [], 'exp': ['n', 'n'], 'Body': 'Prime numbers are numbers with no factors other than one and itself. Factors of a number are always lower or equal to than a given number; so, the larger the number is, the larger the pool of \"possible factors\" that number might have. So the larger the number, it seems like the less likely the number is to be a prime. Surely there must be a number where, simply, every number above it has some other factors.  A \"critical point\" where every number larger than it simply will always have some factors other than one and itself. Has there been any research as to finding this critical point, or has it been proven not to exist?  That for any <math_exp> there is always guaranteed to be a number higher than <math_exp> that has no factors other than one and itself? '}\n",
      "202 {'Id': '202', 'Type': 'answer', 'ParentId': '155', 'urls': ['http://en.wikipedia.org/wiki/Risch_algorithm', 'http://en.wikipedia.org/wiki/Constant_problem'], 'exp': [], 'Body': 'Surprisingly, there is a procedure for determining this, called the Risch Algorithm. However, it is apparently very complicated to implement, and furthermore it is not a true algorithm: if the input does have an anti-derivative, it can find it in a finite amount of time, but demonstrating there is no anti-derivative requires solving the constant problem, which can fail to halt. (In that case, probabilistic methods can be used.) '}\n",
      "203 {'Id': '203', 'Type': 'answer', 'ParentId': '198', 'urls': ['http://en.wikipedia.org/wiki/Axiom_of_choice'], 'exp': [], 'Body': \"The distinction between constructive mathematics and traditional mathematics has nothing to do with Russell's Paradox. Constructive mathematics simply requires working with one less basic postulate that many mathematicians have believed to be sensible and on which some proofs are based, namely the Axiom of Choice \"}\n",
      "204 {'Id': '204', 'Type': 'answer', 'ParentId': '201', 'urls': ['http://math.mit.edu/~ssam/writings/primes.pdf'], 'exp': [], 'Body': 'No there is not, here is a collection of proofs; http://math.mit.edu/~ssam/writings/primes.pdf '}\n",
      "205 {'Id': '205', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://www.python.org/download/'], 'exp': [], 'Body': 'Personally, I always use Python\\'s console. It has history and allows all kinds of math operations. <img src=\"https://i.stack.imgur.com/oB12p.png\" alt=\"Python console Ubuntu\"> It is available for Linux, Windows, Mac, ChromeOS, Android, and others. '}\n",
      "206 {'Id': '206', 'Type': 'question', 'Title': 'What exactly does it mean for a function to be \"well-behaved\"?', 'Tags': ['terminology', 'analysis', 'economics'], 'AcceptedAnswerId': '248', 'urls': [], 'exp': [], 'Body': 'Often in my studies (economics) the assumption of a \"well-behaved\" function will be invoked. I don\\'t exactly know what that entails (I think twice continuously differentiability is one of the requirements), nor do I know why this is necessary (though I imagine the why will depend on each case). Can someone explain it to me, and if there is an explanation of the why as well, I would be grateful. Thanks!  EDIT: To give one example where the term appears, see this Wikipedia entry for utility functions, which says at one point: In order to simplify calculations,   various assumptions have been made of   utility functions. CES (constant elasticity of substitution, or   isoelastic) utility   Exponential utility   Quasilinear utility   Homothetic preferences Most utility functions   used in modeling or theory are   well-behaved. They are usually   monotonic, quasi-concave, continuous   and globally non-satiated. I might be wrong, but I don\\'t think \"well-behaved\" means monotonic, quasi-concave, continuous and globally non-satiated. What about twice differentiable? '}\n",
      "209 {'Id': '209', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://www.r-project.org/'], 'exp': [], 'Body': \"I use R these days. It was build to be a calculator and does the job well. It's syntax might be a bit strange but it allows you to do a lot with little typing. \"}\n",
      "210 {'Id': '210', 'Type': 'answer', 'ParentId': '198', 'urls': [], 'exp': [], 'Body': 'A whole bunch of things in mathematics are inherently nonconstructive. For instance, invariant theory--recall the famous quote by Gordan that Hilbert\\'s mathematics was \"theology.\" (A quote which, I believe, was in jest.)  The Hahn-Banach theorem, a fundamental tool in functional analysis (and a great tool for proving all sorts of results, like approximation results--Runge\\'s theorem, the Stone-Weierstrass theorem, and more) relies on the axiom of choice, and is consequently nonconstructive.  The fact that any proper ideal in a ring is contained in a maximal ideal is frequently used in algebra, and yet it needs the axiom of choice.  The use of ultraproducts in logic (or the construction of hyperreal numbers) is inherently nonconstructive: you can\\'t just exhibit a nonprincipal ultrafilter on the natural numbers. Basically, a lot of mathematics just doesn\\'t work without Zorn\\'s lemma, and this is equivalent to the axiom of choice. '}\n",
      "211 {'Id': '211', 'Type': 'answer', 'ParentId': '201', 'urls': [], 'exp': ['x', 'x+1', 'x', 'x+1', 'x', 'x+1', 'x', 'x+1', 'x', 'x'], 'Body': \"Euclid's famous proof is as follows: Suppose there is a finite number of primes.  Let <math_exp> be the product of all of these primes.  Then look at <math_exp>.  It is clear that <math_exp> is coprime to <math_exp>.  Therefore, no nontrivial factor of <math_exp> is a factor of <math_exp>, but every prime is a factor of <math_exp>.  By the fundamental theorem of arithmetic, <math_exp> admits a prime factorization, and by the above remark, none of these prime factors can be a factor of <math_exp>, but <math_exp> is the product of all primes.  This is a contradiction. \"}\n",
      "212 {'Id': '212', 'Type': 'answer', 'ParentId': '201', 'urls': ['http://xkcd.com/622/'], 'exp': [], 'Body': \"According to XKCD, we have the following Haiku: Top Prime's Divisors' Product (Plus one)'s factors are...? Q.E.D B@%&amp;$ I wonder if we can edit it to make it correct \"}\n",
      "213 {'Id': '213', 'Type': 'question', 'Title': 'Mathematical subjects you wish you learned earlier', 'Tags': ['soft-question', 'learning'], 'AcceptedAnswerId': '498', 'urls': [], 'exp': [], 'Body': 'I am learning geometric algebra, and it is incredible how much it helps me understand other branches of mathematics. I wish I had been exposed to it earlier. Additionally I feel the same way about enumerative combinatorics. What are some less popular mathematical subjects that you think should be more popular? '}\n",
      "214 {'Id': '214', 'Type': 'answer', 'ParentId': '162', 'urls': ['http://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory', 'http://en.wikipedia.org/wiki/New_Foundations'], 'exp': [], 'Body': \"The existence of universal set is incompatible with the Zermelo&ndash;Fraenkel axioms of set theory. However, there are alternative set theories which admit a universal set. One such theory is Quine's New Foundations. \"}\n",
      "215 {'Id': '215', 'Type': 'answer', 'ParentId': '206', 'urls': [], 'exp': [], 'Body': 'The short answer is that there is no \"exact\" meaning. Ideally, additional axioms are introduced to ensure that a certain function (or any mathematical object, for that matter) is \"well-behaved\" which, in effect, makes analysis easier. So, the meaning of \"well-behaved\" should be derived from those specific additional axioms. '}\n",
      "216 {'Id': '216', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': 'I think ideas like cardinality are suitable for getting kids interested in math -- the notion of a one-to-one correspondence is easy to grasp, and the proof of the uncountability of the reals is great fun. Also, elementary number theory can be useful.  This is the sort of thing that would be especially useful for, say, contest math, which is where a lot of kids get interested in math. In general, popular math books can be a good source of ideas. '}\n",
      "217 {'Id': '217', 'Type': 'answer', 'ParentId': '155', 'urls': ['http://www.claymath.org/2005-academy-colloquium-series', 'https://web.archive.org/web/20190128135718/http://www.claymath.org/library/academy/LectureNotes05/Conrad.pdf'], 'exp': [], 'Body': 'Brian Conrad explains this in the following: Impossibility theorems on integration in elementary terms - pdf '}\n",
      "218 {'Id': '218', 'Type': 'answer', 'ParentId': '177', 'urls': ['http://en.wikipedia.org/wiki/Rejection_sampling', 'http://mathworld.wolfram.com/DiskPointPicking.html'], 'exp': [], 'Body': \"Yes this will work; it's called rejection sampling. Even better is to generate a point in polar coordinates though:  pick θ from [0, 2π) and r2 from [0, R2] (ie. multiply R by the square-root of a random number in [0, 1] - without the square-root it is non-uniform). \"}\n",
      "219 {'Id': '219', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': 'Besides all of these mathematics topics, I would contend that it is equally as important to show them the more human side of the subject.  Mathematics has a history and a culture that is accessible and exciting to some people.  Of course there are the exciting lives of Pascal and Galois, but there is also the story of Bourbaki (and the individual stories of their various members). Also, introducing children to actual mathematicians/engineers/scientists is a great way to motivate their interest in the subject.  Let them see that mathematics has not been borne from the head of Zeus, that it is a living and breathing subject. '}\n",
      "220 {'Id': '220', 'Type': 'question', 'Title': 'Simple lowpass frequency response', 'Tags': ['applications', 'signal-processing'], 'urls': [], 'exp': [], 'Body': 'Okay, so hopefully this isn\\'t too hard or off-topic. Let\\'s say I have a very simple lowpass filter (something that smooths out a signal), and the filter object has a position variable and a cutoff variable (between 0 and 1). In every step, a value is put into the following bit of pseudocode as \"input\": position = position*(1-c)+input*c, or more mathematically, f(n) = f(n-1)*(1-c)+x[n]*c. The output is the value of \"position.\" Basically, it moves a percentage of the distance between the current position and then input value, stores this value internally, and returns it as output. It\\'s intentionally simplistic, since the project I\\'m using this for is going to have way too many of these in sequence processing audio in real time. Given the filter design, how do I construct a function that takes input frequency (where 1 means a sine wave with a wavelength of 2 samples, and .5 means a sine wave with wavelength 4 samples, and 0 is a flat line), and cutoff value (between 1 and 0, as shown above) and outputs the amplitude of the resulting sine wave? Sine wave comes in, sine wave comes out, I just want to be able to figure out how much quieter it is at any input and cutoff frequency combination. '}\n",
      "221 {'Id': '221', 'Type': 'answer', 'ParentId': '213', 'urls': [], 'exp': [], 'Body': \"Though I'm sure it's not unpopular, I don't think many people learn it early: Group Theory. It's a real nice area with a lot of cool math and some neat applications (like cryptography). \"}\n",
      "222 {'Id': '222', 'Type': 'answer', 'ParentId': '213', 'urls': [], 'exp': [], 'Body': 'I don\\'t really think that graph theory is a \"less popular mathematical subject,\" but I certainly wish I had been exposed to it earlier. '}\n",
      "223 {'Id': '223', 'Type': 'answer', 'ParentId': '213', 'urls': [], 'exp': [], 'Body': \"Theory of computation, information theory and logic/foundation of mathematics are very interesting topics. I wish I knew them earlier. They are not unpopular(almost every university have a bunch of ToC people in CS depatment...) , but many math major I know have never touched them. They show you the limits of mathematics, computation and communication. Logic shows there are things can't be proved from a set of axioms even if it's true--Godel's incompleteness theorem. There are other interesting theorems in foundation of mathematics. Like the independence of continuum hypothesis to ZFC. Theory of computation showed me things that's not computable. Problems that takes exponential time, exponential space, no matter what kind of algorithm you come up with. Information theory proves the minimum amount of information required to reconstruct some other information. It pops up in unexpected places. There is a proof of there are infinite number of primes by information theory (Sorry I can't find it, I can only tell you it exists. I might find it later). \"}\n",
      "224 {'Id': '224', 'Type': 'answer', 'ParentId': '154', 'urls': ['https://math.stackexchange.com/questions/80745/an-example-of-an-easy-to-understand-undecidable-problem/80772#80772'], 'exp': [], 'Body': 'Are real numbers \"real\"?  It\\'s not even computationally possible to compare two real numbers for equality! Interestingly enough, it is shown in Abstract Algebra courses that the idea of complex numbers arises naturally from the idea of real numbers - you could not say, for instance, that the real numbers are valid but the complex numbers aren\\'t (whatever your definition of valid is...) '}\n",
      "225 {'Id': '225', 'Type': 'answer', 'ParentId': '213', 'urls': ['http://rads.stackoverflow.com/amzn/click/0521792851'], 'exp': [], 'Body': 'I don\\'t know what level of mathematics you are referring to, but here\\'s my opinion after recently finishing my university\\'s undergraduate curriculum. Firstly, I would like to second Jan Gorzny\\'s reply of Group Theory. Second, I wish that I had learned linear algebra earlier.  The topic usually has two semesters: matrix algebra and then an early proof-based introduction to vector spaces and linear transformations.  The real work in this topic can\\'t begin until after both of these classes are completed. I also wish I had been exposed to topology earlier than I was.  Of course there are two \"standard\" approaches here, and I suppose the approach that introduces general topology before advanced calculus would have better suited my tastes. Here is a good book that may give some more insight into the heavily debated area where your question lies: Thomas Garrity, All the Mathematics You Missed But Need to Know for Graduate School '}\n",
      "226 {'Id': '226', 'Type': 'answer', 'ParentId': '206', 'urls': ['http://en.wikipedia.org/wiki/Well-behaved', 'https://math.stackexchange.com/questions/206/what-exactly-does-it-mean-for-a-function-to-be-well-behaved/215#215', 'http://en.wikipedia.org/wiki/Concave_function#Properties'], 'exp': [], 'Body': 'In general, we think of well-behaved functions as simpler, somehow. In any field, we might want to limit ourselves to considering only well-behaved functions in order to avoid having to deal with nasty edge cases. And in each of these domains, the community is free to choose whatever definition of \\'well-behaved\\' makes sense for them. A quick look at the wiki link that e.James posted will show you the diversity in ideas of what it means to be well-behaved. I am no economist, so I will take for granted that the definition you put forth in your question is the one in common use. I can see twice-differentiable as a reasonable requirement for a utility function to be \\'well-behaved\\' is because the derivative of the utility function is marginal utility, and economists often care about the derivative of marginal utility. For example, if the second derivative of utility is negative, this means that the marginal utility has a negative derivative in other words, additional quantity of the good or service does not add utility as quickly. Also commonly referred to as diminishing returns. If we want to be able to take the derivative even once, of course, we need the function to be continuous. You probably don\\'t need to worry about the formal definition here. Pencil test should work fine. The requirement for utility to be monotonic means that it is either always increasing or always decreasing. In other words, that a particular good or service is either desirable or not. If 10 widgets were good, 20 must be better. Of course, as mentioned above, maybe not that much better. Monotonic means that it will also be . (Except for weird stuff like a flat function) That is, they have at most one local maximum. We would prefer that functions be quasiconcave because we wish to avoid cases like the one below. It just makes it so much easier to optimize when you only have one possible maximum to worry about. <img src=\"https://img.sparknotes.com/figures/E/eeb0fa64da5c2a1115cdf73c3ef2f0fc/derivapps1.gif\" alt=\"alt text\"> Globally non-satiated someone else can talk about. I don\\'t know enough to be sure I won\\'t just be misleading you further. '}\n",
      "228 {'Id': '228', 'Type': 'question', 'Title': 'Proof that the sum of two Gaussian variables is another Gaussian', 'Tags': ['probability-distributions'], 'AcceptedAnswerId': '65871', 'urls': [], 'exp': [], 'Body': \"The sum of two Gaussian variables is another Gaussian. It seems natural, but I could not find a proof using Google. What's a short way to prove this? Thanks! Edit: Provided the two variables are independent. \"}\n",
      "229 {'Id': '229', 'Type': 'answer', 'ParentId': '196', 'urls': [], 'exp': [], 'Body': 'That picture confuses things by making it look as though the red line is being \"unwound\" from the circle like paper towel being unwound from a roll. Our brains pick up on that, since it is a real-world example. Both circles complete a single revolution, and both travel the same distance from left to right. If these really were rolls of paper towel, the smaller roll would have to spin faster (and therefore complete more than one full revolution) in order to lay out the same length of paper towel as the larger roll. Alternatively, if the two rolls were spinning at the same rate, the free end of the strip of towel left behind by the smaller roll would also move to the right. In short, the image is a kind of optical/mental illusion, and you\\'re not going crazy :) '}\n",
      "232 {'Id': '232', 'Type': 'answer', 'ParentId': '228', 'urls': ['http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables'], 'exp': [], 'Body': \"I don't know how I missed that one, indeed: http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables Thanks Kaestur Hakarl! \"}\n",
      "233 {'Id': '233', 'Type': 'answer', 'ParentId': '123', 'urls': [], 'exp': ['y = \\\\operatorname{arctanh}(\\\\sin(L))', '\\\\operatorname{arctanh}'], 'Body': 'On a map using the Mercator projection, the relationship between the latitude L of a point and its y coordinate on the map is given by <math_exp>, where <math_exp> is the inverse of the hyperbolic tangent function. '}\n",
      "234 {'Id': '234', 'Type': 'question', 'Title': 'Card doubling paradox', 'Tags': ['paradoxes', 'probability-theory'], 'AcceptedAnswerId': '510', 'urls': [], 'exp': ['x', '50\\\\%', '50\\\\%', '50\\\\%', '0.5x', '50\\\\%', '2x', 'x', '0.5(0.5x)+0.5(2x)=1.25x'], 'Body': 'Suppose there are two face down cards each with a positive real number and with one twice the other. Each card has value equal to its number. You are given one of the cards (with value <math_exp>) and after you have seen it, the dealer offers you an opportunity to swap without anyone having looked at the other card. If you choose to swap, your expected value should be the same, as you still have a <math_exp> chance of getting the higher card and <math_exp> of getting the lower card. However, the other card has a <math_exp> chance of being <math_exp> and a <math_exp> chance of being <math_exp>. If we keep the card, our expected value is <math_exp>, while if we swap it, then our expected value is: <math_exp> so it seems like it is better to swap. Can anyone explain this apparent contradiction? '}\n",
      "235 {'Id': '235', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': ['2 \\\\times 2', '         \\\\begin{vmatrix}         1 &amp; 0 \\\\\\\\         0 &amp; 1 \\\\\\\\         \\\\end{vmatrix} \\\\;', '\\\\;         \\\\begin{vmatrix}         0 &amp; -1 \\\\\\\\         1 &amp; 0 \\\\\\\\         \\\\end{vmatrix} ', '1', 'i'], 'Body': 'To the degree that anything actually \"exists\" in math, yes complex number exist. Once you accept that groups, rings and fields exist, and that isomorphism of rings makes sense, complex numbers can be recognized as (isomorphic to) the subring (which happens to be a field) of the ring of <math_exp> real matrices. Generators of this subring are the following matrices <math_exp> and <math_exp> which correspond to <math_exp> and <math_exp> in the normal notation of complex numbers. As people tend to accept that matrices exist, this may be a convincing argument. '}\n",
      "237 {'Id': '237', 'Type': 'question', 'Title': 'How do you determine if a point sits inside a polygon?', 'Tags': ['geometry', 'algorithms'], 'AcceptedAnswerId': '238', 'urls': [], 'exp': ['(x, y)', '(x_1, y_1), (x_2, y_2), \\\\ldots , (x_n,y_n)'], 'Body': 'Given the coordinates of a point <math_exp>, what is a procedure for determining if it lies within a polygon whose vertices are <math_exp>? '}\n",
      "238 {'Id': '238', 'Type': 'answer', 'ParentId': '237', 'urls': [], 'exp': ['(x_0, y_0)', '(x_1, y_1)', '(x_2, y_2)', 'x_1\\\\leq x_0\\\\leq x_2', 'x_2\\\\leq x_0\\\\leq x_1', 'y'], 'Body': \"I will assume the polygon has no intersections between the edges except at corners. Call the point <math_exp>. First we determine whether we are on a line - this is simple using substitution and range checking. For the range checking, suppose we have a segment <math_exp>, <math_exp>. We check that <math_exp> or <math_exp> and do the same for <math_exp>. Now, if we aren't on a line, we draw a ray from the point and count how many lines you intersect. If the line doesn't intersect a corner or contain a non-degenerate segment of an edge, then an odd number of intersections should mean you are inside and an even number mean outside. If we intersect with a corner, it is more difficult. Clearly, the ray could leave with it only intersecting a corner, but it could also pass through the corner without entering the polygon. One solution is to pick the ray so that it doesn't intersect a corner or go along a line - this should always be possible. We can also use cross product to determine whether we are crossing the lines at their corner or not (just check if the adjacent vertices are on the same side of the line or not). \"}\n",
      "239 {'Id': '239', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': \"When I was that age, I discovered Raymond Smullyan's classic logic puzzle books in the library (such as What is the name of this book?), and really got into it. I remember my amazement when I first understood how a complicated logic puzzle could become trivial, just symbolic manipulation really, with the right notation. \"}\n",
      "240 {'Id': '240', 'Type': 'question', 'Title': 'Are the \"proofs by contradiction\" weaker than other proofs?', 'Tags': ['logic', 'proof-theory'], 'AcceptedAnswerId': '1688', 'urls': [], 'exp': [], 'Body': 'I remember hearing several times the advice that, we should avoid using a proof by contradiction, if it is simple to convert to a direct proof or a proof by contrapositive.  Could you explain the reason? Do logicians think that proofs by contradiction are somewhat weaker than direct proofs? Is there any reason that one would still continue looking for a direct proof of some theorem, although a proof by contradiction has already been found? I don\\'t mean improvements in terms of elegance or exposition, I am asking about logical reasons.  For example, in the case of the \"axiom of choice\", there is obviously reason to look for a proof that does not use the axiom of choice.  Is there a similar case for proofs by contradiction? '}\n",
      "241 {'Id': '241', 'Type': 'answer', 'ParentId': '240', 'urls': [], 'exp': [], 'Body': 'Nearly always the direct proof is easier to understand, shorter, and more helpful! '}\n",
      "242 {'Id': '242', 'Type': 'answer', 'ParentId': '240', 'urls': ['http://math.columbia.edu/~nsnyder/thesismain.pdf'], 'exp': [], 'Body': 'Sometimes you might want to know not just that there exists something, you might want to know how to actually go about finding it (and related questions like how quickly you can find it).  Proofs by contradiction are non-constructive, while direct proofs are typically constructive in the sense that they actually construct an answer. For example, the proof that there are infinitely many primes usually proceeds by contradiction.  However, you can make it a direct proof which gives the stronger result that the nth prime is less than e^{e^n}.  (This is a good exercise to work out for yourself, but you can also find it as Prop 1.1.3 in my senior thesis and probably many other places as well.) '}\n",
      "243 {'Id': '243', 'Type': 'answer', 'ParentId': '240', 'urls': ['http://en.wikipedia.org/wiki/Constructivism_%28mathematics%29'], 'exp': [], 'Body': 'Most logicians consider proofs by contradiction to be equally valid, however some people are constructivists/intuitionists and don\\'t consider them valid. (Edit: This is not strictly true, as explained in comments. Only certain proofs by contradiction are problematic from the constructivist point of view, namely those that prove \"A\" by assuming \"not A\" and getting a contradiction.  In my experience, this is usually exactly the situation that people have in mind when saying \"proof by contradiction.\") One possible reason that the constructivist point of view makes a certain amount of sense is that statements like the continuum hypothesis are independent of the axioms, so it\\'s a bit weird to claim that it\\'s either true or false, in a certain sense it\\'s neither. Nonetheless constructivism is a relatively uncommon position among mathematicians/logicians.  However, it\\'s not considered totally nutty or beyond the pale.  Fortunately, in practice most proofs by contradiction can be translated into constructivist terms and actual constructivists are rather adept at doing so.  So the rest of us mostly don\\'t bother worrying about this issue, figuring it\\'s the constructivists problem. '}\n",
      "244 {'Id': '244', 'Type': 'answer', 'ParentId': '83', 'urls': ['http://www.ted.com/talks/lang/eng/dan_meyer_math_curriculum_makeover.html'], 'exp': [], 'Body': 'Speaking from my own experience with elementary mathematics, yours is a very hard question to answer because there is little in the elementary math curriculum worth getting excited about. Before students are going to get excited about math, the math curriculum has to be changed so that the process of doing mathematics is made to be engaging and thought provoking - not tedious. I think every teacher of elementary mathematics needs to watch this video about how math education can and should be reformed. '}\n",
      "245 {'Id': '245', 'Type': 'answer', 'ParentId': '240', 'urls': ['http://en.wikipedia.org/wiki/Constructivism_%28mathematics%29'], 'exp': [], 'Body': 'At first this seems like a silly question - after all isn\\'t the point of a mathematical proof to be a proof and hence to be beyond question. But of course, to prove anything we need assumptions and some people do disagree with the axioms commonly used by mathematicians. I don\\'t have much knowledge of this view, but I am sure they have theorems that show that contradiction like proofs are valid (given their axioms) within certain conditions. I would recommend going along with what everyone else does and treating \"proofs by contradiction\" as equally valid, unless you have investigated the Constructivism view and you decide that they are correct. As to whether they are clearer, that will depend on the actual proof. Sometimes the clearest way to make a proof is to start from the assumptions and see what they are really saying and why that is going to lead to a contradiction. The most illustrative proof depends on the circumstances. '}\n",
      "246 {'Id': '246', 'Type': 'answer', 'ParentId': '150', 'urls': ['http://en.wikipedia.org/wiki/Weierstrass_function#Density_of_nowhere-differentiable_functions'], 'exp': [], 'Body': 'Actually, in some sense, almost all of the continuous functions are nowhere differentiable: http://en.wikipedia.org/wiki/Weierstrass_function#Density_of_nowhere-differentiable_functions '}\n",
      "247 {'Id': '247', 'Type': 'answer', 'ParentId': '194', 'urls': ['http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes'], 'exp': [], 'Body': 'The Sieve of Eratosthenes is a very intuitive visual representation of why the frequency of prime numbers goes down as you go further out on the number line. '}\n",
      "248 {'Id': '248', 'Type': 'answer', 'ParentId': '206', 'urls': [], 'exp': [], 'Body': 'In the sciences (as opposed to in mathematics) people are often a bit vague about exactly what assumptions they are making about how \"well-behaved\" things are.  The reason for this is that ultimately these theories are made to be put to the test, so why bother worrying about exactly which properties you\\'re assuming when what you care about is functions coming up in real life which are probably going to satisfy all of your assumptions. This is particularly ubiquitous in physics where it is extremely common to make heuristic assumptions about well-behavior. Even in mathematics we do this sometimes.  When people say something is true for n sufficiently large, they often won\\'t bother writing down exactly how large is sufficiently large as long as it\\'s clear from context how to work it out.  Similarly, in an economics paper you could read through the argument and figure out exactly what assumptions they need, but it makes it easier to read to just say \"well-behaved.\" '}\n",
      "250 {'Id': '250', 'Type': 'question', 'Title': 'A challenge by R. P. Feynman: give counter-intuitive theorems that can be translated into everyday language', 'Tags': ['soft-question', 'big-list', 'examples-counterexamples'], 'urls': [], 'exp': [], 'Body': 'The following is a quote from Surely you\\'re joking, Mr. Feynman.  The question is: are there any interesting theorems that you think would be a good example to tell Richard Feynman, as an answer to his challenge?  Theorems should be totally counter-intuitive, and be easily translatable to everyday language.  (Apparently the Banach-Tarski paradox was not a good example.) Then I got an idea.  I challenged   them: \"I bet there isn\\'t a single   theorem that you can tell me - what   the assumptions are and what the   theorem is in terms I can understand -   where I can\\'t tell you right away   whether it\\'s true or false.\" It often went like this: They would   explain to me, \"You\\'ve got an orange,   OK? Now you cut the orange into a   finite number of pieces, put it back   together, and it\\'s as big as the sun.   True or false?\" \"No holes.\" \"Impossible! \"Ha! Everybody gather around! It\\'s   So-and-so\\'s theorem of immeasurable   measure!\" Just when they think they\\'ve got   me, I remind them, \"But you said an   orange! You can\\'t cut the orange peel   any thinner than the atoms.\" \"But we have the condition of   continuity: We can keep on cutting!\" \"No, you said an orange, so I   assumed that you meant a real orange.\" So I always won. If I guessed it   right, great. If I guessed it wrong,   there was always something I could   find in their simplification that they   left out. '}\n",
      "251 {'Id': '251', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': [], 'Body': \"Now that Wiles has done the job, I think that Fermat's Last Theorem may suffice. I find it a bit surprising still. \"}\n",
      "252 {'Id': '252', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel', 'http://demonstrations.wolfram.com/TheHilbertHotel/'], 'exp': [], 'Body': \"An infinite amount of coaches, each containing an infinite amount of people can be accommodated at Hilbert's Grand Hotel. Visual demonstration here. \"}\n",
      "253 {'Id': '253', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://en.wikipedia.org/wiki/Space_filling_curve'], 'exp': [], 'Body': 'All of three dimensional space can be filled up with an infinite curve. '}\n",
      "254 {'Id': '254', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://en.wikipedia.org/wiki/Ham_sandwich_theorem'], 'exp': [], 'Body': 'My first thought is the ham sandwich theorem--given a sandwich formed by two pieces of bread and one piece of ham (these pieces can be of any reasonable/well-behaved shape) in any positions you choose, it is possible to cut this \"sandwich\" exactly in half, that is divide each of the three objects exactly in half by volume, with a single \"cut\" (meaning a single plane). '}\n",
      "255 {'Id': '255', 'Type': 'question', 'Title': 'Why does the series <span class=\"math-container\" id=\"2618\">\\\\sum_{n=1}^\\\\infty\\\\frac1n</span> not converge?', 'Tags': ['calculus', 'sequences-and-series', 'harmonic-numbers'], 'AcceptedAnswerId': '259', 'urls': ['http://en.wikipedia.org/wiki/Harmonic_series_(mathematics)'], 'exp': ['\\\\sum_{n=1}^\\\\infty\\\\frac1n=\\\\frac 1 1 + \\\\frac 12 + \\\\frac 13 + \\\\cdots '], 'Body': \"Can someone give a simple explanation as to why the  <math_exp> doesn't converge, on the other hand it grows very slowly? I'd prefer an easily comprehensible explanation rather than a rigorous proof regularly found in undergraduate textbooks. \"}\n",
      "256 {'Id': '256', 'Type': 'answer', 'ParentId': '113', 'urls': ['http://en.wikipedia.org/wiki/Two_envelopes_problem', 'https://math.stackexchange.com/questions/234/card-doubling-paradox', 'https://mathoverflow.net/questions/9037'], 'exp': [], 'Body': 'The two envelopes problem is a good one. See also: Card doubling paradox and: https://mathoverflow.net/questions/9037 '}\n",
      "258 {'Id': '258', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://en.wikipedia.org/wiki/Monty_Hall_problem'], 'exp': [], 'Body': \"The Monty Hall problem fits the bill pretty well. Almost everyone, including most mathematicians, answered it wrong on their first try, and some took a lot of convincing before they agreed with the correct answer. It's also very easy to explain it to people. \"}\n",
      "259 {'Id': '259', 'Type': 'answer', 'ParentId': '255', 'urls': [], 'exp': ['1', '\\\\displaystyle\\\\frac11\\\\qquad', '1', '2', '\\\\displaystyle\\\\frac12+\\\\frac13\\\\qquad', '2', '3', '\\\\displaystyle\\\\frac14+\\\\frac15+\\\\frac16+\\\\frac17\\\\qquad', '4', '4', '\\\\displaystyle\\\\frac18+\\\\frac19+\\\\cdots+\\\\frac1{15}\\\\qquad', '8', '\\\\quad\\\\vdots', 'n', '2^{n-1}', 'n', '\\\\dfrac1{2^n}', '2', '\\\\dfrac1{2^2}', '2^{n-1} \\\\cdot \\\\dfrac1{2^n} =  \\\\dfrac1{2}', '\\\\dfrac1{2}'], 'Body': \"Let's group the terms as follows: Group <math_exp> : <math_exp>           (<math_exp> term) Group <math_exp> : <math_exp>(<math_exp> terms) Group <math_exp> : <math_exp>(<math_exp> terms) Group <math_exp> : <math_exp>   (<math_exp> terms) <math_exp> In general, group <math_exp> contains <math_exp> terms.  But also, notice that the smallest element in group <math_exp> is larger than <math_exp>.  For example all elements in group <math_exp> are larger than <math_exp>.  So the sum of the terms in each group is larger than <math_exp>.  Since there are infinitely many groups, and the sum in each group is larger than <math_exp>, it follows that the total sum is infinite. This proof is often attributed to Nicole Oresme. \"}\n",
      "260 {'Id': '260', 'Type': 'answer', 'ParentId': '250', 'urls': ['https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory'], 'exp': [], 'Body': \"There are true statements in arithmetic which are unprovable.  Even more remarkably there are explicit polynomial equations where it's unprovable whether or not they have integer solutions with ZFC! (We need ZFC + consistency of ZFC) \"}\n",
      "261 {'Id': '261', 'Type': 'answer', 'ParentId': '255', 'urls': [], 'exp': [' 1+\\\\frac{1}{2}+\\\\frac{1}{3}+\\\\dots+\\\\frac{1}{n}', '1', 'n', '\\\\frac{1}{x}', '1+\\\\frac{1}{2}+\\\\frac{1}{3}+\\\\dots+ \\\\frac{1}{n}', '\\\\ln(n)', '\\\\ln(x)', '\\\\frac{1}{x}'], 'Body': \"This is not as good an answer as AgCl's, nonetheless people may find it interesting. If you're used to calculus then you might notice that the sum <math_exp> is very close to the integral from <math_exp> to <math_exp> of <math_exp>.  This definite integral is ln(n), so you should expect <math_exp> to grow like <math_exp>. Although this argument can be made rigorous, it's still unsatisfying because it depends on the fact that the derivative of <math_exp> is <math_exp>, which is probably harder than the original question.  Nonetheless it does illustrate a good general heuristic for quickly determining how sums behave if you already know calculus. \"}\n",
      "262 {'Id': '262', 'Type': 'question', 'Title': 'What is the single most influential book every mathematician should read?', 'Tags': ['soft-question', 'big-list', 'reference-request'], 'urls': [], 'exp': [], 'Body': 'If you could go back in time and tell yourself to read a specific book at the beginning of your career as a mathematician, which book would it be? '}\n",
      "263 {'Id': '263', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'William Dunham\\'s \"Journey through Genius.\" Well, rather that is the book I read that made me want to be a mathematician. '}\n",
      "264 {'Id': '264', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'Following Noah\\'s lead I will mention; \"The Man Who Loved Only Numbers\" and \"How to Read and Do Proofs\" '}\n",
      "265 {'Id': '265', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'Polya\\'s \"How To Solve It\" '}\n",
      "266 {'Id': '266', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://www.webpages.uidaho.edu/~markn/squares/'], 'exp': [], 'Body': 'Every simple closed curve that you can draw by hand will pass through the corners of some square.  The question was asked by Toeplitz in 1911, and has only been partially answered in 1989 by Stromquist.  As of now, the answer is only known to be positive, for the curves that can be drawn by hand. (i.e. the curves that are piecewise the graph of a continuous function) I find the result beyond my intuition. <img src=\"https://i.stack.imgur.com/gl7Ge.gif\" alt=\"alt text\"> For details, see http://www.webpages.uidaho.edu/~markn/squares/  (the figure is also borrowed from this site) '}\n",
      "267 {'Id': '267', 'Type': 'answer', 'ParentId': '262', 'urls': ['http://omega.albany.edu:8008/JaynesBook.html', 'http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes'], 'exp': [], 'Body': 'Probability Theory: the Logic of Science. Or anything by Edwin T Jaynes. '}\n",
      "270 {'Id': '270', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': \"This is an extremely broad question, especially given the wide variety of mathy people here, but I'll bite. HSM Coxeter's Introduction to Geometry is a book that was very important to the development of my interest in mathematics and inclination towards its geometric aspects. \"}\n",
      "271 {'Id': '271', 'Type': 'question', 'Title': 'Is there a real number lookup algorithm or service?', 'Tags': ['math-software', 'experimental-mathematics'], 'AcceptedAnswerId': '273', 'urls': [], 'exp': ['1.644934', '\\\\displaystyle\\\\frac{\\\\pi^2}{6}', '\\\\displaystyle\\\\sum_{n=1}^\\\\infty \\\\frac{1}{n^2}', '\\\\displaystyle\\\\frac{\\\\pi^2}{6}', '\\\\displaystyle \\\\frac{\\\\pi^2}{6}'], 'Body': 'Is there a way of taking a number known to limited precision (e.g. <math_exp>) and finding out an \"interesting\" real number (e.g. <math_exp>) that\\'s close to it? I\\'m thinking of something like Sloane\\'s Online Encyclopedia of Integer Sequences, only for real numbers. The intended use would be: write a program to calculate an approximation to <math_exp>, look up the answer (\"looks close to <math_exp>\") and then use the likely answer to help find a proof that the sum really is <math_exp>. Does such a thing exist? '}\n",
      "272 {'Id': '272', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': \"Nicolas Bourbaki's Éléments de mathématique (specifically Topologie Générale and Algèbre). \"}\n",
      "273 {'Id': '273', 'Type': 'answer', 'ParentId': '271', 'urls': ['http://www.wolframalpha.com/input/?i=1.644934'], 'exp': [], 'Body': 'Try Wolfram Alpha. It actually does sequences as well. '}\n",
      "274 {'Id': '274', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://www.speedcrunch.org/en_US/index.html'], 'exp': [], 'Body': 'For desktop software, I use SpeedCrunch. Has a history, lots of mathematical functions, supports variable, etc. '}\n",
      "275 {'Id': '275', 'Type': 'question', 'Title': 'Best Maths Books for Non-Mathematicians', 'Tags': ['reference-request', 'soft-question', 'big-list', 'book-recommendation'], 'urls': ['http://rads.stackoverflow.com/amzn/click/014014739X'], 'exp': [], 'Body': 'I\\'m not a real Mathematician, just an enthusiast. I\\'m often in the situation where I want to learn some interesting Maths through a good book, but not through an actual Maths textbook. I\\'m also often trying to give people good Maths books to get them \"hooked\". So the question: What are good books, for laymen, which teach interesting Mathematics, but actually does it in a \"real\" way. For example, \"Fermat\\'s Last Enigma\" doesn\\'t count, since it doesn\\'t actually feature any Maths, just a story, and most textbook don\\'t count, since they don\\'t feature a story. My favorite example of this is \"Journey Through Genius\", which is a brilliant combination of interesting storytelling and large amounts of actual Mathematics. It took my love of Maths to a whole other level. Edit: A few more details on what I\\'m looking for. The audience of \"laymen\" should be anyone who has the ability (and desire) to understand actual mathematics, but does not want to learn from a textbook. Obviously I\\'m thinking about myself here, as a programmer who loves mathematics, I love being exposed to real maths, but I\\'m not going to get into it seriously. That\\'s why books that show actual maths, but give a lot more exposition (and much clearer explanations, especially of what the intuition should be) are great. When I say \"real maths\", I\\'m talking about actual proofs, formulas, or other mathematical theories. Specifically, I\\'m not talking about philosophy, nor am I talking about books which only talk about the history of maths (Simon Singh style), since they only talk about maths, they don\\'t actually show anything. William Dunham\\'s books and Paul J. Nahin\\'s books are good examples. '}\n",
      "276 {'Id': '276', 'Type': 'answer', 'ParentId': '275', 'urls': ['https://rads.stackoverflow.com/amzn/click/014014739X'], 'exp': [], 'Body': ' <img src=\"https://i.stack.imgur.com/nuTFm.jpg\" alt=\"cover\"> A brilliant combination of interesting storytelling and large amounts of actual Mathematics. It took my love of Maths to a whole other level. '}\n",
      "278 {'Id': '278', 'Type': 'answer', 'ParentId': '275', 'urls': ['http://rads.stackoverflow.com/amzn/click/0195105192'], 'exp': [], 'Body': 'I\\'ve been successful in using Courant and Robbins\\' What Is Mathematics? An Elementary Approach to Ideas and Methods for adults who have not had a math class for a few decades, but are open to the idea of learning more about mathematics. <img src=\"https://i.stack.imgur.com/slRn1.jpg\" alt=\"\"> Some sections are too advanced for someone with only high school mathematics, and many more will appear that way to the person at first, but do not actually rely on anything beyond high school mathematics. '}\n",
      "279 {'Id': '279', 'Type': 'answer', 'ParentId': '271', 'urls': ['http://oeis.org/', 'http://oeis.org/A000796'], 'exp': [], 'Body': \"Sometimes the decimal digits of numbers will appear in Sloane's On-Line Encyclopedia of Integer Sequences OIES. E.g. here is the decimal expansion of pi. \"}\n",
      "280 {'Id': '280', 'Type': 'answer', 'ParentId': '262', 'urls': ['http://en.wikipedia.org/wiki/Euclid%27s_Elements', 'http://en.wikipedia.org/wiki/Principia_mathematica'], 'exp': [], 'Body': \"Euclid's Elements Newton's Principia Mathematica Ideally in the original languages of Ancient Greek and Latin respectively! No, just kidding. But they are true classics that any accomplished mathematician should read at some point during their career. Not because they'll teach you something you don't already know, but they provide a unique insight into the mind of these giants. \"}\n",
      "281 {'Id': '281', 'Type': 'answer', 'ParentId': '6', 'urls': [], 'exp': [], 'Body': 'Some good options: '}\n",
      "283 {'Id': '283', 'Type': 'question', 'Title': 'Is <span class=\"math-container\" id=\"2780\">0</span> a natural number?', 'Tags': ['terminology', 'natural-numbers'], 'AcceptedAnswerId': '293', 'urls': [], 'exp': ['0'], 'Body': 'Is there a consensus in the mathematical community, or some accepted authority, to determine whether zero should be classified as a natural number? It seems as though formerly <math_exp> was considered in the set of natural numbers, but now it seems more common to see definitions saying that the natural numbers are precisely the positive integers. '}\n",
      "284 {'Id': '284', 'Type': 'answer', 'ParentId': '90', 'urls': ['http://www.purplemath.com/lessons.htm'], 'exp': [], 'Body': \" has a list for math lessons and tutoring. It's a list with various links and short reviews referring to tutoring and instructional resources. \"}\n",
      "286 {'Id': '286', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://opinionator.blogs.nytimes.com/category/steven-strogatz/'], 'exp': [], 'Body': \"It isn't quite a blog, but Steven Strogatz's 15 part series for the New York Times was excellent. \"}\n",
      "287 {'Id': '287', 'Type': 'question', 'Title': 'How come <span class=\"math-container\" id=\"2908\">32.5 = 31.5</span>? (The \"Missing Square\" puzzle.)', 'Tags': ['geometry', 'recreational-mathematics', 'fake-proofs'], 'AcceptedAnswerId': '290', 'urls': ['http://en.wikipedia.org/wiki/Missing_square_puzzle'], 'exp': ['32.5 = 31.5'], 'Body': 'Below is a visual proof (!) that <math_exp>. How could that be? <img src=\"https://farm1.static.flickr.com/48/152036443_ca28c8d2a1_o.png\" alt=\"alt text\"> (As noted in a comment and answer, this is known as the \"Missing Square\" puzzle.) '}\n",
      "289 {'Id': '289', 'Type': 'answer', 'ParentId': '287', 'urls': [], 'exp': [], 'Body': 'The red and blue triangles are not similar (the ratios of the sides are 3/8 = 0.375 and 2/5 = 0.4 respectively), so the \"hypotenuse\" of your big triangle is not a straight line. '}\n",
      "290 {'Id': '290', 'Type': 'answer', 'ParentId': '287', 'urls': [], 'exp': [], 'Body': \"It's an optical illusion - neither the first nor second set of blocks actually describes a triangle. The diagonal edge of the first is slightly concave and that of the second is slightly convex. To see clearly, look at the gradients of the hypotenuses of the red and blue triangles - they're not 'similar'. gradient of blue triangle hypotenuse = 2/5 gradient of red triangle hypotenuse = 3/8 Since these gradients are different, combining them in the ways shown in the diagram does not produce an overall straight (diagonal) line. \"}\n",
      "291 {'Id': '291', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://www.johndcook.com/blog/', 'http://blogs.mathworks.com/loren/', 'http://unimodular.net/blog/?p=185', 'http://cameroncounts.wordpress.com/', 'http://www.walkingrandomly.com/', 'http://topologicalmusings.wordpress.com/'], 'exp': [], 'Body': \"John D Cook writes The Endeavor One of the MathWorks blogs: Loren on the Art of Matlab ... a few more: eon Peter Cameron's Blog Walking Randomly Todd and Vishal's Blog (Check their blogrolls for more) \"}\n",
      "292 {'Id': '292', 'Type': 'answer', 'ParentId': '287', 'urls': [], 'exp': [], 'Body': \"No, this is not a proof of your statement. If you look very closely, you will see that the hypotenuse of the triangles aren't straight. You can also verify this algebraically by calculating the internal angles of the triangle using trigonometry. The other way to demonstrate the non-straightness of the hypotenuse is in the fact that the red and blue triangles aren't similar, which can be seen by the difference in the ratios of their width and height. For the blue triangle, this is 2/5 and for the red triangle this is 3/8. As they are both right angled triangles, these ratios would be the same for similar triangles and the fact that they aren't means the interior angles are different. \"}\n",
      "293 {'Id': '293', 'Type': 'answer', 'ParentId': '283', 'urls': ['http://en.wikipedia.org/wiki/Natural_number'], 'exp': ['\\\\{1, 2, 3, \\\\dots\\\\}', '\\\\{0, 1, 2,\\\\dots\\\\}'], 'Body': \"Simple answer: sometimes yes, sometimes no, it's usually stated (or implied by notation). From the Wikipedia article: In mathematics, there are two   conventions for the set of natural   numbers: it is either the set of   positive integers  <math_exp>   according to the traditional   definition; or the set of non-negative   integers <math_exp> according to a   definition first appearing in the   nineteenth century. Saying that, more often than not I've seen the natural numbers only representing the 'counting numbers' (i.e. excluding zero). This was the traditional historical definition, and makes more sense to me. Zero is in many ways the 'odd one out' - indeed, historically it was not discovered (described?) until some time after the natural numbers. \"}\n",
      "294 {'Id': '294', 'Type': 'answer', 'ParentId': '283', 'urls': [], 'exp': ['0', '0', '0'], 'Body': 'There are the two definitions, as you say. However the set of strictly positive numbers being the natural numbers is actually the older definition. Inclusion of <math_exp> in the natural numbers is a definition for them that first occurred in the 19th century. The Peano Axioms for natural numbers take <math_exp> to be one though, so if you are working with these axioms (and a lot of natural number theory does) then you take <math_exp> to be a natural number. '}\n",
      "296 {'Id': '296', 'Type': 'answer', 'ParentId': '283', 'urls': ['https://archive.org/details/arithmeticespri00peangoog'], 'exp': ['1', '0', '\\\\mathbb{N}^+', '1', '\\\\mathbb{N}', '0'], 'Body': 'There is no \"official rule\", it depends from what you want to do with natural numbers. Originally they started from <math_exp> because <math_exp> was not given the status of number. Nowadays if you see <math_exp> you may be assured we are talking about numbers from <math_exp> above; <math_exp> is usually for numbers from <math_exp> above. [EDIT: the original definitions of Peano axioms, as found in Arithmetices principia: nova methodo, may be found at https://archive.org/details/arithmeticespri00peangoog : look at it. <img src=\"https://i.stack.imgur.com/0rOia.png\" alt=\"\"> ] '}\n",
      "297 {'Id': '297', 'Type': 'question', 'Title': 'Simple numerical methods for calculating the digits of <span class=\"math-container\" id=\"2934\">\\\\pi</span>', 'Tags': ['approximation', 'numerical-methods', 'pi'], 'AcceptedAnswerId': '315', 'urls': [], 'exp': ['\\\\pi'], 'Body': 'Are there any simple methods for calculating the digits of <math_exp>? Computers are able to calculate billions of digits, so there must be an algorithm for computing them. Is there a simple algorithm that can be computed by hand in order to compute the first few digits? '}\n",
      "299 {'Id': '299', 'Type': 'answer', 'ParentId': '120', 'urls': [], 'exp': [], 'Body': \"actually 1 was considered a prime number until the beginning of 20th century. Unique factorization was a driving force beneath its changing of status, since it's formulation is quickier if 1 is not considered a prime; but I think that group theory was the other force. Indeed I prefer to describe numbers as primes, composites and unities, that is numbers whose inverse exists (so if we take the set of integer numbers Z, we have that 1 and -1 are unities and we still have unique factorization up to unities). We can always amend the defition of a prime number and say it is a number with exactly two divisors: in this way 1 is not a prime by definition :-) \"}\n",
      "300 {'Id': '300', 'Type': 'answer', 'ParentId': '198', 'urls': [], 'exp': [], 'Body': 'when dealing with infinite sets, many propositions cannot be proved mithout using non-constructive proofs. Axiom of Choice (AC, in brief) or an equivalent proposition is required. Russell\\'s paradox is not a problem per se, you just rule out certain collections of things as sets; Banach-Tarski paradox (you may take a ball, \"divide\" it in a finite number of parts, translate and rotate them and obtain two balls equal to the first) may be worse indeed. But few mathematicians would prefer not to do a lot of maths because AC is not allowed! '}\n",
      "301 {'Id': '301', 'Type': 'question', 'Title': 'Why are <span class=\"math-container\" id=\"2958\">\\\\Delta_1</span> sentences of arithmetic called recursive?', 'Tags': ['logic', 'computability', 'proof-theory'], 'AcceptedAnswerId': '312', 'urls': ['http://en.wikipedia.org/wiki/Prenex_normal_form'], 'exp': ['\\\\Pi_1', '\\\\Sigma_1', '\\\\Delta_1', '\\\\Pi_1', '\\\\Sigma_1.'], 'Body': 'The arithmetic hierarchy defines the <math_exp> formulae of arithmetic to be formulae that are provably equivalent to a formula in prenex normal form that only has universal quantifiers, and <math_exp> if it is provably equivalent to a prenex normal form with only existential quantifiers. A formula is <math_exp> if it is both <math_exp> and <math_exp>  These formulae are often called recursive: why? '}\n",
      "302 {'Id': '302', 'Type': 'answer', 'ParentId': '237', 'urls': ['http://en.wikipedia.org/wiki/Polygon_triangulation'], 'exp': [], 'Body': 'Representing a polygon by its edge path might not be the most useful, especially if you want to ask about inclusion for many points.  Consider triangulating the polygon, which is trivial for convex polygons, and not difficult to find O(n log(n)) for hairier cases.  Then determining whether the point is in the polygon reduces to whether it is in anyone of the triangles, which is easy. '}\n",
      "303 {'Id': '303', 'Type': 'question', 'Title': 'What are all the homomorphisms between the rings <span class=\"math-container\" id=\"3019\">\\\\mathbb{Z}_{18}</span> and <span class=\"math-container\" id=\"3020\">\\\\mathbb{Z}_{15}</span>?', 'Tags': ['ring-theory', 'abstract-algebra'], 'AcceptedAnswerId': '310', 'urls': [], 'exp': ['φ', '\\\\mathbb{Z}_{18}', '\\\\mathbb{Z}_{15}', 'φ(1)', '0 = φ(0) = φ(18) = φ(18 \\\\cdot 1) = 18 \\\\cdot φ(1) = 15 \\\\cdot φ(1) + 3 \\\\cdot φ(1) = 3 \\\\cdot φ(1)', 'φ(1)', '5', '10'], 'Body': 'Any homomorphism <math_exp> between the rings <math_exp> and <math_exp> is completely defined by <math_exp>. So from <math_exp> we get that <math_exp> is either <math_exp> or <math_exp>. But how can I prove or disprove that these two are valid homomorphisms? '}\n",
      "304 {'Id': '304', 'Type': 'answer', 'ParentId': '283', 'urls': [], 'exp': ['0', '0', '1', '0', '0'], 'Body': \"I remember all of my courses at University using only positive integers (not including <math_exp>) for the Natural Numbers. It's possible that they had come to an agreement amongst the Maths Faculty, but during at least two courses we generated the set of natural numbers in ways that wouldn't make sense if <math_exp> was included. One involved the cardinality of Sets of Sets, the other defined the natural numbers in terms of the number <math_exp> and addition only (<math_exp> and Negative Integers come into the picture later when you define an inverse to addition). As a result when teaching the difference between Integers and Natural Numbers I always define <math_exp> as an integer that isn't a Natural Number. \"}\n",
      "305 {'Id': '305', 'Type': 'answer', 'ParentId': '90', 'urls': ['http://www.themathpage.com/Arith/arithmetic.htm'], 'exp': [], 'Body': \"For basic mathematics mathpage has a long course, including the list below. Don't be put off by the elementary school style lesson names, it does go into some depth with each one. \"}\n",
      "306 {'Id': '306', 'Type': 'answer', 'ParentId': '196', 'urls': [], 'exp': [], 'Body': \"the smaller wheel does not just rotate, but also slides. If you had cogwheels instead of smooth wheels, you'd notice that movement is not possible. \"}\n",
      "307 {'Id': '307', 'Type': 'question', 'Title': 'Prove that <span class=\"math-container\" id=\"3061\">(n-1)! \\\\equiv -1 \\\\pmod{n}</span> iff <span class=\"math-container\" id=\"3062\">n</span> is prime [Wilson\\'s Theorem]', 'Tags': ['group-theory', 'elementary-number-theory', 'congruences'], 'AcceptedAnswerId': '308', 'urls': [], 'exp': ['(n-1)!', '-1 \\\\pmod{n}', 'n'], 'Body': 'How can I show that <math_exp> is congruent to <math_exp> if and only if <math_exp> is prime? Thanks. '}\n",
      "308 {'Id': '308', 'Type': 'answer', 'ParentId': '307', 'urls': ['https://en.wikipedia.org/wiki/Wilson%27s_theorem#Proofs'], 'exp': ['n\\\\text{ is prime if }(n-1)! \\\\equiv -1 \\\\pmod n', 'n', 'k|n', 'k\\\\lt n', 'k|(n-1)!', 'k \\\\equiv 1 \\\\pmod n', 'k', '1', 'n', '1', '(n-1)! \\\\equiv -1\\\\text{ if }n\\\\text{ is prime}', '1, 2, ... n-1', '\\\\mod n', '\\\\mod p', 'x', '0', 'y', 'xy \\\\equiv 1 \\\\mod n', 'n \\\\nmid x', 'n', 'xn', 'x', '0x', 'n', 'kn \\\\mod n', 'n', 'x, 2x,... nx', 'n', 'y', 'xy \\\\equiv 1 \\\\mod n', 'n'], 'Body': \"<math_exp> This direction is easy. If <math_exp> is composite, then there exists <math_exp> and <math_exp>. So <math_exp> and <math_exp>. This means <math_exp> needs to divide <math_exp>. So <math_exp> must be prime (or <math_exp>, but we can eliminate this by substitution). <math_exp> Wikipedia contains two proofs of this result known as Wilson's theorem. The first proof only uses basic abstract algebra and so should be understandable with a good knowledge of modular arithmetic. Just in case, I prove below that each element <math_exp> has a unique inverse <math_exp>. They use the fact that integers <math_exp> form a group and hence that each element <math_exp> not congruent <math_exp> has a multiplicative inverse (a number <math_exp> such that <math_exp>. We show this as follows. Suppose <math_exp>, for <math_exp> prime. From the uniqueness of prime factorisations, <math_exp> is the first product of <math_exp>, after <math_exp>, divisible by <math_exp> (use prime factorisation theorem). If we look at the series <math_exp>, this cycles and must have cycle length <math_exp>. Therefore, each element <math_exp> must be different modulo <math_exp>, including one, <math_exp>, with <math_exp>. Furthermore, due to the cycle length being <math_exp>, each only one of those elements will be an inverse. So every element has a unique inverse (although 1 and -1 are their own inverses). \"}\n",
      "309 {'Id': '309', 'Type': 'question', 'Title': 'If <span class=\"math-container\" id=\"3196\">A</span> is a subobject of <span class=\"math-container\" id=\"3197\">B</span>, and <span class=\"math-container\" id=\"3198\">B</span> a subobject of <span class=\"math-container\" id=\"3199\">A</span>, are they isomorphic?', 'Tags': ['category-theory'], 'AcceptedAnswerId': '327', 'urls': [], 'exp': ['X', 'Y', 'Y', 'X', 'A', 'B', 'B', 'A'], 'Body': 'In category theory, a subobject of <math_exp> is defined as an object <math_exp> with a monomorphism, from <math_exp> to <math_exp>. If <math_exp> is a subobject of <math_exp>, and <math_exp> a subobject of <math_exp>, are they isomorphic? It is not true in general that having monomorphisms going both ways between two objects is sufficient for isomorphy, so it would seem the answer is no. I ask because I\\'m working through the exercises in Geroch\\'s Mathematical Physics, and one of them asks you to prove that the relation \"is a subobject of\" is reflexive, transitive and antisymmetric. But it can\\'t be antisymmetric if I\\'m right... '}\n",
      "310 {'Id': '310', 'Type': 'answer', 'ParentId': '303', 'urls': [], 'exp': ['R, S', 'R~', 'S', 'x^2=x', '5', '\\\\Bbb Z_{15}', '1 \\\\to 5', '\\\\Bbb Z_{15}', 'T \\\\subset \\\\Bbb Z_{15}', '\\\\Bbb Z_{18} \\\\to T', '1', '10', 'T \\\\to \\\\Bbb Z_{15}'], 'Body': \"If one has a homomorphism of two rings <math_exp>, and <math_exp> has an identity, then the identity must be mapped to an idempotent element of <math_exp>, because the equation <math_exp> is preserved under homomorphisms.  Now <math_exp> is not an idempotent element in <math_exp>, so the map generated by <math_exp> is not a homomorphism. However, 10 is an idempotent element of <math_exp>. In particular, the subring <math_exp> generated by 10 has unit 10.  Since it is annihilated by 3, and consequently by 18, there is a unital homomorphism <math_exp> (i.e., mapping <math_exp> to <math_exp>).  So your second map is a legitimate homomorphism of rings (composing with the injection <math_exp>). Basically, the point of this answer is to check that one of your maps preserves the relations of the two rings, while the other doesn't. \"}\n",
      "311 {'Id': '311', 'Type': 'question', 'Title': 'Higher Order Logics', 'Tags': ['logic', 'higher-order-logic'], 'AcceptedAnswerId': '325', 'urls': ['http://en.wikipedia.org/wiki/Higher_order_logic'], 'exp': [], 'Body': 'I\\'ve read about about higher-order logics (i.e. those that build on first-order predicate logic) but am not too clear on their applications. While they are capable of expressing a greater range of proofs (though never all, by Godel\\'s Incompleteness theorem), they are often said to be less \"well-behaved\". Mathematicians generally seem to stay clear of such logics when possible, yet they are certainly necessary for prooving some more complicated concepts/theorems, as I understand. (For example, it seems the reals can only be constructed using at least 2nd order logic.) Why is this, what makes them less well-behaved or less useful with respect to logic/proof theory/other fields? '}\n",
      "312 {'Id': '312', 'Type': 'answer', 'ParentId': '301', 'urls': [], 'exp': ['\\\\phi(x_1, ... x_n)', '\\\\Sigma_1, \\\\Pi_1', 'x_1,...x_n', '\\\\phi(x_1, ... , x_n)', '\\\\phi', '\\\\Sigma_1', 'k', 'M', 'k'], 'Body': \"If a formula <math_exp> is in both <math_exp>, then one can define a Turing machine to determine whether it is true or false. Namely, in parallel, search for a collection of parameters that makes true the existential formula, and search for a collection of formulas that makes false the universal formula. If the first happens, return true; if the second happens, return false.  One of these must exist, so the Turing machine always halts. (The set of <math_exp> such that <math_exp> is valid if <math_exp> belongs to  <math_exp> is, by contrast, is only recursively enumerable.) By contrast, since the action of any Turing machine is simulable by existential formulas in first-order logic (i.e. there exists a number <math_exp> such that <math_exp> halts in <math_exp> steps), any language which is recursively enumerable can be expressed by existential formulas. Any language whose complement is recursively enumerable can similarly be expressed by universal formulas (by the analog of deMorgan's laws). So any recursive language (i.e., one which is both recursively enumerable and whose complement is r.e.), can be expressed in both ways. \"}\n",
      "313 {'Id': '313', 'Type': 'answer', 'ParentId': '311', 'urls': [], 'exp': ['F', 'P', 'S \\\\times S', 'P', 'P(x,y), P(z,y)', 'x=z', 'w', 'q', 'P(w,q)', 'P', 'x,y,z,w,q', 'T_n', 'x_0, \\\\dots x_n', 'n', 'F', 'T_n'], 'Body': 'Second-order logic does not satisfy the completeness and compactness theorems. Here is a proof that the compactness theorem fails (which itself implies that the completeness theorem fails, because completeness implies compactness).  Namely, there is a second-order way (call it <math_exp>) of expressing that a set is finite: every injective function on the set is surjective. This means that if <math_exp> is a relation on the product set <math_exp> such that <math_exp> satisfies the conditions to be a function, and <math_exp> imply <math_exp>, then for all <math_exp> there is <math_exp> with <math_exp>. This statement is quantified over <math_exp> as well as the variables <math_exp> so is second-order and clearly expresses finiteness. However, we also have the statement <math_exp> \"there exist distinct <math_exp> in the set,\" which is even first order, and states that the set has cardinality at least <math_exp>. So the conjunction of <math_exp> and any finite collection of the <math_exp> is satisfiable, but all of them together cannot be satisfied. '}\n",
      "314 {'Id': '314', 'Type': 'answer', 'ParentId': '271', 'urls': ['http://pi.lacim.uqam.ca/'], 'exp': [], 'Body': 'I\\'ve long used Simon Plouffe\\'s inverse symbolic calculator for this purpose.  It is essentially a searchable list of \"interesting\" numbers. '}\n",
      "315 {'Id': '315', 'Type': 'answer', 'ParentId': '297', 'urls': ['http://mathworld.wolfram.com/TrigonometricAdditionFormulas.html'], 'exp': ['\\\\tan^{-1}x = x - \\\\frac{x^3}{3} + \\\\frac{x^5}{5} - \\\\frac{x^7}{7} + \\\\cdots', '\\\\pi/4 = \\\\tan^{-1} 1', '(\\\\tan (a+b) = (\\\\tan a + \\\\tan b) / (1 - \\\\tan a \\\\tan b))', '\\\\pi/4', '\\\\pi/4 = 2 \\\\tan^{-1} 2/11 + 3 \\\\tan^{-1} 1/7', '\\\\pi = 8 \\\\tan^{-1} 2/11 + 12 \\\\tan^{-1} 1/7'], 'Body': 'One of the simplest is the series for arctangent: <math_exp> <math_exp>, but that converges slowly. You can use the addition formula for the tangent <math_exp> to break down <math_exp>  to the sum of two angles and repeat; this can be used to come up with values for the arctangent that are smaller than 1 and therefore converge faster. exercise for the reader: show that <math_exp> This then means that  <math_exp> Use the arctangent formula with only a few terms and you will have pi to quite a few decimal places. (3 terms yields 3.14160, 4 terms yields 3.1415924, 5 terms yields 3.1415926592) '}\n",
      "316 {'Id': '316', 'Type': 'question', 'Title': 'Why is <span class=\"math-container\" id=\"3266\">\\\\int\\\\limits_0^1 (1-x^7)^{1/5} - (1-x^5)^{1/7} dx=0</span>?', 'Tags': ['calculus'], 'AcceptedAnswerId': '345', 'urls': ['http://integrals.wolfram.com/index.jsp?expr=%281-x%5E7%29%5E%281%2F5%29+-+%281-x%5E5%29%5E%281%2F7%29&amp;random=false'], 'exp': ['\\\\int_{0}^{1} (1-x^7)^{1/5}-(1-x^5)^{1/7}\\\\ dx', '0'], 'Body': \"When I tried to approximate <math_exp> I kept getting answers that were really close to <math_exp>, so I think it might be true. But why? When I ask Mathematica, I get a bunch of symbols I don't understand! \"}\n",
      "319 {'Id': '319', 'Type': 'answer', 'ParentId': '8', 'urls': ['http://en.wikipedia.org/wiki/Fibonacci_number'], 'exp': ['\\\\sqrt{5}', '1+\\\\sqrt{5}', 'F(2n-1) = F(n)^2 + F(n-1)^2', 'F(2n) = (2F(n-1) + F(n)) \\\\times F(n)', 'F(k)', 'k'], 'Body': \"A lot of people have mentioned Binet's formula.  But I suspect this is not the most practical way to compute the nth Fibonacci number for large n, because it requires either having a very accurate value of <math_exp> and carrying around lots of decimal places (if you want to do floating-point arithmetic) or expanding large powers of <math_exp> using the binomial formula.  The latter comes out to writing the Fibonacci number as a sum of binomial coefficients. The following formulas hold, though: <math_exp> <math_exp> which you can find derivations of in the Wikipedia article on Fibonacci numbers.  This lets you find <math_exp>, for any <math_exp> even or odd, in terms of two Fibonacci numbers with approximately half the index.  The result is faster than Binet's formula. \"}\n",
      "320 {'Id': '320', 'Type': 'answer', 'ParentId': '307', 'urls': [], 'exp': ['(p-1)! \\\\equiv -1 (p)', 'p', 'p=2', 'n \\\\ne 0', 'n, 2n, ... (p-1)', 'n', 'p', 'hn \\\\equiv kn (p)', '(h-k)n \\\\equiv 0 (p)', 'p', 'n', 'p', 'n', 'm', 'mn \\\\equiv 1 (p)', 'x^2\\\\equiv 1 (p)', '(x+1)(x-1) \\\\equiv 0 (p)', 'x \\\\equiv 1 (p)', 'x \\\\equiv -1 (p)', 'n', 'm', 'm \\\\neq n', '2', 'p-2', '1 (p)', '1', 'p-1', '-1 (p)'], 'Body': \"[NOTE: it seems that there is some difference between preview and actual output, so instead if using (mod p) I stick with (p)] to show that <math_exp> without explicitly use group theory, maybe the simplest path is: (the following assumes <math_exp> is odd, but if <math_exp> then the result is immediate) given <math_exp>, all values <math_exp> <math_exp> are different mod <math_exp>. Otherwise, if <math_exp> then <math_exp> against the hypothesis that <math_exp> is prime. this means that each <math_exp> has an inverse mod <math_exp>, that is for each <math_exp> there is a <math_exp> such that <math_exp>. the equation <math_exp> may be written as <math_exp>; therefore its only solutions are <math_exp> and <math_exp>. For each other number <math_exp>, an inverse <math_exp> must exist (because of the pigeonhole principle) but <math_exp>. we are nearly done. Let's couple every number from <math_exp> to <math_exp> with its own inverse. Their product is <math_exp>, so they don't count in the overall total. <math_exp> does not count either; it remains just <math_exp>, that is <math_exp> as requested. \"}\n",
      "321 {'Id': '321', 'Type': 'answer', 'ParentId': '303', 'urls': ['http://en.wikipedia.org/wiki/Ring_homomorphism', 'http://en.wikipedia.org/wiki/Ring_homomorphism'], 'exp': [], 'Body': 'I will quote Wikipedia on the definition of a ring homomorphism. More precisely, if R and S are rings,   then a ring homomorphism is a function   f : R → S such that1 The last requirement is being relaxed. We can then sub in our f, in this case f(a1)=5a or f(a1)=10a and see if these relations hold. Given that these are small, finite and well understood groups, the problem is easy to solve from here. '}\n",
      "322 {'Id': '322', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'T.W.Körner, The Pleasures of Counting. It shows how mathematics is alive. '}\n",
      "323 {'Id': '323', 'Type': 'answer', 'ParentId': '275', 'urls': [], 'exp': [], 'Body': 'I think that a non-mathematician could appreciate T.W.Körner\\'s book The Pleasures of Counting; but I still believe that the collection of \"Mathematical Games\" columns from Martin Gardner are the very best thing. '}\n",
      "324 {'Id': '324', 'Type': 'question', 'Title': 'Unital homomorphism', 'Tags': ['abstract-algebra', 'terminology', 'definition'], 'AcceptedAnswerId': '330', 'urls': [], 'exp': [], 'Body': 'What is a unital homomorphism? Why are they important? '}\n",
      "325 {'Id': '325', 'Type': 'answer', 'ParentId': '311', 'urls': [], 'exp': [], 'Body': 'Unfortunately the term is ambiguous: there two kinds of semantics of higher-order languages, and only one is problematic.  Consider the language of second-order arithmetic, where there are quantifiers over both natural numbers and sets of natural numbers. First is what Quine called \"set-theory in sheep\\'s clothing\": this is where  quantification over sets of natural numbers is defined to be over all the sets of number that can be posited.  It\\'s the theory we use when we prove that there can be only one complete, totally ordered field.  It isn\\'t really a logic, there\\'s no complete notion of proof formalisation for it.  Wikipedia calls this \"standard semantics\"; I\\'m not sure if there is an authority for this. Then there\\'s Henkin semantics, which uses the rule analogous to the lambda-calculus to define a  semantics for second-order quantifiers.  This can be seen as still in the realm of first-order logic, in that sense that the second-order system can be translated into a first-order system conserving provability.  This is how second-order arithmetic is defined. All the theorems of the Henkin semantics will be \"theorems\" in the first. '}\n",
      "326 {'Id': '326', 'Type': 'question', 'Title': 'Calculating the probability of two dice getting at least a <span class=\"math-container\" id=\"3279\">1</span> or a <span class=\"math-container\" id=\"3280\">5</span>', 'Tags': ['probability-theory'], 'AcceptedAnswerId': '328', 'urls': [], 'exp': ['2', '1', '5'], 'Body': 'So you have <math_exp> dice and you want to get at least a <math_exp> or a <math_exp> (on the dice not added).  How do you go about calculating the answer for this question. This question comes from the game farkle. '}\n",
      "327 {'Id': '327', 'Type': 'answer', 'ParentId': '309', 'urls': [], 'exp': ['A', 'B', 'f:A\\\\to B', 'g:B\\\\to A', 'f', 'g', 'A', 'B'], 'Body': \"I don't think this can be true in general. What if we just take the category consisting of two objects <math_exp>, <math_exp> and take morphisms <math_exp>, <math_exp> with no relations between the morphisms, but forcing associativity. Then certainly <math_exp> and <math_exp> are monomorphisms but <math_exp> and <math_exp> are not isomorphic (since there are no relations between the morphisms). \"}\n",
      "328 {'Id': '328', 'Type': 'answer', 'ParentId': '326', 'urls': [], 'exp': ['(4/6)^2', '1-(2/3)^2=5/9'], 'Body': 'Go backwards: Calculate the probability that neither of them shows a 1 or 5. That means both show a 2, 3, 4, or 6. Thats <math_exp>. Hence the probability that at least one shows a 1 or 5 is <math_exp>. '}\n",
      "329 {'Id': '329', 'Type': 'question', 'Title': 'Books on Number Theory for Layman', 'Tags': ['number-theory', 'reference-request', 'book-recommendation'], 'AcceptedAnswerId': '483', 'urls': [], 'exp': [], 'Body': 'Books on Number Theory for anyone who loves Mathematics?(Beginner to Advanced &amp; just for someone who has a basic grasp of math) '}\n",
      "330 {'Id': '330', 'Type': 'answer', 'ParentId': '324', 'urls': [], 'exp': [], 'Body': 'A unital homomorphism between rings R and S is a ring homomorphism that sends the identity element of R to the identity element of S. Homomorphisms (between objects in any algebraic category like groups, rings, vector spaces, etc.) preserve the algebraic structure, and if you want a map between rings with an identity element, it is natural to require this to preserve this element (since it satisfies unique properties). '}\n",
      "331 {'Id': '331', 'Type': 'answer', 'ParentId': '329', 'urls': [], 'exp': [], 'Body': 'I would still stick with Hardy and Wright, even if it is quite old. '}\n",
      "332 {'Id': '332', 'Type': 'answer', 'ParentId': '324', 'urls': [], 'exp': ['\\\\operatorname{Spec S} \\\\to \\\\operatorname{Spec} R', 'R \\\\to S'], 'Body': \"A lot of results about rings just won't work otherwise: for instance, a unital homomorphism of rings sends units to units.  A nonunital homomorphism doesn't have to do that.  Nonunital homomorphisms can be very degenerate, e.g. the zero homomorphism. Another reason you want homomorphisms to preserve the unit is that this is how you get a map <math_exp> from a ring-homomorphism <math_exp>. \"}\n",
      "333 {'Id': '333', 'Type': 'answer', 'ParentId': '326', 'urls': [], 'exp': [], 'Body': 'To visually see the answer given by balpha above, you could write out the entire set of dice rolls Total number of possible dice rolls: 36 Dice rolls that contain 1 or a 5: 20 20/36 = 5/9 '}\n",
      "334 {'Id': '334', 'Type': 'answer', 'ParentId': '275', 'urls': [], 'exp': [], 'Body': 'I think any book by John Allen Paulos would be something any Math enthusiast could enjoy and learn from. '}\n",
      "335 {'Id': '335', 'Type': 'answer', 'ParentId': '326', 'urls': ['http://img.skitch.com/20100721-xwruwx7qnntx1pjmkjq8gxpifs.gif'], 'exp': [], 'Body': 'The other way to visualise this would be to draw a probability tree like so: alt text http://img.skitch.com/20100721-xwruwx7qnntx1pjmkjq8gxpifs.gif (apologies for my poor standard of drawing :) ) '}\n",
      "336 {'Id': '336', 'Type': 'question', 'Title': 'Why are <span class=\"math-container\" id=\"3288\">3D</span> transformation matrices <span class=\"math-container\" id=\"3289\">4 \\\\times 4</span> instead of <span class=\"math-container\" id=\"3290\">3 \\\\times 3</span>?', 'Tags': ['linear-algebra', 'matrices', 'geometry'], 'AcceptedAnswerId': '338', 'urls': [], 'exp': ['3D', '4\\\\times 4', 'x', 'y', 'z', '1', '4\\\\times 4', '3\\\\times 3', '3\\\\times 3'], 'Body': 'Background: Many (if not all) of the transformation matrices used in <math_exp> computer graphics are <math_exp>, including the three values for <math_exp>, <math_exp> and <math_exp>, plus an additional term which usually has a value of <math_exp>. Given the extra computing effort required to multiply <math_exp> matrices instead of <math_exp> matrices, there must be a substantial benefit to including that extra fourth term, even though <math_exp> matrices should (?) be sufficient to describe points and transformations in 3D space. Question: Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know why that is the case. '}\n",
      "337 {'Id': '337', 'Type': 'answer', 'ParentId': '329', 'urls': [], 'exp': [], 'Body': 'A concise introduction to the theory of numbers by Alan Baker (1970 Fields medalist) covers a lot of ground in less than 100 pages, and does so in a fluid way that never feels rushed. I love this little book. '}\n",
      "338 {'Id': '338', 'Type': 'answer', 'ParentId': '336', 'urls': [], 'exp': [], 'Body': \"Even though 3x3 matrices should (?) be sufficient to describe points and transformations in 3D space. No, they aren't enough! Suppose you represent points in space using 3D vectors. You can transform these using 3x3 matrices. But if you examine the definition of matrix multiplication you should see immediately that multiplying a zero 3D vector by a 3x3 matrix gives you another zero vector. So simply multiplying by a 3x3 matrix can never move the origin. But translations and rotations do need to move the origin. So 3x3 matrices are not enough. I haven't tried to explain exactly how 4x4 matrices are used. But I hope I've convinced you that 3x3 matrices aren't up to the task and that something more is needed. \"}\n",
      "339 {'Id': '339', 'Type': 'answer', 'ParentId': '275', 'urls': [], 'exp': [], 'Body': 'John Derbyshire\\'s Prime Obsession is about Riemann\\'s hypothesis. One of the stated goals of the author is to explain what \"all non-trivial zeros of the zeta function have real part one-half\" means to readers who have no background in calculus. Odd-numbered chapters tell the story of how Riemann came to his hypothesis, and even-numbered chapters are more mathematical in nature. '}\n",
      "340 {'Id': '340', 'Type': 'answer', 'ParentId': '336', 'urls': [], 'exp': [], 'Body': \"To follow up user80's answer, you want to get transformations of the form v --> Av + b, where A is a 3 by 3 matrix (the linear part of transformation) and b is a 3-vector.  We can encode this transformation in a 4 x 4 matrix by putting A in the top left with three 0's below it and making the last column be (b,1).  Multiplying the 4-vector (v,1) with this matrix will give you (Av + b, 1). \"}\n",
      "342 {'Id': '342', 'Type': 'answer', 'ParentId': '329', 'urls': ['https://books.google.com/books?id=AeiRlwEACAAJ'], 'exp': [], 'Body': 'I like Niven and Zuckerman, Introduction to the Theory of Numbers. '}\n",
      "343 {'Id': '343', 'Type': 'answer', 'ParentId': '336', 'urls': ['https://stackoverflow.com/questions/2465116/understanding-opengl-matrices/2465290#2465290', 'http://en.wikipedia.org/wiki/Homogeneous_coordinates#Use_in_computer_graphics'], 'exp': [], 'Body': 'I\\'m going to copy my answer from Stack Overflow, which also shows why 4-component vectors (and hence 4&times;4 matrices) are used instead of 3-component ones. In most 3D graphics a point is represented by a 4-component vector (x, y, z, w), where w = 1. Usual operations applied on a point include translation, scaling, rotation, reflection, skewing and combination of these. These transformations can be represented by a mathematical object called \"matrix\". A matrix applies on a vector like this: For example, scaling is represented as and translation as One of the reason for the 4th component is to make a translation representable by a matrix. The advantage of using a matrix is that multiple transformations can be combined into one via matrix multiplication. Now, if the purpose is simply to bring translation on the table, then I\\'d say (x, y, z, 1) instead of (x, y, z, w) and make the last row of the matrix always [0 0 0 1], as done usually for 2D graphics. In fact, the 4-component vector will be mapped back to the normal 3-vector vector via this formula: This is called homogeneous coordinates. Allowing this makes the perspective projection expressible with a matrix too, which can again combine with all other transformations. For example, since objects farther away should be smaller on screen, we transform the 3D coordinates into 2D using formula Now if we apply the projection matrix then the real 3D coordinates would become so we just need to chop the z-coordinate out to project to 2D. '}\n",
      "344 {'Id': '344', 'Type': 'answer', 'ParentId': '329', 'urls': [], 'exp': [], 'Body': 'Serre\\'s \"A course in Arithmetic\" is pretty phenomenal. '}\n",
      "345 {'Id': '345', 'Type': 'answer', 'ParentId': '316', 'urls': [], 'exp': [' y = \\\\left(1 - x^7\\\\right)^{1/5} ', ' \\\\left(1 - y^5\\\\right)^{1/7} = x ', '(1-x^7)^{1/5}', '(1-x^5)^{1/7}', ' \\\\int_0^1 \\\\left(1-x^7\\\\right)^{1/5} dx = \\\\int_0^1 \\\\left(1-y^5\\\\right)^{1/7} dy '], 'Body': 'Note that if <math_exp> then <math_exp> This means <math_exp> is the inverse function of <math_exp>. In the graph, one will be the same as the other when reflected along the diagonal line y = x. Also, both functions Therefore, the area under the graph in [0, 1] will be the same for both functions: <math_exp> Grouping the two integrals yield the equation in the title. '}\n",
      "346 {'Id': '346', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://news.bbc.co.uk/2/hi/programmes/more_or_less/8735812.stm', 'http://arxiv.org/pdf/1102.0173v1.pdf'], 'exp': [], 'Body': 'Similar to the Monty Hall problem, but trickier: at the latest Gathering 4 Gardner, Gary Foshee asked I have two children. One is a boy born on a Tuesday. What is the probability I have two boys? We are assuming that births are equally distributed during the week, that every child is a boy or girl with probability 1/2, and that there is no dependence relation between sex and day of birth. His Answer: 13/27. This was in the news a lot recently, see for instance BBC News.  (Later analysis showed the answer depends on why the parent said that.) '}\n",
      "347 {'Id': '347', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://en.wikipedia.org/wiki/Brouwer_fixed_point_theorem'], 'exp': [], 'Body': \"You have two identical pieces of paper with the same picture printed on them. You put one flat on a table and the other one you crumple up (without tearing it) and place it on top of the first one. Brouwer's fixed point theorem states that there is some point in the picture on the crumpled-up page that is directly above the same point on the bottom page. It doesn't matter how you place the pages, or how you deform the top one. \"}\n",
      "348 {'Id': '348', 'Type': 'answer', 'ParentId': '57', 'urls': [], 'exp': [], 'Body': 'It\\'s not clear what the question is asking. Is it asking for logical relations among the properties? e.g. a relation that is transitive and symmetric MUST be reflexive (unless the relation holds between no two objects). Or is it asking for mathematical relations that have these properties? \">\" is transitive, asymmetric and total or complete (for all distinct x,y either xRy or yRx). It is a strict total order. \"greater or equal\" is transitive, complete, symmetric, reflexive. This is a weak total order. \"x is a subset of y\" is reflexive and antisymmetric but not necessarily transitive, symmetric (although it can be either of those things if you restrict yourself to the right sort of sets). \"x is a proper subset of y\" is asymmetric and irreflexive (for all x, it is not the case that xRx), and can be transitive if you restrict yourself to the right sort of sets. Note that in both these cases, one can define one of each pair of relations in terms of the other (plus a notion of identity or non-identity). In logic the notion of \"x entails y\" is transitive, reflexive. The relation \"x has the same integer part as y\" over the real numbers is transitive, reflexive, symmetric. This is called an equivalence relation. '}\n",
      "349 {'Id': '349', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://math-blog.com/', 'http://unapologetic.wordpress.com/2010/07/20/upper-and-lower-ordinate-sets/'], 'exp': [], 'Body': \"Math-blog is one I have in my google reader and I just found this one through google reader but it's a little over my head:The Unapologetic Mathematician \"}\n",
      "350 {'Id': '350', 'Type': 'question', 'Title': 'Where can I find a review of discrete math', 'Tags': ['soft-question', 'reference-request', 'education', 'discrete-mathematics'], 'AcceptedAnswerId': '354', 'urls': [], 'exp': [], 'Body': \"I'm looking for course notes and assignments  and hopefully some example exams for Discrete Math, I'm taking a placement exam in the subject after having taken it 4 years ago. \"}\n",
      "351 {'Id': '351', 'Type': 'answer', 'ParentId': '135', 'urls': [], 'exp': ['a', 'b', 'a^b', 'b', 'a', '2^3', '1', '000, 001, 010, \\\\ldots, 111.', '0^0=1'], 'Body': \"If <math_exp> and <math_exp> are natural numbers, then <math_exp> is the number of ways you can make a sequence of length <math_exp> where each element in the sequence is chosen from a set of size <math_exp>. You're allowed replacements. For example <math_exp> is the number of 3 digit sequences where each digit is zero or <math_exp>: <math_exp> There is precisely one way to make a zero length sequence: one. So you'd expect <math_exp>. \"}\n",
      "352 {'Id': '352', 'Type': 'answer', 'ParentId': '57', 'urls': [], 'exp': [], 'Body': 'I think you\\'re thinking about this in the wrong way. Properties don\\'t \"work\", properties are things that are true for the given relation. For instance, you say that the a relation has the reflexive property if it satisfies the condition that all elements are related to themselves. Similarly, it has the symmetric property if for all a and b, if a is related to b then b is related to a. You could simply say that \"∀a : a R a\" each time, but because this is something that happens often, this particular property has been given a name, i.e. reflexivity. Certain types of specific relations are also given names. For instance, a partial order is reflexive, antisymmetric and transitive, and equivalence relations are reflexive, symmetric and transitive. Again, these are just names that aren\\'t strictly needed, but they make it more convenient talking about these kinds of things. '}\n",
      "353 {'Id': '353', 'Type': 'answer', 'ParentId': '297', 'urls': ['http://en.wikipedia.org/wiki/Bailey-Borwein-Plouffe_formula', 'http://en.literateprograms.org/Pi_with_the_BBP_formula_%28Python%29'], 'exp': [], 'Body': 'Some of the easiest algorithms to use are the spigot algorithms. They allow you to jump rapidly to the nth digit without computing the n-1 digits before. The catch is that they only really work in binary. Here are some implementations in Python. You can see how short they are. '}\n",
      "354 {'Id': '354', 'Type': 'answer', 'ParentId': '350', 'urls': ['http://www.cs.sunysb.edu/~cse547/'], 'exp': [], 'Body': 'This is the discrete math course one in my school. It contain lecture notes, homework and previous exams. http://www.cs.sunysb.edu/~cse547/ '}\n",
      "355 {'Id': '355', 'Type': 'answer', 'ParentId': '275', 'urls': ['http://rads.stackoverflow.com/amzn/click/1589880331', 'http://rads.stackoverflow.com/amzn/click/1589880366', 'http://rads.stackoverflow.com/amzn/click/1589880331', 'http://rads.stackoverflow.com/amzn/click/0465026567', 'http://rads.stackoverflow.com/amzn/click/0060935588'], 'exp': [], 'Body': \"One's that were suggested to me by my Calculus teacher in High School. Even my wife liked them and she hates math now: The Education of T.C. Mits: What modern mathematics means to you Infinity: Beyond the Beyond the Beyond Written and illustrated(Pictures are great ;p) by a couple: Lillian R. Lieber, and Hugh Gray Lieber. These books were hard to find before because they went out of print but I have this new version and like it a lot. The books explains profound topics in a way that is graspable by anyone without being dumbed down. Godel's proof is one I enjoyed. It's was a little hard to understand but there is nothing in this book that makes it inaccessible to someone without a strong math background. Keeping with Godel in the title, Godel, Escher, Bach: An Eternal Golden Braid while not just about math was a good read (a bit long ;p). The Music of the Primes: Searching to Solve the Greatest Mystery in Mathematics  It describes the Riemann Hypothesis and people who were involved with it somehow. My favorite part was learning about the people who attempted to solve it. Many I never heard off before this book. (Side not: I'll have to read pguertin suggestion, sounds in like a similar but more profound book). \"}\n",
      "356 {'Id': '356', 'Type': 'question', 'Title': 'Best Intermediate/Advanced Computer Science book', 'Tags': ['soft-question', 'big-list', 'reference-request'], 'urls': [], 'exp': [], 'Body': 'I\\'m very interested in Computer Science (computational complexity, etc.). I\\'ve already finished a University course in the subject (using Sipser\\'s \"Introduction to the Theory of Computation\"). I know the basics, i.e. Turing Machines, Computability (Halting problem and related reductions), Complexity classes (time and space, P/NP, L/NL, a little about BPP). Now, I\\'m looking for a good book to learn about some more advanced concepts. Any ideas? '}\n",
      "357 {'Id': '357', 'Type': 'answer', 'ParentId': '329', 'urls': [], 'exp': [], 'Body': 'Stewart&amp;Tall\\'s \"Algebraic Number Theory\" is great. '}\n",
      "358 {'Id': '358', 'Type': 'answer', 'ParentId': '275', 'urls': ['http://math.boisestate.edu/~holmes/holmes/A%20Mathematician%27s%20Apology.pdf'], 'exp': [], 'Body': \"Possibly this may not really qualify as presenting much interesting maths, but I think Hardy's A Mathematician's Apology should be on the must-read list. \"}\n",
      "359 {'Id': '359', 'Type': 'answer', 'ParentId': '356', 'urls': [], 'exp': [], 'Body': \"Papadimitriou's Computational Complexity covers complexity theory at a higher level than Sipser, but has essentially no prerequisites. \"}\n",
      "360 {'Id': '360', 'Type': 'answer', 'ParentId': '262', 'urls': ['http://en.wikipedia.org/wiki/A_Mathematician%27s_Apology'], 'exp': [], 'Body': 'A Mathematician\\'s Apology by G H Hardy. I did in fact read this in high school, and it raised my view of mathematics from a thing of utility to a thing of beauty and wonder. It inspired me to go on to study mathematics at Cambridge myself. It\\'s a pity that the \"introduction\" by C P Snow is longer than the original and contains a rather depressing view of Hardy\\'s later life. I would recommend readers to skip the introduction altogether and concentrate on Hardy\\'s own words. '}\n",
      "362 {'Id': '362', 'Type': 'question', 'Title': 'History of the Concept of a Ring', 'Tags': ['math-history'], 'AcceptedAnswerId': '915', 'urls': [], 'exp': ['\\\\mathbb{Z}'], 'Body': \"I am vaguely familiar with the broad strokes of the development of group theory, first when ideas of geometric symmetries were studied in concrete settings without the abstract notion of a group available, and later as it was formalized by Cayley, Lagrange, etc (and later, infinite groups being well-developed). In any case, it's intuitively easy for me to imagine that there was substantial lay, scientific, and artistic interest in several of the concepts well-encoded by a theory of groups. I know a few of the corresponding names for who developed the abstract formulation of rings initially (Wedderburn etc.), but I'm less aware of the ideas and problems that might have given rise to interest in ring structures. Of course, now they're terribly useful in lots of math, and <math_exp> is a natural model for elementary properties of commutative rings, and I'll wager number theorists had an interest in developing the concept. And if I wanted noncommutative models, matrices are a good place to start looking. But I'm not even familiar with what the state of knowledge and formalization of things like matrices/linear operators was at the time rings were developed, so maybe these aren't actually good examples for how rings might have been motivated. Can anyone outline or point me to some basics on the history of the development of basic algebraic structures besides groups? \"}\n",
      "363 {'Id': '363', 'Type': 'answer', 'ParentId': '362', 'urls': ['https://mathoverflow.net/questions/26613/papers-that-debunk-common-myths-in-the-history-of-mathematics/30272#30272', 'https://mathoverflow.net/questions/34806/what-was-the-relative-importance-of-flt-vs-higher-reciprocity-laws-in-kummers-i', 'http://www-gap.dcs.st-and.ac.uk/~history/HistTopics/Ring_theory.html'], 'exp': ['p', 'x^p + y^p = z^p', '\\\\prod (x+\\\\zeta_p^iy) = z^p', '\\\\zeta_p', 'p', 'Z[\\\\zeta_p]', 'p&gt;3', 'Z[\\\\zeta_p]', 'p ', 'xyz', 'Z[\\\\zeta_p]', 'p', 'Z[\\\\zeta_p]'], 'Body': 'Edit: Bill Dubuque has pointed out that much of this answer (specifically, the part about FLT) is essentially a mathematical urban legend, albeit a pervasive one. I cannot delete an accepted answer, so here is a link to an answer of his on MO explaining it. Here is also a link to a related question. There\\'s some of the history here in Bourbaki\\'s Commutative Algebra, in the appendix.  Basically, a fair bit of ring theory was developed for algebraic number theory.  This in turn was because people were trying to prove Fermat\\'s last theorem. Why\\'s this? Let <math_exp> be a prime. Then the equation <math_exp> can be written as <math_exp> for <math_exp> a primitive <math_exp>th root of unity.  All these quantities are elements of the ring <math_exp>.  So if <math_exp> and there is unique factorization in the ring <math_exp>, it isn\\'t terribly hard to show that this is impossible at least in the basic case where <math_exp> does not divide <math_exp> (and can be found, for instance, in Borevich-Shafarevich\\'s book on number theory). Lame actually thought he had a proof of FLT via this argument. But he was wrong: these rings generally don\\'t admit unique factorization.  So, it became a problem to study these \"generalized integers\" <math_exp>, which of course are basic examples of rings.  It wasn\\'t until Dedekind that the right notion of unique factorization -- namely, factorization of ideals -- was found. In fact, the case of FLT I just mentioned generalizes to the case where <math_exp> does not divide the class number of <math_exp> (the class number is the invariant that measures how far it is from being a UFD).  And, according to this article, Dedekind was the first to define a ring. The article I linked to, incidentally, has a fair bit of additional interesting history. '}\n",
      "364 {'Id': '364', 'Type': 'answer', 'ParentId': '275', 'urls': ['http://www.amazon.com/Paul-J.-Nahin/e/B001HCS1XI/ref=sr_ntt_srch_lnk_1?_encoding=UTF8&amp;qid=1279734093&amp;sr=8-1', 'http://www.amazon.com/Ian-Stewart/e/B000APQ9NM/ref=ntt_athr_dp_pel_1'], 'exp': [], 'Body': 'Paul Nahin has a number of accessible mathematics books written for non-mathematicians, the most famous being Professor Ian Stewart also has many books which each give laymen overviews of various fields or surprising mathematical results '}\n",
      "365 {'Id': '365', 'Type': 'question', 'Title': 'Tiling a <span class=\"math-container\" id=\"3319\">3 \\\\times 2n</span> rectangle with dominoes', 'Tags': ['combinatorics', 'tiling'], 'urls': ['http://oeis.org/A001835'], 'exp': ['3 \\\\times 2n', 'f(n)', 'g(n)', '3 \\\\times 2n', '3 \\\\times 2n+1', '2 \\\\times n', '3 \\\\times 2n'], 'Body': \"I'm looking to find out if there's any easy way to calculate the number of ways to tile a <math_exp> rectangle with dominoes. I was able to do it with the two codependent recurrences where <math_exp> is the actual answer and <math_exp> is a helper function that represents the number of ways to tile a <math_exp> rectangle with two extra squares on the end (the same as a <math_exp> rectangle missing one square). By combining these and doing some algebra, I was able to reduce this to which shows up as sequence A001835, confirming that this is the correct recurrence. The number of ways to tile a <math_exp> rectangle is the Fibonacci numbers because every rectangle ends with either a verticle domino or two horizontal ones, which gives the exact recurrence that Fibonacci numbers do. My question is, is there a similar simple explanation for this recurrence for tiling a <math_exp> rectangle? \"}\n",
      "366 {'Id': '366', 'Type': 'answer', 'ParentId': '350', 'urls': ['http://en.wikipedia.org/wiki/Discrete_mathematics#Further_reading'], 'exp': [], 'Body': 'When wanting to know about a particular mathematics subject, I often find that starting with the \"further reading\" section of the relevant wikipedia page is a good way in. '}\n",
      "367 {'Id': '367', 'Type': 'answer', 'ParentId': '365', 'urls': [], 'exp': ['2 \\\\times n', 'f(n) + f(n-2) = 4f(n-1)', 'n', '(n-2)', '(n-1)', '3 \\\\times 2n', 'n', '(n-2)', '(n-1)', '(n-1)', '1, 2, 3,', '4', '(n-1)', 'n', '1', 'n', '(n-2)', '(n-1)', 'n', '2g(n-2)', '2g(n-2) + f(n-2) = f(n-1)', '2g(n-1) + f(n-1) = f(n)'], 'Body': 'Here\\'s my best shot at the sort of explanation you\\'re asking for, although it\\'s not nearly as clear as the <math_exp> case.  The negative sign makes combinatorial proofs difficult, so let\\'s rearrange this as: <math_exp> Then you want to show that the number of <math_exp>-tilings, plus the number of <math_exp>-tilings, is four times the number of <math_exp>-tilings. (An \"n-tiling\" is a tiling of a <math_exp> rectangle by dominoes.) In bijective terms, then, we want a bijection between the set of <math_exp>-tilings and <math_exp>-tilings and the set of <math_exp>-tilings, where the <math_exp>-tilings are each tagged with the number <math_exp> or <math_exp>. Given an <math_exp>-tiling, there are three \"obvious\" ways to obtain an <math_exp>-tiling from it, namely by adding one of the three <math_exp>-tilings on the right end.  These generate tilings which have a vertical line passing all the way through,  two units from the right end; call these \"faulted\" tilings, and those which don\\'t have a vertical line in that position \"faultless\". So it suffices to show that the number of faultless <math_exp>-tilings, plus the number of <math_exp>-tilings, is the number of <math_exp>-tilings.  It\\'s easy to see that the number of faultless <math_exp>-tilings is <math_exp>; a faultless tilings must have a horizontal domino covering the second and third squares from the right in some row, and this assumption forces the placement of some other dominoes.  So we need <math_exp>.  Shifting the indices, we need <math_exp>, which you have already said is true. '}\n",
      "368 {'Id': '368', 'Type': 'question', 'Title': 'Is there a closed-form equation for <span class=\"math-container\" id=\"3360\">n!</span>? If not, why not?', 'Tags': ['combinatorics', 'algorithms', 'factorial', 'closed-form'], 'AcceptedAnswerId': '374', 'urls': [], 'exp': ['n!'], 'Body': \"I know that the Fibonacci sequence can be described via the Binet's formula. However, I was wondering if there was a similar formula for <math_exp>. Is this possible?  If not, why not? \"}\n",
      "369 {'Id': '369', 'Type': 'answer', 'ParentId': '368', 'urls': ['http://en.wikipedia.org/wiki/Stirling%27s_approximation'], 'exp': ['n!\\\\sim\\\\sqrt{2\\\\pi n} \\\\left(\\\\frac{n}{e}\\\\right)^n', 'n!'], 'Body': \"The relative error of Stirling's approximation gets arbitrarily small as n gets larger. <math_exp> However, it is only an approximation, not a closed-form of <math_exp> \"}\n",
      "370 {'Id': '370', 'Type': 'question', 'Title': 'Good books and lecture notes about category theory.', 'Tags': ['reference-request', 'soft-question', 'category-theory', 'big-list', 'book-recommendation'], 'AcceptedAnswerId': '377', 'urls': [], 'exp': [], 'Body': 'What are the best books and lecture notes on category theory? '}\n",
      "371 {'Id': '371', 'Type': 'answer', 'ParentId': '350', 'urls': [], 'exp': [], 'Body': \"if you don't mind shell out a good amount of money, Concrete Mathematics by Graham, Knuth and Patashnik could be nice (I own the first edition) \"}\n",
      "372 {'Id': '372', 'Type': 'answer', 'ParentId': '297', 'urls': ['http://en.wikipedia.org/wiki/Monte_Carlo_integration'], 'exp': ['\\\\pi', '\\\\pi r^2', '\\\\pi', 'r=1', '\\\\frac{\\\\pi}{4}', 'N', 'p', '\\\\frac{\\\\pi}{4} = \\\\frac{p}{N}'], 'Body': \"There is also the option of approximating <math_exp> using Monte Carlo integration. The idea is this: If we agree that the area of a circle is <math_exp>, for simplicity we build a circle of area <math_exp> by setting <math_exp>.  Placing this circle wholly inside of another region of known area, preferably by inscribing it in a square of side length 2, then we have a ratio of the circle's area to the total area of the square  (in this example that ratio is <math_exp>). The Monte Carlo method works by approximating areas based on the ratio of the number of sample points lying within our region of interest and the total number of sample points we choose to try.  If we spread a uniformly distributed sequence of <math_exp> points over our square of area 4, and call the number of points that land inside of the inscribed circle <math_exp>, then we can say <math_exp>. My implementation of this in Matlab requires tens of thousands of test points to achieve 3.14159xxxx, but I have not tried it for low-discrepancy sequences, or any other uniformly distributed point sets. \"}\n",
      "373 {'Id': '373', 'Type': 'answer', 'ParentId': '370', 'urls': [], 'exp': [], 'Body': 'Categories for the Working mathematician by Mac Lane Categories and Sheaves by Kashiwara and Schapira '}\n",
      "374 {'Id': '374', 'Type': 'answer', 'ParentId': '368', 'urls': [], 'exp': ['n! = \\\\int_0^\\\\infty t^n e^{-t} \\\\: dt'], 'Body': \"If you're willing to accept an integral as an answer, then <math_exp>. \"}\n",
      "375 {'Id': '375', 'Type': 'question', 'Title': 'Are <span class=\"math-container\" id=\"3381\">x \\\\cdot 0 = 0</span>, <span class=\"math-container\" id=\"3382\">x \\\\cdot 1 = x</span>, and <span class=\"math-container\" id=\"3383\">-(-x) = x</span> axioms?', 'Tags': ['abstract-algebra', 'definition', 'axioms'], 'AcceptedAnswerId': '376', 'urls': [], 'exp': ['x \\\\cdot 0 = 0', 'x \\\\cdot 1 = x', '-(-x) = x'], 'Body': \"Context: Rings. Are <math_exp> and <math_exp> and <math_exp> axioms? Arguably three questions in one, but since they all are properties of the multiplication, I'll try my luck... \"}\n",
      "376 {'Id': '376', 'Type': 'answer', 'ParentId': '375', 'urls': [], 'exp': ['0', 'x + 0 = x', 'x', 'x*0 = 0', 'x*0 = x*(0+0) = x*0 + x*0', 'x*0 = 0', 'x*0', 'x*1 = x', '-x', 'x + (-x) = 0', '(-x) + x', 'x', '-x'], 'Body': \"I will assume this is in the context of rings (e.g., real numbers, integers, etc).  In this case, the axiom defining <math_exp> is that <math_exp> for all <math_exp>.  <math_exp> is a result of this since we have <math_exp> which implies <math_exp> (canceling one of the <math_exp>'s). I am guessing that for the second one you mean <math_exp>.  This is a definition (axiom). The third one is a consequence of the definition of <math_exp> being the element such that <math_exp>.  For then we have <math_exp> is also zero so that <math_exp> is the negative of <math_exp>. \"}\n",
      "377 {'Id': '377', 'Type': 'answer', 'ParentId': '370', 'urls': [], 'exp': [], 'Body': \"Lang's Algebra contains a lot of introductory material on categories, which is really nice since it's done with constant motivation from algebra (e.g. coproducts are introduced right before the free product of groups is discussed). \"}\n",
      "378 {'Id': '378', 'Type': 'answer', 'ParentId': '350', 'urls': ['http://rads.stackoverflow.com/amzn/click/0073229725'], 'exp': [], 'Body': \"When it comes to textbooks, the Kenneth Rosen text Discrete Mathematics and its Applications is highly recommended. I was first introduced to it at my university, but I've seen it cited in several places. \"}\n",
      "379 {'Id': '379', 'Type': 'answer', 'ParentId': '375', 'urls': ['http://en.wikipedia.org/wiki/Unit_ring', 'http://en.wikipedia.org/wiki/Field_%28mathematics%29'], 'exp': ['+', '\\\\cdot', '\\\\times', '\\\\ast', '+', '\\\\{ \\\\ldots, -2, -1, 0, 1, 2, \\\\ldots\\\\}', 'x * 1 = x', '0 * x = (0+0) * x = 0*x + 0*x', '-(0*x)', '0*x', '0 = 0*x', '-(-x)', '-x', '-(-x) = x', 'x', '-x', 'x + -x = 0', '-x + x = 0', '\\\\{1, 2, 3, \\\\ldots\\\\}', '0'], 'Body': 'The question is more profound than is initially seems, and is really about algebraic structures. The first question you have to ask yourself is where you\\'re working: In general, addition and multiplication are defined on a structure, which in this case  is a set (basically a collection of \"things\") with two operators we call addition (marked <math_exp>) and multiplication (marked <math_exp> or <math_exp> or <math_exp> or whatever). If this structure holds some properties, which are sometimes called axioms, then it is called a unit ring. The properties are: While this is a long list, and introduces the operator <math_exp> which is not even explicitly mentioned in the question, these properties are quite natural. For example, the integers <math_exp> we all know and love indeed form a ring. The real numbers also form a ring  (in fact they form a field, which means they hold even more properties). In regard to your question, the identity <math_exp> (I assume that\\'s what you meant) is in fact an axiom - it is axiom 7. However, the other two identities are results of the other axioms. First identity: We use axioms 2 and 9 to get  <math_exp>  and then by adding <math_exp> (the additive inverse of <math_exp>, from axiom 5) to both sides,  <math_exp>. Second identity: As stated in axiom 5, <math_exp> is just a notation used which means \"the additive inverse of <math_exp>\". To show that <math_exp> we need to show that <math_exp> is in fact the additive inverse of <math_exp>, or in other words that <math_exp> and <math_exp>. But that\\'s just what axiom 5 says, so we\\'re done. Last point: You might be wondering why did we have to go and introduce addition to answer a question about multiplication? Well, it so happens that without addition the other two identities are simply not true. For example, if we look at the positive integers <math_exp> with only multiplication, then there is no <math_exp> there! Simply put, this is because the positive integers do not form a ring. '}\n",
      "380 {'Id': '380', 'Type': 'answer', 'ParentId': '370', 'urls': [], 'exp': [], 'Body': \"Arbib, Arrows, Structures, and Functors: The Categorical Imperative More elementary than MacLane. I don't know very much about this, but some stripes of computer scientist have taken an interest in category theory recently, and there are lecture notes floating around with that orientation. \"}\n",
      "381 {'Id': '381', 'Type': 'question', 'Title': 'Applications of the Fibonacci sequence', 'Tags': ['combinatorics', 'big-list', 'applications', 'fibonacci-numbers'], 'AcceptedAnswerId': '449', 'urls': [], 'exp': ['n'], 'Body': 'The Fibonacci sequence is very well known, and is often explained with a story about how many rabbits there are after <math_exp> generations if they each produce a new pair every generation. Is there any other reason you would care about the Fibonacci sequence? '}\n",
      "382 {'Id': '382', 'Type': 'answer', 'ParentId': '381', 'urls': [], 'exp': ['R(x,y,z)', 'x^y = z', 'T(x,y,z,w)=0', 'R(x,y,z)', 'T(x,y,z,w)=0', 'w'], 'Body': \"Yep, the solution by Matiyasevich to Hilbert's tenth problem relies heavily on showing exponentiation is Diophantine.  In other words, the relation R given by <math_exp> if and only if <math_exp> can be expressed by a polynomial equation <math_exp>, such that <math_exp> holds if and only if <math_exp> has a solution with <math_exp> integral.  This is a very difficult and technical result, as well as rather counterintuitive. Recall that the Fibonacci numbers grow essentially exponentially. One of the lemmas in showing that exponentiation is Diophantine is to show that the Fibonacci sequence (and its variants) are Diophantine.  See the book by Matiyasevich on Hilbert's 10th problem. \"}\n",
      "383 {'Id': '383', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://en.wikipedia.org/wiki/Elliot_wave'], 'exp': [], 'Body': 'They are sometimes used or occur in financial applications, e.g. Elliot Wave Theory.  This seems more like magic than math to me, but I am not a trader. '}\n",
      "384 {'Id': '384', 'Type': 'answer', 'ParentId': '381', 'urls': [], 'exp': ['k', 'F_{k+2}', 'F_n', 'n^{th}'], 'Body': \"It isn't exactly an application as such, but the upper bound of the size of a subtree in a Fibonacci heap whose root is a node with degree <math_exp> is <math_exp> where <math_exp> is the <math_exp> Fibonacci number. \"}\n",
      "385 {'Id': '385', 'Type': 'answer', 'ParentId': '375', 'urls': [], 'exp': [], 'Body': \"Answering what you asked, the question is poorly formed because you didn't specify the theory you are talking about. To do that, well, you must define symbols, deduction rules... and axioms. Answering what you probably meant to ask, no. Those are properties that - and × have in the Z set, not axioms. They can be deduced from the operations themselves. \"}\n",
      "386 {'Id': '386', 'Type': 'answer', 'ParentId': '381', 'urls': [], 'exp': [], 'Body': \"They're useful in determining the amortized run time of the appropriately named Fibonacci Heap in computer science. \"}\n",
      "387 {'Id': '387', 'Type': 'question', 'Title': 'Sum of reciprocals of numbers with certain terms omitted', 'Tags': ['sequences-and-series', 'convergence-divergence'], 'AcceptedAnswerId': '388', 'urls': [], 'exp': ['1 + \\\\frac12 + \\\\frac13 + \\\\frac14 + \\\\cdots', '\\\\frac12 + \\\\frac13 + \\\\frac15 + \\\\frac17 + \\\\frac1{11} + \\\\cdots', 'O(\\\\log \\\\log n)'], 'Body': \"I know that the harmonic series <math_exp> diverges. I also know that the sum of the inverse of prime numbers <math_exp> diverges too, even if really slowly since it's <math_exp>. But I think I read that if we consider the numbers whose decimal representation does not have a certain digit (say, 7) and sum the inverse of these numbers, the sum is finite (usually between 19 and 20, it depends from the missing digit). Does anybody know the result, and some way to prove that the sum is finite? \"}\n",
      "388 {'Id': '388', 'Type': 'answer', 'ParentId': '387', 'urls': ['http://mathworld.wolfram.com/KempnerSeries.html', 'http://www.jstor.org/stable/2974352', 'http://www.jstor.org/stable/2974352'], 'exp': [], 'Body': \"EDIT: This might be what you're looking for. Found it from looking at the source below. They're called Kempner series. An article here (and cited below) says that one Dr. Kempner proved in 1914 that the series 1+ 1/2 + 1/3 + ..., with any term that has a 9 in the denominator removed, is convergent (though he doesn't say what it converges to in the introductory paragraph). The article goes on to generalize the result. A Curious Convergent Series Frank Irwin The American Mathematical Monthly, Vol. 23, No. 5 (May, 1916), pp. 149-152 Published by: Mathematical Association of America Stable URL: http://www.jstor.org/stable/2974352 \"}\n",
      "389 {'Id': '389', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://en.wikipedia.org/wiki/Wythoff%27s_game'], 'exp': [], 'Body': \"it pops up every now and then. For example, positions (F2n, F2n+1) are winning ones in Wythoff's Game. \"}\n",
      "392 {'Id': '392', 'Type': 'question', 'Title': 'Intuitive understanding of the derivatives of <span class=\"math-container\" id=\"3532\">\\\\sin x</span> and <span class=\"math-container\" id=\"3533\">\\\\cos x</span>', 'Tags': ['calculus', 'trigonometry'], 'AcceptedAnswerId': '1093', 'urls': [], 'exp': ['\\\\sin x', '\\\\cos x', '\\\\cos x', '\\\\sin x', '\\\\pi/4', '\\\\sin x', 'x', '\\\\sin x', 'x', '\\\\cos x'], 'Body': 'One of the first things ever taught in a differential calculus class: This leads to a rather neat (and convenient?) chain of derivatives: An analysis of the shape of their graphs confirms some points; for example, when <math_exp> is at a maximum, <math_exp> is zero and moving downwards; when <math_exp>  is at a maximum, <math_exp> is zero and moving upwards.  But these \"matching points\" only work for multiples of <math_exp>. Let us move back towards the original definition(s) of sine and cosine: At the most basic level, <math_exp> is defined as -- for a right triangle with internal angle <math_exp> -- the length of the side opposite of the angle divided by the hypotenuse of the triangle. To generalize this to the domain of all real numbers, <math_exp> was then defined as the Y-coordinate of a point on the unit circle that is an angle <math_exp> from the positive X-axis. The definition of <math_exp> was then made the same way, but with adj/hyp and the X-coordinate, as we all know. Is there anything about this basic definition that allows someone to look at these definitions, alone, and think, \"Hey, the derivative of the sine function with respect to angle is the cosine function!\" That is, from the unit circle definition alone.  Or, even more amazingly, the right triangle definition alone.  Ignoring graphical analysis of their plot. In essence, I am asking, essentially, \"Intuitively why is the derivative of the sine the cosine?\" '}\n",
      "393 {'Id': '393', 'Type': 'answer', 'ParentId': '392', 'urls': ['http://en.wikipedia.org/wiki/Taylor_series'], 'exp': [' \\\\sin x = \\\\sum^{\\\\infty}_{n=0} \\\\frac{(-1)^n}{(2n+1)!} x^{2n+1} = x - \\\\frac{x^3}{3!} + \\\\frac{x^5}{5!} - \\\\cdots\\\\text{ for all } x\\\\!', '\\\\cos x = \\\\sum^{\\\\infty}_{n=0} \\\\frac{(-1)^n}{(2n)!} x^{2n} = 1 - \\\\frac{x^2}{2!} + \\\\frac{x^4}{4!} - \\\\cdots\\\\text{ for all } x\\\\! '], 'Body': \"This isn't exactly what you asked, but look at the Taylor series for the polynomials: <math_exp> <math_exp> The relationships between the derivatives are clear from this. \"}\n",
      "394 {'Id': '394', 'Type': 'answer', 'ParentId': '387', 'urls': [], 'exp': ['S', 'k', 'S =S_1 + S_2 + S_3 + \\\\ldots', 'S_i', 'i', 'k', 'i', 'k', '8\\\\cdot9^{i-1}', '8', '0', 'k', '9', 'k=0', '9', '8\\\\cdot9^{i-1}', 'S_i', 'S_i', '\\\\frac1a', 'a', 'i', 'a \\\\geq 10^{i-1}', '\\\\frac1a \\\\leq \\\\frac1{10^{i-1}}', 'S_i \\\\leq 8\\\\cdot\\\\dfrac{9^{i-1} }{10^{i-1}} = 8\\\\cdot\\\\left(\\\\frac9{10}\\\\right)^{i-1}', 'S= \\\\sum S_i \\\\leq \\\\sum 8\\\\cdot\\\\left(\\\\frac9{10}\\\\right)^{i-1}', '\\\\frac9{10} &lt; 1', 'S', 'S'], 'Body': \"It is not very surprising that the sum is finite, since numbers without a 7 (or any other digit) get rarer and rarer as the number of digits increases. Here's a proof. Let <math_exp> be the harmonic series with all terms whose denominator contains the digit <math_exp> removed. We can write <math_exp>, where <math_exp> is the sum of all terms whose denominator contains exactly <math_exp> digits, all different from <math_exp>. Now, the number of <math_exp>-digit numbers that do not contain the digit <math_exp> is <math_exp> (there are <math_exp> choices for the first digit, excluding <math_exp> and <math_exp>, and <math_exp> choices for the other digits). [Well, if <math_exp> there are <math_exp> choices for the first digit, but the proof still works.] So there are <math_exp> numbers in the sum <math_exp>. Now each number in <math_exp> is of the form <math_exp>, where <math_exp> is an <math_exp>-digit number. So <math_exp>, which implies <math_exp>. Therefore <math_exp>. So <math_exp> which is a  geometric series of ratio <math_exp>, which converges. Since <math_exp> is a positive series bounded above by a converging series, <math_exp> converges. \"}\n",
      "395 {'Id': '395', 'Type': 'answer', 'ParentId': '392', 'urls': [], 'exp': [], 'Body': 'As a Physics Major, I would like to propose an answer that comes from my understanding of seeing sine and cosine in the real world. In doing this, I will examine uniform circular motion. Because of the point-on-a-unit-circle definition of sine and cosine, we can say that: Is a proper parametric function to describe a point moving along the unit circle. Let us consider what the first derivate, in a physical context, should be.  The first derivative of position should represent, ideally, the velocity of the point. In a physical context, we would expect the velocity to be the line tangent to the direction of motion at a given time t.  Following from this, it would be tangent to the circle at angle t.  Also, because the angular velocity is constant, the magnitude of the velocity should be constant as well. As expected, the velocity is constant, so the derivatives of sine and cosine are behaving as they should. We can also think about what the direction of the velocity would be, as well, compared to the position vector. I\\'m not sure if this is \"cheating\" by the bounds of the question, but by visualizing the graph we can see that the velocity, by nature of being tangent to the circle, must be perpendicular to the position vector. If this is true, then position * velocity = 0 (dot product). Life is good.  If we assume that the definition of cos(t) is -sin(t) and that the definition of sin(t) is cos(t), we find physical behavior exactly like expected: a constant velocity that is always perpendicular to the position vector. We can take this further and look at the acceleration.  In Physics, we would call this the restoring force.  In a circle, what acceleration would have to exist in order to keep a point moving in a circle? More specifically, in what direction would this acceleration have to be? It takes little thought to arrive at the idea that acceleration would have to be center-seeking, and pointing towards the center.  So, if we can find that acceleration is in the opposite direction as the position vector, the we can be almost certain about the derivatives of sine and cosine.  That is, their internal angle should be pi. '}\n",
      "396 {'Id': '396', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://en.wikipedia.org/wiki/Fibonacci_search_technique'], 'exp': [], 'Body': \"Suppose you're writing a computer program to search a sorted array for a particular value. Usually the best method to use is a binary search. But binary search assumes it's the same cost to read from anywhere in the array. If it costs something to move from one array element to another, and the cost is proportional to how many array elements you need to skip over to get from one element you read to the next, then Fibonacci search works better. This can apply to situations like searching through arrays that don't fit entirely in your computer's cache so it's generally cheaper to read nearby elements that distant ones. \"}\n",
      "397 {'Id': '397', 'Type': 'answer', 'ParentId': '392', 'urls': [], 'exp': [], 'Body': \"I don't think you can get an intuitive feel for the derivatives without looking at the plots personally. When you consider that a derivative is a rate of change, you need to be looking at a function that is varying, which implies you are looking at the plot/graph of the function. When you further consider that a derivative (by definition of it being a rate of change) is a gradient function, the intuitive answer is that cos is the gradient function of sin, and -sin is the gradient function of cos (and so on). So if you calculate the gradient of the sin curve at any point, the value you get will be the cosine value for that point. \"}\n",
      "398 {'Id': '398', 'Type': 'question', 'Title': 'What is larger -- the set of all positive even numbers, or the set of all positive integers?', 'Tags': ['elementary-set-theory', 'infinity'], 'AcceptedAnswerId': '406', 'urls': [], 'exp': [], 'Body': 'We will call the set of all positive even numbers E and the set of all positive integers N. At first glance, it seems obvious that E is smaller than N, because for E is basically N with half of its terms taken out.  The size of E is the size of N divided by two. You could see this as, for every item in E, two items in N could be matched (the item x and x-1).  This implies that N is twice as large as E On second glance though, it seems less obvious.  Each item in N could be mapped with one item in E (the item x*2). Which is larger, then?  Or are they both equal in size?  Why? (My background in Set theory is quite extremely scant) '}\n",
      "399 {'Id': '399', 'Type': 'answer', 'ParentId': '398', 'urls': ['http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument'], 'exp': [], 'Body': \"They are both the same size, the size being 'countable infinity' or 'aleph-null'. The reasoning behind it is exactly that which you have already identified - you can assign each item in E to a single value in N. This is true for the Natural numbers, the Integers, the Rationals but not the Reals (see the Diagonal Slash argument for details on this result). -- Added explanation from comment -- The first reasoning is invalid because the cardinality of infinite sets doesn't follow 'normal' multiplication rules. If you multiply a set with cardinality of aleph-0 by 2, you still have aleph-0. The same is true if you divide it, add to it, subtract from it by any finite amount. \"}\n",
      "400 {'Id': '400', 'Type': 'answer', 'ParentId': '398', 'urls': ['http://en.wikipedia.org/wiki/Cardinality'], 'exp': [], 'Body': 'Each item in N could be mapped with one item in E  (the item x*2). Yes. Both sets have cardinality aleph-0. '}\n",
      "401 {'Id': '401', 'Type': 'answer', 'ParentId': '398', 'urls': [], 'exp': [], 'Body': \"The word 'size' doesn't have a intuitive meaning for set of infinite items. Mathematicians defined cardinality by one-to-one correspondence(bijection), and it's generally what it means by 'size'. If there exist a bijection between A and B, then the two sets have the same cardinality. You have shown the existence of a bijection, therefore E and N have the same cardinality. You might mean the 'density' of N is twice as large as E. density of A(a subset of natural number) is limit of |{a&lt;=n|a\\\\in A}|/n as n goes to infinity. \"}\n",
      "402 {'Id': '402', 'Type': 'answer', 'ParentId': '275', 'urls': [], 'exp': [], 'Body': 'As an undergrad, I read a fair number of pop math books. The best by far was Ash and Gross\\' \"Fearless Symmetry\". This book is very beautiful. It sustains a nice level of rigor while being approachable by those who aren\\'t professionals. Additionally, it weaves the tale of one of the most beautiful recent stories in mathematics. Everyone I know of who have read the book have found it wonderful. '}\n",
      "403 {'Id': '403', 'Type': 'answer', 'ParentId': '392', 'urls': ['http://en.wikipedia.org/wiki/Euler%27s_formula'], 'exp': [], 'Body': \"This interesting pattern of derivatives involving sine and cosine is related to the fact that e^x is its own derivative and that e^(ix) = cos(x) + i*sin(x) (Euler's Formula). These two facts are in some sense the math hiding behind Justin L's more physical explanation, which you might well find more intuitive. \"}\n",
      "404 {'Id': '404', 'Type': 'answer', 'ParentId': '392', 'urls': [], 'exp': [\"y'' = -y\", 'f', 'g', \"f' = i f\", \"g' = -i g\", \"f' = -g\", \"g' = f\", \"f' = g\", \"g' = -f\"], 'Body': 'One of the main ways that sine and cosine come up is as the fundamental solutions to the differential equation <math_exp>, known as the wave equation. Why is this an important differential equation?  Well, interpreting it using Newton\\'s second law it says \"the force is proportional and opposite to the position.\" For example, this is what happens with a spring! Now that\\'s a 2nd degree equation, so it has a 2-dimensional space of solutions. How to pick a nice basis for that space? Well, one way would be to pick <math_exp> and <math_exp> such that <math_exp> and <math_exp>. However, that involves too many imaginary numbers, so another option is <math_exp>, and <math_exp>. Thus if you\\'re trying to find two functions which explain oscillatory motion you\\'re naturally lead to picking functions that have <math_exp>, <math_exp>, etc. (On the other hand it\\'s totally unclear from this point of view why Sine and Cosine should have anything to do with triangles...) '}\n",
      "405 {'Id': '405', 'Type': 'question', 'Title': 'Simple explanation of a monad', 'Tags': ['category-theory', 'intuition', 'universal-algebra', 'monads'], 'AcceptedAnswerId': '408', 'urls': [], 'exp': [], 'Body': 'I have been learning some functional programming recently and I so I have come across monads. I understand what they are in programming terms, but I would like to understand what they are mathematically. Can anyone explain what a monad is using as little category theory as possible? '}\n",
      "406 {'Id': '406', 'Type': 'answer', 'ParentId': '398', 'urls': [], 'exp': [], 'Body': 'Mathematics is the art of clever forgetting.  The first mathematical breakthrough, numbers, came about when people realized that if you just forgot about whether it was 5 cows + 3 cows, or 5 rocks + 3 rocks or whatever you always got 8.  Numbers are what you get when you look at collections of objects and forget what kind of object they are. When you say \"as sets\" you mean you\\'re forgetting a lot of information, in particular you don\\'t care about what the names of the elements in that set are or what properties those elements have.  As sets the positive numbers and the positive even numbers are \"the same\" (that is are in bijection) because you can take 1,2,3,...  and just rename 1 to 2, and 2 to 4, and n to 2n, and you\\'ve just renamed all the elements and got the even numbers! However, if you want to remember more about these sets, for example that they\\'re not arbitrary sets they\\'re both subsets of the natural numbers, then they become distinguishable.  Depending on how you want to measure \"size of a subset of the natural numbers\" they might be different sizes.  For example, a common way to measure \"size of a subset of the natural numbers\" is by its \"density.\"  That is you look at the first N numbers and calculate what fraction of them are in your set, and then take the limit as N goes to infinity (warning for sufficiently complicated sets this limit might not exist).  So for your two examples, one has density 1 and the other has density 1/2, which is one way to make precise the intuition that the former is bigger as a subset of the natural numbers (though not as a set) than the latter. '}\n",
      "407 {'Id': '407', 'Type': 'answer', 'ParentId': '398', 'urls': ['http://en.wikipedia.org/wiki/Countable_set', 'http://en.wikipedia.org/wiki/Aleph_number'], 'exp': [], 'Body': 'Your example is an example of a countable sets: http://en.wikipedia.org/wiki/Countable_set As others have explained, if you find a bijection between these two sets then they are of the same cardinality or have the same number of elements.(I hope that statement is true :)) But in this case you have found one with N which is what makes it a countable set. f(x)=2x would be an example of a bijection between N and 2N. Since for every x in the set of natural numbers there is a corresponding number in the set of 2N (only one) then you can say that they have the same cardinality. A cooler example would be the interval of the real numbers (0, 1) and R itself, those are not countable but their cardinality is the same. Other notes, if you are wondering what aleph is: http://en.wikipedia.org/wiki/Aleph_number '}\n",
      "408 {'Id': '408', 'Type': 'answer', 'ParentId': '405', 'urls': ['http://www.haskell.org/tutorial/monads.html', 'http://en.wikibooks.org/wiki/Haskell/Category_theory'], 'exp': [], 'Body': 'If you want to avoid too much category theory, you can first read this link to understand the definition of monads in Haskell. Then look at Wikibooks for a more mathematical look (thanks Jonathan Fischoff). There are two descriptions that I know of. The first can easily be found by looking at wiki under Monad or consulting Harry\\'s nice summary. The second is more interesting in my opinion. I will assume that you don\\'t know the definition of a monoidal action, if you do, just skip ahead. A monoidal action is a functor from a monoid to the category of endofunctors on a category satisfying two coherence relations. These two coherence relations simply verify that your monoidal product is the same as composition in the target, and that the identity object behaves with the action. The relations are normally written as diagrams, but without latex implement, I wont type them here. To get an idea of a monoidal action, consider a group action, and formulate it a little more categorically, by writing the two axioms as diagrams. These diagrams, when converted to the language of monoidal categories, are exactly those of a monoidal action. Now the best part is once you have monoidal action, monads on a category are simply the category of monoidal actions from the trivial monoidal category to your category. Note here that the trivial monoidal category will be the monoidal category with one object one morphism and all the other monoidal data is trivially determined. The monadic coherence relations come for free from your monoidal action coherence relations. So, my simple explanation? In this way, we can formulate monads functorially as \"representations\" of the trivial monoidal category. One can readily show the two definitions are the same. '}\n",
      "409 {'Id': '409', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://rads.stackoverflow.com/amzn/click/052171916X'], 'exp': [], 'Body': 'Another book that is more elementary, not requiring any algebraic topology for motivation, and formulating the basics through a question and answer approach is: Conceptual Mathematics An added benefit is that it is written by an expert! '}\n",
      "411 {'Id': '411', 'Type': 'question', 'Title': 'Why do complex functions have a finite radius of convergence?', 'Tags': ['sequences-and-series', 'complex-analysis'], 'AcceptedAnswerId': '413', 'urls': [], 'exp': ['\\\\displaystyle f(z)=\\\\sum_{n=0}^\\\\infty a_n z^n', 'R&gt;0', 'R', 'z_0', '|z_0|=R', 'f(z)=\\\\begin{cases}   0 &amp; \\\\text{for $z=0$} \\\\\\\\\\\\   e^{-\\\\frac{1}{z^2}} &amp; \\\\text{for $z \\\\neq 0$} \\\\end{cases}', '0'], 'Body': 'Say we have a function <math_exp> with radius of convergence <math_exp>. Why is the radius of convergence only <math_exp>? Can we conclude that there must be a pole, branch cut or discontinuity for some <math_exp> with <math_exp>? What does that mean for functions like <math_exp> that have a radius of convergence <math_exp>? '}\n",
      "412 {'Id': '412', 'Type': 'answer', 'ParentId': '405', 'urls': ['http://en.wikipedia.org/wiki/Monoid_object'], 'exp': ['C', 'C', '\\\\mathcal{End}(C)=\\\\mathcal{Hom}_{Cat}(C,C),', 'C\\\\to D', 'Fun(C,D)', '\\\\mathcal{Hom}_{Cat}(C,D)', 'Cat(C,D)'], 'Body': 'Let <math_exp> be a category.  Then a monad based at <math_exp> is a monoid in the strict monoidal category <math_exp> where the natural monoidal product is given by composition of endofunctors, and the monoidal unit is the identity functor. A monoid in a monoidal category is defined here. If you need more explanation, just give me a call. Notation: The category of functors <math_exp> is also written as <math_exp>, but this notation is nonstandard.  The standard notations are <math_exp> or simply <math_exp>. '}\n",
      "413 {'Id': '413', 'Type': 'answer', 'ParentId': '411', 'urls': [], 'exp': ['R', '|z| = R', '\\\\xi', 'R', '\\\\xi', 'D_R(0)', '\\\\sum z^{2^j}', '\\\\sum_j \\\\frac{z^j}{j^2}'], 'Body': 'If the radius of convergence is <math_exp>, that means there is a singular point on the circle <math_exp>. In other words, there is a point <math_exp> on the circle of radius <math_exp> such that the function cannot be extended via \"analytic continuation\" in a neighborhood of <math_exp>.  This is a straightforward application of compactness of the circle and can be found in books on complex analysis, e.g. Rudin\\'s. However, it does not mean that there is a pole, branch cut, or discontinuity, though those would cause singular values. Indeed, a \"pole\" on the boundary would only make sense if you can analytically continue the power series to some proper domain containing the disk <math_exp>, and this is generally impossible. For instance, the power series <math_exp> cannot be continued in any way outside the unit disk, because it is unbounded along any ray whose angle is a dyadic fraction.  The unit circle is its natural boundary, though it does not make sense to say that the function has a branch point or pole there.  (More generally, one can show that given any domain in the plane, there is a holomorphic function in that domain which cannot be extended any further, essentially using variations on the same theme.) The function <math_exp>, incidentally, is continuous on the closed unit disk, but even though there is a singular point there. So continuity may happen at singular points. The last function you mention does not have a power series expansion in a neighborhood of zero. In fact, it is not continuous at zero, because it blows up if you approach zero along the imaginary axis. '}\n",
      "414 {'Id': '414', 'Type': 'question', 'Title': \"Why do we use the commutator bracket for Lie algebra's\", 'Tags': ['notation', 'lie-algebras'], 'AcceptedAnswerId': '417', 'urls': [], 'exp': ['[a,b]=ab-ba'], 'Body': 'We define Lie algebras abstractly as algebras whose multiplication satisfies anti-commutativity and Jacobi\\'s Identity. A particular instance of this is an associative algebra equipped with the commutator bracket:  <math_exp>. However, the notation suggests that this bracket is the one we think about. Additionally, the right adjoint to the functor I just mentioned creates the universal enveloping algebra by quotienting the tensor algebra by the tensor version of this bracket; but we could always start with some arbitrary Lie algebra with  some other satisfactory bracket and apply this functor. My question is \"Why the commutator bracket?\" Is it purely from a historical standpoint (and if so could you explain why)? Or is there a result that says any Lie algebra is essentially one with the commutator bracket (maybe something about the faithfulness of the functor from above)? I know of (a colleague told me) a proof that the Jacobi identity is also an artifact of the right adjoint to the universal enveloping algebra. He can show that it is the necessary identity for the universal enveloping algebra to be associative (if someone knows of this in the literature I would also appreciate the link to this!) I hope this question is clear, if not, I can revise and try to make it a bit more specific. '}\n",
      "415 {'Id': '415', 'Type': 'question', 'Title': 'What property of certain regular polygons allows them to be faces of the Platonic Solids?', 'Tags': ['geometry', 'polyhedra', 'platonic-solids'], 'AcceptedAnswerId': '416', 'urls': [], 'exp': [], 'Body': 'It appears to me that only Triangles, Squares, and Pentagons are able to \"tessellate\" (is that the proper word in this context?) to become regular 3D convex polytopes. What property of those regular polygons themselves allow them to faces of regular convex polyhedron?  Is it something in their angles?  Their number of sides? Also, why are there more Triangle-based Platonic Solids (three) than Square- and Pentagon- based ones? (one each) Similarly, is this the same property that allows certain Platonic Solids to be used as \"faces\" of regular polychoron (4D polytopes)? '}\n",
      "416 {'Id': '416', 'Type': 'answer', 'ParentId': '415', 'urls': [], 'exp': ['3\\\\alpha&lt;2\\\\pi', '\\\\frac{\\\\pi}{3}', '3\\\\cdot\\\\frac{\\\\pi}{3}&lt;2\\\\pi', '4\\\\cdot\\\\frac{\\\\pi}{3}&lt;2\\\\pi', '5\\\\cdot\\\\frac{\\\\pi}{3}&lt;2\\\\pi', '6\\\\cdot\\\\frac{\\\\pi}{3}=2\\\\pi', '\\\\frac{\\\\pi}{2}', '3\\\\cdot\\\\frac{\\\\pi}{2}&lt;2\\\\pi', '4\\\\cdot\\\\frac{\\\\pi}{2}=2\\\\pi', '\\\\frac{3\\\\pi}{5}', '3\\\\cdot\\\\frac{3\\\\pi}{5}&lt;2\\\\pi', '2\\\\pi', '\\\\frac{2\\\\pi}{3}', '3\\\\cdot\\\\frac{2\\\\pi}{3}=2\\\\pi'], 'Body': 'The regular polygons that form the Platonic solids are those for which the measure of the interior angles, say &alpha; for convenience, is such that <math_exp> (360&deg;) so that three (or more) of the polygons can be assembled around a vertex of the solid. Regular (equilateral) triangles have interior angles of measure <math_exp> (60&deg;), so they can be assembled 3, 4, or 5 at a vertex (<math_exp>, <math_exp>, <math_exp>), but not 6 (<math_exp>--they tesselate the plane). Regular quadrilaterals (squares) have interior angles of measure <math_exp> (90&deg;), so they can be assembled 3 at a vertex (<math_exp>), but not 4 (<math_exp>--they tesselate the plane). Regular pentagons have interior angles of measure <math_exp> (108&deg;), so they can be assembled 3 at a vertex (<math_exp>), but not 4 (<math_exp>). Regular hexagons have interior angles of measure <math_exp> (120&deg;), so they cannot be assembled 3 at a vertex (<math_exp>--they tesselate the plane). Any other regular polygon will have larger interior angles, so cannot be assembled into a regular solid. '}\n",
      "417 {'Id': '417', 'Type': 'answer', 'ParentId': '414', 'urls': [], 'exp': [], 'Body': 'Well Lie algebras naturally arise from the Lie bracket of vector fields and from taking the Lie algebra of a Lie group.  If we look at a the Lie algebra of a matrix subgroup, then the Lie bracket is the commutator of matrices. '}\n",
      "418 {'Id': '418', 'Type': 'question', 'Title': 'Construct a bijection from <span class=\"math-container\" id=\"3757\">\\\\mathbb{R}</span> to <span class=\"math-container\" id=\"3758\">\\\\mathbb{R}\\\\setminus S</span>, where <span class=\"math-container\" id=\"3759\">S</span> is countable', 'Tags': ['elementary-set-theory'], 'AcceptedAnswerId': '420', 'urls': [], 'exp': ['(0,1)', '[0,1]', 'S', 'f:\\\\mathbb{N}\\\\to\\\\mathbb{N}\\\\setminus S', '|\\\\mathbb{N}| = |\\\\mathbb{N}\\\\setminus S|', 'f(n)', 'n^{\\\\text{th}}', '\\\\mathbb{N}\\\\setminus S', 'f:\\\\mathbb{R}\\\\to\\\\mathbb{R}\\\\setminus S'], 'Body': \"Two questions: Find a bijective function from <math_exp> to <math_exp>. I haven't found the solution to this since I saw it a few days ago. It strikes me as odd--mapping a open set into a closed set. <math_exp> is countable. It's trivial to find a bijective function <math_exp> when <math_exp>; let <math_exp> equal the <math_exp> smallest number in <math_exp>. Are there any analogous trivial solutions to <math_exp>? \"}\n",
      "419 {'Id': '419', 'Type': 'question', 'Title': 'What can we conclude from correlation?', 'Tags': ['statistics'], 'AcceptedAnswerId': '421', 'urls': [], 'exp': [], 'Body': \"I just got my statistics test back and I am totally confused about one of the questions! A study was done that took a simple random sample of 40 people and measured whether   the subjects were right-handed or   left-handed, as well as their ages.   The study showed that the proportion   of left-handed people and the ages had   a strong negative correlation. What   can we conclude? Explain your answer. I know that we can't conclude that getting older causes people to become right-handed. Something else might be causing it, not the age. If two things are correlated, we can only conclude association, not causation. So I wrote: We can conclude that many people   become right-handed as they grow   older, but we cannot tell why. That's exactly what association means, but my teacher marked me wrong! What mistake did I make? Is 40 too small of a sample size to make any conclusions? \"}\n",
      "420 {'Id': '420', 'Type': 'answer', 'ParentId': '418', 'urls': ['http://en.wikipedia.org/wiki/Cantor%E2%80%93Bernstein%E2%80%93Schroeder_theorem'], 'exp': ['(0,1) \\\\to [0,1]', 'f: [0,1] \\\\to [1/4, 3/4] \\\\subset (0,1)', 'x \\\\to x/2 +1/4', 'R \\\\to R - N', 'S', 'R', 'N', 'S', '(0,1)'], 'Body': \"The proof of the Schroeder-Bernstein theorem allows you to get a bijection for 1, since we have an injection <math_exp> and a bijection <math_exp> (say <math_exp>).  The function's definition will be somewhat messy (basically, it depends on how many times you can lift a point under these to injections already defined, and specifically the parity of the number of times), but it'll do it. For 2, iterate this construction to get a bijection <math_exp>.  Then given any countable set <math_exp>, define the map of <math_exp> that interchanges <math_exp> and <math_exp> and leaves every other point fixed.  Then the composition of the first bijection with this second map is your bijection. Continuity considerations imply that the map can't be continuous: in 1, for instance, we'd otherwise have that <math_exp> is compact, which it's not. \"}\n",
      "421 {'Id': '421', 'Type': 'answer', 'ParentId': '419', 'urls': [], 'exp': [], 'Body': 'This is wrong: \"We can conclude that many people become right-handed as they grow older.\" We cannot conclude this at all from the given data. For one, the study only takes a sample at one point in time, rather than selecting a sample and monitoring their progress through many decades. This is what would be needed for us to even entertain the possibility that aging causes a change in handedness. Other possible causes include that left handed people might have a shorter life expectancy, or perhaps there was a spike in the birth rate of right handed people in the past. There are many other possibilities that have been mentioned in others answers which would also account for the skewed proportions without requiring people to change handedness with age, which is what you falsely concluded in the test. Also, just an observation, but it appears the \"study\" was conducted under false pretenses. Handedness is a false dichotomy, people can also be ambidextrous. '}\n",
      "422 {'Id': '422', 'Type': 'answer', 'ParentId': '240', 'urls': [], 'exp': [], 'Body': 'In mathematics you can construct a mathematical theory with different sets of axioms. This can be really useful. When mathematicians ignored the parallel line axiom in Euclidian geometry it gave raise to non-Euclidian geometries, which became really important in Einsteinian physics. An axiom of logic is the law of the excluded third which basically says that one statement is either true or false. This means that any theorem that depends completely in this axiom is not valid on mathematical theories that decide to ignore the axiom. A proof by contradiction is using the axiom directly; if the consequent is false then the antecent is false, then the converse of the consequent is true (because it must be either true or false). If the theorem can be proved in a constructive way, then it does not depend on the Law of Excluded Third and is valid in theories that does not use the law. '}\n",
      "423 {'Id': '423', 'Type': 'question', 'Title': 'Intuitive reasoning behind the Chain Rule in multiple variables?', 'Tags': ['calculus', 'linear-algebra'], 'AcceptedAnswerId': '429', 'urls': [], 'exp': ['2\\\\times 2 = 4', 'z = f(x,y),', 'x = g(t)', 'y = h(t)', '\\\\frac{dz}{dt} = \\\\frac{\\\\partial z}{dx} \\\\frac{dx}{dt} + \\\\frac{\\\\partial z}{dy} \\\\frac{dy}{dt}.'], 'Body': 'I\\'ve sort of gotten a grasp on the Chain rule with one variable.  If you hike up a mountain at 2 feet an hour, and the temperature decreases at 2 degrees per feet, the temperature would be decreasing for you at <math_exp> degrees per hour. But I\\'m having a bit more trouble understanding the Chain Rule as applied to multiple variables.  Even the case of 2 dimensions <math_exp> where <math_exp> and <math_exp>, so <math_exp> Now, this is easy enough to \"calculate\" (and figure out what goes where).  My teacher taught me a neat tree-based graphical method for figuring out partial derivatives using chain rule.  All-in-all, it was rather hand-wavey.  However, I\\'m not sure exactly how this works, intuitively. Why, intuitively, is the equation above true?  Why addition?  Why not multiplication, like the other chain rule?  Why are some multiplied and some added? '}\n",
      "424 {'Id': '424', 'Type': 'answer', 'ParentId': '419', 'urls': [], 'exp': [], 'Body': 'We can only conclude that the result is interesting and it deserves more research. Without further study we can not say if right-handedness causes age (ie. being left handed causes biological alterations that shorten the life span), age causes right-handedness (ie. as people age they become right handed), they are correlated because they are caused by another variable (ie. older people became educated in a different system that discouraged left-handedness), the study had bad luck selecting its sample, the study sample was bad designed, the study was bad designed, etc. '}\n",
      "425 {'Id': '425', 'Type': 'answer', 'ParentId': '423', 'urls': [], 'exp': [], 'Body': \"Changing time is the same as changing x and changing y. If the changes each one caused in z didn't interact, then the total change would be the sum of both changes. If the function is well behaved (differentiatable) then the interaction caused by an infinitesimal change in x with an infinitesimal change in y will be doubly infinitesimal. The proof of the double chain rule just shows this formally. \"}\n",
      "426 {'Id': '426', 'Type': 'answer', 'ParentId': '423', 'urls': [], 'exp': ['f:X\\\\to Y', 'g:Y\\\\to Z', 'f', '\\\\vec{v}', 'g', 'f(\\\\vec{v})', 'T_\\\\vec{v}(g\\\\circ f)=T_{f(\\\\vec{v})}(g) \\\\circ T_\\\\vec{v}(f).', '{}^t[x,y,z]', '\\\\vec{w}'], 'Body': \"The correct context of the chain rule is that taking the tangent bundle is functorial.  A more down-to-earth answer is provided by working coordinate free using linear algebra. Suppose <math_exp> and <math_exp> are functions between Banach spaces (these are a generalized version of R^n) such that <math_exp> is differentiable at <math_exp> and <math_exp> is differentiable at <math_exp> (note that in the general case we must require that their derivatives are toplinear (continuous and linear), since not all linear maps are continuous in the context of infinite dimensional spaces). Then taking the total differential, we see that the chain rule is equivalent to saying that: <math_exp> The description you get with coordinates comes from this very much simpler presentation (Where T denotes the total differential) as follows: To derive the formula with coordinates (say, for example, in three dimensions), we present the total differentials (which are linear transformations) as their Jacobian matrices and test along the column vector <math_exp>. (Where the leftthand exponent t denotes the matrix transpose). Note: When we present linear operators by their matrices, composition of linear transformations becomes matrix multiplication, and evaluation at a vector <math_exp> becomes righthand multiplication by a column matrix. The main point is that coordinates obscure what's actually going on here.  The beauty of the coordinate free definition is destroyed by the complicated description of matrix multiplication. \"}\n",
      "427 {'Id': '427', 'Type': 'answer', 'ParentId': '240', 'urls': ['http://en.wikipedia.org/wiki/Principle_of_explosion', 'http://en.wikipedia.org/wiki/Natural_deduction#Proofs_and_type-theory'], 'exp': ['\\\\Gamma', '\\\\Gamma\\\\cup\\\\{(\\\\neg\\\\phi)\\\\}', '\\\\neg\\\\phi', '\\\\Gamma', '\\\\phi', '\\\\psi', '\\\\Gamma\\\\cup\\\\{(\\\\neg\\\\phi)\\\\}\\\\vdash\\\\psi', '\\\\Gamma\\\\cup\\\\{(\\\\neg\\\\phi)\\\\}\\\\vdash \\\\neg\\\\psi', '\\\\Gamma\\\\cup\\\\{(\\\\neg\\\\phi)\\\\}\\\\vdash\\\\phi', '\\\\phi', '\\\\Gamma\\\\vdash(\\\\neg\\\\phi\\\\Rightarrow\\\\phi)', '((\\\\neg\\\\phi\\\\Rightarrow\\\\phi)\\\\Rightarrow\\\\phi', '\\\\Gamma\\\\vdash\\\\phi', '\\\\phi', '\\\\Gamma', '\\\\Gamma', '\\\\phi'], 'Body': 'Proof by contradiction is just as logically valid as any other type of proof. If you are unsure, I think it might help to consider exactly what a proof by contradiction entails. Say we have a set of statements <math_exp>, and that <math_exp> is not consistent. That is, the statement <math_exp> contradicts something in <math_exp>. (In other words, we supposed <math_exp> was false, and reached a contradiction.) Say that statement was <math_exp>. Then <math_exp> and <math_exp>. By the principle of explosion, we conclude that <math_exp>. (We can prove any statement, so we prove <math_exp>. By deduction, we know that <math_exp>. Most first-order logic systems have an axiom that gives us <math_exp>. I hope you can convince yourself that this is true without trouble. This yields <math_exp>. We started with the idea that the negation of the statement <math_exp> was incompatible with your working set of axioms and theorems <math_exp>, and concluded that therefore <math_exp> proves <math_exp>. Of course, there is more than one way to prove anything. Other methods can often be more intuitive, more elegant, or may lead to some other useful results. However, that is distinct from \"weaker.\" Proof by contradiction is perfectly sound. '}\n",
      "428 {'Id': '428', 'Type': 'answer', 'ParentId': '423', 'urls': [], 'exp': ['z = f(x,y)', 'y', '\\\\frac{dz}{dt} = \\\\frac{df}{dx} * \\\\frac{dx}{dt}', 'x', '\\\\frac{dz}{dt} = \\\\frac{df}{fy} * \\\\frac{dy}{dt}'], 'Body': 'Think of it in terms of causality &amp; superposition. <math_exp> If you keep <math_exp> fixed then <math_exp> If you keep <math_exp> fixed then <math_exp>. Superposition says you can just add the two together. '}\n",
      "429 {'Id': '429', 'Type': 'answer', 'ParentId': '423', 'urls': [], 'exp': ['f: R^m \\\\to R^n', 'q=f(p)', 'p', 'f', 'q', 'p', 'L: R^m \\\\to R^n', 'g: R^n \\\\to R^s', 'r = g(q)', 'g', 'q', 'r', 'N', 'p,q,r', 'f', 'L', 'g', 'N', 'g \\\\circ f', 'N \\\\circ L', 'N \\\\circ L', 'g \\\\circ f', 'g \\\\circ f'], 'Body': 'The basic reason is that one is simply composing the derivatives just as one composes the functions. Derivatives are linear approximations to functions. When you compose the functions, you compose the linear approximations---not a surprise. I\\'m going to try to expand on Harry Gindi\\'s answer, because that was the only way I could grok it, but in somewhat simpler terms. The way to think of a derivative in multiple variables is as a linear approximation.   In particular, let <math_exp> and <math_exp>.  Then near <math_exp>, we can write <math_exp> as <math_exp> basically something linear plus some \"noise\" which \"doesn\\'t matter\" (i.e. is little oh of the distance to <math_exp>).  Call this linear map <math_exp>. Now, suppose <math_exp> is some map and <math_exp>.  We can approximate <math_exp> near <math_exp> by <math_exp> plus some linear map <math_exp> plus some \"garbage\" which is, again, small. For simplicity, I\\'m going to assume that <math_exp> are all zero. This is ok, because one can just move one\\'s origin around a bit. So, as before, applying <math_exp> to a point near zero corresponds loosely to applying the linear transformation <math_exp>. Applying <math_exp> to a point near zero corresponds loosely to applying <math_exp>.  Hence applying <math_exp> corresponds up to some ignorable \"garbage\" to the map <math_exp>. This means that <math_exp> is the linear approximation to <math_exp> at zero, in particular this composition is the derivative of <math_exp>. '}\n",
      "431 {'Id': '431', 'Type': 'answer', 'ParentId': '419', 'urls': [], 'exp': [], 'Body': 'Here is one example of a plausible explanation that disagrees with your analysis: Cultural expectations for left- and right-handedness have changed over time. Older people may have gone to school at a time where left-handedness was discouraged and students were forced to write with their right hands, training children never to use the left hand instead of the right. Younger participants in the study were in school more recently and learned to write at a time where left-handedness was not discouraged, creating a positive correlation between left-handedness and youth. '}\n",
      "432 {'Id': '432', 'Type': 'question', 'Title': 'Why does the discriminant of a cubic polynomial being less than <span class=\"math-container\" id=\"3961\">0</span> indicate complex roots?', 'Tags': ['algebra-precalculus', 'polynomials', 'roots', 'symmetric-polynomials'], 'AcceptedAnswerId': '670', 'urls': ['http://en.wikipedia.org/wiki/Discriminant', 'http://en.wikipedia.org/wiki/Cubic_equation#The_nature_of_the_roots'], 'exp': ['\\\\Delta = 18abcd - 4b^3d + b^2 c^2 - 4ac^3 - 27a^2d^2', 'ax^3 + bx^2 + cx+ d', '\\\\Delta &gt; 0', '\\\\Delta &lt; 0', '\\\\Delta &lt; 0', '\\\\Delta', '0', '0'], 'Body': 'The discriminant <math_exp> of the cubic polynomial <math_exp>  vanishes, but also that there are three distinct, real roots if <math_exp>, and that there is one real root and two complex roots (complex conjugates) if <math_exp>. Why does <math_exp> indicate complex roots? I understand that because of the way that the discriminant is defined, it indicates that there is a repeated root if it vanishes, but why does <math_exp> greater than <math_exp> or less than <math_exp> have special meaning, too? '}\n",
      "433 {'Id': '433', 'Type': 'answer', 'ParentId': '432', 'urls': ['http://en.wikipedia.org/wiki/Discriminant#Discriminant_of_a_polynomial'], 'exp': ['\\\\prod_{i \\\\neq j} (x_i - x_j)^2', 'C', 'a_0^{2n-2}', 'a_0', 'n'], 'Body': 'The discriminant of any monic polynomial is the product <math_exp> of the squares of the differences of the roots (in an algebraic closure, e.g. <math_exp>). Cf. the Wikipedia article on this. Consequently, if the roots are all real and distinct, this must be positive. (If the polynomial is not monic, the factor <math_exp> is thrown in, for <math_exp> the leading coefficient and <math_exp> the degree; this is positive for a real polynomial.) '}\n",
      "434 {'Id': '434', 'Type': 'question', 'Title': 'How can there be explicit polynomial equations for which the existence of integer solutions is unprovable?', 'Tags': ['logic', 'diophantine-equations'], 'AcceptedAnswerId': '437', 'urls': ['https://math.stackexchange.com/questions/250/a-challenge-by-r-p-feynman-give-counter-intuitive-theorems-that-can-be-transl/260#260'], 'exp': [], 'Body': 'This answer suggests that there are explicit polynomial equations for which the existence  (or nonexistence) of integer solutions is unprovable. How can this be? '}\n",
      "437 {'Id': '437', 'Type': 'answer', 'ParentId': '434', 'urls': ['https://mathoverflow.net/questions/32892/does-anyone-know-a-polynomial-whose-lack-of-roots-cant-be-proved'], 'exp': ['\\\\phi(x_1, \\\\dots, x_n)', 'n', 'P(x_1, \\\\dots, x_n, z_1, \\\\dots, z_m)', 'z_1, \\\\dots, z_m', '\\\\phi(x_1, \\\\dots, x_n)', 'P(z_1, \\\\dots, z_m)'], 'Body': 'Edit: This has been explained better by Joel David Hamkins; cf. this MO thread. This is because given any first-order formula <math_exp> with <math_exp> parameters in the integers, there is a polynomial <math_exp> which can be proved to have a root in <math_exp> in the integers if and only if <math_exp>---this is a version of Matiyasevich\\'s solution to Hilbert\\'s tenth problem. Now, it\\'s possible to construct within any formal system explicit first-order formulas which can\\'t be proved or disproved (for instance, the statement \"this formal system is consistent\").  This is  the second incompleteness theorem.  These first-order formulas correspond to polynomials by the first paragraph. In particular, one can show that you can construct a polynomial <math_exp> corresponding to the statement \"mathematics* is inconsistent\" translated to a formula.  Thus, if mathematics is consistent, there\\'s no mathematical proof that this polynomial doesn\\'t have a root. *Mathematics=ZFC here. For more on these lines, see Ebbinghaus-Frum-Thomas\\'s very accessible book on mathematical logic. '}\n",
      "438 {'Id': '438', 'Type': 'question', 'Title': 'Why <span class=\"math-container\" id=\"4002\">\\\\sqrt{-1 \\\\times {-1}} \\\\neq \\\\sqrt{-1}^2</span>?', 'Tags': ['algebra-precalculus', 'arithmetic', 'complex-numbers', 'fake-proofs'], 'AcceptedAnswerId': '439', 'urls': [], 'exp': [], 'Body': \"I know there must be something unmathematical in the following but I don't know where it is: \\\\begin{align} \\\\sqrt{-1} &amp;= i \\\\\\\\ \\\\\\\\ \\\\frac1{\\\\sqrt{-1}} &amp;= \\\\frac1i \\\\\\\\ \\\\\\\\ \\\\frac{\\\\sqrt1}{\\\\sqrt{-1}} &amp;= \\\\frac1i \\\\\\\\ \\\\\\\\ \\\\sqrt{\\\\frac1{-1}} &amp;= \\\\frac1i \\\\\\\\ \\\\\\\\ \\\\sqrt{\\\\frac{-1}1} &amp;= \\\\frac1i \\\\\\\\ \\\\\\\\ \\\\sqrt{-1} &amp;= \\\\frac1i \\\\\\\\ \\\\\\\\ i &amp;= \\\\frac1i \\\\\\\\ \\\\\\\\ i^2 &amp;= 1 \\\\\\\\ \\\\\\\\ -1 &amp;= 1 \\\\quad !!! \\\\end{align} \"}\n",
      "439 {'Id': '439', 'Type': 'answer', 'ParentId': '438', 'urls': ['https://math.stackexchange.com/questions/49169/i2-why-is-it-1-when-you-can-show-it-is-1'], 'exp': ['\\\\frac{\\\\sqrt{a}}{\\\\sqrt{b}}=\\\\sqrt{\\\\frac{a}{b}}', 'a\\\\ge 0', 'b&gt;0', '\\\\frac{\\\\sqrt{a}}{\\\\sqrt{b}}=\\\\sqrt{\\\\frac{a}{b}}', 'a\\\\ge 0', 'b&gt;0', '\\\\sqrt{x}', 'x', 'x&lt;0', 'i\\\\sqrt{|x|}', '-1=i^2=(\\\\sqrt{-1})^2=\\\\sqrt{-1}\\\\sqrt{-1}\\\\overset{!}{=}\\\\sqrt{-1\\\\cdot-1}=\\\\sqrt{1}=1', '\\\\sqrt{a}\\\\sqrt{b}=\\\\sqrt{ab}', 'a\\\\ge 0', 'b\\\\ge 0', 'a=b=-1'], 'Body': 'Between your third and fourth lines, you use <math_exp>.  This is only (guaranteed to be) true when <math_exp> and <math_exp>. edit:  As pointed out in the comments, what I meant was that the identity <math_exp> has domain <math_exp> and <math_exp>.  Outside that domain, applying the identity is inappropriate, whether or not it \"works.\" In general (and this is the crux of most \"fake\" proofs involving square roots of negative numbers), <math_exp> where <math_exp> is a negative real number (<math_exp>) must first be rewritten as <math_exp> before any other algebraic manipulations can be applied (because the identities relating to manipulation of square roots [perhaps exponentiation with non-integer exponents in general] require nonnegative numbers). This similar question, focused on <math_exp>, is using the similar identity <math_exp>, which has domain <math_exp> and <math_exp>, so applying it when <math_exp> is invalid. '}\n",
      "440 {'Id': '440', 'Type': 'question', 'Title': \"Why isn't reflexivity redundant in the definition of equivalence relation?\", 'Tags': ['elementary-set-theory', 'relations', 'equivalence-relations'], 'AcceptedAnswerId': '441', 'urls': [], 'exp': ['a', 'b', 'a R b', 'b R a', 'a R a'], 'Body': \"An equivalence relation is defined by three properties: reflexivity, symmetry and transitivity. Doesn't symmetry and transitivity implies reflexivity? Consider the following argument. For any <math_exp> and <math_exp>, <math_exp> implies <math_exp> by symmetry. Using transitivity, we have <math_exp>. Source: Exercise 8.46, P195 of Mathematical Proofs, 2nd (not 3rd) ed. by Chartrand et al \"}\n",
      "441 {'Id': '441', 'Type': 'answer', 'ParentId': '440', 'urls': [], 'exp': ['a', 'b', 'aRb'], 'Body': 'Actually, without the reflexivity condition,   the empty relation would count as an equivalence relation, which is non-ideal. Your argument used the hypothesis that for each <math_exp>, there exists <math_exp> such that <math_exp> holds. If this is true, then symmetry and transitivity imply reflexivity, but this is not true in general. '}\n",
      "442 {'Id': '442', 'Type': 'answer', 'ParentId': '440', 'urls': [], 'exp': [], 'Body': \"No. The missing condition is sometimes called  'seriality' -- for any x there must be an y such that x R y. If you add seriality to the symmetry and transitivity you get a reflexive relation again. \"}\n",
      "443 {'Id': '443', 'Type': 'question', 'Title': 'Why is the decimal representation of <span class=\"math-container\" id=\"4210\">\\\\frac17</span> \"cyclical\"?', 'Tags': ['number-theory', 'fractions', 'decimal-expansion'], 'AcceptedAnswerId': '445', 'urls': ['http://en.wikipedia.org/wiki/142857_(number)', 'http://en.wikipedia.org/wiki/Cyclic_number'], 'exp': ['\\\\frac17 = 0.(142857)', '7', '10', '7', '7', '10', '2\\\\cdot 5'], 'Body': '<math_exp>... with the digits in the parentheses repeating. I understand that the reason it\\'s a repeating fraction is because <math_exp> and <math_exp> are coprime.  But this...cyclical nature is something that is not observed by any other reciprocal of any natural number that I know of (besides multiples of <math_exp>). (if I am wrong, I hope that I may find others through this question) By \"cyclical,\" I mean: Where all of the repeating digits are the same string of digits, but shifted.  Not just a simple \"they are all the same digits re-arranged\", but the same digits in the same order, but shifted. Or perhaps more strikingly, from the wikipedia article: What is it about the number <math_exp> in relation to the base <math_exp> (and its prime factorization <math_exp>?) that allows its reciprocal to behave this way?  Is it (and its multiples) unique in having this property? Wikipedia has an article on this subject, and gives a form for deriving them and constructing arbitrary ones, but does little to show the \"why\", and finding what numbers have cyclic inverses. '}\n",
      "444 {'Id': '444', 'Type': 'answer', 'ParentId': '443', 'urls': ['http://en.wikipedia.org/wiki/Cyclic_number', 'http://en.wikipedia.org/wiki/Repeating_decimal'], 'exp': [], 'Body': 'It works with 1/19 = 0.(052631578947368421) too, while n/13 has two cycles: 1/13 = 0.(076923), 2/13 = 0.(153846), 3/13 = 0.(230769), 4/13 = 0.(307692), 5/13 = 0.(384615), and so on. That a cycle must appear when you have a prime number p different from the base in which we work (so in base 10 different from 2 and 5) is clear: if you perform the long division 1/p, sooner or later partial quotients must be repeated, and from that point on the quotients repeat themselves. The length of the cycle must be a divisor of p-1: it may be short (think at 1/11 = 0.(09) ) or have the maximum possible lenght like the cases of 7 and 19. Wikipedia has an article on Cyclic numbers, and some other example is also here;  unfortunately no sufficient rule is given for a number to have its inverse cyclical. '}\n",
      "445 {'Id': '445', 'Type': 'answer', 'ParentId': '443', 'urls': ['https://oeis.org/A001913'], 'exp': ['\\\\frac{1}{p}', 'p|(10^k-1)', 'k|(p-1)', 'k\\\\leq p-1', 'k=p-1', '\\\\frac{1}{p}'], 'Body': \"For a prime p, the length of the repeating block of <math_exp> is the least positive integer k for which <math_exp>.  As in mau's answer, <math_exp>, so <math_exp>.  When <math_exp>, then <math_exp> and its multiples behave as discussed in the question. Of the first 100 primes, this is true for 7, 17, 19, 23, 29, 47, 59, 61, 97, 109, 113, 131, 149, 167, 179, 181, 193, 223, 229, 233, 257, 263, 269, 313, 337, 367, 379, 383, 389, 419, 433, 461, 487, 491, 499, 503, 509, 541 (sequence A001913 in OEIS). (List generated in Mathematica using Select[Table[Prime[n], {n, 1, 100}], # - 1 == Length[RealDigits[1/#][[1]][[1]]]&amp;].) \"}\n",
      "446 {'Id': '446', 'Type': 'answer', 'ParentId': '418', 'urls': [], 'exp': ['(0,1) \\\\to [0,1]', '1', 'f\\\\left(\\\\frac{1}{2}\\\\right) = 0,\\\\quad f\\\\left(\\\\frac{1}{3}\\\\right) = 1,\\\\quad f\\\\left(\\\\frac{1}{n}\\\\right) = \\\\frac{1}{n-2}\\\\ \\\\textrm{for}\\\\ n &gt; 3,', 'f(x) = x\\\\ \\\\textrm{for}\\\\ x\\\\ \\\\textrm{not equal to a reciprocal of an integer}', '\\\\mathbb{R}', '\\\\mathbb{R}\\\\setminus S', 'S', 'S', 's_1, s_2, s_3, \\\\dots', 't_1, t_2, \\\\dots', '\\\\mathbb{R}', 'S', 'g(s_i) = t_{2i},\\\\quad g(t_i) = t_{2i+1},\\\\quad g(x) = x\\\\ (\\\\textrm{otherwise}).', '\\\\mathbb{R}', '\\\\mathbb{R}\\\\setminus S'], 'Body': 'An explicit bijection <math_exp> for part <math_exp> is given by: <math_exp> <math_exp> For a bijection <math_exp> to <math_exp>, we can number the elements of <math_exp> (because <math_exp> is countable) as <math_exp> Choose an infinite sequence <math_exp> of distinct elements of <math_exp>, none of which are in <math_exp>. Then define: <math_exp> This is a bijection from <math_exp> to <math_exp>. '}\n",
      "448 {'Id': '448', 'Type': 'question', 'Title': 'In how many different ways can I sort balls of two different colors', 'Tags': ['combinatorics'], 'AcceptedAnswerId': '452', 'urls': [], 'exp': [], 'Body': \"Let's say, I have 4 yellow and 5 blue balls. How do I calculate in how many different orders I  can place them? And what if I also have 3 red balls? \"}\n",
      "449 {'Id': '449', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://www.catonmat.net/blog/using-fibonacci-numbers-to-convert-from-miles-to-kilometers', 'http://mathworld.wolfram.com/GoldenRatio.html'], 'exp': [], 'Body': \"Perhaps it's not an entirely practical application, but Fibonacci numbers can be used to convert from miles to kilometers and vice versa: Take two consecutive Fibonacci   numbers, for example 5 and 8. And   you're done converting. No kidding –   there are 8 kilometers in 5 miles. To   convert back just read the result from   the other end - there are 5 miles in 8   km! But why does it work? Fibonacci numbers have a property that   the ratio of two consecutive numbers   tends to the Golden ratio as   numbers get bigger and bigger. The   Golden ratio is a number and it   happens to be approximately 1.618. Coincidentally, there are 1.609   kilometers in a mile, which is within   0.5% of the Golden ratio. \"}\n",
      "450 {'Id': '450', 'Type': 'question', 'Title': 'Can non-linear transformations be represented as Transformation Matrices?', 'Tags': ['linear-algebra', 'transformation'], 'urls': [], 'exp': ['x', 'y', 'r', '\\\\theta', 'r', 'r', '\\\\theta', 'y'], 'Body': \"I just came back from an intense linear algebra lecture which showed that linear transformations could be represented by transformation matrices; with more generalization, it was later shown that affine transformations (linear + translation) could be represented by matrix multiplication as well. This got me to thinking about all those other transformations I've picked up over the past years I've been studying mathematics.  For example, polar transformations -- transforming <math_exp> and <math_exp> to two new variables <math_exp> and <math_exp>. If you mapped <math_exp> to the <math_exp> axis and <math_exp> to the <math_exp> axis, you'd basically have a coordinate transformation.  A rather warped one, at that. Is there a way to represent this using a transformation matrix?  I've tried fiddling around with the numbers but everything I've tried to work with has fallen apart quite embarrassingly. More importantly, is there a way to, given a specific non-linear transformation, construct a transformation matrix from it? \"}\n",
      "451 {'Id': '451', 'Type': 'answer', 'ParentId': '448', 'urls': [], 'exp': [], 'Body': \"The case of two colors is simple: if you have m yellow balls and n blue ones you only need to choose m positions among (m+n) possibilities, that is (m+n)!/(m!&middot;n!). The other balls' positions are automatically set up. \"}\n",
      "452 {'Id': '452', 'Type': 'answer', 'ParentId': '448', 'urls': ['http://en.wikipedia.org/wiki/Combination'], 'exp': ['(4 + 5)! = 9!', '9', '8', '4!', '5!', '\\\\text{total arrangements} = \\\\frac{\\\\text{arrangements of all balls}}{\\\\text{arrangements of yellow balls} \\\\times \\\\text{arrangements of blue balls}}', '\\\\text{total arrangements} = \\\\frac{9!}{5! \\\\times 4!} = 126'], 'Body': \"This is a standard problem involving the combinations of sets, though perhaps not very obvious intuitively. Firstly consider the number of ways you can rearrange the entire set of balls, counting each ball as indepndent (effectively ignoring colours for now). This is simply <math_exp>, since the 1st ball can be any of the <math_exp>, the 2nd can be any of the remaining <math_exp>, and so on. Then we calculate how many different ways the yellow balls can be arranged within themselves, since for the purpose of this problem they are considered equivalent. The number of combinations is of course <math_exp>; similarly, for the blue balls the number is <math_exp>. Hence, overall we find: <math_exp> Therefore in our case we have: <math_exp> I'm sure you can see how this can be easily extended if we also have 3 red balls too. (Hint: the total changes and we have another multiple of identical arrangements to account for.) \"}\n",
      "453 {'Id': '453', 'Type': 'answer', 'ParentId': '450', 'urls': [], 'exp': [], 'Body': \"No.  Everything is determined by a choice of basis.  For a more in-depth answer, I would need to explain the first two weeks of linear algebra and draw some commutative diagrams. If you'd like a better explanation, see pages 12-14 of Emil Artin's monograph Geometric Algebra. \"}\n",
      "454 {'Id': '454', 'Type': 'answer', 'ParentId': '356', 'urls': ['http://en.wikipedia.org/wiki/The_art_of_computer_programming'], 'exp': [], 'Body': \"(Donald Knuth) The legendary book (of multiple volumes, still incomplete) can't go without mention. For learning about algorithms and their complexities, there is no rival. It's written with practicality in mind, though from a largely theoretical perspective.  \"}\n",
      "455 {'Id': '455', 'Type': 'answer', 'ParentId': '450', 'urls': [], 'exp': ['3\\\\text{D}', '3\\\\times3', '3\\\\text{D}', '3\\\\text{D}', '4\\\\times4', '4\\\\times4', '3\\\\text{D}'], 'Body': \"You can't represent a non linear transformation with a matrix, however there are some tricks (for want of a better word) available if you use homogenous co-ordinates. For example, <math_exp> translation is a non-linear transformation in a <math_exp> <math_exp> transformation matrix, but is a linear transformation in <math_exp> homogenous co-ordinates using a <math_exp> transformation matrix. The same is true of other things like perspective projections. This is why <math_exp> matrices are used in <math_exp> graphics as the homogenous co-ordinate system simplifies things a lot. To clarify - using homogenous co-ordinates increases the range of transformations representable using matrices from plain linear transformations to affine transformations and some projections, but it doesn't make all non-linear transformations representable using matrices. The non-linear transformation provided as an example is still beyond representation as an affine transformation (Thanks to @Harry for prompting this clarification in the comments) \"}\n",
      "456 {'Id': '456', 'Type': 'answer', 'ParentId': '450', 'urls': [], 'exp': [], 'Body': \"As Harry says, you can't (the example of affine transformations can be tweaked to work because they're just linear ones with the origin translated). However, approximating a nonlinear function by a linear one is something we do all the time in calculus through the derivative, and is what we often have to do to make a mathematical model of some real-world phenomenon tractable. \"}\n",
      "457 {'Id': '457', 'Type': 'question', 'Title': 'Good Physical Demonstrations of Abstract Mathematics', 'Tags': ['soft-question', 'big-list', 'physics', 'education'], 'urls': [], 'exp': ['\\\\sum \\\\frac{1}{n^2} = \\\\frac{\\\\pi^2}{6}', 'i^{th}', 'A_i', 'n_i', '\\\\sum A_i \\\\cdot n_i = 0', 'i_th', 'A_i \\\\cdot n_i', 'SO(3)', 'SU(2)'], 'Body': 'I like to use physical demonstrations when teaching mathematics (putting physics in the service of mathematics, for once, instead of the other way around), and it\\'d be great to get some more ideas to use. I\\'m looking for nontrivial ideas in abstract mathematics that can be demonstrated with some contraption, construction or physical intuition. For example, one can restate Euler\\'s proof that <math_exp> in terms of the flow of an incompressible fluid with sources at the integer points in the plane. Or, consider the problem of showing that, for a convex polyhedron whose <math_exp> face has area <math_exp> and outward facing normal vector <math_exp>, <math_exp>. One can intuitively show this by pretending the polyhedron is filled with gas at uniform pressure. The force the gas exerts on the <math_exp> face is proportional to <math_exp>, with the same proportionality for every face. But the sum of all the forces must be zero; otherwise this polyhedron (considered as a solid) could achieve perpetual motion. For an example showing less basic mathematics, consider \"showing\" the double cover of <math_exp> by <math_exp> by needing to rotate your hand 720 degrees to get it back to the same orientation. Anyone have more demonstrations of this kind? '}\n",
      "458 {'Id': '458', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://library.thinkquest.org/27890/applications5.html', 'http://www.popmath.org.uk/rpamaths/rpampages/sunflower.html', 'http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/'], 'exp': [], 'Body': 'They appear in pattern formation, for instance in the numbers of petals on a plant, the segements on a pineapple or pinecone, and the structure of nautilus shells. http://library.thinkquest.org/27890/applications5.html http://www.popmath.org.uk/rpamaths/rpampages/sunflower.html http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/ '}\n",
      "460 {'Id': '460', 'Type': 'question', 'Title': \"Are there any interesting semigroups that aren't monoids?\", 'Tags': ['big-list', 'abstract-algebra', 'semigroups', 'monoid'], 'AcceptedAnswerId': '467', 'urls': [], 'exp': ['(X, \\\\ast)', '(X, \\\\ast, e)', 'e', 'X'], 'Body': \"Are there any interesting and natural examples of semigroups that are not monoids (that is, they don't have an identity element)? To be a bit more precise, I guess I should ask if there any interesting examples of semigroups <math_exp> for which there is not a monoid <math_exp> where <math_exp> is in <math_exp>. I don't consider an example like the set of real numbers greater than 10 (considered under addition) to be a sufficiently 'natural' semigroup for my purposes; if the domain can be extended in an obvious way to include an identity element then that's not what I'm after. \"}\n",
      "461 {'Id': '461', 'Type': 'answer', 'ParentId': '448', 'urls': [], 'exp': [], 'Body': 'For some reason I find it easier to think in terms of letters of a word being rearranged, and your problem is equivalent to asking how many permutations there are of the word YYYYBBBBB. The formula for counting permutations of words with repeated letters (whose reasoning has been described by Noldorin) gives us the correct answer of 9!/(4!5!) = 126. '}\n",
      "462 {'Id': '462', 'Type': 'question', 'Title': 'How do proof verifiers work?', 'Tags': ['logic', 'math-software', 'theorem-provers'], 'AcceptedAnswerId': '463', 'urls': ['http://en.wikipedia.org/wiki/HOL_Light', 'http://en.wikipedia.org/wiki/Calculus_of_constructions', 'http://us.metamath.org/'], 'exp': [], 'Body': \"I'm currently trying to understand the concepts and theory behind some of the common proof verifiers out there, but am not quite sure on the exact nature and construction of the sort of systems/proof calculi they use. Are they essentially based on higher-order logics that use Henkin semantics, or is there something more to it? As I understand, extending Henkin semantics to higher-order logic does not render the formal system any less sound, though I am not too clear on that. Though I'm mainly looking for a general answer with useful examples, here are a few specific questions: The Wikipedia pages on proof verification programs such as HOL Light Coq, and Metamath give some idea, but these pages contain limited/unclear information, and there are rather few specific high-level resources elsewhere. There are so many variations on formal logics/systems used in proof theory that I'm not sure quite what the base ideas of these systems are - what is required or optimal and what is open to experimentation. Perhaps a good way of answering this, certainly one I would appreciate, would be a brief guide (albeit with some technical detail/specifics) on how one might go about generating a complete proof calculus (proof verification system) from scratch? Any other information in the form of explanations and examples would be great too, however. \"}\n",
      "463 {'Id': '463', 'Type': 'answer', 'ParentId': '462', 'urls': [], 'exp': [], 'Body': \"I'll answer just part of your question: I think the other parts will become clearer based on this. A proof verifier is essentially a program that takes one argument, a proof representation, and checks that this is properly constructed, and says OK if it is, and either fails silently otherwise, or highlights what is invalid otherwise. In principle, the proof representation could just be a sequence of formulae in a Hilbert system: all logics (at least, first-orderisable logics) can be represented in such a way.  You don't even need to say which rule is specified at each step, since it is decidable whether any formula follows by a rule application from earlier formulae. In practice, though, the proof representations are more complex.  Metamath is rather close to Hilbert systems, but has a rich set of rules.  Coq and LF use (different) typed lambda calculi with definitions to represent the steps, which are computationally quite expensive to check (IIRC, both are PSPACE hard).  And the proof verifier can do much more: Coq allows ML programs to be extracted from proofs. \"}\n",
      "465 {'Id': '465', 'Type': 'answer', 'ParentId': '83', 'urls': ['http://scratch.mit.edu/', 'http://en.wikipedia.org/wiki/Mindstorms%3a_Children,_Computers,_and_Powerful_Ideas'], 'exp': [], 'Body': 'Seymour Papert\\'s ideas about \"maths you can build\".  Look at Scratch (much inspired by Papert), and read Mindstorms; note that quite a few of his ideas there are contentious, but there is a lot of maths you can make exciting this way. '}\n",
      "467 {'Id': '467', 'Type': 'answer', 'ParentId': '460', 'urls': ['http://planetmath.org/encyclopedia/Folded.html', 'http://en.wikipedia.org/wiki/Dirac_delta_function'], 'exp': [], 'Body': 'Convolution of functions/distributions is useful in a variety of fields, and the identity element, the dirac delta, is not strictly a function. '}\n",
      "468 {'Id': '468', 'Type': 'answer', 'ParentId': '460', 'urls': [], 'exp': ['lim_{x\\\\to \\\\infty}f(x) = 0', 'f(x) \\\\neq 0'], 'Body': 'Let G be the set of (continuous) functions f: R -> R where f(x) tends to 0 as x tends to infinity: <math_exp>. The operator is the usual point-wise multiplication of functions. G is closed under * since lim(f(x)) = 0 and lim(g(x)) = 0 imply lim(f(x)*g(x)) = 0.  G is a subgroup of {f: R->R}, so the identity must be the same - the function which is constantly 1. But this identity is not in G. EDIT: As Harry correctly points out, {f: R->R} is not a group. Therefore the following correction is needed: consider only functions such that <math_exp> everywhere. '}\n",
      "469 {'Id': '469', 'Type': 'question', 'Title': 'What is the meaning of the double turnstile symbol (<span class=\"math-container\" id=\"4429\">\\\\models</span>)?', 'Tags': ['logic', 'notation'], 'AcceptedAnswerId': '478', 'urls': [], 'exp': ['\\\\models'], 'Body': \"What's the meaning of the double turnstile symbol in logic or mathematical notation? : <math_exp> \"}\n",
      "470 {'Id': '470', 'Type': 'answer', 'ParentId': '469', 'urls': ['http://en.wikipedia.org/wiki/Entailment'], 'exp': [], 'Body': 'It is a symbol from model theory denoting entailment. A ⊧ B is read as \"A entails B\". '}\n",
      "472 {'Id': '472', 'Type': 'answer', 'ParentId': '469', 'urls': ['http://en.wikipedia.org/wiki/Double_turnstile'], 'exp': [], 'Body': \"It is called a 'double turnstile': http://en.wikipedia.org/wiki/Double_turnstile. \"}\n",
      "473 {'Id': '473', 'Type': 'answer', 'ParentId': '469', 'urls': ['http://planetmath.org/encyclopedia/SatisfactionRelation.html', 'http://www.trinity.edu/cbrown/topics_in_logic/struct/node2.html'], 'exp': ['\\\\models', '\\\\mathcal{M}=(M,I)', '\\\\mathcal{M}', '\\\\nu', '(\\\\mathcal{M},\\\\nu)\\\\models \\\\varphi', '\\\\varphi', '\\\\nu'], 'Body': '<math_exp> is also known as the satisfication relation. For a structure <math_exp> and an <math_exp>-assignment <math_exp>, <math_exp> means that the formula <math_exp> is true with the particular assignment <math_exp>. See http://www.trinity.edu/cbrown/topics_in_logic/struct/node2.html '}\n",
      "474 {'Id': '474', 'Type': 'answer', 'ParentId': '460', 'urls': [], 'exp': [], 'Body': 'Finite sets of matrices of varying dimensions, where the product A*B={PQ|P in A &amp; Q in B &amp; dim(Q)=codim(P)}, and dim &amp; codim are the dimensions of the source &amp; target spaces of a matrix. The infinite case has an obvious unit. '}\n",
      "475 {'Id': '475', 'Type': 'answer', 'ParentId': '457', 'urls': [], 'exp': [], 'Body': 'The book \"The Mathematical Mechanic\" by Mark Levi is a very good source of such examples, which Levi has been collecting for some time. The first two here are in the book, if I recall correctly. '}\n",
      "476 {'Id': '476', 'Type': 'question', 'Title': 'Looking for a book similar to \"Think of a Number\"', 'Tags': ['reference-request', 'soft-question', 'big-list', 'book-recommendation'], 'urls': [], 'exp': [], 'Body': 'Many years ago, I had read a book entitled \"Think of a Number\" by Malcolm E. Lines, and it was an eminently readable and thought provoking book. In the book, there were topics like Fibonacci numbers (along with the live examples from the nature) and Golden Section. Now I\\'m looking for a similar book. Can anyone recommend me one? '}\n",
      "477 {'Id': '477', 'Type': 'question', 'Title': 'Cardinality of set of real continuous functions', 'Tags': ['real-analysis', 'elementary-set-theory', 'cardinals'], 'AcceptedAnswerId': '479', 'urls': ['http://en.wikipedia.org/wiki/Cardinality_of_the_continuum'], 'exp': ['\\\\mathbb{R\\\\to R}', '\\\\mathbb R^n'], 'Body': 'The set of all <math_exp> continuous functions is . How to show that? Is there any bijection between <math_exp> and the set of continuous functions? '}\n",
      "478 {'Id': '478', 'Type': 'answer', 'ParentId': '469', 'urls': ['http://en.wikipedia.org/wiki/Sentential_calculus'], 'exp': ['\\\\vdash', '\\\\wedge', '\\\\vdash', '\\\\wedge', '\\\\models', '\\\\Gamma', '\\\\Gamma', '\\\\models', '\\\\Gamma \\\\vdash A \\\\iff \\\\Gamma \\\\models A', 'A \\\\vdash B \\\\Rightarrow A \\\\models B', 'A \\\\models B \\\\Rightarrow A \\\\vdash B', '\\\\models', '\\\\vdash'], 'Body': 'Just to enlarge on Harry\\'s answer: Your symbol denotes one of two specified notions of implication in formal logic <math_exp> -the turnstile symbol denotes syntactic implication (syntactic here means  related to syntax, the structure of a sentence), where the \\'algebra\\' of the logical system in play (for example sentential calculus) allows us to \\'rearrange and cancel\\' the stuff we know on the left into the thing we want to prove on the right. An example might be the classic \"all men are mortal <math_exp> socrates is a man <math_exp> socrates is mortal\" (\\'<math_exp>\\' of course here just means \\'and\\'). You can almost imagine cancelling out the \\'man bit\\' on the left to just give the sentence on the right (although the truth may be more complex...). <math_exp> -the double turnstile, on the other hand, is not so much about algebra as meaning (formally it denotes semantic implication)- it means that any interpretation of the stuff we know on the left must have the corresponding interpretation of the thing we want to prove on the right true. An example would be if we had an infinite set of sentences: <math_exp>:= {\"1 is lovely\", \"2 is lovely\", ...} in which all numbers appear, and the sentence A= \" the natural numbers are precisely {1,2,...}\" listing all numbers. Any interpretation would give us B=\"all natural numbers are lovely\". So <math_exp>, A <math_exp> B. Now, the goal of any logician trying to set up a formal system is to have <math_exp>, meaning that the \\'algebra\\' must line up with the interpretation, and this is not something we can take as given. Take the second example above- can we be sure that algebraic operations can \\'parse\\' those infinitely many sentences and make the simple sentence on the right?? (this is to do with a property called compactness) The goal can be split into two distict subgoals: Soundness: <math_exp> Completeness: <math_exp> Where the first stops you proving things that aren\\'t true when we interpret them and the second means that everything we know to be true on interpretation, we must be able to prove. Sentential calculus, for example, can be proved complete (and was in Godel\\'s lesser known, but celebrated completeness theorem), but other for other systems Godel\\'s incompleteness theorem, give us a terrible choice between the two. In summary: The interplay of meaning and axiomatic machine mathematics, captured by the difference between <math_exp> and <math_exp>, is a subtle and interesting thing. '}\n",
      "479 {'Id': '479', 'Type': 'answer', 'ParentId': '477', 'urls': ['http://en.wikipedia.org/wiki/Cantor%E2%80%93Bernstein%E2%80%93Schroeder_theorem'], 'exp': ['R^{N}', '.a_1a_2..., .b_1b_2..., .c_1c_2...', '.a_1 b_1 a_2 c_1 b_2 a_3...'], 'Body': 'The cardinality is at least that of the continuum because every real number corresponds to a constant function.  The cardinality is at most that of the continuum because the set of real continuous functions injects into the sequence space <math_exp> by mapping each continuous function to its values on all the rational points. Since the rational points are dense, this determines the function. The Schroeder-Bernstein theorem now implies the cardinality is precisely that of the continuum. Note that  then the set of sequences of reals is also of the same cardinality as the reals.  This is because if we have a sequence of binary representations <math_exp>, we can splice them together via <math_exp> so that a sequence of reals can be encoded by one real number. '}\n",
      "480 {'Id': '480', 'Type': 'answer', 'ParentId': '457', 'urls': [], 'exp': ['S^2'], 'Body': 'There is the hairy ball theorem, which states that no even dimensional sphere admits a nowherevanishing continuous vector field.  In the case of <math_exp>, the physical demonstration of this is that one cannot comb the hair of a ball without getting a cowlick. '}\n",
      "481 {'Id': '481', 'Type': 'answer', 'ParentId': '476', 'urls': [], 'exp': [], 'Body': 'did you try the books of Eastaway and Wyndham? Why Do Buses Come in Threes? and How Long Is a Piece of String? Blurb for the first one says \"An amusing explanation of how maths is relevant to almost everything in life. Citing many examples of the way mathematics can explain common phenomena\"; for the second, \"This title is for anyone wanting to remind themselves - or discover for the first time - that maths is relevant to almost everything we do. Dating, cooking, travelling by car, gambling and ranking sportsmen all have links with intriguing mathematical problems that are explained in this book\". '}\n",
      "482 {'Id': '482', 'Type': 'answer', 'ParentId': '438', 'urls': ['https://math.stackexchange.com/questions/438/1-is-not-1-so-where-is-the-mistake/439#439'], 'exp': ['\\\\begin{align*} \\\\sqrt{-1} &amp;= \\\\hat\\\\imath &amp; \\\\mathrm{LHS}&amp;=i, \\\\mathrm{RHS}=i \\\\\\\\ 1/\\\\sqrt{-1} &amp;= 1/\\\\hat\\\\imath &amp; \\\\mathrm{LHS}&amp;=1/i=-i, \\\\mathrm{RHS}=-i \\\\\\\\ \\\\sqrt{1}/\\\\sqrt{-1} &amp;= 1/\\\\hat\\\\imath &amp; \\\\mathrm{LHS}&amp;=1/i=-i, \\\\mathrm{RHS}=-i \\\\\\\\ \\\\textstyle\\\\sqrt{1/-1} &amp;= 1/\\\\hat\\\\imath &amp; \\\\mathrm{LHS}&amp;=\\\\sqrt{-1}=i, \\\\mathrm{RHS}=-i \\\\end{align*}', '\\\\textstyle\\\\sqrt{1}/\\\\sqrt{-1}=\\\\sqrt{1/-1}'], 'Body': \"Isaac's answer is correct, but it can be hard to see if you don't have a strong knowledge of your laws. These problems are generally easy to solve if you examine it line by line and simplify both sides. <math_exp> We can then see that the error must be assuming <math_exp>. \"}\n",
      "483 {'Id': '483', 'Type': 'answer', 'ParentId': '329', 'urls': ['http://rads.stackoverflow.com/amzn/click/038797329X'], 'exp': [], 'Body': 'A Classical Introduction to Modern Number Theory by Ireland and Rosen  hands down! '}\n",
      "484 {'Id': '484', 'Type': 'answer', 'ParentId': '164', 'urls': [], 'exp': ['\\\\int 4\\\\pi\\\\cdot r^2 dr = 4/3 \\\\pi r^3.'], 'Body': 'Integration. <math_exp> '}\n",
      "486 {'Id': '486', 'Type': 'question', 'Title': 'Software for solving geometry questions', 'Tags': ['algebraic-geometry', 'logic', 'euclidean-geometry', 'math-software', 'quantifier-elimination'], 'AcceptedAnswerId': '495', 'urls': [], 'exp': [], 'Body': \"When I used to compete in Olympiad Competitions back in high school, a decent number of the easier geometry questions were solvable by what we called a geometry bash. Basically, you'd label every angle in the diagram with the variable then use a limited set of basic geometry operations to find relations between the elements, eliminate equations and then you'd eventually get the result. It seems like the kind of thing you could program a computer to do. So, I'm curious, does there exist any software to do this? I know there is lots of software for solving equations, but is there anything that lets you actually input a geometry problem without manually converting to equations? I'm not looking for anything too advance, even seeing just an attempt would be interesting. If there is anything decent, I think it'd be rather interesting to run the results on various competitions and see how many of the questions it solves. \"}\n",
      "493 {'Id': '493', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://amathew.wordpress.com/'], 'exp': [], 'Body': 'http://amathew.wordpress.com/ He climbs Mount Bourbaki with the grace of a leaping gazelle. '}\n",
      "494 {'Id': '494', 'Type': 'answer', 'ParentId': '476', 'urls': ['http://rads.stackoverflow.com/amzn/click/0198516231'], 'exp': [], 'Body': \"I don't know the book you are speaking of, but for some reason I am reminded of 1089, and all that by David Acheson.  This is a charming book about some mathematical curiosities and can be appreciated by the professional and enjoyed by the layperson. \"}\n",
      "495 {'Id': '495', 'Type': 'answer', 'ParentId': '486', 'urls': ['http://woody.cs.wichita.edu/', 'http://woody.cs.wichita.edu/', 'http://woody.cs.wichita.edu/help/pdf/', 'http://woody.cs.wichita.edu/help/atp.html'], 'exp': ['AD', '\\\\angle BAC', '\\\\triangle ABC', 'XYZ', '\\\\triangle XYZ', 'XY', 'XY', '\\\\frac{AB}{AC} = \\\\frac{BD}{DC}', 'ABC', 'AD', 'L', 'A,B,C,D', '\\\\frac{AB}{AC} \\\\frac{DC}{BD} = 1', '\\\\frac{AB}{AC} \\\\frac{DC}{BD} = \\\\frac{AB}{AC} \\\\frac{ACD}{ABD}', '=\\\\frac{AB}{AC} \\\\frac{\\\\frac{1}{2} AC\\\\cdot AD \\\\sin \\\\angle CAD}{\\\\frac{1}{2} AB\\\\cdot AD \\\\sin \\\\angle BAD}', 'D', '=\\\\frac{AB}{AC} \\\\frac{AC}{AB} =1'], 'Body': 'This is just a special case of automated theorem proving. A nice thing is some geometry problem can indeed be solved by an algorithm. There are theories show such system can be realized algorithmically. I don\\'t know if anyone have really wrote the program to do it. JGEX seems to do that. A mechanical geometry proof technique was popularized in China by Jingzhong Zhang. He first introduced it as a way for machines to solve geometric problems relating the proportions between areas, lengths or angles. Then some Olympiad people I know start using it to bash that kind of problem. I don\\'t know what the name is in English, but a literal translation of the method is \"point removal method\". Although it\\'s not exactly same as what you are talking about, because \"input a geometry problem\" requires you to provide the construction of the problem from a straight edge and a compass, which is almost like \"manually converting to equations\". the basic idea: Construct the original problem by compass and straightedge, make a list of every constructed point ordered by the order of construction, let it be L. Record which points are used in the construction of every point.(Only the points used to construct point P are used in the substitution steps 3 and 4) Translate the theorem into a equivalent form. Usually a/b = 1, where a and b are functions of length and area of certain segments or triangles. Let\\'s call this equation E. Let P be the last point of L. For every P appears in E, substitute it with another relation using other points from L(P itself is also allowed), usually if we are proving about lengths, we might use area. A list of possible operations are required for this step. It can branches off as a proof tree when the program decide to use different substitutions. Do another substitution that eliminates the point P. For example, if in the step before, we substitute length to area, then we want to find something involve the length. Do 3 to 4 over and over until we have 1=1 An example: Angle bisector theorem Given: <math_exp> is the angle bisector of <math_exp> of triangle <math_exp>. Let <math_exp> be the area of triangle <math_exp>, and <math_exp> be the length of segment <math_exp>. Prove: <math_exp> Proof: First construct <math_exp>. Then construct <math_exp>. The points in the list <math_exp> are <math_exp>. The equivalent equation to the theorem is <math_exp> <math_exp> (substitute length with area) <math_exp>, this step successfully remove point <math_exp> by cancellation. <math_exp> This is only a non-formal explanation of how such automated system would work. I think the following book from Zhang will tell you more about it: Machine proofs in geometry: automated production of readable proofs for geometry theorems. I did not read the book, but the description of it seems like what you are seeking. A few paper by Zhang and his colleague can be found in the JGEX\\'s website. The JGEX documentation on it\\'s automated theorem prover is also a great resource. '}\n",
      "496 {'Id': '496', 'Type': 'answer', 'ParentId': '476', 'urls': ['http://rads.stackoverflow.com/amzn/click/0805062998'], 'exp': [], 'Body': \"The Number Devil may be something like what you're looking for. Young Robert's dreams have taken a decided turn for the weird. Instead of falling down holes and such, he's visiting a bizarre magical land of number tricks with the number devil as his host. Starting at one and adding zero and all the rest of the numbers, Robert and the number devil use giant furry calculators, piles of coconuts, and endlessly scrolling paper to introduce basic concepts of numeracy, from interesting number sequences to exponents to matrices. (It's not a watered-down kids' book, even though the description might suggest it.) \"}\n",
      "497 {'Id': '497', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://qchu.wordpress.com/', 'http://rigtriv.wordpress.com/'], 'exp': [], 'Body': 'I find Annoying Precision to be wonderfully readable, and has many many interesting topics. Additionally, Rigorous Trivialities is a bit higher level, but has a really useful intro to Algebraic Geometry. '}\n",
      "498 {'Id': '498', 'Type': 'answer', 'ParentId': '213', 'urls': [], 'exp': [], 'Body': 'Category theory and algebraic geometry. I spent a lot of time in undergrad studying things that were kinda nifty, but way too classical to be of any use/interest beyond \"fun math\". When I got to grad school, category theory was assumed and made some of my courses much harder than they should\\'ve been. In the words of Ravi Vakil, \"algebraic geometry should be learned slowly over a number of years\". I currently NEED algebraic geometry, so I don\\'t have this number of years. I wish I would\\'ve started that a long time ago. Additionally, both of these topics would\\'ve helped me learn the things I was thinking about anyways, in particular commutative algebra. '}\n",
      "499 {'Id': '499', 'Type': 'answer', 'ParentId': '476', 'urls': ['http://rads.stackoverflow.com/amzn/click/0393059456', 'http://rads.stackoverflow.com/amzn/click/0767908163'], 'exp': [], 'Body': \"Another more general pop math book covering mathematical curiosities is Coincidences, Chaos, and All that Math Jazz. It is at the level you're talking about. A more focused book on the golden ratio is that by Mario Livio. One nice feature of this second book is that he does a nice job of pointing out places where people believe in coincidences that aren't really there. He stays nicely objective, where many others fail to. \"}\n",
      "500 {'Id': '500', 'Type': 'answer', 'ParentId': '213', 'urls': [], 'exp': [], 'Body': \"I did mathematics as an undergrad, and I thought that differential equations were boring and pointless. Type of diff eq -> existence and uniqueness proofs for solutions -> rinse and repeat. Yawn. But now I find my lack of knowledge of differential equations is hampering my learning some interesting parts of physics that I'd like to know more about... \"}\n",
      "501 {'Id': '501', 'Type': 'question', 'Title': 'If all sets were finite, how could the real numbers be defined?', 'Tags': ['logic', 'set-theory', 'philosophy', 'proof-theory', 'finitism'], 'AcceptedAnswerId': '1878', 'urls': ['http://en.wikipedia.org/wiki/Constructivism_(mathematics)#Example_from_real_analysis', 'http://meta.math.stackexchange.com/questions/172/why-did-you-close-my-question-if-all-sets-were-finite'], 'exp': ['e', '\\\\sqrt{2}'], 'Body': \"An extreme form of constructivism is called finitisim.  In this form, unlike the standard axiom system, infinite sets are not allowed.  There are important mathematicians, such as Kronecker, who supported such a system.  I can see that the natural numbers and rational numbers can easily defined in a finitist system, by easy adaptations of the standard definitions.   But in order to do any significant mathematics, we need to have definitions for the irrational numbers that one is likely to encounter in practice, such as <math_exp>  or <math_exp>.  In the standard constructions, real numbers are defined as Dedekind cuts or Cauchy sequences, which are actually sets of infinite cardinality, so they are of no use here. My question is, how would a real number like those be defined in a finitist axiom system (Of course we have no hope to construct the entire set of real numbers, since that set is uncountably infinite). After doing a little research  I found a constructivist definition in Wikipedia http://en.wikipedia.org/wiki/Constructivism_(mathematics)#Example_from_real_analysis , but we need a finitist definition of a function for this definition to work (Because in the standard system, a function over the set of natural numbers is actually an infinite set). So my question boils down to this:  How can we define a function f over the natural numbers in a finitist axiom system? Original version of this question, which had been closed during private beta, is as follows: If all sets were finite, how would mathematics be like? If we replace the axiom that 'there   exists an infinite set' with 'all sets   are finite', how would mathematics be   like? My guess is that, all the theory   that has practical importance would   still show up, but everything would be   very very unreadable for humans. Is   that true? We would have the natural numbers,   athough the class of all natural   numbers would not be a set. In the   same sense, we could have the rational   numbers. But could we have the real   numbers? Can the standard   constructions be adapted to this   setting? \"}\n",
      "504 {'Id': '504', 'Type': 'answer', 'ParentId': '501', 'urls': ['http://en.wikipedia.org/wiki/Peano_axioms', 'http://en.wikipedia.org/wiki/G%C3%B6del_numbering', 'http://en.wikipedia.org/wiki/Goodstein%27s_theorem', 'http://en.wikipedia.org/wiki/Cauchy_sequence', 'http://en.wikipedia.org/wiki/Reverse_mathematics', 'http://www.andrew.cmu.edu/user/avigad/Papers/elementary.pdf', 'http://en.wikipedia.org/wiki/Analytic_number_theory'], 'exp': [], 'Body': 'There is a fragment of mathematics that is given by a set of axioms known as the Peano axioms. Using these rules you can carry out a vast amount of mathematics relating to natural numbers. For example you can prove lots of theorems in number theory using these axioms. The Peano axioms make no reference to sets at all, whether finite or infinite. The only things that exist in this theory are naturals. You can\\'t even form the set of all integers. You can only talk about the naturals themselves. So a vast amount of mathematics would work absolutely fine. Even though Peano\\'s axioms are about naturals, you can already use them to talk about finite sets. The idea is that any finite set could be encoded as a finite sequence of symbols which in turn could be represented as naturals using Godel numbering. So questions like \"is this set a subset of that one?\" could be turned into purely arithmetical statements about Godel numbers. So I\\'m pretty sure that declaring that there is no infinite set would make little difference to people working within the system defined by Peano\\'s axioms. We\\'d still have all of the natural numbers to work with, we just wouldn\\'t be able to assemble them into a single entity, the set of all natural numbers. On the other hand, there are theorems that make essential use of an infinite set. Like Goodstein\\'s theorem. Without infinite sets (or a substitute of some sort) it would be impossible to prove this result. So the overall result would be, I think, that you could still do lots of mathematics fine. The mathematics you could do wouldn\\'t be all that weird. And you\\'d simply be depriving yourself of a useful proof technique. By the way, you\\'d still be able to say many things about real numbers. A real number can be thought of as a Cauchy sequence. A Cauchy sequence is a certain type of sequence of rational numbers. So many statements about real numbers, when unpacked, are really statements about rational, and hence naturals, but in disguise. Update: Uncovering precisely what parts of mathematics you need in order to prove things is a field known as reverse mathematics. Hilbert, and others mathematicians, were interested in trying to prove as much mathematics as possible using finite methods. Although it was ultimately shown that you can\\'t carry out all mathematics using finite methods, it\\'s surprising how much you can. Here\\'s a paper that talks about a system called EA which has no infinite sets. Amazingly we can use results from analytic number theory in EA. This is because propositions about analytic functions can be interpreted as statements about natural numbers. '}\n",
      "505 {'Id': '505', 'Type': 'question', 'Title': 'Can there be two distinct, continuous functions that are equal at all rationals?', 'Tags': ['calculus', 'real-analysis'], 'AcceptedAnswerId': '507', 'urls': ['https://math.stackexchange.com/questions/477/cardinality-of-set-of-real-continuous-functions/479#479'], 'exp': [], 'Body': \"Akhil showed that the Cardinality of set of real continuous functions is the same as the continuum, using as a step the observation that continuous functions that agree at rational points must agree everywhere, since the rationals are dense in the reals. This isn't an obvious step, so why is it true? \"}\n",
      "506 {'Id': '506', 'Type': 'answer', 'ParentId': '505', 'urls': [], 'exp': ['f(x)', 'g(x)', '\\\\lim_{x \\\\to a} f(x) - g(x) = 0', 'a', '\\\\lim_{x \\\\to a} f(x) - g(x) = f(a) - g(a)', 'a', 'f(a) - g(a) = 0', 'f(x) = g(x)', 'f', 'g'], 'Body': 'If there were two continuous functions <math_exp> and <math_exp> that were equal at all rationals, then (because the rationals are dense) we can show that <math_exp> for all values of <math_exp> using a delta-epsilon proof. Since the difference of two continuous functions is continuous, we know <math_exp> for all <math_exp>, and therefore <math_exp> and <math_exp>, proving that <math_exp> and <math_exp> must be identical. '}\n",
      "507 {'Id': '507', 'Type': 'answer', 'ParentId': '505', 'urls': [], 'exp': ['f', 'g', 'f(x) = g(x)', 'x', 'c', 'c', '\\\\lim_{n \\\\to \\\\infty}x_{n}=c', 'f', 'g', '\\\\lim_{n \\\\to \\\\infty}f({x_{n}})=f({c})', '\\\\lim_{n \\\\to \\\\infty}g({x_{n}})=g({c})', 'x_n', 'f(x_n) = g(x_n)', 'n', 'f(c) = g(c)', 'c'], 'Body': 'Without resorting to &epsilon;-&delta; arguments: Let <math_exp> and <math_exp> be continuous real functions and <math_exp> for all rational <math_exp>.  For any real number <math_exp> (in particular, an irrational <math_exp>), there exists a Cauchy sequence of rational numbers  such that <math_exp>.  Since <math_exp> and <math_exp> are continuous, <math_exp> and <math_exp>.  Since <math_exp> is rational, <math_exp> for all <math_exp>, so the two limits must be equal and so <math_exp> for all real <math_exp>. '}\n",
      "508 {'Id': '508', 'Type': 'answer', 'ParentId': '486', 'urls': ['http://www.math.rutgers.edu/~zeilberg/PG/gt.html'], 'exp': [], 'Body': 'You might be interested in Doron Zeilberger\\'s website. He has a page entitled \"Plane Geometry: An Elementary Textbook (Circa 2050)\" where he envisioned a world in which computers can derive all of plane geometry without human intervention or interference. The accompanying Maple package proves many statements by computer. The page exists at http://www.math.rutgers.edu/~zeilberg/PG/gt.html. '}\n",
      "509 {'Id': '509', 'Type': 'answer', 'ParentId': '234', 'urls': [], 'exp': [], 'Body': 'This paradox has always interested me.  Something to think about is that there does not exist a uniform probability distribution over the positive real numbers (since they are infinite).  In arriving at your paradox, it seems you are assuming that any real number is equally likely, but this cannot be the case. '}\n",
      "510 {'Id': '510', 'Type': 'answer', 'ParentId': '234', 'urls': ['http://www.franzdietrich.net/Papers/DietrichList-TwoEnvelopeParadox.pdf'], 'exp': [], 'Body': 'This puzzle is known as the two envelope paradox. This paper contains a nice explanation of the two envelope paradox, and some references to further literature regarding the puzzle. '}\n",
      "511 {'Id': '511', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://wildaboutmath.com/', 'http://satmathblog.com/'], 'exp': [], 'Body': 'Wild about Math! and SatMathBlog (more for amateurs) '}\n",
      "513 {'Id': '513', 'Type': 'answer', 'ParentId': '1', 'urls': [], 'exp': [], 'Body': 'Infinity is an overloaded term that can mean many things. One common non-mathematical use of infinity is to refer to everything in the universe. This is not what mathematicians mean when they say infinity. That would be a kin to the set of all sets, which is a paradoxical concept that is not part of mathematical discourse. Mathematicians will use infinity as a way to represent a process that continues indefinitely. This is a kin to saying \"take the limit as n goes to infinity\", which is close to saying \"continue this process indefinitely.\" Infinity is also use infinity to talk about size. All sets are either infinite or finite. The story doesn\\'t stop there. There is something fundamentally different about sets like the points on a line, where there are no holes, and sets like the integers where there are holes. They are both infinite but one seems denser then the other. That\\'s where whole countable uncountable thing comes in. Infinite sets have a size, but it is not a number in the traditional sense. Its more like \"relative size\". Bijections are how we determine size for infinite sets, which are explained well on this page, so I won\\'t repeat the explanation. A more in-depth, but still understandable explanation is given in Computability and Logic by George Boolos. '}\n",
      "514 {'Id': '514', 'Type': 'question', 'Title': 'Conjectures that have been disproved with extremely large counterexamples?', 'Tags': ['big-list', 'conjectures', 'big-numbers'], 'AcceptedAnswerId': '1101', 'urls': [], 'exp': ['$n/2$', '$3n+1$', '$1$', '$1-4-2-1-4-2-1$', '$5.76 \\\\times 10^{18}$', '$1$'], 'Body': 'I just came back from my Number Theory course, and during the lecture there was mention of the Collatz Conjecture. I\\'m sure that everyone here is familiar with it; it describes an operation on a natural number – <math_exp> if it is even, <math_exp> if it is odd. The conjecture states that if this operation is repeated, all numbers will eventually wind up at <math_exp> (or rather, in an infinite loop of <math_exp>). I fired up Python and ran a quick test on this for all numbers up to <math_exp> (using the powers of cloud computing and dynamic programming magic).  Which is millions of millions of millions. And all of them eventually ended up at <math_exp>. Surely I am close to testing every natural number? How many natural numbers could there be?  Surely not much more than millions of millions of millions. (I kid.) I explained this to my friend, who told me, \"Why would numbers suddenly get different at a certain point?  Wouldn\\'t they all be expected to behave the same?\" To which I said, \"No, you are wrong!  In fact, I am sure there are many conjectures which have been disproved by counterexamples that are extremely large!\" And he said, \"It is my conjecture that there are none! (and if any, they are rare)\". Please help me, smart math people.  Can you provide a counterexample to his conjecture?  Perhaps, more convincingly, several?  I\\'ve only managed to find one! (Polya\\'s conjecture). One, out of the many thousands (I presume) of conjectures. It\\'s also one that is hard to explain the finer points to the layman. Are there any more famous or accessible examples? '}\n",
      "515 {'Id': '515', 'Type': 'answer', 'ParentId': '514', 'urls': ['http://en.wikipedia.org/wiki/P%C3%B3lya_conjecture', 'http://en.wikipedia.org/wiki/Mertens_conjecture', 'http://en.wikipedia.org/wiki/Skewes%27_number'], 'exp': [], 'Body': 'The wikipedia article on the Collatz conjecture gives these three examples of conjectures that were disproved with large numbers: Polya conjecture. Mertens conjecture. Skewes number. '}\n",
      "516 {'Id': '516', 'Type': 'answer', 'ParentId': '514', 'urls': ['http://en.wikipedia.org/wiki/Skewes%27_number'], 'exp': [], 'Body': \"The first example which came to my mind is the Skewes' number, that is the smallest natural number n for which &pi;(n) > li(n). Wikipedia states that now the limit is near e727.952, but the first estimation was much higher. \"}\n",
      "517 {'Id': '517', 'Type': 'answer', 'ParentId': '118', 'urls': [], 'exp': [], 'Body': 'Calculus is basically a way of calculating rates of changes (similar to slopes, but called derivatives in calculus), and areas, volumes, and surface areas (for starters). It\\'s easy to calculate these kinds of things with algebra and geometry if the shapes you\\'re interested in are simple.  For example, if you have a straight line you can calculate the slope easily.  But if you want to know the slope at an arbitrary point (any random point) on the graph of some function like x-squared or some other polynomial, then you would need to use calculus.  In this case, calculus gives you a way of \"zooming in\" on the point you\\'re interested in to find the slope EXACTLY at that point.  This is called a derivative. If you have a cube or a sphere, you can calculate the volume and surface area easily.  If you have an odd shape, you need to use calculus.  You use calculus to make an infinite number of really small slices of the object you\\'re interested in, determine the sizes of the slices, and then add all those sizes up.  This process is called integration.  It turns out that integration is the reverse of derivation (finding a derivative). In summary, calculus is a tool that lets you do calculations with complicated curves, shapes, etc., that you would normally not be able to do with just algebra and geometry. '}\n",
      "518 {'Id': '518', 'Type': 'question', 'Title': 'Why does a minimal prime ideal consist of zerodivisors?', 'Tags': ['commutative-algebra'], 'AcceptedAnswerId': '27734', 'urls': [], 'exp': ['A', 'P \\\\subset A', 'P', 'A', 'A_P', 'PA_P', 'P', 'A', 'PA_P', 'A_P', 'A_P'], 'Body': 'Let <math_exp> be a commutative ring. Suppose <math_exp> is a minimal prime ideal. Then it is a theorem that <math_exp> consists of zero-divisors. This can be proved using localization, when <math_exp> is noetherian: <math_exp> is local artinian, so every element of <math_exp> is nilpotent.  Hence every element of <math_exp> is a zero-divisor. (As Matt E has observed, when <math_exp> is nonnoetherian, one can still use a similar argument: <math_exp> is the only prime in <math_exp>, hence is the radical of <math_exp> by elementary commutative algebra.) Can this be proved without using localization? '}\n",
      "519 {'Id': '519', 'Type': 'question', 'Title': 'How do you prove that a group specified by a presentation is infinite?', 'Tags': ['group-theory', 'geometric-group-theory', 'group-presentation'], 'AcceptedAnswerId': '597', 'urls': [], 'exp': [' G = \\\\left\\\\langle x, y \\\\;  \\\\left| \\\\;  x^2 = y^3 = (xy)^7 = 1\\\\right. \\\\right\\\\rangle '], 'Body': \"The group: <math_exp> is infinite, or so I've been told. How would I go about proving this? (To prove finiteness of a finitely presented group, I could do a coset enumeration, but I don't see how this helps if I want to prove that it's infinite.) \"}\n",
      "521 {'Id': '521', 'Type': 'answer', 'ParentId': '11', 'urls': ['https://web.archive.org/web/20100725014132/http://www.google.com:80/buzz/114134834346472219368/RarPutThCJv/In-the-foundations-of-mathematics-the-standard', 'https://terrytao.files.wordpress.com/2011/06/blog-book.pdf'], 'exp': [], 'Body': 'Okay I burned a lot of reputation points (at least for me) on MathOverflow to gain clarity on how to give some intuition into this problem, so hopefully this answer will be at least be somewhat illuminating. To gain a deeper understanding of what is going on, first we need to answer the question, \"What is a number?\" There are a lot of ways to define numbers, but in general numbers are thought of as symbols that represent sets. This is easy for things like the natural numbers. So 10 would correspond to the set with ten things -- like a bag of ten stones. Pretty straight forward. The tricky part is that when we consider ten a subset of the real numbers, we actually redefine it. This is not emphasized even in higher mathematics classes, like real analysis; it just happens when we define the real numbers. So what is 10 when constructed in the real numbers? Well, at least with the Dedekind cut version of the real numbers, all real numbers correspond to a set with an infinite amount of elements. This makes 10 under the hood look drastically different, although in practice it operates exactly the same. So let\\'s return to the question: Why is 10 the same as 9.99999? Because the real numbers have this completely surprising quality, where there is no next real number. So when you have two real numbers that are as close together as possible, they are the same. I can\\'t think of any physical object that has this quality, but it\\'s how the real numbers work (makes \"real\" seem ironic). With integers (bag of stones version) this is not the same. When you have two integers as close to each other as possible they are still different, and they are distance one apart. Put another way, 10 bag of stones are not the same as 9.9999999 but 10 the natural number, where natural numbers are a subset of the real numbers is. The bottom line is that the real numbers have these tricky edge cases that are hard to understand intuitively. Don\\'t worry, your intuition is not really failing you. :) I didn\\'t feel confident answering until I got this Terence Tao link: (Wayback Machine) https://web.archive.org/web/20100725014132/http://www.google.com:80/buzz/114134834346472219368/RarPutThCJv/In-the-foundations-of-mathematics-the-standard (PDF, page 12) https://terrytao.files.wordpress.com/2011/06/blog-book.pdf '}\n",
      "522 {'Id': '522', 'Type': 'answer', 'ParentId': '514', 'urls': ['http://mathworld.wolfram.com/ChebyshevBias.html'], 'exp': [], 'Body': 'A famous example that is not quite as large as these others is the prime race. The conjecture states, roughly: Consider the first n primes, not counting 2 or 3. Divide them into two groups: A contains all of those primes congruent to 1 modulo 3 and B contains those primes congruent to 2 modulo 3. A will never contain more numbers than B. The smallest value of n for which this is false is 23338590792. '}\n",
      "523 {'Id': '523', 'Type': 'question', 'Title': 'Is the set of all unique (convex) polygons countable? If so, by what bijection to the natural numbers?', 'Tags': ['geometry', 'elementary-set-theory'], 'AcceptedAnswerId': '526', 'urls': [], 'exp': [], 'Body': 'Polygons are, in this question, defined as non-unique if they similar to another (by rotation, reflection, translation, or scaling). Would this answer be any different if similar but non-identical polygons were allowed?  And if only if rotated/translated by rational coefficients? Would this answer be any different if we constrained the length and internal angles of all polygons to rational numbers? Assume the number of sides is finite but unbounded, and greater than two. '}\n",
      "524 {'Id': '524', 'Type': 'question', 'Title': 'How can I tell which matrix decomposition to use for OLS?', 'Tags': ['linear-algebra', 'numerical-methods', 'numerical-linear-algebra'], 'AcceptedAnswerId': '1206', 'urls': ['http://en.wikipedia.org/wiki/QR_decomposition', 'http://en.wikipedia.org/wiki/Singular_value_decomposition#Pseudoinverse'], 'exp': ['\\\\boldsymbol{Ax}=\\\\boldsymbol{b}', '\\\\boldsymbol{A}'], 'Body': \"I want to find the least squares solution to <math_exp> where <math_exp> is a highly sparse square matrix. I found two methods that look like they might lead me to a solution: QR factorization, and singular value decomposition. Unfortunately, I haven't taken linear algebra yet, so I can't really understand most of what those pages are saying. I can calculate both in Matlab though, and it looks like the SVD gave me a smaller squared error. Why did that happen? How can I know which one I should be using in the future? \"}\n",
      "526 {'Id': '526', 'Type': 'answer', 'ParentId': '523', 'urls': [], 'exp': ['A', 'n', 'A', 'n'], 'Body': 'There are uncountably many, because for example one can have rectangles with arbitrary side ratios.  For your second question, if everything is constrained to be rational, there will be countably many, because a polygon is uniquely determined by its ordered collection of sides and angles. This is part of a general fact: if <math_exp> is a countable set, then the collection of ordered <math_exp>-tuples of elements of <math_exp> for all <math_exp> is still countable. '}\n",
      "527 {'Id': '527', 'Type': 'answer', 'ParentId': '514', 'urls': ['http://www.ams.sunysb.edu/~estie/estie.html', 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5143&amp;rep=rep1&amp;type=pdf'], 'exp': [], 'Body': 'I heard this story from Professor Estie Arkin at Stony Brook (sorry, I don\\'t know what conjecture she was talking about): For weeks we tried to prove the conjecture (without success) while we left a computer running looking for counter-examples.  One morning we came in to find the computer screen flashing: \"Counter-example found\".  We all thought that there must have been a bug in the algorithm, but sure enough, it was a valid counter-example. I tell this story to my students to emphasize that \"proof by lack of counter-example\" is not a proof at all! [Edit] Here was the response from Estie: It is mentioned in our paper:   Hamiltonian Triangulations for Fast Rendering   E.M. Arkin, M. Held, J.S.B. Mitchell, S.S. Skiena (1994). Algorithms -- ESA\\'94, Springer-Verlag, LNCS 855, J. van Leeuwen (ed.), pp. 36-47; Utrecht, The Netherlands, Sep 26-28, 1994. Specifically section 4 of the paper, that gives an example of a set of points that does not have a so-called \"sequential triangulation\". The person who wrote the code I talked about is Martin Held. '}\n",
      "528 {'Id': '528', 'Type': 'answer', 'ParentId': '356', 'urls': [], 'exp': [], 'Body': 'I think you should determine what kinds of applications you want to apply CS to and then learn general theory relevant to those applications.  Not all theory is equally applicable everywhere. '}\n",
      "529 {'Id': '529', 'Type': 'question', 'Title': 'Why are the only division algebras over the real numbers the real numbers, the complex numbers, and the quaternions?', 'Tags': ['quaternions', 'ring-theory', 'abstract-algebra'], 'AcceptedAnswerId': '547', 'urls': [], 'exp': [], 'Body': \"Why are the only (associative) division algebras over the real numbers the real numbers, the complex numbers, and the quaternions? Here a division algebra is an associative algebra where every nonzero number is invertible (like a field, but without assuming commutativity of multiplication). This is an old result proved by Frobenius, but I can't remember how the argument goes.  Anyone have a quick proof? \"}\n",
      "530 {'Id': '530', 'Type': 'question', 'Title': 'Can you find a domain where <span class=\"math-container\" id=\"5003\">ax+by=1</span> has a solution for all <span class=\"math-container\" id=\"5004\">a</span> and <span class=\"math-container\" id=\"5005\">b</span> relatively prime, but which is not a PID?', 'Tags': ['number-theory', 'abstract-algebra'], 'AcceptedAnswerId': '562', 'urls': [], 'exp': ['a', 'b', 'x', 'y', 'ax+by=1'], 'Body': 'In Intro Number Theory a key lemma is that if <math_exp> and <math_exp> are relatively prime integers, then there exist integers <math_exp> and <math_exp> such that <math_exp>.  In a more advanced course instead you would use the theorem that the integers are a PID, i.e. that all ideals are principal.  Then the old lemma can be used to prove that \"any ideal generated by two elements is actually principal.\"  Induction then says that any finitely generated ideal is principal.  But, what if all finitely generated ideals are principal but there are some ideals that aren\\'t finitely generated?  Can that happen? '}\n",
      "531 {'Id': '531', 'Type': 'question', 'Title': 'What is \"ultrafinitism\" and why do people believe it?', 'Tags': ['soft-question', 'philosophy', 'constructive-mathematics'], 'urls': [], 'exp': [], 'Body': 'I know there\\'s something called \"ultrafinitism\" which is a very radical form of constructivism that I\\'ve heard said means people don\\'t believe that really large integers actually exist.  Could someone make this a little bit more precise?  Are there good reasons for taking this point of view?  Can you actually get math done from that perspective? '}\n",
      "532 {'Id': '532', 'Type': 'answer', 'ParentId': '531', 'urls': ['http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimPDF/real.pdf', 'http://en.wikipedia.org/wiki/Alexander_Esenin-Volpin'], 'exp': [], 'Body': \"The philosophy is explained in Doron Zeilberger's article.  Basically, it's the belief that there is a largest natural number! I've heard a funny story (on Scott Aaronson's blog) about someone who was an ultrafinitist. -Do you believe in 1? -Yes, he responded immediately -Do you believe in 2? -Yes, he responded after a brief pause -Do you believe in 3?  -Yes, he responded after a slightly longer pause -Do you believe in 4? -Yes, after several seconds It soon become clear that he would take twice as long to answer the next question as the previous one. (I believe Alexander Esesin-Volpin was the person.) \"}\n",
      "533 {'Id': '533', 'Type': 'question', 'Title': 'Classifying Quasi-coherent Sheaves on Projective Schemes', 'Tags': ['algebraic-geometry', 'projective-schemes', 'quasicoherent-sheaves', 'projective-space'], 'AcceptedAnswerId': '561', 'urls': [], 'exp': ['\\\\mathbb{P}^n'], 'Body': 'I know some references where I can find this, but they seem tedious. Both Hartshorne and Ueno cover this. I am wondering if there is an elegant way to describe these. If this task is too difficult in general, how about just <math_exp>? Thanks! '}\n",
      "534 {'Id': '534', 'Type': 'question', 'Title': 'Applications of the \"soft maximum\"', 'Tags': ['numerical-methods', 'analysis'], 'AcceptedAnswerId': '565', 'urls': ['http://www.johndcook.com/blog/2010/01/13/soft-maximum', 'http://www.johndcook.com/blog/'], 'exp': ['x_1,x_2,\\\\ldots,x_n', 'g(x_1,x_2,\\\\ldots,x_n) = \\\\log(\\\\exp(x_1) + \\\\exp(x_2) + \\\\cdots + \\\\exp(x_n))', 'x_i', 'x_i', '\\\\exp(x_i)', 'really', 'x_i', 'max(x_i)'], 'Body': 'There is a little triviality that has been referred to as the \"soft maximum\" over on John Cook\\'s Blog that I find to be fun, at the very least. The idea is this:  given a list of values, say <math_exp> , the function <math_exp> returns a value very near the maximum in the list. This happens because that exponentiation exaggerates the differences between the <math_exp> values.  For the largest <math_exp>, <math_exp> will be <math_exp> large.  This largest exponential will significantly outweigh all of the others combined. Taking the logarithm, i.e. undoing the exponentiation,  we essentially recover the largest of the <math_exp>\\'s.  (Of course, if two of the values were very near one another, we aren\\'t guaranteed to get the true maximum, but it won\\'t be far off!) About this, John Cook says: \"The soft maximum approximates the hard maximum but it also rounds off the corners.\"  This couldn\\'t really be said any better. I recall trying to cleverly construct sequences for proofs in advanced calculus where not-everywhere-differentiable operations would have been great to use if they didn\\'t have that pesky non-differentiable trait.  I can\\'t recall a specific incidence where I was tempted to use <math_exp>, but this seems at least plausible that it would have come up. Has anyone used this before or have a scenario off hand where it would be useful? '}\n",
      "535 {'Id': '535', 'Type': 'question', 'Title': 'Describe the locus in the complex plane of the zeros of a quartic polynomial as the constant term varies', 'Tags': ['algebra-precalculus', 'polynomials', 'roots', 'quartic-equations'], 'urls': [], 'exp': ['p_c(x)=4x^4+8x^3-3x^2-9x+c'], 'Body': '<img src=\"https://i.stack.imgur.com/pWnq4.png\" alt=\"complex plot of the zeros\"> (Diagram and setup from UCSMP Precaluclus and Discrete Mathematics, 3rd ed.) Above is a partial plot of the zeros of <math_exp>.  The text stops at showing the diagram and does not discuss the shape of the locus of the zeros or describe the resulting curves.  Are the curves in the locus some specific (named) type of curve?  Is there a simple way to describe the curves (equations)? The question need not be limited to the specific polynomial given--a similar sort of locus is generated by the zeros of nearly any quartic polynomial as the constant term is varied. '}\n",
      "536 {'Id': '536', 'Type': 'question', 'Title': 'Proving that 1- and 2-d simple symmetric random walks return to the origin with probability 1', 'Tags': ['probability-theory', 'stochastic-processes', 'random-walk', 'symmetry'], 'urls': [], 'exp': ['1', '1', '2', '1', '(p = 1)', '1', '2', '1', '2', '3', '\\\\frac14', '\\\\frac16'], 'Body': 'How does one prove that a simple (steps of length <math_exp> in directions parallel to the axes) symmetric (each possible direction is equally likely) random walk in <math_exp> or <math_exp> dimensions returns to the origin with probability <math_exp>? Edit: note that while returning to the origin is guaranteed <math_exp> in <math_exp> and <math_exp> dimensions, it is not guaranteed in higher dimensions; this means that something in a correct justification for the <math_exp>- or <math_exp>-d case must fail to extend to <math_exp>-d (or fail when the probability for each direction drops from <math_exp> to <math_exp>). '}\n",
      "537 {'Id': '537', 'Type': 'answer', 'ParentId': '536', 'urls': [], 'exp': [], 'Body': \"I'll do 1D. 1D walks are building binary strings, 010101, etc. Say take six steps. Then 111111 is just as likely as 101010. However, how many of the possible sequences have six ones? 1. How many of the possibly sequences have three ones and three zeros? Much more. That number is called multiplicity, and it grows mighty fast. In the limit its log becomes Shannon entropy. Sequences are equally likely, but combinations are not. In the limit the combinations with maximum entropy are going dominate all the rest. So the walk is going to have gone an equal number of right and left steps...almost surely. \"}\n",
      "538 {'Id': '538', 'Type': 'answer', 'ParentId': '536', 'urls': ['https://math.stackexchange.com/questions/536/proving-that-1-and-2-d-simple-symmetric-random-walks-return-to-the-origin-with-p/537#537'], 'exp': [], 'Body': 'I can prove the 1 dimensional case a bit more formally than Jonathan. First we only look at the absolute values. Let us try to calculate the probability of this never exceeding x. The probability of 2x+1 consecutive moves all being the same is some p>0. If this ever occurs, then the absolute value will exceed x. Consider n groups of 2x+1 moves, the probability that at least one of these is all the same is 1-(1-p)^n, which approaches 1. So the probability of reaching each absolute value x (other than 0) is 1. Now, lets consider the probability of reaching 0 again. Without loss of generality, suppose our first move is +1. We have a 100% chance of reaching a point a distance of 1 from this. There is a 50% that the first such point is 0 and 50% that it is 2. From 2, we have a 100% chance of reaching a point two away from this. 50% chance that this is 0, 50% that this is 4. Repeating, it is easy to see that we have to reach 0 again. Furthermore, after we have reached an absolute value x, the probability of reaching it again is 1. So we reach each absolute value an infinite number of times. Due to symmetry, we can expect to reach each x an infinite number of times. EDIT: I originally tried to apply it to the 2d case as you can see below. This is incorrect, as even though there will be no maximal absolute value, there is no reason we will necessarily come back down. Similar reasoning will work in the 2d   case. Instead of using symmetry, we   simply note that each point with the   same Manhattan distance from 0 has an   expected value that is a non-zero   proportion of the expected value all   points a particular Manhattan distance   away. '}\n",
      "540 {'Id': '540', 'Type': 'question', 'Title': 'What is the Riemann-Zeta function?', 'Tags': ['terminology', 'complex-analysis', 'prime-numbers', 'riemann-zeta'], 'AcceptedAnswerId': '832', 'urls': [], 'exp': [], 'Body': \"In laymen's terms, as much as possible: What is the Riemann-Zeta function, and why does it come up so often with relation to prime numbers? \"}\n",
      "541 {'Id': '541', 'Type': 'question', 'Title': 'Which books would you recommend about Recreational Mathematics?', 'Tags': ['soft-question', 'big-list', 'reference-request', 'recreational-mathematics'], 'AcceptedAnswerId': '611', 'urls': [], 'exp': [], 'Body': 'By this I mean books  with math puzzles and problems similar to the ones you would find in mathematical olympiads. '}\n",
      "542 {'Id': '542', 'Type': 'answer', 'ParentId': '540', 'urls': ['http://en.wikipedia.org/wiki/Riemann_zeta_function', 'http://en.wikipedia.org/wiki/Complex_function#Complex_functions', 'http://en.wikipedia.org/wiki/Closed-form_expression', 'http://en.wikipedia.org/wiki/Complex_plane', 'http://en.wikipedia.org/wiki/Riemann_hypothesis'], 'exp': ['\\\\displaystyle\\\\sum_{n=1}^\\\\infty\\\\dfrac1{n^s},\\\\quad\\\\Re(s)\\\\gt1'], 'Body': \"Giving an explanation in layman's terms is always going to be challenging, given that the Riemann-Zeta function (and related hypothesis) inevitably lies in the domain of abstract mathematics, but I shall do my best. The Riemann-Zeta function is a complex function that tells us many things about the theory of numbers. Its mystery is increased by the fact it has no closed form - i.e. it can't be expressed a single formula that contains other standard (elementary) functions. Although there are many different ways of expressing the Riemann-Zeta function (the Wikipedia article gives several), it can ultimately be derived from the following simple series of real numbers: <math_exp> by extending it into the complex plane. The reason this strange and esoteric function is so famous and actively discussed in mathematics is due to the Riemann hypothesis - proposed in 1859 by the great Bernhard Riemann and still unsolved. The Wiki article states the problem in quite simple terms: The Riemann zeta-function ζ(s) is   defined for all complex numbers s ≠ 1.   It has zeros at the negative even   integers (i.e. at s = −2, −4, −6,   ...). These are called the trivial   zeros. The Riemann hypothesis is   concerned with the non-trivial zeros,   and states that: Thus the non-trivial zeros should lie   on the critical line, 1/2 + it, where   t is a real number and i is the   imaginary unit. Although the conjecture (it is only that at the moment) has many consequences for mathematics (number theory in particular), the primary one, at least the one Riemann originally proposed, is about the distribution of prime numbers. In other words, it tells us with great precision what the average gaps between primes are as we move to greater and greater numbers. Many of the other implications are rather more esoteric, though perhaps equally important for pure mathematicians. \"}\n",
      "544 {'Id': '544', 'Type': 'question', 'Title': 'What is a Markov Chain?', 'Tags': ['terminology', 'probability-theory', 'intuition', 'stochastic-processes'], 'AcceptedAnswerId': '545', 'urls': [], 'exp': [], 'Body': 'What is a intuitive explanation of a Markov Chain, and how they work? Please provide at least one practical example. '}\n",
      "545 {'Id': '545', 'Type': 'answer', 'ParentId': '544', 'urls': ['http://en.wikipedia.org/wiki/Markov_chain'], 'exp': ['P(X_n | X_1, X_2, \\\\dots X_{n-1}) = P(X_n | X_{n-1})', '0.3', '0.4', 'P(R|S) \\\\cdot P(S|R) + P(S|S) \\\\cdot P(S|S) = 0.3 \\\\cdot 0.4+0.6 \\\\cdot 0.6 = 0.48'], 'Body': \"A Markov chain is a discrete random   process with the property that the   next state depends only on the current   state (wikipedia) So <math_exp>. An example could be when you are modelling the weather. You then can take the assumption that the weather of today can be predicted by only using the knowledge of yesterday. Let's say we have Rainy and Sunny. When it is rainy on one day the next day is Sunny with probability <math_exp>. When it is Sunny, the probability for Rain next day is <math_exp>. Now when it is today Sunny we can predict the weather of the day after tomorrow, by simply calculating the probability for Rain tomorrow, multiplying that with the probablity for Sun after rain plus the probability of Sun tomorrow times the probability of Sun after sun. In total the probability of Sunny of the day after tomorrow is <math_exp>. \"}\n",
      "546 {'Id': '546', 'Type': 'answer', 'ParentId': '544', 'urls': ['http://en.wikipedia.org/wiki/Markov_chain', 'http://mathworld.wolfram.com/MarkovChain.html'], 'exp': [], 'Body': 'In a nutshell, a Markov chain is (the behavior of) a random process which may only find itself in a (not necessarily finite) number of different states. The process moves from a state to another in discrete times (that is, you define a sequence S(t) of states at time t=0,1,2,...), and for which the probability of going from state S to state R depends just from S and R; that is, there is no \"memory of the past\" and the process is \"timeless\". This means that the Markov chain may be modeled as a n*n matrix, where n is the number of possible states. An example of a process which may be modeled by a Markov chain is the sequence of faces of a die showing up, if you are allowed to rotate the die wrt an edge. The corresponding matrix is As usual, Wikipedia and MathWorld are your friends. '}\n",
      "547 {'Id': '547', 'Type': 'answer', 'ParentId': '529', 'urls': ['http://en.wikipedia.org/wiki/Frobenius_theorem_(real_division_algebras)'], 'exp': ['D', 'v^2=\\\\langle v, v\\\\rangle', 'D', '\\\\mathbb R\\\\oplus D_0', 'D_0', 'Tr=0', 'x^2-a=0', 'x'], 'Body': \"Essentially one first proves that any real division algebra <math_exp> is a Clifford algebra (i.e. it's generated by elements of some inner product vector space I subject to relations <math_exp>): first one splits <math_exp> as <math_exp> where <math_exp> is the space of elements with <math_exp> and then one observes that minimal polynomial of a traceless element has the form <math_exp> (it's quadratic because it's irreducible and the coefficient of <math_exp> is zero because it is the trace). Now it remains to find out which Clifford algebras are division algebras which is pretty straightforward (well, and it follows from the classification of Clifford algebras). This proof is written in Wikipedia. \"}\n",
      "548 {'Id': '548', 'Type': 'question', 'Title': 'Explanation of method for showing that <span class=\"math-container\" id=\"5423\">\\\\frac{0}{0}</span> is undefined', 'Tags': ['algebra-precalculus'], 'urls': ['https://stackoverflow.com/questions/3236489/why-is-0-divided-by-0-an-error/3236541#3236541'], 'exp': ['\\\\frac00', 'x = \\\\frac00', 'x \\\\cdot 0 = 0', 'x', '0', '0', 'x', '0', 'x \\\\cdot 0 = \\\\frac00 \\\\cdot 0', '0', '\\\\frac00'], 'Body': '(This was asked due to the comments and downvotes on this Stackoverflow answer. I am not that good at maths, so was wondering if I had made any basic mistakes) Ignoring limits, I would like to know if this is a valid explanation for why <math_exp> is undefined: <math_exp>    <math_exp> Hence There are an infinite number of values for <math_exp> as anything multiplied by <math_exp> is <math_exp>. However, it seems to have got comments, with two general themes. Once is that you lose the values of <math_exp> by multiplying by <math_exp>. The other is that the last line is: <math_exp> as it involves a division by <math_exp>. Is there any merit to either argument? More to the point, are there any major flaws in my explanation and is there a better way of showing why <math_exp> is undefined? '}\n",
      "550 {'Id': '550', 'Type': 'answer', 'ParentId': '392', 'urls': [], 'exp': [\"\\\\sin'(x) = \\\\lim\\\\limits_{ h\\\\to 0}\\\\frac{\\\\sin(x+h)-\\\\sin(x)}{h}\", '\\\\sin(x+h) = \\\\sin(x)\\\\cos(h)+\\\\cos(x)\\\\sin(h)', \"\\\\Rightarrow  \\\\sin'(x) =  \\\\lim\\\\limits_{ h\\\\to 0}\\\\frac{(\\\\sin(x)(\\\\cos(h)-1) + \\\\cos(x)\\\\sin(h))}{h}\", 'x', '\\\\sin(x)\\\\sim x', '\\\\lim\\\\limits_{ h\\\\to 0}\\\\frac{\\\\sin h}{h}=1', '\\\\cos(x)\\\\sim 1 -\\\\frac {x^2} 2 ', '\\\\lim\\\\limits_{ h\\\\to 0}\\\\frac{\\\\cos h-1}{h}=0', \" \\\\sin'(x) = \\\\cos(x)\", \"\\\\cos'(x) = \\\\lim\\\\limits_{ h\\\\to 0}\\\\frac{\\\\cos(x+h)-\\\\cos(x)}{h}\", '\\\\cos(x+h) = \\\\cos(x)\\\\cos(h) - \\\\sin(x)\\\\sin(h)', \"\\\\Rightarrow \\\\cos'(x) = \\\\lim\\\\limits_{h\\\\to0}\\\\frac{\\\\cos(x)(\\\\cos(h)-1) - \\\\sin(x)\\\\sin(h)}{h}\", '= -\\\\sin(x)'], 'Body': 'From first principles,using trig identities and small-angle approximations: <math_exp> <math_exp> <math_exp> For <math_exp> small, <math_exp>, so <math_exp>and <math_exp> so <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> by the same reasoning above. '}\n",
      "551 {'Id': '551', 'Type': 'answer', 'ParentId': '531', 'urls': ['http://en.wikipedia.org/wiki/Self-verifying_theories'], 'exp': [], 'Body': 'Ultrafinitism is basically resource-bounded  constructivism: proofs have constructive content, and what you get out of these constructions isn\\'t much more than you put in. Looking at the universal and existential quantifier should help clarify things.  Constructively, a universally quantified sentence means that if I am given a parameter, I can construct something that satisfies the quantified predicate.  Ultrafinitistically, the thing you give back won\\'t be much bigger: typically there will be a polynomial bound on the size of what you get back. For existentially quantified statements, the constructive content is a pair of the value of the parameter, and the construction that satisfies the predicate.  Here the resource is the size of the proof: the size of the parameter and construction will be related to the size of the proof of the existential. Typically, addition and multiplication are total functions, but exponentiation is not.  Self-verifying theories are more extreme: addition is total in the strongest of these theories, but multiplication cannot be.  So the resource bound is linear for these theories, not polynomial. A foundational problem with ultrafinitism is that there aren\\'t nice ultrafinitist logics that support an attractive formulae-as-types correspondence in the way that intuitionistic logic does.  This makes ultrafinitism a less comfy kind of constructivism than intuitionism. Why do people believe it?  For the same kinds of reasons people believe in constructivism: they want mathematical claims to be backed up by something they can regard as concrete.  Just as an intuitionist might be bothered by the idea of cutting a ball into non-measurable pieces and putting them back together into two balls, so too  an ultrafinitist might be concerned about the idea that towers of exponentials are meaningful ways of constructing numbers.  Wittgenstein argued this point in his \"Lectures on the Foundations of Mathematics\". Can you actually get math done from that perspective? Yes.  If intuitionism is the mathematics of the computable, ultrafinitism is the mathematics of the feasibly computable.  But the difference in ease of working with between ultrafinitism and intuitionism is much bigger than that between intuitionism and classical mathematics. '}\n",
      "552 {'Id': '552', 'Type': 'answer', 'ParentId': '548', 'urls': [], 'exp': ['f(x,y)=x/y', 'x,y \\\\to 0', 'f(0,0)', 'f(0,0)', 'f(0,0)', '\\\\lim g(x) = g(\\\\lim x)', '\\\\displaystyle \\\\lim_{x \\\\to 0} \\\\frac{\\\\sin(x)}{x} = 1', '\\\\sin(x)', 'x', '\\\\displaystyle \\\\lim_{x \\\\to 0} \\\\frac{\\\\cos x - 1}{x} = 0', '\\\\displaystyle \\\\lim_{x \\\\to 0} \\\\frac{\\\\sqrt{x}}{x} =+\\\\infty'], 'Body': 'I think that ignoring limits is problematic. If there was a limit of the function <math_exp> for <math_exp> regardless of how the limit is performed, then one would define that value to be <math_exp>, even if everything else is strange. Since the limiting value depends on the way the limit is done, choosing a value for <math_exp> is counterproductive as it gives a non-continuous function. Better to have a continuous function over a slightly smaller domain. This also forces the point that if you do have a limit process that results in the evaluation of <math_exp>, you realize early on that you should examine the limit carefully rather than use <math_exp> (which is only true for continuous functions, of course.) By the way, this might be too trivial, but I\\'ll give an example of how the limiting value depends on the limit: <math_exp>, (both <math_exp> and <math_exp> go to 0) <math_exp> (again, both numerator and denominator go to 0, but numerator goes \"faster\") <math_exp>. '}\n",
      "553 {'Id': '553', 'Type': 'answer', 'ParentId': '548', 'urls': [], 'exp': ['\\\\frac0x = 0 \\\\overset{?}{\\\\implies} \\\\frac00 = 0', '\\\\frac x x = 1 \\\\overset{?}{\\\\implies} \\\\frac00 = 1', '\\\\frac00 = k, \\\\forall k', '2 = 3', 'k = k', '\\\\frac00', '\\\\frac00'], 'Body': 'For \"all\" x, <math_exp> For \"all\" x, <math_exp> Moreover, if one could say <math_exp>, we could then say <math_exp> &mdash; just divide both sides by 0 and get <math_exp>, which is patently true. Since there is no reasonable value <math_exp> can have, <math_exp> must be undefined. '}\n",
      "554 {'Id': '554', 'Type': 'answer', 'ParentId': '536', 'urls': ['http://en.wikipedia.org/wiki/Generating_function', 'http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter12.pdf'], 'exp': ['u_0=1', 'u_{2n} = p(n,n,0,0)+ p(n-1,n-1,1,1)+...+p(n-k,n-k,k,k)+...+p(0,0,n,n)', 'n\\\\neq0', 'p(u,d,l,r)', '\\\\frac{1}{4^{2n}}', '\\\\frac{(2n)!}{u!d!l!r!}', 'p(n-k,n-k,k,k)=\\\\frac{1}{4^{2n}} \\\\frac{(2n)!}{(n-k)!(n-k)!k!k!}', '(2n)!=n! n! \\\\binom{2n}{n}', 'p(n-k,n-k,k,k)=\\\\frac{1}{4^{2n}} \\\\binom{2n}{n} \\\\binom{n}{k}^2', 'u_{2n}= \\\\frac{1}{4^{2n}}  \\\\Sigma_k \\\\binom{2n}{n} \\\\binom{n}{k}^2', 'u_{2n}= \\\\frac{1}{4^{2n}} \\\\binom{2n}{n}^2', 'f_{2n}', 'u_{2n}= f_2 u_{2n-2}+ f_4 u_{2n-4} +....+f_{2n-2}u_{2} + f_{2n}', 'f_0=0', 'u_0=1', 'u_{2n}= f_0 u_{2n}+ f_2 u_{2n-2}+ f_4 u_{2n-4} +....+f_{2n-2}u_{2} + f_{2n}u_0', 'u_n', 'f_n', 'U(x)= \\\\Sigma_m u_{2m} x^{2m}', 'F(x)= \\\\Sigma_m f_{2m} x^{2m}', 'U(x)= 1+ U(x)F(x)', 'u_0', 'F(x)= \\\\frac{U(x)-1}{U(x)}', '\\\\Sigma f_n= F(1)', 'U(1)', '^2'], 'Body': \"I'll try it for 2D and then we can get 1D as a corollary [excercise!]... This is the only proof I know of, there may be a more intuitive (and less messy without tex!) proof out there, but I like this one- it uses generating functions in a really nifty way. Consider the probability of being at the origin after 2n steps (notice we cannot return in an odd number of steps): <math_exp> <math_exp> (when <math_exp>) Here <math_exp> is the probability of the first 2n steps being u up, d down, l left and r right in any order. Each order has probability <math_exp>, and there are <math_exp> distinct orders, giving <math_exp> Now, since <math_exp> we have <math_exp> giving <math_exp> which, by one of those silly binomial results, can be contracted to <math_exp> Let us put that in our back pocket for now and consider instead the probability of first returning after 2n steps <math_exp> --- this is rather difficult to tackle directly but we can make ourselves a cunning little formula involving it, jazz out some generating function fun and seal the deal with some. The formula in question is: <math_exp> Which we shall not prove so much as explain: to return to the origin after 2n steps (LHS) you must either first return after 2 steps and do a 'return to the origin in 2n-2 steps walk' (first term RHS) or first return after 4 steps and do a 'return to the origin in 2n-4 steps walk' (2nd term) or... or first return after 2n-2 steps and do a 'return to the origin in 2 steps walk or first return to the origin after 2n steps. We shall now tweak our formula just a tiny bit so it has the right symmetry properties for what is to come, we do this by adding <math_exp> and <math_exp> to give <math_exp> Which is secretly a statement about generating functions (this is the sweet bit!), see look at the generating functions of <math_exp>, <math_exp>: <math_exp>, <math_exp> We see: <math_exp> (where the '1+' is to compensate for the fact that <math_exp> does not appear in the product) Rearranging to: <math_exp> Observe that the probability of return is <math_exp>, which comes out as 1  because <math_exp> diverges by some tedious stirlings formula bounds that I forget. Edit: Until Tex comes online, this is pretty unreadable, so here's a link to some lecture notes I found with the same proof (and, fortunately, the same notation!). Enjoy! Edit<math_exp>: Hooray, Tex has come online!!! Enjoy. \"}\n",
      "555 {'Id': '555', 'Type': 'answer', 'ParentId': '514', 'urls': ['http://en.wikipedia.org/wiki/Mersenne_conjectures'], 'exp': ['M_n=2^n  − 1', 'M_{31}', 'M_{31}'], 'Body': 'For an old example, Mersenne made the following conjecture in 1644: The Mersenne numbers, <math_exp>, are prime for n = 2, 3, 5, 7, 13, 17, 19, 31, 67, 127 and 257, and no others. Euler observed that the Mersenne number at <math_exp> is prime, so refuting the conjecture. <math_exp> is quite large by the standards of the day: 2 147 483 647. According to Wikipedia, there are 51 known Mersenne primes as of 2018 '}\n",
      "556 {'Id': '556', 'Type': 'answer', 'ParentId': '540', 'urls': ['http://mathworld.wolfram.com/AnalyticContinuation.html'], 'exp': ['1 + 1/2 + 1/3 + \\\\cdots', '1/(1^2) + 1/(2^2) + 1/(3^2) + \\\\cdots', '\\\\pi^2/6', '\\\\displaystyle f(x)=\\\\sum_{n=1}^\\\\infty\\\\dfrac1{n^x}', 'x \\\\gt 1', '\\\\displaystyle \\\\zeta(s)=\\\\sum_{n=1}^\\\\infty\\\\dfrac1{n^s}', 's', 'x', '\\\\zeta(0)=-1/2', '-1/2', 's = -2n', 'n', '\\\\zeta(s) = 0', \"s'=(x,y)\", \"\\\\zeta(s') = 0\", '0 \\\\lt x \\\\lt 1', 'x = 1/2', '\\\\pi(n)', 'n', '\\\\displaystyle\\\\sum_{n=1}^\\\\infty\\\\frac1{n^s}=\\\\prod_{p \\\\text{ prime}}\\\\frac1{1-p^{-s}}'], 'Body': \"Here there is another attempt at an explanation. We know that the sum of the inverse of the positive numbers, <math_exp>, diverges. Euler shown that the sum of the inverse of the squares,  <math_exp>, has a finite sum, namely <math_exp>. Mathematicians love to generalize things, so they thought at the function <math_exp> which is defined for <math_exp>. But this was not enough: they decided that the variable could be a complex number and not a real one. There is a standard tecnique (Analytic continuation) which allows us to extend the function to nearly all the complex plane. So we now have a function which formally is <math_exp> (the variable being <math_exp> and not <math_exp> to show that we are dealing with complex numbers) but is not computed in this way. Just to make an example, <math_exp>, and sum of an infinity of ones is not <math_exp>. :-) It may be shown that for <math_exp> (<math_exp> positive integer) <math_exp>. But there are infinite other point <math_exp> where <math_exp>. For all of these points, <math_exp>; Riemann's hypothesis says that for all such points <math_exp>. If it were true, we could have the best asymptotic expression to count <math_exp>, that is the number of primes below <math_exp>. Why does the function pop up when we talk about primes? I don't know, but in the case of integer values Euler proved that <math_exp> Maybe this could be a good start. \"}\n",
      "558 {'Id': '558', 'Type': 'answer', 'ParentId': '213', 'urls': [], 'exp': [], 'Body': \"Statistics is the topic in which I am still poor and it is still useful to me which I learned so late and that's why I am poor in Statistics. \"}\n",
      "560 {'Id': '560', 'Type': 'answer', 'ParentId': '135', 'urls': [], 'exp': ['A^B:=\\\\{f:B\\\\to A\\\\}', '|A|^{|B|}:=|A^B|', 'B', '0', '0^0', '0^0=|\\\\emptyset|^{|\\\\emptyset|}=|\\\\emptyset^\\\\emptyset|   =|\\\\{f:\\\\emptyset\\\\to\\\\emptyset\\\\}|=1'], 'Body': \"If we use the idea of set exponentiation to define exponentiation of cardinals, we have the following natural idea: <math_exp> We define the exponential of cardinals as follows: <math_exp>.  It's easy to check that this agrees with our intuition for exponentiation of natural numbers when <math_exp> is nonempty. There is only one set representing the cardinal <math_exp>, namely the empty set.  Then we may look at <math_exp> as follows: <math_exp> \"}\n",
      "561 {'Id': '561', 'Type': 'answer', 'ParentId': '533', 'urls': [], 'exp': ['Spec(A)', 'A', 'M', 'M', 'A', 'Spec(A)', 'R', 'R = R_0 + R_1 + \\\\dots', 'R', 'M', '\\\\tilde{M}', 'P', 'M_{(P)}', 'm/s', 's', 'm', 'P', 'M= M_0 + M_1 + \\\\dots', 'M', \"M' = M_1 + M_2 + \\\\dots\", 'Proj(R)', 'R'], 'Body': \"Quasi-coherent sheaves on affine schemes (say <math_exp>) are obtained by taking an <math_exp>-module <math_exp> and the associated sheaf (by localizing <math_exp>).  This gives an equivalence of categories between <math_exp>-modules and q-c sheaves on <math_exp>. Let <math_exp> be a graded ring, <math_exp> (direct sum).  Then we can, given a graded <math_exp>-module <math_exp>, consider its associated sheaf <math_exp>.  The stalk of this at  a homogeneous prime ideal <math_exp> is defined to be the localization <math_exp>, which is defined as generated by quotients <math_exp> for <math_exp> homogeneous of the same degree as <math_exp> and not in <math_exp>. In short, we get sheaves of modules on the affine scheme just as we get the normal  sheaves of rings.  We get sheaves of modules on the projective scheme in the same homogeneous localization way as we get the sheaf of rings. However, it's no longer an equivalence of categories. Why? Say you had a graded module <math_exp> (in general, we allow negative gradings as well).  Then it is easy to check that the sheaves associated to <math_exp> and <math_exp> are exactly the same. Nevertheless, it is possible to get every sheaf on <math_exp> for <math_exp> a graded ring in this way.  See Proposition II.5.15 in Hartshorne. \"}\n",
      "562 {'Id': '562', 'Type': 'answer', 'ParentId': '530', 'urls': [], 'exp': ['U  \\\\subset \\\\mathbb{C}', 'a,b', '{1-\\\\frac{1}{n}}'], 'Body': \"If I'm not mistaken, the integral domain of holomorphic functions on a connected open set <math_exp> works. It is a theorem (in Chapter 15 of Rudin's Real and Complex Analysis, and essentially a corollary of the Weierstrass factorization theorem), that every finitely generated ideal in this domain is principal. This implies that if <math_exp> have no common factor, they generate the unit ideal.  However, for instance, the ideal of holomorphic functions in the unit disk that vanish on all but finitely many of <math_exp> is nonprincipal. \"}\n",
      "563 {'Id': '563', 'Type': 'answer', 'ParentId': '536', 'urls': ['https://www.math.duke.edu/~rtd/PTE/PTE4_1.pdf'], 'exp': ['0', '2n', '\\\\rho_1(2n) = \\\\binom{2n}{n}/2^{2n}', '\\\\binom{2n}{n}', 'n', 'n', '\\\\rho_2(2n)', '0', '2n', '0', '\\\\rho_2(2n) = \\\\left(\\\\binom{2n}{n}/2^{2n}\\\\right)^2', '1/(\\\\pi n)', '\\\\sum_{n \\\\geq 1} \\\\rho_2(2n)', '1'], 'Body': 'See  (link goes to online copy of the fourth edition).  On p. 164 Durrett gives a proof that simple random walk is recurrent in two dimensions. First find the probability that simple random walk in one dimension is at <math_exp> after <math_exp> steps; this is clearly <math_exp>, since <math_exp> is the number of paths with <math_exp> right steps and <math_exp> left steps. Next, the probability that simple random walk in two dimensions -- call this <math_exp> -- is at <math_exp> after <math_exp> steps is the square of the previous probability.  Consider the simple random walk which makes steps to the northeast, northwest, southeast, and southwest with equal probability. The projections of this walk onto the x- and y-axes are independent simple random walks in one dimension. Rotating and rescaling gives the \"usual\" SRW in two dimensions (with steps north, east, south and west) and doesn\\'t change the probability of being at <math_exp>. So <math_exp>.  This is asymptotic to <math_exp>, and the expected number of returns to the origin is the <math_exp>, so the expected number of returns to the origin is infinite. It\\'s not hard to show (and is in fact true, but the proof is unenlightening so I\\'ll leave it to Durrett) that in this case the probability of eventually returning to the origin is <math_exp>. '}\n",
      "564 {'Id': '564', 'Type': 'answer', 'ParentId': '544', 'urls': ['http://www.thefreedictionary.com/run', 'http://en.wikipedia.org/wiki/Viterbi_algorithm'], 'exp': [], 'Body': 'Markov chains, especially hidden Markov models are hugely important in computation linguistics. A hidden Markov model is one where we can\\'t directly view the state, but we do have some information about what the state might be. For example, consider breaking down a sentence into what is called \"parts of speech\" such as verbs, adjectives, ect. We don\\'t know what the parts of speech are, but we can attempt to deduce them from the word. For example, the word run might be used 80% as a verb, 18% of the time as a noun and 2% of the time as an adjective. We also have (Markov) relations between the parts of speech, so for example an adjective might be followed by a noun 70% of the time and another adjective 30% of the time. We can use the Viterbi algorithm to decide which sequence is most likely to have generated the observed sentence. This algorithm takes into account two factors: '}\n",
      "565 {'Id': '565', 'Type': 'answer', 'ParentId': '534', 'urls': [], 'exp': [], 'Body': 'This is close to being the flipside of the geometric mean, which is the nth root of the product of the numbers, and can be expressed as the exponential of the sum of the logarithms. Another pair of dual mean measures is the regular mean and the harmonic mean (n divided by the sum of the reciprocals). I say the soft maximum is close to being the flipside of the geometric mean, but it lacks the good property that all of the others have of taking a list of the same value to that value (for definedness, let all values be positive).  Let\\'s call the hyperbolic mean the \"soft maximum\" of the nth roots of the terms in the list: then this has that good property. The hyperbolic mean the emphasises large values in a roughly symmetric manner to the way that the geometric mean emphasises small values (which is always smaller than the regular mean), and is, of course, much smaller for long list of large values. So I say, consider it an amplified version of a useful addition to the family of of mean operators. '}\n",
      "566 {'Id': '566', 'Type': 'question', 'Title': 'Your favourite maths puzzles', 'Tags': ['soft-question', 'big-list', 'puzzle'], 'urls': ['http://en.wikipedia.org/wiki/Latin_squares', 'http://en.wikipedia.org/wiki/Magic_squares'], 'exp': ['\\\\sqrt{2+\\\\sqrt{3}}', '\\\\mathbb{R}'], 'Body': \"Okay, so this question was bound to come up sooner or later- the hope was to ask it well before someone asked it badly... To a certain extent, any piece of mathematics is a puzzle in some sense: whether we are classifying the homological intersection forms of four manifolds or calculating the optimum dimensions of a cylinder, it is an element of investigation and inherently puzzlish intrigue that drives us. Indeed most puzzles (cryptic crosswords aside) are somewhat mathematical (the mathematics of sudoku for example is hidden in latin squares). Mathematicians and puzzles get on, it seems, rather well. Okay, so in order to make this question worthwhile (and not a ten-page wadeathon through 57 varieties of the men with red and blue hats puzzle), we are going to have to impose some limitations. Not every puzzle-based answer that pops into your head will qualify for answerhood- to do so it must And should For ease of voting- one puzzle per post is bestest. Simplify <math_exp> From: problem solving magazine Hint: Try a two term solution Can one make an equilateral triangle with all vertices at integer coordinates? From: Durham distance maths challenge 2010 Hint: This is equivalent to the rational case nxn Magic squares form a vector space over <math_exp> prove this, and by way of a linear transformation, derive the dimension of this vector space. From: Me, I made this up (you can tell, can't you!) Hint: Apply the rank nullity theorem Happy puzzling! \"}\n",
      "567 {'Id': '567', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': 'From Mathematical Puzzles by Peter Winkler: Divide an hexagon in equilateral triangles, like in the figure. Now fill all the hexagon with the three kinds of diamonds made from two triangles, also shown in the figure. Prove that the number of each kind of diamond is the same. <img src=\"https://i.stack.imgur.com/THuvE.png\" alt=\"diagram of hexagon filled with diamonds\"> '}\n",
      "568 {'Id': '568', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': 'unknown source: Could the plane be colored with two different colors (say, red and blue) so that there is no equilateral triangle whose vertices are all of the same color? '}\n",
      "571 {'Id': '571', 'Type': 'question', 'Title': 'What is the optimum angle of projection when throwing a stone off a cliff?', 'Tags': ['calculus', 'trigonometry', 'physics'], 'AcceptedAnswerId': '578', 'urls': [], 'exp': ['h', 'v', 'a', 'a', 'd', 'h', 'a', '\\\\large\\\\frac{\\\\pi}{4}', '45°', 'h', 'a', 'h', 'a', '45°', 'a', 'h'], 'Body': \"You are standing on a cliff at a height <math_exp> above the sea. You are capable of throwing a stone with velocity <math_exp> at any angle <math_exp> between horizontal and vertical. What is the value of <math_exp> when the horizontal distance travelled <math_exp> is at a maximum? On level ground, when <math_exp> is zero, it's easy to show that <math_exp> needs to be midway between horizontal and vertical, and thus <math_exp> or <math_exp>. As <math_exp> increases, however, we can see by heuristic reasoning that <math_exp> decreases to zero, because you can put more of the velocity into the horizontal component as the height of the cliff begins to make up for the loss in the vertical component. For small negative values of <math_exp> (throwing up onto a platform), <math_exp> will actually be greater than <math_exp>. Is there a fully-solved, closed-form expression for the value of <math_exp> when <math_exp> is not zero? \"}\n",
      "572 {'Id': '572', 'Type': 'answer', 'ParentId': '541', 'urls': ['http://www.artofproblemsolving.com/Store/index.php'], 'exp': [], 'Body': 'Art of Problem Solving: http://www.artofproblemsolving.com/Store/index.php Anything by Martin Gardner. '}\n",
      "573 {'Id': '573', 'Type': 'question', 'Title': 'Varying definitions of cohomology', 'Tags': ['homology-cohomology', 'algebraic-topology'], 'AcceptedAnswerId': '579', 'urls': [], 'exp': ['d', '\\\\ker{d}/\\\\mathrm{im}_{d+1}'], 'Body': \"So I know that given a chain complex we can define the <math_exp>-th cohomology by taking <math_exp>. But I don't know how this corresponds to the idea of holes in topological spaces (maybe this is homology, I'm a tad confused). \"}\n",
      "574 {'Id': '574', 'Type': 'question', 'Title': 'genericness and the Zariski topology', 'Tags': ['general-topology', 'algebraic-geometry', 'intuition'], 'AcceptedAnswerId': '878', 'urls': [], 'exp': [], 'Body': 'What does it mean (in a mathematically rigorous way) to claim something is \"generic?\" How does this coincide with the Zariski topology? '}\n",
      "575 {'Id': '575', 'Type': 'question', 'Title': 'Projective duality', 'Tags': ['curves', 'projective-space'], 'AcceptedAnswerId': '830', 'urls': [], 'exp': [], 'Body': 'Given a curve how do you intuitively construct the picture of its projective dual? I know points --> lines, lines--> points but for something like the swallowtail this is not really obvious. '}\n",
      "576 {'Id': '576', 'Type': 'answer', 'ParentId': '571', 'urls': ['https://math.stackexchange.com/questions/571/what-is-the-optimum-angle-of-projection-when-throwing-a-stone-off-a-cliff/578#578'], 'exp': [\"x'=v \\\\cos a\", \"y''= -g\", \"t=0) \\\\quad y'= v \\\\sin a\", \"y'= v \\\\sin a -gt\", 'x_0=0', 'x=vt \\\\cos a', 'y_0=h', 'y=vt \\\\sin a - \\\\frac12 gt^2+c', 't', 'h, y=vt \\\\sin a - \\\\frac12 gt^2+h', 'y=0', 't', 'x', 'x', 'y'], 'Body': \"I don't have a complete solution, but I attempted to solve this problem using calculus. <math_exp>      <math_exp> and (at  <math_exp>      So, <math_exp>      <math_exp>, so <math_exp>      <math_exp>, so <math_exp> (integrating with respect to <math_exp>)      Subbing in <math_exp> The ball will hit the ground when <math_exp>. This is as far as I got, but it appears that you can find a closed solution after all. I originally tried solving the quadratic for <math_exp> and subbing that into <math_exp>, but it seems to work much better to do the substitution the other way round. I will leave this solution here in case anyone wants to see how to derive the basic equations for <math_exp> and <math_exp>. \"}\n",
      "577 {'Id': '577', 'Type': 'question', 'Title': 'Has anyone ever proposed additional axioms?', 'Tags': ['axioms'], 'AcceptedAnswerId': '1760', 'urls': [], 'exp': [], 'Body': 'According to Wikipedia, Godel\\'s incompleteness theorem states: No consistent system of axioms whose   theorems can be listed by an   \"effective procedure\" (essentially, a   computer program) is capable of   proving all facts about the natural   numbers. This obviously includes our current system. So has anyone proposed any additional axioms that seem credible? '}\n",
      "578 {'Id': '578', 'Type': 'answer', 'ParentId': '571', 'urls': [], 'exp': ['t = \\\\frac d{v\\\\cos\\\\theta}', ' 0 = h + d\\\\tan\\\\theta - \\\\frac{gd^2\\\\sec^2\\\\theta}{2v^2}\\\\qquad(3) ', '\\\\frac{dd}{d\\\\theta}=0', ' d = \\\\frac{v^2}{g\\\\tan\\\\theta} '], 'Body': 'Assume no friction and uniform gravity g. If you throw a stone at point (0, h), with velocity (v cos &theta;, v sin &theta;), then we get \\\\begin{align} d &amp;= vt\\\\cos\\\\theta &amp;&amp; (1) \\\\\\\\ 0 &amp;= h + vt\\\\sin\\\\theta - \\\\frac12 gt^2 &amp;&amp; (2) \\\\end{align} The only unknown to be solved is t (total travel time). We could eliminate it by using <math_exp> to get <math_exp> Then we compute the total derivative with respect to θ: \\\\begin{align} 0 &amp;= \\\\frac d{d\\\\theta}\\\\left(d\\\\tan\\\\theta\\\\right) - \\\\frac g{2v^2}\\\\frac d{d\\\\theta}\\\\left(d^2\\\\sec^2\\\\theta\\\\right) \\\\\\\\ &amp;= \\\\ldots \\\\end{align} and then set <math_exp> (because it is maximum) to solve d: <math_exp> Substitute this back to (3) gives: \\\\begin{align} h &amp;= \\\\frac{v^2}g \\\\left( \\\\frac1{2\\\\sin^2\\\\theta} - 1\\\\right) \\\\\\\\ \\\\Rightarrow \\\\sin\\\\theta &amp;= \\\\left( 2 \\\\left(\\\\frac{gh}{v^2} + 1\\\\right) \\\\right)^{-1/2} \\\\end{align} This is the closed form of θ in terms of h. '}\n",
      "579 {'Id': '579', 'Type': 'answer', 'ParentId': '573', 'urls': [], 'exp': ['0', 'd^2 = 0', 'ker( d ) / im (d)', 'i', '-i', '\\\\Bbb Z', '\\\\Bbb R'], 'Body': 'Edited to clear some things up: Simplicial and singular (co)homology were invented to detect holes in spaces.  To get an intuitive idea of how this works, consider subspaces of the plane.  Here the 2-chains are formal sums of things homeomorphic to the closed disk, and 1-chains are formal sums of things homeomorphic to a line segment.  The operator d takes the boundary of a chain.  For example, the boundary of the closed disk is a circle.  If we take d of the circle we get <math_exp> since a circle has no boundary.  And in general it happens that <math_exp>, that is boundaries always have no boundaries themselves.  Now suppose we remove the origin from the plane and take a circle around the origin.  This circle is in the kernel of d since it has no boundary.  However, it does not bound any 2-chain in the space (since the origin is removed) and so it is not in the image of the boundary operator on two-dimensions.  Thus the circle represents a non-trivial element in the quotient space <math_exp>. The way I have defined things makes the above a homology theory simply because the d operator decreases dimension.  Cohomology is the same thing only the operator increases dimension (for example the exterior derivative on differential forms).  Thus algebraically there really is no difference between cohomology and homology since we can just change the grading from <math_exp> to <math_exp>. From a homology we can get a corresponding cohomology theory by dualizing, that is by looking at maps from the group of chains to the underlying group (e.g. <math_exp> or <math_exp>).  Then d on the cohomology theory becomes the adjoint of the previous boundary operator and thus increases degrees. '}\n",
      "580 {'Id': '580', 'Type': 'answer', 'ParentId': '566', 'urls': ['http://en.wikipedia.org/wiki/Pseudorandom_number_generator'], 'exp': [], 'Body': \"Most of us know that, being deterministic, computers cannot generate true random numbers. However, let's say you have a box which generates truly random binary numbers, but is biased:  it's more likely to generate either a 1 or a 0, but you don't know the exact probabilities, or even which is more likely (both probabilities are > 0 and sum to 1, obviously) Can you use this box to create an unbiased random generator of binary numbers? \"}\n",
      "582 {'Id': '582', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': \"Assuming you have unlimited time and cash, is there a strategy that's guaranteed to win at roulette? \"}\n",
      "584 {'Id': '584', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': \"Frk n th rd 1 Y'r n pth n n slnd, cme t  frk n th rd.  Bth pths ld t vllgs f ntvs; th ntr vllg thr lwys tlls th trth r lwys ls (bth villgs cld b trth-tllng r lyng vllgs, r n f ch).  Thr r tw ntvs t th frk - thy cld bth b frm th sm vllg, r frm dffrnt vllgs (s bth cld b trth-tllrs, both lrs, r ne f ch). n pth lds t sfty, th thr t dm.  Y'r llwd t sk nly n qstn t ch ntv t fgr t whch pth s whch. Wht d y sk? \"}\n",
      "585 {'Id': '585', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': 'Fork in the road 2 You\\'re once again at a fork in the road, and again, one path leads to safety, the other to doom. There are three natives at the fork.  One is from a village of truth-tellers, one from a village of liars, one from a village of random answerers.  Of course you don\\'t know which is which. Moreover, the natives answer \"pish\" and \"posh\" for yes and no, but you don\\'t know which means \"yes\" and which means \"no.\" You\\'re allowed to ask only two yes-or-no questions, each question being directed at one native. What do you ask? '}\n",
      "586 {'Id': '586', 'Type': 'answer', 'ParentId': '107', 'urls': [], 'exp': [], 'Body': \"I've always thought this sort of puzzle is a wonderful example of the difference between behaviour as you approach a limit, versus behaviour AT the limit. You can make the puzzle more revealing like this: Say I have an infinite number of banknotes, each with a unique serial number. I give you bill number 1. Now, you have two options, either you keep that bill and the game ends, or I give you 10 more banknotes, but you have to burn the lowest numbered bill. It seems obvious that option two is much better. Now, we repeat this game over and over. You keep getting more and more money. But at the limit, every banknote has been burned and you're worse off than you would have been had you just taken the one banknote and left. \"}\n",
      "587 {'Id': '587', 'Type': 'answer', 'ParentId': '577', 'urls': [], 'exp': [], 'Body': \"Adding additional axioms would make more truths provable. But it wouldn't make all truths provable (unless the axiom was inconsistent with the already given ones, in which case all falsehoods would also be provable too). So adding additional axioms isn't going to help make all the truths provable. I guess you could just add, say, Goldbach's conjecture and the Riemann hypothesis as extra axioms and carry on doing mathematics-PLUS! but why would you want to? \"}\n",
      "588 {'Id': '588', 'Type': 'question', 'Title': 'What functions can be represented as power series?', 'Tags': ['calculus', 'complex-analysis', 'sequences-and-series'], 'urls': [], 'exp': [], 'Body': 'How do we know if a particular function can be represented as a power series? And once we have come up with a power series representation, how does one figure out its radius of convergence ? '}\n",
      "589 {'Id': '589', 'Type': 'answer', 'ParentId': '588', 'urls': [], 'exp': [], 'Body': 'This is a very general question, as one can create all sorts of power series for different functions. (e.g. Taylor series, Laurent series, Fourier series). To give the obvious example of Taylor series: a power series representation of a function can be found if the function is infinitely differentiable in the neighbourhood of the given point. With all power series, you will need to find the recursion relation (formula giving a successive term from the current term) and then use the ratio test to solve for the value of the input variable that gives a ratio of convergence of 1. '}\n",
      "590 {'Id': '590', 'Type': 'answer', 'ParentId': '540', 'urls': ['https://math.stackexchange.com/questions/255/'], 'exp': ['\\\\zeta(s)', 's\\\\to 1', '\\\\displaystyle\\\\sum_{n=1}^\\\\infty \\\\frac{1} {n^s} = \\\\prod_{p\\\\in\\\\text{prime}} \\\\frac{1} {1-p^-s}', '\\\\displaystyle\\\\log \\\\zeta(s) = \\\\sum_{p\\\\in\\\\text{prime}} \\\\log \\\\frac{1} {1-p^-s}.', '\\\\sum 1/p'], 'Body': 'The key point is that the Riemann zeta function is a function whose properties encode properties about the prime numbers.  As mentioned by Noldorin, in order to fully understand the Riemann zeta function you need to \"analytically continue it to the complex plane\" which is a tricky process which takes serious study.  Fortunately for some easier properties of the primes you can just use the definition of the zeta function for real s. Claim (due to Euler): The fact that <math_exp> goes to infinity as <math_exp> tells you that there are infinitely many primes. Sketch of proof: Use the \"Euler factorization\" mentioned by mau (expand the RHS as a geometric series and then multiply it out using unique factorization into primes): <math_exp> Now take log of both sides to get: <math_exp> Now use the taylor series for \\\\log and send s to one.  You\\'ll get that the left hand side goes to infinity ( not converge?), while the right hand side looks like <math_exp> + bounded terms.  So there must be infinitely many primes. '}\n",
      "591 {'Id': '591', 'Type': 'answer', 'ParentId': '588', 'urls': [], 'exp': ['e^{-1/x^2}', 'C^\\\\infty'], 'Body': \"A function can be represented as a power series if and only if it is complex differentiable in an open set. This follows from the general form of Taylor's theorem for complex functions. Being real differentiable--even infinitely many times--is not enough, as the function <math_exp> on the real line (equal to 0 at 0) is <math_exp> yet does not equal its power series expansion since all its derivatives at zero vanish. The reason is that the complexified version of the function is not even continuous at the origin. \"}\n",
      "592 {'Id': '592', 'Type': 'answer', 'ParentId': '577', 'urls': ['http://en.wikipedia.org/wiki/Continuum_hypothesis', 'http://www.tricki.org/article/How_to_use_the_continuum_hypothesis'], 'exp': [], 'Body': \"Sure, the continuum hypothesis.  We know, thanks to the work of Godel and Paul Cohen, that this is a legitimate axiom, in that it doesn't destroy consistency: you can't disprove it (or prove it) using Zermelo-Frank set theory. People do actually use the continuum hypothesis in model theory, for instance, where there are several results that depend on it.  For an article about its uses, cf. the Tricki page. \"}\n",
      "593 {'Id': '593', 'Type': 'question', 'Title': 'Is there a relationship between <span class=\"math-container\" id=\"5628\">e</span> and the sum of <span class=\"math-container\" id=\"5629\">n</span>-simplexes volumes?', 'Tags': ['geometry', 'intuition', 'sequences-and-series'], 'AcceptedAnswerId': '682', 'urls': ['http://en.wikipedia.org/wiki/Simplex#Geometric_properties', 'http://en.wikipedia.org/wiki/E_%28mathematical_constant%29#Complex_numbers'], 'exp': ['e^x', 'e^x', 'n', '\\\\infty'], 'Body': 'When I look at the Taylor series for <math_exp> and the volume formula for oriented simplexes, it makes <math_exp> look like it is, at least almost, the sum of simplexes volumes from <math_exp> to <math_exp>. Does anyone know of a stronger relationship beyond, \"they sort of look similar\"? Here are some links: Volume formula http://en.wikipedia.org/wiki/Simplex#Geometric_properties Taylor Series http://en.wikipedia.org/wiki/E_%28mathematical_constant%29#Complex_numbers '}\n",
      "594 {'Id': '594', 'Type': 'question', 'Title': 'How do you prove that a prime is the sum of two squares iff it is congruent to 1 mod 4?', 'Tags': ['number-theory', 'prime-numbers'], 'urls': [], 'exp': ['p'], 'Body': 'It is a theorem in elementary number theory that if <math_exp> is a prime and congruent to 1 mod 4, then it is the sum of two squares. Apparently there is a trick involving arithmetic in the gaussian integers that lets you prove this quickly. Can anyone explain it? '}\n",
      "595 {'Id': '595', 'Type': 'question', 'Title': 'What is the most efficient way to determine if a matrix is invertible?', 'Tags': ['linear-algebra', 'matrices'], 'AcceptedAnswerId': '596', 'urls': ['http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2005/'], 'exp': ['n \\\\times n', 'O(n!)'], 'Body': 'I\\'m learning Linear Algebra using MIT\\'s Open Courseware Course 18.06 Quite often, the professor says \"... assuming that the matrix is invertible ...\". Somewhere in the lecture he says that using a determinant on an <math_exp> matrix is on the order of <math_exp> operations, where an operation is a multiplication and a subtraction. Is there a more efficient way?  If the aim is to get the inverse, rather than just determine the invertibility, what is the most effecient way to do this? '}\n",
      "596 {'Id': '596', 'Type': 'answer', 'ParentId': '595', 'urls': ['http://en.wikipedia.org/wiki/Gauss%E2%80%93Jordan_elimination'], 'exp': [], 'Body': 'Gauss-Jordan elimination can be used to determine when a matrix is invertible and can be done in polynomial (in fact, cubic) time.  The same method (when you apply the opposite row operation to identity matrix) works to calculate the inverse in polynomial time as wel. '}\n",
      "597 {'Id': '597', 'Type': 'answer', 'ParentId': '519', 'urls': ['http://en.wikipedia.org/wiki/(2,3,7)_triangle_group'], 'exp': ['\\\\langle x,y \\\\; | \\\\; x^2=y^3=1 \\\\rangle \\\\cong \\\\operatorname{PSL}_2(\\\\mathbb Z)', '\\\\operatorname{PSL}_2/T^7=1', 'T:z\\\\mapsto z+1'], 'Body': '<math_exp> and this isomorphism identifies G with <math_exp> (where <math_exp>). Result is the symmetry group of the tiling of the hyperbolic plane. From this description one can see that G is infinite (e.g. because there are infinitely many triangles in the tiling and G acts on them transitively). '}\n",
      "598 {'Id': '598', 'Type': 'answer', 'ParentId': '594', 'urls': [], 'exp': ['p', 'p = x^2 + y^2', 'x,y', 'p = (x+iy)(x-iy) = N(x+iy)', 'N', '\\\\mathbb{Z}[i]', 'p', '\\\\mathbb{Z}[i]', 'p', '\\\\mathbb{Z}', '\\\\mathbb{Z}[i]', 'X^2+1', 'p', '-1', 'p', 'p \\\\equiv 1 \\\\mod 4', 't \\\\in \\\\mathbb{Z}', 't^2 \\\\equiv -1 \\\\mod p', 'X^2+1', 'p', 'p', '\\\\mathbb{Z}[i]', 'p', 'p \\\\mid (t+i)(t-i)', 'p \\\\mid t+i', 't \\\\mid t-i', 'x+iy', '\\\\mathbb{Z}[i]', 'p', 'N(x+iy) = x^2+y^2', 'p^2', 'p', '1', 'x+iy', 'x^2+y^2 = p'], 'Body': \"Let <math_exp> be a prime congruent to 1 mod 4. Then to write <math_exp> for <math_exp> integers is the same as writing <math_exp> for <math_exp> the norm. It is well-known that the ring of Gaussian integers <math_exp> is a principal ideal domain, even a euclidean domain.   Now I claim that <math_exp> is not prime in <math_exp>. To determine how a prime <math_exp> of <math_exp> splits in <math_exp> is equivalent to determining how the polynomial <math_exp> splits modulo <math_exp>. First off, <math_exp> is a quadratic residue modulo <math_exp> because <math_exp>.  Consequently, there is <math_exp> with <math_exp>, so <math_exp> splits modulo <math_exp>, and <math_exp> does not remain prime in <math_exp>.  (Another way of seeing this is to note that if <math_exp> remained prime, then we'd have <math_exp>, which means that <math_exp> or <math_exp>.) Anyway, as a result there is a non-unit <math_exp> of <math_exp> that properly divides <math_exp>.  This means that the norms properly divide as well.  In particular, <math_exp> properly divides <math_exp>, so is <math_exp> or <math_exp>. It cannot be the latter since otherwise <math_exp> would be a unit.  So <math_exp>. \"}\n",
      "599 {'Id': '599', 'Type': 'answer', 'ParentId': '595', 'urls': ['http://en.wikipedia.org/wiki/Determinant#Algorithmic_implementation'], 'exp': [], 'Body': 'A matrix is invertible iff its determinant is non-zero. There are algorithms which find the determinant in slightly worse than O(n2) '}\n",
      "600 {'Id': '600', 'Type': 'question', 'Title': 'Circular permutations with indistinguishable objects', 'Tags': ['combinatorics'], 'AcceptedAnswerId': '825', 'urls': [], 'exp': ['n!', 'n!/n', '1234', '2341', '4321', 'n', 'k', 'r_i', 'i^{th}'], 'Body': 'Given n distinct objects, there are <math_exp> permutations of the objects and <math_exp> \"circular permutations\" of the objects (orientation of the circle matters, but there is no starting point, so <math_exp> and <math_exp> are the same, but <math_exp> is different). Given <math_exp> objects of <math_exp> types (where the objects within each type are indistinguishable), <math_exp> of the <math_exp> type, there are \\\\begin{equation*} \\\\frac{n!}{r_1!r_2!\\\\cdots r_k!} \\\\end{equation*} permutations.  How many circular permutations are there of such a set? '}\n",
      "601 {'Id': '601', 'Type': 'question', 'Title': 'Counting how many hands of cards use all four suits', 'Tags': ['combinatorics', 'card-games'], 'AcceptedAnswerId': '602', 'urls': [], 'exp': ['52', 'k', 'k', 'k', '4', '(1)', '(2)'], 'Body': \"From a standard <math_exp>-card deck, how many ways are there to pick a hand of <math_exp> cards that includes one card from all four suits? I know that for any specific <math_exp>, it's possible to break it up into cases based on the partitions of <math_exp> into <math_exp> parts. For example, if I want to choose a hand of six cards, I can break it up into two cases based on whether there are <math_exp> three cards from one suit and one card from each of the other three or <math_exp> two cards from each of two suits and one card from each of the other two. Is there a simpler, more general solution that doesn't require splitting the problem into many different cases? \"}\n",
      "602 {'Id': '602', 'Type': 'answer', 'ParentId': '601', 'urls': [], 'exp': ['N(\\\\dots)', '\\\\begin{align} &amp;N(\\\\mathrm{no\\\\ }\\\\heartsuit)+N(\\\\mathrm{no\\\\ }\\\\spadesuit)+N(\\\\mathrm{no\\\\ }\\\\clubsuit)+N(\\\\mathrm{no\\\\ }\\\\diamondsuit) \\\\\\\\ &amp;\\\\quad\\\\quad-N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\spadesuit)-N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\clubsuit)-N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\diamondsuit)-N(\\\\mathrm{no\\\\ }\\\\spadesuit\\\\clubsuit)-N(\\\\mathrm{no\\\\ }\\\\spadesuit\\\\diamondsuit)-N(\\\\mathrm{no\\\\ }\\\\clubsuit\\\\diamondsuit) \\\\\\\\ &amp;\\\\quad\\\\quad+N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\spadesuit\\\\clubsuit)+N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\spadesuit\\\\diamondsuit)+N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\clubsuit\\\\diamondsuit)+N(\\\\mathrm{no\\\\ }\\\\spadesuit\\\\clubsuit\\\\diamondsuit) \\\\\\\\ &amp;\\\\quad\\\\quad-N(\\\\mathrm{no\\\\ }\\\\heartsuit\\\\spadesuit\\\\clubsuit\\\\diamondsuit) \\\\\\\\ &amp;=4{39 \\\\choose k}-6{26 \\\\choose k}+4{13 \\\\choose k}-{0 \\\\choose k}. \\\\end{align}', '{52 \\\\choose k}-4{39 \\\\choose k}+6{26 \\\\choose k}-4{13 \\\\choose k}+{0 \\\\choose k}.'], 'Body': 'Count the number of hands that do not contain at least one card from every suit and subtract from the total number of k-card hands.  To count the number of hands that do not contain at least one card from every suit, use inclusion-exclusion considering what suits are not in a given hand.  That is, letting <math_exp> mean the number of hands meeting the given criteria, <math_exp> So, the number of hands of k cards that include at least one card from every suit is <math_exp>  [Drop terms as appropriate for larger values of k.] '}\n",
      "603 {'Id': '603', 'Type': 'question', 'Title': 'What transformations of the plane are geometrically constructable (compass & straight edge)?', 'Tags': ['geometry', 'euclidean-geometry', 'geometric-construction', 'transformational-geometry'], 'AcceptedAnswerId': '634', 'urls': [], 'exp': [], 'Body': 'Congruence transformations (isometries) and similarity transformations (isometries + dilations) should be constructable.  What about other affine transformations?  Other conformal mappings? edit: by constructable, I mean given the defining information for the transformation in a geometric way (e.g. a dilation requires a center and a ratio, so the given could be a point and two segments), can you construct the image of a point under the transformation from its preimage? '}\n",
      "604 {'Id': '604', 'Type': 'question', 'Title': 'Why does the log-log scale on my Slide Rule work?', 'Tags': ['intuition'], 'AcceptedAnswerId': '605', 'urls': ['http://www.antiquark.com/sliderule/sim/virtual-slide-rule.html'], 'exp': ['LL_3', '3^n', '3', 'LL_3', '1', 'C', '3^n', 'n', '3^2', '2', 'C', 'LL_3', '9', '3', 'C', '27', 'LL_3', '4', '81', 'LL_3', '1.3,\\\\cdots', 'LL_2', '3^{0.5}', '5', 'C', 'LL_2', '1.732', '\\\\ln(m) + \\\\ln(n) = \\\\ln(mn)'], 'Body': \"For a long time I've eschewed bulky and inelegant calculators for the use of my trusty trig/log-log slide rule.  For those unfamiliar, here is a simple slide rule simulator using Javascript. To demonstrate, find the <math_exp> scale, which is on the back of the virtual one.  Let's say we want to solve  <math_exp>. First, you would move the cursor (the red line) over where <math_exp> is on the <math_exp> scale.  Then, you would slide the middle slider until the <math_exp> on the <math_exp> scale is lined up to the cursor. And voila, your slide rule is set up to find <math_exp> for any arbitrary <math_exp>.   For example, to find <math_exp>, move the cursor to <math_exp> on the <math_exp> scale, and your answer is what the cursor is on on the <math_exp> scale (<math_exp>).  Move your cursor to <math_exp> on <math_exp>, and it should be lined up with <math_exp> on <math_exp>.  To <math_exp> on C, it is on <math_exp> on <math_exp>. You can even do this for non-integer exponents (<math_exp> etc.) You can also do this for exponents less than one, by using the <math_exp> scale.  For example, to do <math_exp>, you would find <math_exp> on the <math_exp> scale, and look where the cursor is lined up at on the <math_exp> scale (which is about <math_exp>). Anyways, I was wondering if anyone could explain to me how this all works?  It works, but...why?  What property of logarithms and exponents (and logarithms of logarithms?) allows this to work? I already understand how the basics of the Slide Rule works (<math_exp>), with only multiplication, but this exponentiation eludes me. \"}\n",
      "605 {'Id': '605', 'Type': 'answer', 'ParentId': '604', 'urls': [], 'exp': [], 'Body': 'If x = 3n, then log x = n log 3. The C scale is logarithmic, which means if the reading is p, then the distance is proportional to &nbsp; log p. Similarly, in the LLx scale the distance is proportional to &nbsp; log log p. Thus, when you align 1 to \"3\" in LL3, you introduce an offset of (log log 3). Suppose you get a reading of n in the C scale, then the corresponding value in LL3 would be: eliminating one level of log gives eliminating one more level of log gives LL2 is the same as LL3 except it covers a different range. '}\n",
      "606 {'Id': '606', 'Type': 'answer', 'ParentId': '519', 'urls': [], 'exp': [], 'Body': 'Grigory has already answered your particular question.  However, I wanted to point out that your question \"How do you prove that a group specified by a presentation is infinite?\" has no good answer in general.  Indeed, in general the question of whether a group presentation defines the trivial group is undecidable. '}\n",
      "607 {'Id': '607', 'Type': 'question', 'Title': 'Given enough time, what are the chances I can come out ahead in a coin toss contest?', 'Tags': ['probability'], 'AcceptedAnswerId': '608', 'urls': [], 'exp': [], 'Body': 'Assuming I can play forever, what are my chances of coming out ahead in a coin flipping series? Let\\'s say I want \"heads\"...then if I flip once, and get heads, then I win, because I\\'ve reached a point where I have more heads than tails (1-0).  If it was tails, I can flip again.  If I\\'m lucky, and I get two heads in a row after this, this is another way for me to win (2-1). Obviously, if I can play forever, my chances are probably pretty decent.  They are at least greater than 50%, since I can get that from the first flip.  After that, though, it starts getting sticky. I\\'ve drawn a tree graph to try to get to the point where I could start see the formula hopefully dropping out, but so far it\\'s eluding me. Your chances of coming out ahead after 1 flip are 50%.  Fine.  Assuming you don\\'t win, you have to flip at least twice more.  This step gives you 1 chance out of 4.  The next level would be after 5 flips, where you have an addtional 2 chances out of 12, followed by 7 flips, giving you 4 out of 40. I suspect I may be able to work through this given some time, but I\\'d like to see what other people think...is there an easy way to approach this?  Is this a known problem? '}\n",
      "608 {'Id': '608', 'Type': 'answer', 'ParentId': '607', 'urls': ['https://math.stackexchange.com/questions/536/proving-that-1-and-2-d-simple-symmetric-random-walks-return-to-the-origin-with-p'], 'exp': [], 'Body': '100%, for the same reason as the 1-D walk In fact (again for the same reason), your chances are 100% of eventually reaching X-greater heads than tails (or tails than heads), where X is any non-negative integer. '}\n",
      "610 {'Id': '610', 'Type': 'answer', 'ParentId': '600', 'urls': ['http://en.wikipedia.org/wiki/P%C3%B3lya_enumeration_theorem'], 'exp': [], 'Body': \"This problem is best solved with Pólya's enumeration theorem, which follows from Burnside's lemma.  See the first section of this Wikipedia article. \"}\n",
      "611 {'Id': '611', 'Type': 'answer', 'ParentId': '541', 'urls': ['http://books.google.com/books?id=rVvr2p76cKQC&amp;printsec=frontcover', 'http://books.google.com/books?id=Go_iAAAACAAJ&amp;printsec=frontcover', 'http://books.google.com/books?id=B3EYPeKViAwC&amp;printsec=frontcover'], 'exp': [], 'Body': 'If you\\'re after Olympiad-level books, get The IMO Compendium which is a collection of problems from the International Math Olympiad, 1959-2004. You can find similar books with national Olympiad problems by going to Amazon and searching for \"mathematical Olympiad\". Two books that offer collections of techniques useful for olympiad-level contests are Paul Zeitz\\'s The Art and Craft of Problem Solving and Arthur Engel\\'s Problem Solving Strategies. There are lots of other books with similar titles and descriptions. Just follow Google Books\\'s suggestions. '}\n",
      "612 {'Id': '612', 'Type': 'question', 'Title': 'What are some applications outside of mathematics for algebraic geometry?', 'Tags': ['algebraic-geometry'], 'AcceptedAnswerId': '615', 'urls': [], 'exp': [], 'Body': 'Are there any results from algebraic geometry that have led to an interesting \"real world\" application? '}\n",
      "613 {'Id': '613', 'Type': 'answer', 'ParentId': '612', 'urls': ['http://erikdemaine.org/theses/tabbott.pdf', 'http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.3585'], 'exp': ['\\\\mathbb{R}^2'], 'Body': 'Broadly speaking, algebraic geometry is used a lot in some areas of robotics and mechanical engineering. Real algebraic geometry, for example, is important to the development of CAD systems (think NURBS, computing intersections of primitives, etc.) And AG comes up in robotics when it is important to figure out, say, what motions a robotic arm in a given configuration is capable of, or to construct some kind of linkage that draws a prescribed curve. Something specific in that vein: Kempe\\'s Universality Theorem gives that any bounded algebraic curve in <math_exp> is the locus of some linkage. The \"locus of a linkage\" being the path drawn out by all the vertices of a graph, where the edge lengths are all specified and one or more vertices remains still. Interestingly, Kempe\\'s orginal proof of the theorem was flawed, and more recent proofs have been more involved. However, Timothy Abbott\\'s MIT masters thesis gives a simpler proof that gives a working linkage for a given curve, and makes for interesting reading concerning the problem in general. Edit: The NURBS connection is, in part, that can construct a B-spline that approximates a given real algebraic curve, which is crucial in displaying intersection curves, for example. See here for more details (I\\'m afraid I don\\'t know many on this.) '}\n",
      "614 {'Id': '614', 'Type': 'question', 'Title': 'How to determine annual payments on a partially repaid loan?', 'Tags': ['finance'], 'AcceptedAnswerId': '616', 'urls': [], 'exp': ['500 is repaid with   payments at the end of each year. The lender charges interest at an   annual effective rate of 10%.    Each of the first ten payments is   150% of the amount of interest due.    Each of the last ten payments is X.    Calculate X. I came across this practice question while studying for my actuary exam. I tried it on my own and got stuck: ', ' ', '\\\\', '50 interest each year, so each of the first 10 payments must be ', ' ', '\\\\', '1296.87. So the balance would be ', ' ', '\\\\', '750=', ' ', '\\\\', '546.97/10=', ' '], 'Body': \"A 10-year loan of <math_exp>\\\\<math_exp>500 will earn <math_exp> <math_exp>\\\\<math_exp>75. Then after 10 years, a total of 750 has been repaid. In 10 years, I can find the accumulated debt by saying PV=500, I/Y=10, N=10, giving me FV=<math_exp> <math_exp>\\\\<math_exp>1296.87-<math_exp> <math_exp>\\\\<math_exp>546.97. Now I am stuck! How do I find out what the last payments should be? I know I can't just divide <math_exp> <math_exp>\\\\<math_exp>54.697, because the lender is still charging interest while the borrower pays off this remaining debt, so there would still be the interest left over. This situation isn't mentioned anywhere in my calculator manual! Can one of you give me some explanation about what is going on so that I can do it by hand? I tried working on it some more, and came up with a really great idea! Since the payments are 1.5x the 10% interest, it's just like paying off 5% of the principal each year! This saves me a lot of time, because I can just set I/Y=-5 and get FV=598.74 on my calculator. I did it the long way by calculating the future value of each interest payment (turns out they were not all $75, because the outstanding principal got smaller), and they were the same. Is this always going to work, or did I just get lucky here? Another update! I think I solved it. All I needed to do was to set FV=0, I/Y=10, N=10, PV=598.74, and then I got PMT=97.44. I never used the PMT button before, though, so is there some other way I can check the answer is right? \"}\n",
      "615 {'Id': '615', 'Type': 'answer', 'ParentId': '612', 'urls': ['http://www.dms.uaf.edu/~eallman/mega.pdf', 'http://rigtriv.wordpress.com/2008/08/07/phylogenetics-and-algebraic-geometry/'], 'exp': [], 'Body': 'The following slideshow gives an explanation of how algebraic geometry can be used in phylogenetics. See also this post of Charles Siegel on Rigorous Trivialties. This is not an area I\\'ve looked at in much detail at all, but it appears that the idea is to use a graph to model evolutionary processes, and such that the \"transition function\" for these processes is given by a polynomial map.  In particular, it\\'d be of interest to look at the potential outcomes, namely the image of the transition function; that corresponds to the image of a polynomial map (which is not necessarily an algebraic variety, but it is a constructible set, so not that badly behaved either).  (In practice, though, it seems that one studies the closure, which is a legitimate algebraic set.) '}\n",
      "616 {'Id': '616', 'Type': 'answer', 'ParentId': '614', 'urls': ['http://en.wikipedia.org/wiki/Geometric_progression', 'https://math.stackexchange.com/questions/618/repayments-of-a-loan-with-compound-interest/685#685'], 'exp': ['\\\\', '750 from the future value, when in fact each ', ' ', '75\\\\times(1.1)^{10-n}', '75(1.1^9)+75(1.1^8)+\\\\cdots+75(1.1^0).', 'a', 'r', 'n', '\\\\frac{a{r^{n-1}}}{r-1}=\\\\frac{75{1.1^{10-1}}}{0.1}\\\\approx\\\\$1195.31'], 'Body': 'This problem is in two stages. For the first stage, notice that you are paying 150% interest, but ending up owing more. This is because you subtracted <math_exp> <math_exp>\\\\<math_exp>75 amount was paid at a time in the past and needs be converted to a future value too. The payment in the nth year has a future value of <math_exp>. The total future value of the repayment is: <math_exp> Note that I have assumed that the interest is charged before the repayments are made. This sequence is a geometric progression. We consider it as a geometric sequence in reverse to make the maths easier. It has first term (<math_exp>) 75, each term 1.1 times the previous (<math_exp>) and 10 terms (<math_exp>). The sum is given by the formula: <math_exp> After we have solved this first part, then it is just a standard interest with repayments problem.. '}\n",
      "617 {'Id': '617', 'Type': 'answer', 'ParentId': '607', 'urls': ['http://en.wikipedia.org/wiki/Catalan_number', 'http://en.wikipedia.org/wiki/Stirling%27s_approximation'], 'exp': ['2^{2n}', 'C_n / 2^{2n}', 'C_n = \\\\frac{(2n)!}{(n+1)!n!}', 'C_n / 2^{2n} = \\\\frac{(2n)!}{2^{2n}} \\\\cdot \\\\frac{1}{(n+1)!n!}', '(2n/e)^{2n}'], 'Body': \"The question can be answered using Catalan numbers. Let C_n denote the number of sequences of 2n coin tosses in which you are never ahead. Formally, we count sequences in which every prefix has no less T's than H's. We call this property A. The number of total sequences of length 2n is <math_exp>. We then show that as n→∞, the ratio <math_exp> tends to 0. This means that in almost every sequence you will eventually be ahead (the chances of a random sequence having property A tend to 0 as the sequence gets longer). Indeed, <math_exp> so <math_exp> and it can be shown that this tends to 0 by Stirling's approximation (multiply and divide by <math_exp>). \"}\n",
      "618 {'Id': '618', 'Type': 'question', 'Title': 'Repayments of a loan with compound interest', 'Tags': ['finance'], 'AcceptedAnswerId': '685', 'urls': [], 'exp': [], 'Body': 'Suppose I have a loan of M dollars. At the end of each year, I am charged interest at rate R and make a repayment of P. The loan is repaid after n years. '}\n",
      "619 {'Id': '619', 'Type': 'question', 'Title': 'Problem: Two Trains and a Fly', 'Tags': ['sequences-and-series'], 'AcceptedAnswerId': '620', 'urls': [], 'exp': [], 'Body': \"The Problem: Two trains travel on the same track towards each other, each going at a speed of 50 kph. They start out 50km apart. A fly starts at the front of one train and flies at 75 kph to the front of the other; when it gets there, it turns around and flies back towards the first. It continues flying back and forth til the two trains meet and it gets squashed (the least of our worries, perhaps). How far did the fly travel before it got squashed? Attempt at a solution: I can do this by summing the infinite series of the fly's distance for each leg. I get an answer of 37.5 km: but that's so nice! There must be a more intuitive way...is there? \"}\n",
      "620 {'Id': '620', 'Type': 'answer', 'ParentId': '619', 'urls': [], 'exp': [], 'Body': 'The trains take half an hour to collide, which, at a rate of 75kph, leads to the fly travelling 37.5km. '}\n",
      "621 {'Id': '621', 'Type': 'answer', 'ParentId': '618', 'urls': ['http://en.wikipedia.org/wiki/Amortization_calculator'], 'exp': [], 'Body': 'Suggest you look into amortization calculators. '}\n",
      "622 {'Id': '622', 'Type': 'question', 'Title': 'Importance of Representation Theory', 'Tags': ['representation-theory', 'physics'], 'urls': ['https://en.wikipedia.org/wiki/Particle_physics_and_representation_theory'], 'exp': ['$\\\\mathrm{SO}(3)$', '$\\\\mathrm{GL}(n)$', '\\\\psi(x,t) \\\\mapsto \\\\psi(Ax,t)', '$A$', '$\\\\mathrm{SO}(n)$', '$G$', '$G$', '$G$'], 'Body': \"Representation theory is a subject I want to like (it can be fun finding the representations of a group), but it's hard for me to see it as a subject that arises naturally or why it is important. I can think of two mathematical reasons for studying it: The character table of a group is packs a lot of information about the group and is concise. It is practically/computationally nice to have explicit matrices that model a group. But there must certainly be deeper things that I am missing. I can understand why one would want to study group actions (the axioms for a group beg you to think of elements as operators), but why look at group actions on vector spaces? Is it because linear algebra is so easy/well-known (when compared to just modules, say)? I am also told that representation theory is important in quantum mechanics. For example, physics should be <math_exp> invariant and when we represent this on a Hilbert space of wave-functions, we are led to information about angular momentum. But this seems to only trivially invoke representation theory since we already start with a subgroup of <math_exp> and then extend it to act on wave functions by <math_exp> for <math_exp> in <math_exp>. This Wikipedia article on particle physics and representation theory claims that if our physical system has <math_exp> as a symmetry group, then there is a correspondence between particles and representations of <math_exp>. I'm not sure if I understand this correspondence since it seems to be saying that if we act an element of G on a state that corresponds to some particle, then this new state also corresponds to the same particle. So a particle is an orbit of the <math_exp> action? Anyone know of good sources that talk about this? \"}\n",
      "623 {'Id': '623', 'Type': 'question', 'Title': 'Why is the volume of a cone one third of the volume of a cylinder?', 'Tags': ['geometry'], 'AcceptedAnswerId': '644', 'urls': ['http://en.wikipedia.org/wiki/Solid_of_revolution', 'https://i.stack.imgur.com/CMqwh.gif'], 'exp': ['h', 'r', '\\\\frac{1}{3} \\\\pi r^2 h'], 'Body': 'The volume of a cone with height <math_exp> and radius <math_exp> is <math_exp>, which is exactly one third the volume of the smallest cylinder that it fits inside. This can be proved easily by considering a cone as a solid of revolution, but I would like to know if it can be proved or at least visual demonstrated without using calculus.  '}\n",
      "624 {'Id': '624', 'Type': 'question', 'Title': 'Balance chemical equations without trial and error?', 'Tags': ['linear-algebra', 'systems-of-equations', 'chemistry'], 'AcceptedAnswerId': '629', 'urls': [], 'exp': [' \\\\mathrm{Al} + \\\\text O_2 \\\\to \\\\mathrm{Al}_2 \\\\mathrm O_3 ', ' 4 \\\\mathrm{Al} + 3 \\\\mathrm{ O_2} \\\\to 2 \\\\mathrm{Al}_2 \\\\mathrm{ O_3} '], 'Body': 'In my AP chemistry class, I often have to balance chemical equations like the following: <math_exp> The goal is to make both side of the arrow have the same amount of atoms by adding compounds in the equation to each side. A solution: <math_exp> When the subscripts become really large, or there are a lot of atoms involved, trial and error is impossible unless performed by a computer. What if some chemical equation can not be balanced?  (Do such equations exist?) I tried one for a long time only to realize the problem was wrong. My teacher said trial and error is the only way. Are there other methods? '}\n",
      "625 {'Id': '625', 'Type': 'question', 'Title': \"Why is the derivative of a circle's area its perimeter (and similarly for spheres)?\", 'Tags': ['calculus', 'geometry', 'derivatives', 'circles', 'area'], 'urls': [], 'exp': ['r', '\\\\pi r^2', '2 \\\\pi r', '\\\\frac{4}{3} \\\\pi r^3', 'r', '4 \\\\pi r^2'], 'Body': \"When differentiated with respect to <math_exp>, the derivative of <math_exp> is <math_exp>, which is the circumference of a circle. Similarly, when the formula for a sphere's volume <math_exp> is differentiated with respect to <math_exp>, we get <math_exp>. Is this just a coincidence, or is there some deep explanation for why we should expect this? \"}\n",
      "626 {'Id': '626', 'Type': 'answer', 'ParentId': '622', 'urls': ['http://en.wikipedia.org/wiki/Burnside%27s_theorem'], 'exp': ['p^a q^b', 'GL_n(\\\\mathbb{A}_K)', '\\\\mathbb{A}_K', 'n=1'], 'Body': 'The representation theory of finite groups can be used to prove results about finite groups themselves that are otherwise much harder to prove by \"elementary\" means. For instance, the proof of Burnside\\'s theorem (that a group of order <math_exp> is solvable).  A lot of the classification proof of finite simple groups relies on representation theory (or so I\\'m told, I haven\\'t read the proof...). Mathematical physics. Lie algebras and Lie groups definitely come up here, but I\\'m not familiar enough to explain anything. In addition, the classification of complex simple Lie algebras relies on the root space decomposition, which is a significant (and nontrivial) fact about the representation theory of semisimple Lie algebras. Number theory.  The nonabelian version of L-functions (Artin L-functions) rely on the representations of the Galois group (in the abelian case, these just correspond to sums of 1-dimensional characters).  For instance, the proof that Artin L-functions are meromorphic in the whole plane relies on (I think) Artin Brauer\\'s theorem (i.e., a corollary of the usual statement) that any irreducible character is an rational integer combination of induced characters from cyclic subgroups -- this is in Serre\\'s Linear Representations of Finite Groups. Also, the Langlands program studies  representations of groups <math_exp> for <math_exp> the adele ring of a global field. This is a generalization of standard \"abelian\" class field theory (when <math_exp> and one is determining the character group of the ideles). Combinatorics. The representation theory of the symmetric group has a lot of connections to combinatorics, because you can parametrize the irreducibles explicitly (via Young diagrams), and this leads to the problem of determining how these Young diagrams interact. For instance, what does the tensor product of two Young diagrams look like when decomposed as a sum of Young diagrams? What is the dimension of the irreducible representation associated to a Young diagram? These problems have a combinatorial flavor. I should add the disclaimer that I have not formally studied representation theory, and these are likely to be an unrepresentative sample of topics (some of which I have only vaguely heard about). '}\n",
      "628 {'Id': '628', 'Type': 'answer', 'ParentId': '625', 'urls': ['http://en.wikipedia.org/wiki/Annulus_%28mathematics%29'], 'exp': ['dr', '2 \\\\pi r', '2\\\\pi(r+dr)', '2\\\\pi r', 'dr', '2\\\\pi(r+dr)', '2\\\\pi r', '2\\\\pi r\\\\cdot dr', 'r', 'dr', '2\\\\pi r'], 'Body': 'Consider increasing the radius of a circle by an infinitesimally small amount, <math_exp>. This increases the area by an annulus (or ring) with inner radius <math_exp> and outer radius <math_exp>. As this ring is extremely thin, we can imagine cutting the ring and then flattening it out to form a rectangle with width <math_exp> and height <math_exp> (the side of length <math_exp> is close enough to <math_exp> that we can ignore that). So the area gain is <math_exp> and to determine the rate of change with respect to <math_exp>, we divide by <math_exp> and so we get <math_exp>. Please note that this is just an informative, intuitive explanation as opposed to a formal proof. The same reasoning works with a sphere, we just flatten it out to a rectangular prism instead. '}\n",
      "629 {'Id': '629', 'Type': 'answer', 'ParentId': '624', 'urls': [], 'exp': ['A (\\\\mathrm{Al}) + B (\\\\mathrm{O_2}) \\\\rightarrow C (\\\\mathrm{Al_2 O_3})', 'A = 2C', '2B = 3C', 'A=1', '(A,B,C) = (1,\\\\frac{3}{4},\\\\frac{1}{2})', '4', '(4,3,2)'], 'Body': \"Yes; it's possible to write a system of equations that can be solved to find the correct coefficients. Here's an example for the given formula. We're trying to find coefficients A, B, and C such that <math_exp> In order to do this, we can write an equation for each element based on how many atoms are on each side of the equation. for Al: <math_exp> for O: <math_exp> This is an uninteresting example, but these will always be linear equations in terms of the coefficients. Note that we have fewer equations than variables. This means that there's more than one way to correctly balance the equation (and there is, because any set of coefficients can be scaled by any factor). We just need to find one integral solution to these equations. To solve, we can arbitrarily set one of the variables to 1 and we'll get a solution with (probably fractional) coefficients. If we add <math_exp>, the solution is <math_exp>. To get the smallest solution with integer coefficients, just multiply by the least common multiple of the denominators (<math_exp> in this case), giving us <math_exp>. If the set of equations has no solution where the coefficients are nonzero, then you know that the equation cannot be balanced. \"}\n",
      "631 {'Id': '631', 'Type': 'question', 'Title': 'Is the <span class=\"math-container\" id=\"6433\">24</span> game NP-complete?', 'Tags': ['computer-science'], 'urls': [], 'exp': ['24', '24', 'n+1', 'n', '?'], 'Body': \"The <math_exp> game is as follows. Four numbers are drawn; the player's objective is to make <math_exp> from the four numbers using the four basic arithmetic operations (in any order) and parentheses however one pleases. Consider the following generalization. Given <math_exp> numbers, determine whether the last one can be obtained from the first <math_exp> using elementary arithmetical operations as above. This problem admits succinct certificates so is in NP. Is it NP-complete<math_exp> \"}\n",
      "632 {'Id': '632', 'Type': 'question', 'Title': 'Validating a mathematical model (Lagrange formulation and geometry)', 'Tags': ['calculus', 'geometry', 'applications'], 'AcceptedAnswerId': '1245', 'urls': ['http://web.cos.gmu.edu/~tstephe3/talks/SIAMMaterialsScience2010.pdf', 'https://math.stackexchange.com/questions/632/validating-a-mathematical-model-lagrange-formulation-and-geometry/1245#1245'], 'exp': ['\\\\widetilde{G}(x_1, x_2) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2),', 'f^{(1)}x_1 + f^{(2)}x_2 = c_1,', 'f^{(1)} + f^{(2)} = 1. ', 'x_{i} &gt; 0', 'f^{(i)} &gt; 0', 'i=1,2', 'L(x_1,x_2,f^{(1)},f^{(2)},\\\\lambda_1, \\\\lambda_2, \\\\lambda_3) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2)', '- \\\\lambda_{1}(f^{(1)}x_1 + f^{(2)}x_2 - c_1)', '- \\\\lambda_{2}(f^{(1)} + f^{(2)} - 1) ', '\\\\widetilde{G}', 'x_{i}', '\\\\nabla L = 0:', \"\\\\frac{\\\\partial L}{\\\\partial x_{1}}   = f^{(1)}G_{1}'(x_1) - \\\\lambda_{1}f^{(1)} = 0\", \"\\\\frac{\\\\partial L}{\\\\partial x_2}     = f^{(2)}G_{2}'(x_2) - \\\\lambda_{1}f^{(2)} = 0\", '\\\\frac{\\\\partial L}{\\\\partial f^{(1)}} = G_{1}(x_1) - \\\\lambda_{1}x_{1} - \\\\lambda_2 = 0', '\\\\frac{\\\\partial L}{\\\\partial f^{(2)}} = G_{2}(x_2) - \\\\lambda_{1}x_{2} - \\\\lambda_2 = 0', \"(*) f^{(1)}\\\\left[G_{1}'(x_1) - \\\\lambda_1 \\\\right] = 0\", \"(**) f^{(2)}\\\\left[G_{2}'(x_2) - \\\\lambda_1 \\\\right]= 0 \", '(\\\\***) G_{1}(x_1) - G_{2}(x_2) = \\\\lambda_1 \\\\left[ x_1 - x_2\\\\right]', 'f^{(1)}', 'f^{(2)}', \"G_{1}'(x_1) = G_{2}'(x_2) = \\\\lambda_{1}.\", '\\\\frac{G_{1}(x_1) -G_{2}(x_2)}{x_1 - x_2} = \\\\lambda_{1}.', 'G_{i}', 'x_1', 'x_2', '(x_1,G_{1}(x_1))', '(x_2,G_{2}(x_2))', 'x_1', 'x_2', '\\\\widetilde{G}', 'x_1,x_2,x_3', 'x_3 = 1- x_1 - x_2', '\\\\nabla L = 0'], 'Body': 'I am working on computing phase diagrams for alloys.  These are blueprints for a material that show what phase, or combination of phases, a material will exist in for a range of concentrations and temperatures (see this pdf presentation). The crucial step in drawing the boundaries that separate one phase from another on these diagrams involves minimizing a free energy function subject to basic physical conservation constraints.  I am going to leave out the chemistry/physics and hope that we can move forward with the minimization using Lagrange multipliers. The free energy that is to be minimized is this: <math_exp> subject to: <math_exp> <math_exp> (and also that the <math_exp> and <math_exp>, for <math_exp>.) The Lagrange formulation is: <math_exp> <math_exp> <math_exp> The minimization of <math_exp> follows from finding the <math_exp>\\'s  that satisfy <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> which yields: <math_exp> <math_exp> <math_exp> Because <math_exp> and <math_exp> are not to be zero, from (*) and (**) we have that <math_exp> And, a manipulation of equation (***) looks like <math_exp> Now, think of <math_exp> as an even degree polynomial (which it isn\\'t, but it\\'s graph sometimes resembles one) in the plane.  Let the points <math_exp> and <math_exp> be locations along the x-axis that lie roughly below the minima of this curve.  The constraints (*),(**), and (***) describe the condition that the line drawn between <math_exp> and <math_exp> form a common tangent to the \"wells\" of the curve.  It is these points <math_exp> and <math_exp>, which represent concentrations of pure components in our alloy, that become mapped onto a phase diagram.  It is essentially by repeating this procedure for many temperatures that we can trace out the boundaries in the desired phase diagram. The question is:  Looking at this from a purely analytic geometry perspective, how would one derive the \"variational\" approach to find a common tangent line that we seem to have found using the above Lagrangian?  (warning: I don\\'t really know how to model things using variational methods.) And, secondly: I have presented a model of a binary alloy, meaning two variables to keep track of representing concentrations.  I have been working on ternary alloys, where this free energy <math_exp> is a function of three variables (two independent: <math_exp>, where <math_exp>) and is therefore a surface over a Gibbs triangle.  Then <math_exp> produces partial derivatives that no longer \"speak geometry\" to me, although the solution is a common tangent plane.  (I have attempted to characterize a common tangent plane based purely in analytic geometry - completely disregarding the Lagrangian - and have come up with several relations between directional derivatives... How might directional derivatives relate to the optimality conditions set forth by the Lagrangian?) EDIT:  Thank you Greg Graviton for wading through this sub-optimal notation and pointing out several mistakes in the statement of the problem.  (Also, thank you for the excellent discussion below.) '}\n",
      "633 {'Id': '633', 'Type': 'answer', 'ParentId': '623', 'urls': ['https://math.stackexchange.com/questions/164/why-is-the-volume-of-a-sphere-frac43-pi-r3/174#174'], 'exp': [], 'Body': 'You can use Pappus\\'s centroid theorem as in my answer here, but it does not provide much insight. If instead of a cylinder and a cone, you consider a cube and a square-based pyramid where the \"top\" vertex of the pyramid (the one opposite the square base) is shifted to be directly above one vertex of the base, you can fit three such pyramids together to form the complete cube.  (I\\'ve seen this as physical toy/puzzle with three pyramidal pieces and a cubic container.)  This may give some insight into the 1/3 \"pointy thing rule\" (for pointy things with similar, linearly-related cross-sections) that Katie Banks discussed in her comment. '}\n",
      "634 {'Id': '634', 'Type': 'answer', 'ParentId': '603', 'urls': ['http://mathworld.wolfram.com/AffineTransformation.html', 'http://en.wikipedia.org/wiki/Affine_transformation#Affine_transformation_of_the_plane', 'http://mathworld.wolfram.com/AffineTransformation.html', 'http://en.wikipedia.org/wiki/Affine_transformation#Affine_transformation_of_the_plane'], 'exp': ['P&#39;', '\\\\overleftrightarrow{RP}', '\\\\frac{RP&#39;}{RP}', 'P', 'R', 'P', '\\\\ell', '\\\\ell', 'P', '\\\\ell', 'P', 'P', '\\\\ell', 'P', 'P', '\\\\overrightarrow{AB}', 'A', 'B', 'A', 'P', 'B', '\\\\overleftrightarrow{PA}', 'P', '\\\\overrightarrow{AB}', 'P', '\\\\overrightarrow{AB}', 'P', 'R', '\\\\angle ABC', 'A', 'C', 'P', 'R', '\\\\angle ABC', 'B', 'R', 'A', '\\\\overrightarrow{RP}', 'C', 'D', 'R', 'P', 'P', '\\\\angle ABC', 'R', '\\\\overrightarrow{RD}', 'P', 'R', '\\\\frac{AC}{AB}', 'C', '\\\\overrightarrow{AB}', 'B', 'B&#39;', 'C', 'C&#39;', 'A', 'R', '\\\\overleftrightarrow{B&#39;P}', 'C&#39;', '\\\\overleftrightarrow{B&#39;P}', '\\\\overrightarrow{RP}', 'P', 'R', '\\\\frac{AC}{AB}', '\\\\overrightarrow{RP}', 'C&#39;', '\\\\overleftrightarrow{B&#39;P}', '\\\\overleftrightarrow{MN}', '\\\\overleftrightarrow{PQ}', '\\\\overleftrightarrow{M&#39;N&#39;}', '\\\\overleftrightarrow{P&#39;Q&#39;}', 'X&#39;', 'X', 'X', '\\\\overleftrightarrow{MN}', '\\\\overleftrightarrow{PQ}', '\\\\overleftrightarrow{M&#39;N&#39;}', '\\\\overleftrightarrow{P&#39;Q&#39;}', '\\\\triangle ABC', '\\\\triangle A&#39;B&#39;C&#39;', 'P', '\\\\triangle ABC', '\\\\triangle A&#39;B&#39;C&#39;', '\\\\ell_1', 'P', '\\\\overline{AB}', '\\\\ell_2', 'P', '\\\\overline{AC}', '\\\\ell_1', '\\\\overline{AC}', 'I_1', '\\\\ell_2', '\\\\overline{AB}', 'I_2', '\\\\overline{A&#39;B&#39;}', 'B&#39;', 'I&#39;_2', '\\\\frac{AB}{AI_2}=\\\\frac{A&#39;B&#39;}{A&#39;I&#39;_2}', '\\\\overline{A&#39;C&#39;}', 'C&#39;', 'I&#39;_1', '\\\\frac{AC}{AI_1}=\\\\frac{A&#39;C&#39;}{A&#39;I&#39;_1}', '\\\\ell&#39;_1', 'I&#39;_1', '\\\\overline{A&#39;B&#39;}', '\\\\ell&#39;_2', 'I&#39;_2', '\\\\overline{A&#39;C&#39;}', 'P', '\\\\triangle ABC', '\\\\triangle A&#39;B&#39;C&#39;', '\\\\ell&#39;_1', '\\\\ell&#39;_2', 'P', 'O', '\\\\overrightarrow{OP}', 'X', '\\\\overrightarrow{OP}', 'P', 'X', '\\\\frac{OX}{OP}', 'O'], 'Body': \"edit (2010-07-26): The question is much more involved than I'd originally thought.  As implied in the question, I knew that congruence and similarity transformations are constructible.  Immediately below this section is my original answer, which only demonstrates congruence transformations and was intended more to give an idea of what an answer might look like (since, at the time, there was another answer that was not particularly helpful).  In the last section of this answer is my justification that all affine transformations of the plane are constructible.  In re-reading that now, I realize that I'd assumed the ability to construct a point, say <math_exp>, on a line, say <math_exp>, such that <math_exp> is equal to some known ratio.  This is equivalent to being able to construct the dilation of <math_exp> by the known ratio about center <math_exp>.  I've added the construction of such a dilation below the congruence transformation section. edit (2012-01-28):  A conversation with some colleagues reminded me about this problem and in starting to ask them about it, I realized I'd completely missed that all Möbius transformations are constructible.  Since any Möbius transformation can be expressed as a composition of translation, reflection, inversion, dilation, rotation, and translation (I think there's a typical decomposition that's roughly in that order, hence my listing translation twice).  The only one of these that I have not yet shown is constructible is inversion, so I have appended that construction. As a partial answer, here are constructions of the basic congruence transformations, assuming basic construction techniques like constructing a line parallel or perpendicular to a given line through a given point and angle-copying: reflection  Given a point <math_exp> and a line <math_exp>, construct the line perpendicular to <math_exp> through <math_exp>, and construct the circle centered at the intersection of this new line and <math_exp> and passing through <math_exp>.  The image of <math_exp> under a reflection over the line <math_exp> is the point of intersection of the circle and the new line (the one not at <math_exp>). translation  Given a point <math_exp> and a vector <math_exp> (from <math_exp> to <math_exp>), construct the line through <math_exp> and <math_exp>, the line through <math_exp> parallel to line <math_exp>, and the line through <math_exp> parallel to <math_exp>.  The image of <math_exp> under translation by vector <math_exp> is the intersection of the two constructed parallels. rotation  Given a point <math_exp>, a center of rotation <math_exp>, and an <math_exp> (from <math_exp> to <math_exp>), construct the line through <math_exp> and <math_exp>, copy <math_exp> such that the copy of <math_exp> coincides with <math_exp> and the copy of <math_exp> is on ray <math_exp> and let the copy of <math_exp> be called <math_exp>, construct the circle with center at <math_exp> and passing through <math_exp>.  The image of <math_exp> under rotation by <math_exp> about point <math_exp> is the intersection of the circle with ray <math_exp>. dilation Given a point <math_exp>, a center of dilation <math_exp>, and a ratio <math_exp> (where point <math_exp> lies on ray <math_exp>), translate <math_exp> to <math_exp> and <math_exp> to <math_exp> by the translation that takes <math_exp> to <math_exp>, construct line <math_exp>, construct the line through <math_exp> parallel to <math_exp>, and construct ray <math_exp>.  The image of <math_exp> under a dilation about <math_exp> by a factor of <math_exp> is the intersection of ray <math_exp> and the line through <math_exp> parallel to <math_exp>. All affine transformations are constructible.  Per MathWorld and Wikipedia, an affine transformation of the plane is a transformation of the plane that preserves collinearity and preserves ratios of distances on any given line. First, to show that affine transformations preserve parallelism, suppose that two lines <math_exp> and <math_exp> are parallel, and that their images, lines <math_exp> and <math_exp>, intersect at <math_exp>, the image of <math_exp>.  Since affine transformations preserve collinearity, <math_exp> must be on <math_exp> and on <math_exp>, which is a contradiction, so <math_exp> and <math_exp> cannot intersect.  Thus, affine transformations preserve parallelism. An affine transformation is determined by a <math_exp> and its image, <math_exp> (per MathWorld; Wikipedia talks about defining an affine transformation by a parallelogram and its image, which is equivalent since affine transformations preserve parallelism).  Given point <math_exp> and triangles <math_exp> and <math_exp>, construct line <math_exp> through <math_exp> parallel to <math_exp> and line <math_exp> through <math_exp> and parallel to <math_exp>, call the intersection of <math_exp> with <math_exp> <math_exp> and call the intersection of <math_exp> with <math_exp> <math_exp>, extend <math_exp> past <math_exp> to a point <math_exp> such that <math_exp>, extend <math_exp> past <math_exp> to a point <math_exp> such that <math_exp>, construct line <math_exp> through <math_exp> parallel to <math_exp> and line <math_exp> through <math_exp> parallel to <math_exp>.  The image of <math_exp> under the affine transformation mapping <math_exp> onto <math_exp> is the intersection of lines <math_exp> and <math_exp>. inversion Given a point <math_exp> and a circle centered at <math_exp>, construct ray <math_exp>, let <math_exp> be the point of intersection of <math_exp> with the circle.  The image of <math_exp> under an inversion through the circle is the image of <math_exp> under a dilation by <math_exp> centered at <math_exp>. \"}\n",
      "635 {'Id': '635', 'Type': 'answer', 'ParentId': '623', 'urls': ['http://en.wikipedia.org/wiki/Cavalieris_principle'], 'exp': [], 'Body': \"One can cut a cube into 3 pyramids with square bases -- so for such pyramids the volume is indeed 1/3 hS. And then one uses Cavalieri's principle to prove that the volume of any cone is 1/3 hS. \"}\n",
      "636 {'Id': '636', 'Type': 'question', 'Title': \"How many knight's tours are there?\", 'Tags': ['combinatorics', 'graph-theory', 'hamiltonian-path', 'knight-tours'], 'AcceptedAnswerId': '919', 'urls': [], 'exp': [], 'Body': \"The knight's tour is a sequence of 64 squares on a chess board, where each square is visted once, and each subsequent square can be reached from the previous by a knight's move.  Tours can be cyclic, if the last square is a knight's move away from the first, and acyclic otherwise. There are several symmetries among knight's tours.  Both acyclic and cyclic tours have eight reflectional symmetries, and cyclic tours additionally have symmetries arising from starting at any square in the cycle, and from running the sequence backwards. Is it known how many knight's tours there are, up to all the symmetries? \"}\n",
      "639 {'Id': '639', 'Type': 'question', 'Title': 'Logic problem: Identifying poisoned wines out of a sample, minimizing test subjects with constraints', 'Tags': ['puzzle', 'recreational-mathematics'], 'AcceptedAnswerId': '1106', 'urls': [], 'exp': ['1000', '1', '10', '2', 'N', 'N \\\\gt 1', 'k', '0 \\\\lt k \\\\lt N', 's(N,k)', ' log_2 {N \\\\choose k} \\\\le s(N,k) \\\\le N-1 ', 'log_2 {N \\\\choose k}', 'N \\\\choose k', 'k', 'N', 'N-1', 'k'], 'Body': 'I just got out from my Math and Logic class with my friend.  During the lecture, a well-known math/logic puzzle was presented: The King has <math_exp> wines, <math_exp> of which is poisoned.  He needs to identify the poisoned wine as soon as possible, and with the least resources, so he hires the protagonist, a Mathematician.  The king offers you his expendable servants to help you test which wine is poisoned. The poisoned wine is very potent, so much that one molecule of the wine will cause anyone who drinks it to die.  However, it is slow-acting.  The nature of the slow-acting poison means that there is only time to test one \"drink\" per servant. (A drink may be a mixture of any number of wines)  (Assume that the King needs to know within an hour, and that any poison in the drink takes an hour to show any symptoms) What is the minimum amount of servants you would need to identify the poisoned wine? With enough time and reasoning, one can eventually see that this requires at most ten (<math_exp>) servants (in fact, you could test 24 more wines on top of that 1000 before requiring an eleventh servant).  The proof/procedure is left to the reader. My friend and I, however, was not content with resting upon this answer.  My friend added the question: What would be different if there were <math_exp> wines that were poisoned out of the 1000?  What is the new minimum then? We eventually generalized the problem to this: Given <math_exp> bottles of wine (<math_exp>) and, of those, <math_exp> poisoned wines  (<math_exp>), what is the optimum method to identify the all of the poisoned wines, and how many servants are required (<math_exp>)? After some mathsing, my friend and I managed to find some (possibly unhelpful) lower and upper bounds: <math_exp> This is because <math_exp> is the minimum number of servants to uniquely identify the <math_exp> possible configurations of <math_exp> poisoned wines in <math_exp> total wines. Can anyone help us find an optimum strategy?  Besides the trivial one requiring <math_exp> servants.  How about a possible approach to start? Would this problem be any different if you were only required to find a strategy that would for sure find a wine that is not poisoned, instead of identifying all poisoned wines? (other than the slightly trivial solution of <math_exp> servants) '}\n",
      "640 {'Id': '640', 'Type': 'question', 'Title': 'Why are differentiable complex functions infinitely differentiable?', 'Tags': ['complex-analysis'], 'AcceptedAnswerId': '820', 'urls': [], 'exp': ['\\\\mathbb R ^2', '\\\\mathbb R ^2'], 'Body': \"When I studied complex analysis, I could never understand how once-differentiable complex functions could be possibly be infinitely differentiable. After all, this doesn't hold for functions from <math_exp> to <math_exp>. Can anyone explain what is different about complex numbers? \"}\n",
      "641 {'Id': '641', 'Type': 'answer', 'ParentId': '640', 'urls': ['http://en.wikipedia.org/wiki/Cauchy%27s_integral_formula'], 'exp': [], 'Body': \"The proofs I have seen derive this as a corollary of Cauchy's integral formula.  Look at the difference quotient as an integral, play around with it, and you get that it converges to what you'd get if you differentiated under the integral sign. Note that since harmonic functions also satisfy a similar integral equation, they are also infinitely differentiable in the same way (this also follows since they are real and imaginary parts of holomorphic functions). \"}\n",
      "642 {'Id': '642', 'Type': 'answer', 'ParentId': '622', 'urls': ['http://en.wikipedia.org/wiki/Eightfold_Way_(physics)'], 'exp': ['$G$', '$8$', '$8$', '$\\\\mathrm{SU}(3)$', '$\\\\mathrm{SU}(3)$', '$\\\\mathrm{SO}(3)$', '$\\\\mathrm{SU}(2)$'], 'Body': 'Particles correspond to specific vectors in a representation, not to <math_exp>-orbits! The reason has to do with \"symmetry breaking.\"  The <math_exp> particles in the meson octet correspond to a basis of a certain <math_exp>-dimensional representation of the group <math_exp> called the \"adjoint representation.\" At high enough energies these particles would be indistinguishable. But at low energies the \"<math_exp> symmetry has been broken\" and the particles become distinguishable. Another good physics example that\\'s easier to understand is that the orbital states of electrons in atoms correspond to representations of the group <math_exp> of symmetries of space (well, really <math_exp> if you want to incorporate spin). Try reading a standard quantum mechanics textbook for a little bit of this picture and then try thinking about it in terms of representation theory. '}\n",
      "644 {'Id': '644', 'Type': 'answer', 'ParentId': '623', 'urls': ['https://math.stackexchange.com/questions/623/why-is-the-volume-of-a-cone-one-third-of-the-volume-of-a-cylinder/635#635', 'http://en.wikipedia.org/wiki/Cavalieris_principle'], 'exp': [' r\\\\sqrt\\\\pi', '\\\\frac13 \\\\cdot h \\\\cdot \\\\pi \\\\cdot r^2. ', 'a/h \\\\times r', '\\\\frac ah \\\\cdot   r\\\\sqrt\\\\pi.', ' \\\\frac13\\\\cdot h \\\\cdot \\\\pi \\\\cdot r^2'], 'Body': '<img src=\"https://i.stack.imgur.com/ibgrF.gif\" alt=\"alt text\"> A visual demonstration for the case of a pyramid with a square base. As Grigory states, Cavalieri&#39;s principle can be used to get the formula for the volume of a cone. We just need the base of the square pyramid to have side length <math_exp>. Such a pyramid has volume <math_exp> <img src=\"https://i.stack.imgur.com/Y3IH1.png\" alt=\"alt text\"> Then the area of the base is clearly the same. The cross-sectional area at distance a from the peak is a simple matter of similar triangles: The radius of the cone&#39;s cross section will be <math_exp>. The side length of the square pyramid&#39;s cross section will be <math_exp> Once again, we see that the areas must be equal. So by Cavalieri&#39;s principle, the cone and square pyramid must have the same volume:<math_exp> '}\n",
      "645 {'Id': '645', 'Type': 'question', 'Title': \"How is prisoner's dilemma different from chicken?\", 'Tags': ['game-theory'], 'AcceptedAnswerId': '647', 'urls': [], 'exp': [], 'Body': \"Chicken is a famous game where two people drive on a collision course straight towards each other. Whoever swerves is considered a 'chicken' and loses, but if nobody swerves, they will both crash. So the payoff matrix looks something like this: But I have heard of another situation called the prisoner's dilemma, where two prisoners are each given the choice to testify against the other, or remain silent. The payoff matrix for prisoner's dilemma also looks like I remember hearing that in the prisoner's dilemma, it was always best for both prisoners to testify. But that makes no sense if you try to apply it to chicken: both drivers would crash every time, and in real life, almost always someone ends up swerving. What's the difference between the two situations? \"}\n",
      "646 {'Id': '646', 'Type': 'answer', 'ParentId': '645', 'urls': [], 'exp': [], 'Body': \"The games aren't just about winning or losing, but also about utility. Here is a more accurate table for chicken: Here is one for prisoners dilemma: In the prisoners dilemma, an individual prisoner will always do better by testifying (look at the table), however, by both testifying they end up in a worse position than if both were silent. In contrast, in chicken, going straight will be better if the other swerves and swerving will be better if the other goes straight. Your tables represent some strategies as being equally good for players when they are not. \"}\n",
      "647 {'Id': '647', 'Type': 'answer', 'ParentId': '645', 'urls': ['http://en.wikipedia.org/wiki/Chicken_%28game%29#Prisoner.27s_dilemma', 'http://en.wikipedia.org/wiki/Chicken_%28game%29'], 'exp': [], 'Body': '(See http://en.wikipedia.org/wiki/Chicken_%28game%29#Prisoner.27s_dilemma.) The difference is in the payoff. In the \"chicken\" game, the payoff matrix is like While in the PD game: Both games have this structure in the payoff table: But: This leads a different Nash equilibria. In the PD game, if A remains silent, B chooses to testify because T > R, while if A testifies, B should also testify because P > S. So testifying is B\\'s most rational choice after considering all possibilities. But in the Chicken game, as S > P, if A goes straight, B should swerve. This leads to two Nash equilibria in the pure game: (St, Sw) and (Sw, St). '}\n",
      "648 {'Id': '648', 'Type': 'answer', 'ParentId': '434', 'urls': [], 'exp': [], 'Body': 'The answer to this question depends on how the problem is defined, but the answer is no, at least without defining the problem in a misleading way. Since my first solution was completely off the mark, I have deleted it and posted this new one. Consider polynomial p. If it has an integer solution, then the solution will eventually be found by random guessing. So if it is impossible to prove the existential status, there must be no solution. Now we know from this link that there is a polynomial, q, that is unsolvable in the integers iff ZFC is consistent. It is well known that ZFC cannot prove its own consistency. So if it ZFC is consistent, then q is unsolvable, but we cannot prove this as then we could prove ZFC. So it seems like it is accurate to say that if mathematics is consistent, we have a polynomial with no integer roots, but we can\\'t prove it. However, if we are assuming maths is consistent, we can use this to prove that the equation is unsolvable (indeed that is what we have done). So, it really isn\\'t accurate an accurate statement at all. To further clarify, when considering mathematical truth, there are two basic ways of viewing it. The first is where we are assuming that are axioms are true, which necessarily means assuming consistency. If we show any problem is equivalent to consistency, then we consider it to be true. The other is where we are considering a formal set of statements, of which the axioms have been defined to be true and seeing which statements can be derived to be true. From this viewpoint, we don\\'t actually know whether the axioms are consistent or not. In fact, Godel\\'s second incompleteness theorem shows that no \"non-trivial\" atomic system can prove its own consistency. So showing a problem is equivalent to consistency is actually the same as showing that the problem is unprovable by the atomic system. The confusion comes from assuming ZFC is consistent to eliminate one possibility in a choice, yet not allowing this assumption to be used as an axiom in the proofs. '}\n",
      "649 {'Id': '649', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': 'A probability problem I love. Take a shuffled deck of cards. Deal off the cards one by one until you reach any Ace. Turn over the next card, and note what it is. The question: which card has a higher probability of being turned over, the Ace of Spades or the Two of Hearts? '}\n",
      "650 {'Id': '650', 'Type': 'answer', 'ParentId': '123', 'urls': [], 'exp': [], 'Body': 'I don\\'t know if you consider General Relativity \"outside acadamia\"(and I don\\'t care to argue the point!) but if you do, the group of symmetries with respect to the Lorentzian Metric can be written as Matrices containing hyperbolic trig functions as elements. Note Kenny\\'s comment. '}\n",
      "652 {'Id': '652', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://rads.stackoverflow.com/amzn/click/0521010608'], 'exp': [], 'Body': \"I've read a fair amount of Sets for Mathematics and found it to be a gentle introduction. http://www.amazon.com/Sets-Mathematics-F-William-Lawvere/dp/0521010608/ref=pd_sim_b_5 \"}\n",
      "653 {'Id': '653', 'Type': 'question', 'Title': 'Real world uses of homotopy theory', 'Tags': ['soft-question', 'big-list', 'applications', 'homotopy-theory'], 'urls': [], 'exp': [], 'Body': 'I covered homotopy theory in a recent maths course. However I was never presented with any reasons as to why (or even if) it is useful. Is there any good examples of its use outside academia? '}\n",
      "654 {'Id': '654', 'Type': 'answer', 'ParentId': '653', 'urls': ['http://www.math.uiuc.edu/~ghrist/index_files/research.htm'], 'exp': [], 'Body': 'Robert Ghrist is an amazing applied mathematician who uses a lot of interesting algebraic topology for engineering applications. He uses homology and sheaf theory. I claim this answers your question since homology is a generalization of homotopy theory. Relevant link: http://www.math.uiuc.edu/~ghrist/index_files/research.htm '}\n",
      "655 {'Id': '655', 'Type': 'question', 'Title': 'Proof for multiplying generating functions', 'Tags': ['generating-functions', 'sequences-and-series'], 'AcceptedAnswerId': '658', 'urls': [], 'exp': ['f(x)', 'g(x)'], 'Body': \"I've learned that multiplying two generating functions <math_exp> and <math_exp> will give the result \\\\begin{equation*} \\\\sum_{k=0}^\\\\infty\\\\left(\\\\sum_{j=0}^k a_j\\\\,b_{k-j}\\\\right)x^k. \\\\end{equation*} I've used the result, but it was presented in my class without proof and I'm having some trouble tracking one down.  Weak google-foo today, I suppose.  Can anyone give me a pointer to a proof?  If this is a question better answered in book form, that is fine as well. \"}\n",
      "656 {'Id': '656', 'Type': 'question', 'Title': 'How can I write an equation that matches any sequence?', 'Tags': ['algebra-precalculus'], 'AcceptedAnswerId': '657', 'urls': [], 'exp': ['$y=4x+1$', ' y=x^2+1', 'x=0', '$c$', 'y=ax^2+bx+c'], 'Body': \"One thing I have been wondering about lately is how to write an equation that describes a pattern of numbers. What I mean is: If I have this, I can tell that an equation that describes this would be <math_exp>. In fact, I don't even need the third pair of numbers. It's very easy when the equation is a straight line. But when the equation is a parabola, its not always that easy. For example: I can tell this is <math_exp>, because I recognize the pattern. But I can't always tell just by looking at the numbers what the right equation should be. Is there some way to always know the right equation? I know that if you get the <math_exp> term you can get the <math_exp> in <math_exp>, but that's not enough to let me solve it like I can when the equation is just a line. For example, can someone show me how you would do it for It's not a homework question, I promise! \"}\n",
      "657 {'Id': '657', 'Type': 'answer', 'ParentId': '656', 'urls': [], 'exp': ['A', '= 0'], 'Body': 'If you know your relationship is going to be a polynomial, then there are some pretty (conceptually) simple ways you can do this. If you know what degree your polynomial is (line, parabola, cubic, etc.) then your job will be much easier.  But if not, then you simply need to look at the number of points you have. When I say \"the best you can do\", what I mean is -- if you have a parabola, but are only given two points, then you really can\\'t identify the parabola.  But you can say that it\\'s a simple line. Let\\'s assume you have three points.  The \"best you can do\" is assume that it is degree 2.  If it is actually of degree one, your answer will magically turn into a line ( your x^2 coefficient will be 0 ) The basic idea of solving relationships/equations is: If you have n unknowns, you need n equations/points. Notice how, in the form of the Parabola ( y = A x2 + B x + C ), you have three unknowns?  And also three equations! (points) Let\\'s pick three arbitrary points You would set up three equations: 6 = 12 * A + 1 * B + C 7 = 22 * A + 2 * B + C 3 = 42 * A + 4 * B + C Three equations, three unknowns.  You should be able to solve this with a combination of most system-of-equation-solving rules. In our case, we find: So our equation is y = -x2 + 4 x + 3 Note that, if your original three points formed a line, your <math_exp> would <math_exp> However, if your equation is NOT a polynomial, then you are left with little more than guess and check, plugging in various coefficients and trials (exponential?  trigonometric?) The beauty of the polynomial approach is that a polynomial of high enough degree will always fit any list of points. (provided that the points form a function) '}\n",
      "658 {'Id': '658', 'Type': 'answer', 'ParentId': '655', 'urls': [], 'exp': [], 'Body': 'It is actually the other way round. A generating function is generally defined to have an addition operation where the components are added and a multiplication operation like that you mentioned. Once we have made these definitions, we observe that polynomials obey the same laws and so that it is convenient to represent generating functions as infinite polynomials rather than just an infinite tuple. '}\n",
      "659 {'Id': '659', 'Type': 'question', 'Title': 'Is this the right categorical generalisation of dual space', 'Tags': ['terminology', 'reference-request', 'category-theory'], 'AcceptedAnswerId': '663', 'urls': [], 'exp': ['V', 'k', 'V^*', 'w^* : V \\\\to k', 'A', 'B', 'C', 'B=hom(A,C)', 'B=A^*', 'V \\\\otimes V^*', 'hom(V,V)', 'k', 'V', 'C', 'A', 'A', 'C', 'A^*:=hom(A,C)', 'A', 'A'], 'Body': \"I am presently looking at a structure that I am trying to pin down- my strategy being to pull the thing up into the greatest possible generality (based on the bits I'm sure about) and narrow it down from there. The situation I have is somewhat similar to the dual space structure in vector spaces, though almost certainly less well-behaved and vector spaces alone will not cut it. Consider a vector space <math_exp> over a field <math_exp>- its dual space <math_exp> appears naturally as the set linear of maps <math_exp> which, coincidentally, form a vector space in themselves. We can generalise this to arbitrary categories <math_exp>, <math_exp>, <math_exp> by setting <math_exp>. Then, at least in some sense, <math_exp>. So far so standard, but I want more: a nice property of dual spaces is that an element of <math_exp> can be canonically seen as an element of <math_exp>- this is because of the way that <math_exp> acts on <math_exp> by multiplication. We can mimic this by letting <math_exp> be a monoid acting on <math_exp>. In summary: A category <math_exp> acted on by a monoid <math_exp> and a dual A <math_exp> I am particularly interested in when <math_exp> is also a monoid, especially so when <math_exp> is a space of stochastic matrices. So this isn't too unlikely a construction, in fact it's probably forehead-slappingly well known, so: As you can probably tell, I am no category theorist, so any help would be awesome. \"}\n",
      "660 {'Id': '660', 'Type': 'answer', 'ParentId': '655', 'urls': ['http://rads.stackoverflow.com/amzn/click/007054235X'], 'exp': ['\\\\sum_{n=0}^{\\\\inf} a_n z^n \\\\cdot \\\\sum_{n=0}^{\\\\inf} b_n z^n = (a_0+a_1z+a_2z^2+ \\\\cdots)(b_0+b_1z+b_2z^2+ \\\\cdots)', '=a_0b_0+(a_0b_1 + a_1b_0)z + (a_0b_2+a_1b_1+a_2b_0)z^2 + \\\\cdots', '=c_0+c_1z+c_2z^2+ \\\\cdots ', 'c_n=\\\\sum_{k=0}^n a_k b_{n-k}'], 'Body': 'Casebash is correct that this is a definition and not a theorem. But the motivation from 3.48 (Defintion of product of series) of little Rudin may convince you that this is a good definition: <math_exp> <math_exp> <math_exp> where <math_exp> '}\n",
      "661 {'Id': '661', 'Type': 'answer', 'ParentId': '656', 'urls': [], 'exp': [], 'Body': \"Given a list of terms of a sequence as you describe, one technique that may be of use (supplementary to Justin's answer) is finite differences.  Calculate the differences between successive terms.  If these first differences are constant, then a linear equation fits the terms you have.  If not, compute the differences of the differences.  If these second differences are constant, then a quadratic equation fits the terms you have.  If not, you can continue to compute differences until you reach a constant difference (in the nth differences means an nth degree polynomial), differences that are a constant multiple of the previous differences (exponential of some sort), or you run out of terms.  In any case, what you find is limited to matching the terms you know, as without some kind of general rule for the sequence, the first unknown term could be anything and completely alter the pattern (and with n known terms, a polynomial of degree n-1 will always perfectly fit). \"}\n",
      "662 {'Id': '662', 'Type': 'answer', 'ParentId': '656', 'urls': ['http://en.wikipedia.org/wiki/Inductive_inference'], 'exp': [], 'Body': 'In the general case of trying to predict some infinite sequence of integers, there is no formula. This is because there is no reason to expect a pattern to continue, since all sequences are possible. However, you can say a certain number is more likely given a set of functions. For instance if you considered all Turing machines, you could say that given n elements of a sequence look at all the Turing machines that predict the current sequence, and then find the most predicted next number. There still isn\\'t a efficient way to compute what the most likely next number is. Ray Solomonoff called this \"Universal Probabilistic Induction\" This is explained in more depth here: http://en.wikipedia.org/wiki/Inductive_inference '}\n",
      "663 {'Id': '663', 'Type': 'answer', 'ParentId': '659', 'urls': ['http://en.wikipedia.org/wiki/Dual_object', 'http://en.wikipedia.org/wiki/Tensor_category', 'http://www-math.mit.edu/~etingof/tenscat1.pdf'], 'exp': ['V \\\\times V^* \\\\to k', 'k', 'V \\\\otimes V^* \\\\to k', 'k \\\\to V \\\\times V^*', '\\\\sum e_i \\\\otimes e_i^{\\\\vee}', 'e_i', 'V', 'e_i^{\\\\vee}', 'e_i', 'V \\\\to (k) \\\\otimes V \\\\to (V \\\\otimes V^*) \\\\otimes V \\\\to V \\\\otimes (V^* \\\\otimes V)', 'V', 'V^*', 'k'], 'Body': 'I believe the right generalization of dual spaces is that of a dual object in a tensor category, which I will assume symmetric for convenience. Recall what makes a dual space of a vector space work: We have a map <math_exp> (for <math_exp> the ground field). The problem is, this isn\\'t a homomorphism in the category of vector spaces; it is rather a bilinear map. So you can think of it as a map <math_exp> instead.  This is why you need a tensor structure to think of duals. This isn\\'t enough, though, because we need to know that the pairing is nondegenerate.  One way to express this is that there\\'s a map <math_exp> mapping 1 to the \"Casimir element\" (which is the sum <math_exp> where <math_exp> ranges over a basis of <math_exp> and <math_exp> the dual basis; it is independent of the choice of <math_exp> as a quick computation shows).  The Casimir morphism satisfies the condition that <math_exp> is just the identity. Conversely, this is enough to show that the pairing is nondegenerate. So, anyway, how does this make sense in a symmetric tensor category?  Basically, <math_exp> is the object, <math_exp> the putative dual, and <math_exp> replaced by the unital object. This definition is entirely arrow-theoretic, and it all goes through as usual. It is an exercise to check that the dual is unique. Some examples: This coincides with the usual dual in the category of vector spaces This coincides with the dual sheaf if one is working in the category of locally free sheaves on a scheme This corresponds to the dual (contragredient) representation in the (tensor) category of representations of any Hopf algebra (so this includes representations of finite groups and Lie algebras) Oh, and what happens if you don\\'t have a symmetric tensor category? Then you have to worry about \"left\" and \"right\" duals, respectively.  For more about all this, I recommend the notes of Pavel Etingof on tensor categories. '}\n",
      "664 {'Id': '664', 'Type': 'question', 'Title': 'Finding Moment of Inertia (Rotational Inertia?) <span class=\"math-container\" id=\"6751\">I</span> Using Integration?', 'Tags': ['calculus', 'physics'], 'AcceptedAnswerId': '665', 'urls': [], 'exp': ['F = m a', '\\\\tau = I \\\\alpha', '\\\\tau', '\\\\alpha', 'I', 'I', 'm r^2', 'r', 'I = \\\\int r^2 \\\\,dm', ' I = m r^2 ', 'r', 'r', 'm', 'r', 'm', 'm', 'r', 'w'], 'Body': \"I just came back from my Introduction to Rotational Kinematics class, and one of the important concepts they described was Rotational Inertia, or Moment of Inertia. It's basically the equivalent of mass in Netwon's <math_exp> in linear motion.  The equivalent rotational equation is <math_exp>, where <math_exp> is rotational force, <math_exp> is rotational acceleration, and <math_exp> is rotational inertia. For a point about an axis, <math_exp> is <math_exp>, where <math_exp> is the distance from the point to the axis of rotation. For a continuous body, this is an integral -- <math_exp>. This really doesn't make any sense to me...you have two independent variables?  I am only used to having one independent variable and one constant.  So I would solve this, using my experience with calculus (which encompasses a read through the Sparks Notes packet) as <math_exp> But obviously, this is wrong?  <math_exp> is not a constant!  How do I deal with it?  Do I need to replace <math_exp> with an expression that varies with <math_exp>?  But how could <math_exp> possibly vary with <math_exp>?  Isn't it more likely the other way around?  But how can <math_exp> vary with <math_exp>?  It's all rather confusing me. Could someone help me figure out what to do with all these substitutions for, example, figuring out the Moment of Inertia of a hoop with no thickness and width <math_exp>, with the axis of rotation running through its center orthogonal to its plane? \"}\n",
      "665 {'Id': '665', 'Type': 'answer', 'ParentId': '664', 'urls': [], 'exp': ['I = \\\\int r^2 dm', 'I = \\\\int x^2 dx', 'I = \\\\int r^2 dm = \\\\int |x|^2 dx = \\\\int x^2 dx = (x^3)/3+c', ' I = \\\\frac{(L/8)}{3}-\\\\frac{(-L/8)}{3}=\\\\frac{L}{12} ', 'R^2-r^2', '\\\\frac{m}{R^2-r^2}', '2\\\\pi~k~\\\\mathrm{d}k', '2\\\\pi~k~\\\\mathrm{d}k\\\\frac{m}{R^2-r^2}', '2\\\\pi~k^3~\\\\mathrm{d}k\\\\frac{m}{R^2-r^2}', '\\\\frac{\\\\pi}{2}k^4\\\\frac{m}{R^2-r^2}+c', ' \\\\frac{\\\\pi}{2}(R^4-r^4)\\\\frac{m}{R^2-r^2} = \\\\frac{\\\\pi}{2}(R^2+r^2)m '], 'Body': 'Seeing an expression like <math_exp> is certainly confusing the first time you see it. When you see an expression like <math_exp> we effectively iterating over a range on the x axis and adding up the area of an infinite amount of infinitesimally small strips. Note that the area of each strip is approximated as the function value (x^2) times the strip length (dx). When you see an expression like dm, we are iterating over masses instead. We still sum up the function value (r^2), but this time we multiply it by the strip mass (dm). To solve the problem, we usually put m in terms of another variable which we can iterate over more easily. For example, consider the moment of inertia of a rod of length L around its center with total mass of L. Each bit of length (dx) has mass (dm) and r=|x|. Solving for <math_exp>. Now, we have definite values for x to sub in (-L/2 and L/2), so we write <math_exp> Now lets calculate the moment of inertia of the hoop case you described. We break the hoop up into infinitesimally small rings the same distance from the center. Let the hoop have inner thickness r and outer thickness R. The area is <math_exp>. The area density, d, is therefore <math_exp>. A ring at radius k with thickness dk has area <math_exp>, mass is <math_exp> and moment of inertia around the central axis <math_exp>. The integral is <math_exp>. Subbing in the exact values, we get <math_exp> '}\n",
      "667 {'Id': '667', 'Type': 'question', 'Title': 'Stacks are just sheaves up to Isomorphism', 'Tags': ['intuition', 'algebraic-geometry'], 'AcceptedAnswerId': '900', 'urls': [], 'exp': [], 'Body': 'I have heard that one can think of stacks on a site as taking sheaves but instead of the restrictions being equal, we just loosen it to isomorphic, and treat the sheaf conditions with the \"obvious\" coherence relations. How seriously can one take this analogy? Note that my background is stacks is feeble at best. I hope this question isn\\'t too vague. One may choose to respond to this question with \"That analogy is stupid because of __ gotcha\". Thanks in advance! '}\n",
      "668 {'Id': '668', 'Type': 'question', 'Title': \"What's an intuitive way to think about the determinant?\", 'Tags': ['linear-algebra', 'matrices', 'determinant', 'intuition'], 'AcceptedAnswerId': '669', 'urls': [], 'exp': ['2\\\\times 2', '$n \\\\times n$'], 'Body': 'In my linear algebra class, we just talked about determinants. So far I’ve been understanding the material okay, but now I’m very confused. I get that when the determinant is zero, the matrix doesn’t have an inverse. I can find the determinant of a <math_exp> matrix by the formula. Our teacher showed us how to compute the determinant of an <math_exp> matrix by breaking it up into the determinants of smaller matrices. Apparently there is a way by summing over a bunch of permutations. But the notation is really hard for me and I don’t really know what’s going on with them anymore. Can someone help me figure out what a determinant is, intuitively, and how all those definitions of it are related? '}\n",
      "669 {'Id': '669', 'Type': 'answer', 'ParentId': '668', 'urls': [], 'exp': ['\\\\det: \\\\mathbb{R}^{n^2} \\\\rightarrow \\\\mathbb{R}', ' \\\\det(a \\\\vec v_1 +b \\\\vec w_1 , \\\\vec v_2 ,\\\\ldots,\\\\vec v_n ) = a \\\\det(\\\\vec v_1,\\\\vec v_2,\\\\ldots,\\\\vec v_n) + b \\\\det(\\\\vec w_1, \\\\vec v_2, \\\\ldots,\\\\vec v_n),', 'I', '1', '(a,b)', '(c,d)', '\\\\Big|{}^{a\\\\;b}_{c\\\\;d}\\\\Big|'], 'Body': 'Your trouble with determinants is pretty common. They’re a hard thing to teach well, too, for two main reasons that I can see: the formulas you learn for computing them are messy and complicated, and there’s no “natural” way to interpret the value of the determinant, the way it’s easy to interpret the derivatives you do in calculus at first as the slope of the tangent line. It’s hard to believe things like the invertibility condition you’ve stated when it’s not even clear what the numbers mean and where they come from. Rather than show that the many usual definitions are all the same by comparing them to each other, I’m going to state some general properties of the determinant that I claim are enough to specify uniquely what number you should get when you put in a given matrix. Then it’s not too bad to check that all of the definitions for determinant that you’ve seen satisfy those properties I’ll state. The first thing to think about if you want an “abstract” definition of the determinant to unify all those others is that it’s not an array of numbers with bars on the side. What we’re really looking for is a function that takes N vectors (the N columns of the matrix) and returns a number. Let’s assume we’re working with real numbers for now. Remember how those operations you mentioned change the value of the determinant? Switching two rows or columns changes the sign. Multiplying one row by a constant multiplies the whole determinant by that constant. The general fact that number two draws from: the determinant is linear in each row. That is, if you think of it as a function <math_exp>, then <math_exp> and the corresponding condition in each other slot. The determinant of the identity matrix <math_exp> is <math_exp>. I claim that these facts are enough to define a unique function that takes in N vectors (each of length N) and returns a real number, the determinant of the matrix given by those vectors. I won’t prove that, but I’ll show you how it helps with some other interpretations of the determinant. In particular, there’s a nice geometric way to think of a determinant. Consider the unit cube in N dimensional space: the set of vectors of length N with coordinates 0 or 1 in each spot. The determinant of the linear transformation (matrix) T is the signed volume of the region gotten by applying T to the unit cube. (Don’t worry too much if you don’t know what the “signed” part means, for now). How does that follow from our abstract definition? Well, if you apply the identity to the unit cube, you get back the unit cube. And the volume of the unit cube is 1. If you stretch the cube by a constant factor in one direction only, the new volume is that constant. And if you stack two blocks together aligned on the same direction, their combined volume is the sum of their volumes: this all shows that the signed volume we have is linear in each coordinate when considered as a function of the input vectors. Finally, when you switch two of the vectors that define the unit cube, you flip the orientation. (Again, this is something to come back to later if you don’t know what that means). So there are ways to think about the determinant that aren’t symbol-pushing. If you’ve studied multivariable calculus, you could think about, with this geometric definition of determinant, why determinants (the Jacobian) pop up when we change coordinates doing integration. Hint: a derivative is a linear approximations of the associated function, and consider a “differential volume element” in your starting coordinate system. It’s not too much work to check that the area of the parallelogram formed by vectors <math_exp> and <math_exp> is <math_exp> either: you might try that to get a sense for things. '}\n",
      "670 {'Id': '670', 'Type': 'answer', 'ParentId': '432', 'urls': ['http://en.wikipedia.org/wiki/Complex_conjugate_root_theorem'], 'exp': ['\\\\{ r_1, r_2, r_3 \\\\}', 'r_1', 'r_2', '0', 'r_1 - r_2', '0', '\\\\rho = x+ yi', '\\\\overline{\\\\rho} = x - yi', '  \\\\begin{align*}  (\\\\rho - \\\\overline{\\\\rho})^2 (\\\\rho - r_3)^2 (\\\\overline{\\\\rho} - r_3)^2   &amp;= (2yi)^2 (x + yi - r^3)^2  (x - yi - r^3)^2   \\\\\\\\ &amp;= -4y^2 [((x - r_3) + yi) ((x - r_3) - yi) ]^2   \\\\\\\\ &amp;= -4y^2 ((x - r_3)^2 + y^2)^2   \\\\end{align*}  ', '0', '\\\\Delta'], 'Body': 'These implications are reached by considering the three, different cases for the roots <math_exp> of the polynomial: repeated root, all distinct real roots, or two complex roots and one real root. When one of the roots is repeated, say <math_exp> and <math_exp>, then it is clear that the discriminant is <math_exp> because the <math_exp> term of the product is <math_exp>. When one root is a complex number <math_exp>, then by the complex conjugate root theorem, <math_exp> is also a root. By the same theorem, the remaining third root must be real. Evaluating the product in the discriminant for this case, <math_exp> which is less than or equal to <math_exp>. Finally, when all roots are real, the product is clearly positive. Putting it all together, <math_exp>: '}\n",
      "671 {'Id': '671', 'Type': 'question', 'Title': 'When you randomly shuffle a deck of cards, what is the probability that it is a unique permutation never before configured?', 'Tags': ['probability'], 'AcceptedAnswerId': '672', 'urls': ['http://en.wikipedia.org/wiki/Birthday_Paradox'], 'exp': ['3 \\\\cdot 10^{14}', '1', '3 \\\\cdot 10^{14}', '\\\\frac{1}{52!} \\\\cdot 3 \\\\cdot 10^{14}', '0.5', '0.5', '0.9'], 'Body': 'I just came back from a class on Probability in Game Theory, and was musing over something in my head. Assuming, for the sake of the question: This would, approximately, be on the order of <math_exp> random shuffles in the history of playing cards. If I were to shuffle a new deck today, completely randomly, what are the probabilistic odds (out of <math_exp>) that you create a new unique permutation of the playing cards that has never before been achieved in the history of <math_exp> similarly random shuffles? My first thought was to think that it was a simple matter of <math_exp>, but then I ran into things like Birthday Paradox.  While it is not analogous (I would have to be asking about the odds that any two shuffled decks in the history of shuffled decks ever matched), it has caused me to question my intuitive notions of Probability. What is wrong in my initial approach, if it is wrong? What is the true probability? And, if the probability is less than <math_exp>, if we how many more years (centuries?) must we wait, assuming the current rate of one billion shuffles per day, until we reach a state where the probability is <math_exp>+?   <math_exp>+? (Out of curiosity, it would be neat to know the analogous birthday paradox answer, as well) '}\n",
      "672 {'Id': '672', 'Type': 'answer', 'ParentId': '671', 'urls': [], 'exp': ['\\\\dfrac{3 \\\\times 10^{14}}{52!}', '\\\\left(1-\\\\frac1{52!}\\\\right)^{(3\\\\times10^{14})}', 'n\\\\epsilon', '(1+\\\\epsilon)^n', '1+n\\\\epsilon', '52!\\\\approx 8\\\\times 10^{67}', '\\\\dfrac{3\\\\times10^{14}}{52!}\\\\approx 3.75\\\\times 10^{-54}', '1-\\\\left(1-\\\\frac1{52!}\\\\right)^{(3\\\\times10^{14})}', '\\\\frac1{52!}\\\\times (3\\\\times10^{14})'], 'Body': 'Your original answer of <math_exp> is not far from being right. That is in fact the expected number of times any ordering of the cards has occurred. The probability that any particular ordering of the cards has not occurred, given your initial assumptions, is <math_exp>, and the probability that it has occurred is 1 minus this value.  But for small values of <math_exp>, <math_exp> is nearly <math_exp>.  In particular, since <math_exp> and so <math_exp> is microscopically small, <math_exp> is very nearly <math_exp>. '}\n",
      "673 {'Id': '673', 'Type': 'answer', 'ParentId': '671', 'urls': [], 'exp': [], 'Body': \"Suppose we shuffle a deck and get a permutation p. For each previous shuffling there is a 1-1/52! chance that p doesn't match it. Each previous shuffling is independent, in that regardless of what p and the other permutations are, the chance of p matching the shuffling is 1-1/52! When probabilities are independent we can simply multiple them to find the chance of all the events happening. In this case, the each event is actually a match not happening, so the chance of no matches given n previous shuffles is (1-1/52!)^n. We can then complete the calculations as Michael did. \"}\n",
      "674 {'Id': '674', 'Type': 'question', 'Title': 'How do Lagrange multipliers work to find the lowest value of a function subject to a constraint?', 'Tags': ['calculus', 'optimization', 'lagrange-multiplier'], 'AcceptedAnswerId': '2405', 'urls': [], 'exp': [], 'Body': \"I have been using Lagrange multipliers in constrained optimization problems, but I don't see how they actually work to simultaneously satisfy the constraint and find the lowest possible value of an objective function. \"}\n",
      "675 {'Id': '675', 'Type': 'answer', 'ParentId': '674', 'urls': ['http://en.wikipedia.org/wiki/Constraint_optimization', 'http://en.wikipedia.org/wiki/Lagrange_multipliers', 'http://en.wikipedia.org/wiki/Lagrange_multipliers#Interpretation_of_the_Lagrange_multipliers'], 'exp': ['f(x,y) = x^2 + y^2', 'x=0', 'y=0', 'x', 'y', '3x+y=6', '3\\\\cdot 0 + 1 \\\\cdot 0 \\\\neq 6', 'f(x,y) = x^2 + y^2', '3x+y=6', 'L', 'x', 'y', '\\\\lambda', 'L(x,y,\\\\lambda) = x^2 + y^2 + \\\\lambda(3x+y-6)', 'L', 'x^2 + y^2', '\\\\lambda', '3x+y-6', 'L', 'L', 'x,y', '\\\\lambda', '\\\\nabla L = 0:', '\\\\frac{\\\\partial L}{\\\\partial x} = 2x + 3 \\\\lambda = 0', '\\\\frac{\\\\partial L}{\\\\partial y} = 2y + \\\\lambda = 0', '\\\\frac{\\\\partial L}{\\\\partial \\\\lambda} = 3x + y - 6 = 0', 'x, y', '\\\\lambda', '(x,y,f(x,y))'], 'Body': 'This type of problem is generally referred to as constrained optimization.  A general technique to solve many of these types of problems is known as the method of Lagrange multipliers, here is an example of such a problem using Lagrange multipliers and a short justification as to why the technique works. Consider the parabaloid given by <math_exp>.  The global minimum of this surface lies at the origin (at <math_exp>, <math_exp>).  If we are given the constraint, a requirement on the relationship between <math_exp> and <math_exp>, that <math_exp>, then the origin can no longer be our solution (since <math_exp>).  Yet, there is a lowest point on this function satisfying the given constraint. What we have so far: Objective function: <math_exp>,  subject to: <math_exp>. From here we can derive the Lagrange formulation of our constrained minimization problem.  This will be a function <math_exp> of <math_exp>, <math_exp>, and a single Lagrange multiplier <math_exp> (since we have only a single constraint). It will be this new function that we minimize. <math_exp> The Lagrange formulation incorporates our original function along with our constraint(s).  On the way toward minimizing <math_exp>, we will have to minimize the objective function <math_exp>, as well as minimize the contribution from the constraint, which is now weighted by a factor of <math_exp>.  If the constraint is met, then the expression <math_exp> will necessarily be zero, and will not contribute anything the value of <math_exp>. This is the trick of the technique. Minimizing the Lagrange formulation: To minimize <math_exp> we simply find the <math_exp>, and <math_exp> values that make its gradient zero.  (This is exactly analogous to setting the first derivative to zero in calculus.) <math_exp> <math_exp> <math_exp> <math_exp>, In our example we have arrived at a system of simultaneous linear equations which can (and should) be solved with matrix algebra.  The solution will be a vector holding values for <math_exp> and <math_exp>.  The lowest value of the objective function, subject to the given constraint, sits at <math_exp>, and the Lagrange multiplier does not have an immediate physical interpretation.  (The multipliers have meaning when appearing in certain contexts, more info on that here.) '}\n",
      "676 {'Id': '676', 'Type': 'question', 'Title': 'Probability that a stick randomly broken in two places can form a triangle', 'Tags': ['probability', 'monte-carlo'], 'AcceptedAnswerId': '679', 'urls': [], 'exp': ['\\\\frac14', '0', '1', 'x', 'y', '1000', '0.19', '0.25', '0.19', '10'], 'Body': \"Randomly break a stick (or a piece of dry spaghetti, etc.) in two places, forming three pieces.  The probability that these three pieces can form a triangle is <math_exp> (coordinatize the stick form <math_exp> to <math_exp>, call the breaking points <math_exp> and <math_exp>, consider the unit square of the coordinate plane, shade the areas that satisfy the triangle inequality edit: see comments on the question, below, for a better explanation of this). The other day in class*, my professor was demonstrating how to do a Monte Carlo simulation of this problem on a calculator and wrote a program that, for each trial did the following: He ran around <math_exp> trials and was getting <math_exp>, which he said was probably just random-chance error off <math_exp>, but every time the program was run, no matter who's calculator we used, the result was around <math_exp>. What's wrong with the simulation method?  What is the theoretical answer to the problem actually being simulated? (* the other day was more than <math_exp> years ago) \"}\n",
      "677 {'Id': '677', 'Type': 'question', 'Title': 'Proving that this Game on Polygons Ends', 'Tags': ['open-problem', 'contest-math'], 'AcceptedAnswerId': '1105', 'urls': [], 'exp': ['N', 'S', 'N', 'S', '2003^{2003}', 'n=4', 'S', 'N', 'S'], 'Body': 'About two years ago, I started thinking about the following problem: You\\'re given an <math_exp> and an <math_exp>, positive integers. You start with an <math_exp>-gon that has positive integer labels at each vertex, such that the labels sum to <math_exp>. A \"move\" is replacing the label at some vertex with the positive difference of the labels at its two adjacent vertices. You win if you can exhibit a sequence of moves such that all of the vertex labels are zero at the end. This is a generalization of 2003 USAMO problem B3, \"A positive integer is written at each vertex of a hexagon. A move is to replace a number by the (non-negative) difference between the two numbers at the adjacent vertices. If the starting numbers sum to <math_exp>, show that it is always possible to make a sequence of moves ending with zeros at every vertex.\" The USAMO problem itself can be answered in the affirmative by doing a modified greedy algorithm for most of the moves, showing there\\'s always a move where you can reduce the problem to a smaller sum while changing the parities systematically, then doing casework for the \"end game\" to show that each configuration of 1\\'s and 0\\'s (or k\\'s and zeros, the cases are of course equivalent) can be gotten down to all zeros. It\\'s easy to check that for <math_exp> and <math_exp> odd, there\\'s a winning strategy. I have lots of simulation/numerical evidence and some half baked ideas to suggest that, in general, there\\'s a winning strategy if and only if <math_exp> and <math_exp> have opposite parities. But in two years of coming back to it on and off, I haven\\'t really made progress. It\\'s open, as far as I know, though I knew that more surely two years ago than I do now. Ideas? '}\n",
      "679 {'Id': '679', 'Type': 'answer', 'ParentId': '676', 'urls': [], 'exp': ['1/2', '1/3', 'y', '[0, 1-x]', '1/4', 'y (1-x)', 'y', 'y', '0', '1', 'y=1-x', '\\\\int_0^{1/2}\\\\frac1{2-2x}-\\\\frac{2x-1}{2x-2}\\\\,dx=\\\\ln 2-\\\\frac12\\\\approx0.19314'], 'Body': 'The three triangle inequalities are \\\\begin{align} x + y &amp;&gt; 1-x-y \\\\\\\\ x + (1-x-y) &amp;&gt; y \\\\\\\\ y + (1-x-y) &amp;&gt; x \\\\\\\\ \\\\end{align} Your problem is that in picking the smaller number first from a uniform distribution, it\\'s going to end up being bigger than it would if you had just picked two random numbers and taken the smaller one. (You\\'ll end up with an average value of <math_exp> for the smaller instead of <math_exp> like you actually want.) Now when you pick <math_exp> on <math_exp>, you\\'re making it smaller than it should be (ending up with average value of <math_exp>). To understand this unequal distribution, we can substitute <math_exp> for <math_exp> in the original inequalities and we\\'ll see the proper distribution. <img src=\"https://i.imgur.com/CARdj.png\" alt=\"\"> (Note that the <math_exp>-axis of the graph doesn\\'t really go from <math_exp> to <math_exp>; instead the top represents the line <math_exp>. I\\'m showing it as a square because that\\'s how the probabilities you were calculating were being generated.) Now the probability you\\'re measuring is the area of the strangely-shaped region on the left, which is <math_exp> I believe that\\'s the answer you calculated. '}\n",
      "680 {'Id': '680', 'Type': 'question', 'Title': 'Probability that two people see each other at the coffee shop', 'Tags': ['probability'], 'AcceptedAnswerId': '681', 'urls': [], 'exp': [], 'Body': 'Two mathematicians each come into a coffee shop at a random time between 8:00 a.m. and 9:00 a.m. each day. Each orders a cup of coffee then sits at a table, reading a newspaper for 20 minutes before leaving to go to work. On any day, what is the probability that both mathematicians are at the coffee shop at the same time (that is, their arrival times are within 20 minutes of each other)? '}\n",
      "681 {'Id': '681', 'Type': 'answer', 'ParentId': '680', 'urls': [], 'exp': [], 'Body': 'Working in hours and letting 8:00 a.m. be t=0, each mathematician\\'s arrival time is a number between 0 and 1.  The sample space can be represented by the unit square in the coordinate plane with one professor\\'s arrival time as x and the other\\'s as y, where regions with equal areas are equally likely.  We want x - 1/3 &lt; y &lt; x + 1/3 -- that is, the second professor arrives earlier than the first by no more than 1/3 of an hour or later than the first by no more than 1/3 of an hour. <img src=\"https://i.stack.imgur.com/Z2M3h.png\" alt=\"plot of the inequalities\"> The area of the desired region is 5/9. '}\n",
      "682 {'Id': '682', 'Type': 'answer', 'ParentId': '593', 'urls': [], 'exp': ['\\\\frac{\\\\partial}{\\\\partial x}vol(\\\\text{n-simplex w. edge x}) = vol(\\\\text{(n-1)-simplex w. edge x})', 'e(x):=\\\\sum_n vol\\\\text{(n-simplex w. edge x)}', \"e'(x)=e(x)\", 'e(x)=Ce^x'], 'Body': \"The answer is, it's just a fact &ldquo;cone over a simplex is a simplex&rdquo; rewritten in terms of the generating function: observe that because n-simplex is a cone over (n-1)-simplex <math_exp>; in other words <math_exp> satisfies an equvation <math_exp>. So <math_exp> -- and C=1 because e(0)=1. \"}\n",
      "683 {'Id': '683', 'Type': 'question', 'Title': 'How can I randomly generate trees?', 'Tags': ['probability-theory', 'random'], 'AcceptedAnswerId': '787667', 'urls': [], 'exp': [], 'Body': 'I want to randomly generate trees, i.e. undirected acyclic graphs with a single root, making sure that all possible trees with  a fixed number of nodes n are equally likely. '}\n",
      "684 {'Id': '684', 'Type': 'answer', 'ParentId': '683', 'urls': ['http://www-cs-faculty.stanford.edu/~uno/fasc4a.ps'], 'exp': [], 'Body': 'Knuth says to look at it as generating all nested parentheses in lexicographic order. Look here for the details http://www-cs-faculty.stanford.edu/~uno/fasc4a.ps. '}\n",
      "685 {'Id': '685', 'Type': 'answer', 'ParentId': '618', 'urls': ['http://en.wikipedia.org/wiki/Geometric_series'], 'exp': ['n', 'r=1+R/100', 'P', 'k', 'n-k', 'Pr^{n-k}', '\\\\sum\\\\limits_{k=1}^n Pr^{n-k}', '\\\\sum\\\\limits_{k=0}^{n-1} Pr^k', '\\\\frac{ar^{n-1}}{r-1}', 'a', 'r', 'n', 'n', 'Mr^n', '\\\\frac{Pr^{n-1}}{r-1} = Mr^n', 'n', 'r^n', '\\\\frac{P}{r-1} = r^n\\\\frac{M-P}{r-1}', 'r^n = \\\\frac{P}{M(r-1)-P}', 'n', 'M', 'r', 'P', 'n', 'P=\\\\frac{Mr^n(r-1)}{r^{n-1}}', '\\\\sum\\\\limits_{k=1}^n Pr^k'], 'Body': \"Basic Theory The way to solve this problem is to calculate how much each payment reduces your debt after you have been repaying your loan for <math_exp> years. Let <math_exp>, ie. this converts the interest rate from a percentage to a value you can multiply your debt by to calculate how much you owe after adding one time period's interest. If I make a payment of <math_exp> at the end of the <math_exp>th year, then we avoid paying interest on this money <math_exp> times and so we reduce our debt by <math_exp>. We sum up the future values of all our payments: <math_exp> If we reverse this, it is equivalent to: <math_exp> This is a geometric series, which can be solved using the formula <math_exp> where <math_exp> is the first term, <math_exp> is the factor and <math_exp> is the number of terms being summed. We then attempt to equate this with the debt owed after <math_exp> years, which is <math_exp>. We now compare the two equations: <math_exp> Calculating <math_exp> We group the <math_exp> terms: <math_exp> <math_exp> So we just take the <math_exp>th log of the right hand side. Calculating repayments Given the principal (<math_exp>) and the interest rate (<math_exp>), what will my payment-per-term (<math_exp>) be over <math_exp> accruation terms? <math_exp> Payments made at the start of the year In this case, the future values of our interest payment simply become: <math_exp> We proceed as we did before. Notes We could also solve this problem using present value instead of future value. \"}\n",
      "686 {'Id': '686', 'Type': 'question', 'Title': 'Combinations of selecting <span class=\"math-container\" id=\"7242\">n</span> objects with <span class=\"math-container\" id=\"7243\">k</span> different types', 'Tags': ['combinatorics'], 'AcceptedAnswerId': '691', 'urls': [], 'exp': ['k', 'n'], 'Body': 'Suppose that I am buying cakes for a party. There are <math_exp> different types and I intend to buy a total of <math_exp> cakes. How many different combinations of cakes could I possibly bring to the party? '}\n",
      "687 {'Id': '687', 'Type': 'question', 'Title': 'Increasing network throughput by cutting routes', 'Tags': ['graph-theory', 'scoring-algorithm'], 'AcceptedAnswerId': '1050', 'urls': [], 'exp': [], 'Body': 'Suppose we model traffic flow between two points with a directed graph. Each route has either a constant travel time or one that linearly increases with traffic. We assume that each driver wishes to minimise their own travel time and we assume that the drivers form a Nash equilibria. Can removing a route ever decrease the average travelling time? Note that the existence of multiple Nash equilibria makes this question a bit complicated. To clarify, I am looking for a route removal that will guarantee a decrease in the average traveling time regardless of the Nash equilibria that are chosen before and after. '}\n",
      "690 {'Id': '690', 'Type': 'answer', 'ParentId': '686', 'urls': ['http://en.wikipedia.org/wiki/Combination#Number_of_combinations_with_repetition'], 'exp': [], 'Body': \"Let g(n,k) = # combinations of cakes. Notice that: If we think of k as a radix rather than the # of cakes, then this problem is equivalent to expressing the # of distinct n-digit numbers in base k whose digits are in sorted order. (e.g. 1122399 is equivalent to 9921231) I think I can express it as a nonrecursive sum: g(n,k) = sum from j=1 to max(n,k) of { (k choose j) * h(n,j) } where h(n,j) is the # of ways to partition N cakes using j different types. (the term in the sum is when there are j distinct cakes actually chosen.) But that's about as far as I can get... :/ edit: looks like it's combinations with repetitions = ((k+n-1) choose n). (same as the wikipedia article with n and k swapped) \"}\n",
      "691 {'Id': '691', 'Type': 'answer', 'ParentId': '686', 'urls': [], 'exp': ['n', 'k-1', 'n=6', 'k=5', 'k-1', 'n+k-1', ' \\\\binom{n+k-1}{k-1}. '], 'Body': 'Using a method that\\'s often called \"stars and bars\": We draw <math_exp> stars in a row to represent the cakes, and <math_exp> bars to divide them up. All of the stars to the left of the first bar are cakes of the first type; stars between the first two bars are of the second type; .\\xa0. .\\xa0. Here\\'s an example with <math_exp> and <math_exp>. We\\'re getting 2 of the first type, 3 of the second type, 0 of the third type, 1 of the fourth type, and 0 of the fifth type. In order to solve the problem, we just need to reorder the stars and bars by choosing the <math_exp> spots for the bars out of the <math_exp> total spots, so our answer is: <math_exp> '}\n",
      "692 {'Id': '692', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': [], 'Body': 'Complex numbers are the final step in a sequence of increasingly \"unreal\" extensions to the number system that humans have found it necessary to add over the centuries in order to express significant numerical concepts. The first such \"unreal number\" was zero, back in the mists of time. It seems obvious to us now, but it must have seemed strange at first. How can the number of sheep I have be zero, when I don\\'t actually have any sheep? Negative numbers are the next most obvious addition to the family of numbers. But what does it mean to have -2 apples? If I have 3 apples and you have 5, it\\'s convenient to be able to say that I have -2 more apples than you. Even so, during the middle ages many mathematicians were very uncomfortable with the idea of negative numbers and tried to arrange their equations so that they didn\\'t occur. Rationals (fractions) seem real enough, since I\\'m happy to have 2/5 of a pizza. However, this is not the number of pizzas that I have, just a ratio between 0 and 1 pizzas, and so is further removed from the concept of counting. More significant philosophical problems problems arose when irrational numbers were first discovered by the classical Greeks. They were astonished when Euclid (or one of his predecessors) proved that the diagonal of a unit square could not be represented by ratio of two integers. This was such an outrageous idea to them that they called these numbers \"irrational\". However, they couldn\\'t easily deny their existence since they have a direct geometric representation. Irrational numbers were first understood as the solutions to algebraic equations such as x^2 = 2 but this still doesn\\'t cover all the possible numbers that we need. For example, the ratio of a circle\\'s diameter to its circumference is π and so has a direct geometric representation. However, in 1882 π was proved to be transcendental, meaning that it can\\'t be defined as the solution to a specific algebraic equation. This begins to seem a lot less \"real\", especially when you consider that there are many important transcendental numbers that, unlike π, don\\'t have a geometric interpretation. There are of course many algebraic equations that don\\'t have a solution even among the irrational numbers, and in some ways there\\'s no reason why they should. However, when 16th century mathematicians like Gerolamo Cardano began working on solutions to cubic equations they found that the square roots of negative numbers began cropping up very naturally in their procedures, even though the solutions themselves were purely real. This eventually led people to explore the arithmetic of complex numbers and they were surprised to find that it produced a consistent and beautiful theory. However, complex numbers don\\'t have such an intuitively obvious geometrical meaning as the numbers that came before. They are typically represented graphically as points in the 2D plane, and the rules of addition and multiplication are equivalent to certain operations on lengths and angles, but those operations aren\\'t driven by geometrical necessity in quite the way that squares and circles are. Even so, complex numbers are a perfect representation for various physical phenomena such as the state of particles in quantum mechanics and the behaviour of varying currents in electrical circuits. They are also very useful for reducing the cost of computation in 3D computer graphics. The really special thing about complex numbers, though, is that they are the end of this journey that has been going on for millenia. There is no need to invent further number systems to provide solutions to problems expressed in terms of complex numbers because now every non-contradictory equation, algebraic or transcendental, has a solution within the complex numbers. They are a self-sufficient, consistent system. Physicists have been breaking matter down into smaller and smaller particles over the years. Molecules, atoms, nuclei, protons, quarks. When will the process stop? For several decades it was felt that quarks could be the final, indivisible particle. However, new theoretical frameworks such as string theory are suggesting that there may be more fundamental entities than quarks. Because physics ultimately relies on experimental verification, we can never be sure that there aren\\'t going to be more steps in the sequence. In mathematics, however, we can prove things for all time. Complex numbers are the final step in the sequence, the numbers that we have been reaching for since before the beginning of recorded history. Every other number system is just a subset of the complex numbers, just a part of the true picture. Complex numbers are the real thing. '}\n",
      "693 {'Id': '693', 'Type': 'question', 'Title': 'Picking cakes if we need at least one of each type', 'Tags': ['combinatorics'], 'AcceptedAnswerId': '694', 'urls': [], 'exp': ['n', 'k'], 'Body': \"I need <math_exp> cakes for a party. I go to the cake shop and there are <math_exp> different kinds of cake. For variety, I'd like to get at least one of each cake. How many ways can I do this? \"}\n",
      "694 {'Id': '694', 'Type': 'answer', 'ParentId': '693', 'urls': ['https://math.stackexchange.com/questions/686/combinations-of-selecting-n-objects-with-k-different-types/691#691', 'https://math.stackexchange.com/questions/686/combinations-of-selecting-n-objects-with-k-different-types/691#691'], 'exp': ['{n-1 \\\\choose k-1}', '{(n-k)+k-1 \\\\choose k-1} = {n-1 \\\\choose k-1}'], 'Body': 'Similar to the stars and bars technique, consider the n cakes as a row of n stars *.  Instead of permuting them with k-1 bars | (which allows two bars next to each other, giving 0 of a type, place the k-1 bars (needed to split the n stars into k types) into the n-1 spaces between the stars, allowing at most one bar per space.  The number of ways to do this is <math_exp>. Alternately, since you need one of each type, there are only n-k cakes for which you are choosing types.  Using the stars and bars technique, there are <math_exp> ways to do it. '}\n",
      "696 {'Id': '696', 'Type': 'answer', 'ParentId': '656', 'urls': ['http://en.wikipedia.org/wiki/Least_squares'], 'exp': [], 'Body': 'Of course there are infinite equation (even if you require them to be infinitely differentiable...) that satisfy the given constraints.  As Isaac and Justin already wrote, you may always find a polynomial of degree at most n-1 (where n is the number of points given) which satisfies the given data; but you cannot be sure that this is the right answer.  Moreover, if data is not exact but approximate the resulting polynomial may be quite different from the correct function, since it would likely hade huge peaks and falls. In such cases, an approximate method like least squares could be more useful. '}\n",
      "697 {'Id': '697', 'Type': 'question', 'Title': 'Are there variations on least-squares approximations?', 'Tags': ['linear-algebra', 'big-list', 'approximation', 'regression'], 'urls': [], 'exp': ['\\\\mathbb{R}^d'], 'Body': 'In least-squares approximations the normal equations act to project a vector existing in N-dimensional space onto a lower dimensional space, where our problem actually lies, thus providing the \"best\" solution we can hope for (the orthogonal projection of the N-vector onto our solution space).  The \"best\" solution is the one that minimizes the Euclidean distance (two-norm) between the N-dimensional vector and our lower dimensional space. There exist other norms and other spaces besides <math_exp>, what are the analogues of least-squares under a different norm, or in a different space? '}\n",
      "698 {'Id': '698', 'Type': 'question', 'Title': 'Looking for a probability distribution', 'Tags': ['probability-theory', 'probability'], 'AcceptedAnswerId': '1472', 'urls': ['https://mathoverflow.net/questions/33335/looking-for-a-probability-distribution'], 'exp': ['100,000', '0', '2', '1', '1', '5', '50', '95', '99', '4', '10'], 'Body': \"Recently I discussed an experiment with a friend. Assume we start a random experiment. At first there is an array with size <math_exp>, all set to <math_exp>. We calculate at each round a random number modulo <math_exp> and select one random position in that array. If the number in the array is <math_exp>, nothing is changed and otherwise the pre-computed value is set. The question is: how many distinct hash values would we have added in <math_exp>%, <math_exp>%, <math_exp>%, <math_exp>%, <math_exp>% of all cases? Example: <math_exp> rounds with array of size <math_exp>: First we considered this a somehow simple problem, but after thinking for some hours, searching the web, and asking some math students, we couldn't find a solution. Do you know a probability distribution for this problem? Remark: Was also posted on Math Overflow and got its answer there. \"}\n",
      "699 {'Id': '699', 'Type': 'answer', 'ParentId': '697', 'urls': [], 'exp': [], 'Body': 'Sure, there are variations on least-squares approximations. Here\\'s an engineering answer (not really a pure math answer): On one project I did at my company, we had a thermal model that was approximating the thermal transfer function from measured power to device temperature. The cost of error in the positive direction (underestimating temperature) was worse than the cost of the same error in the negative direction (overestimating temperature) -- so we used a weighting function that was (K * error ^ 2) where K was 1 for negative temperature error and greater than 1 (e.g. 1.5 or 2) for positive temperature error. We thought about using more complicated mappings (10 degrees underestimate much worse than 1 degree underestimate) but didn\\'t want to go there... I assume this has some analog to a utility function (e.g. expected monetary gain of a system with random outcome has nonlinear mapping to \"happiness\" or \"utility\") where nonlinearity is intentionally introduced. You could do something similar for approximating functions with polynomials: a least-squares fit treats error in a linear way, but there may be places in the function (e.g. at the ends or at the center) where minimizing error is more or less important. '}\n",
      "700 {'Id': '700', 'Type': 'answer', 'ParentId': '57', 'urls': [], 'exp': [], 'Body': \"The calculus of relations is an algebra of operations over sets of pairs of individuals, where for any relation R, we can express the relation in the usual infix manner: x R y iff (x,y) &isin; R.  This allows all of the properties of relations to be expressed in equational form. There are four fundamental relations we want to define, each of which make non-useless examples of three of your five properties of relations, plus one other useful property, irreflexivity: Three basic binary operations on relations, and one unary relation: Observe that Never = All&ndash;All, and Neq = All&ndash;Eq. We say R -> S when S holds whenever R holds.  This is the same as saying either that  R is a subset of S, or that R&cap;S=R, or that R-S=Never.  So Never -> Eq, and Eq -> All. Then we can express your five relations: Also, how can a relation be a- and antisymmetrical at the same time? Don't they cancel each other out?  &mdash; Look at Eq: it is both symmetric and anti-symmetric, although it is not asymmetric.  In fact, all asymmetric relations are anti-symmetric, but not vice versa: the difference is that asymmetric and anti-symmetric differ in what they assert about the diagonal: anti-symmetric doesn't care about what pairs there might be along the diagonal, whilst asymmetric insists that there are no pairs along the diagonal, which is irreflexivity. \"}\n",
      "701 {'Id': '701', 'Type': 'question', 'Title': 'How many circles of a given radius can be packed into a given rectangular box?', 'Tags': ['geometry', 'packing-problem'], 'urls': [], 'exp': ['l', 'w', 'r', 'r', 'l', 'w'], 'Body': \"I've just came back from my Mathematics of Packing and Shipping lecture, and I've run into a problem I've been trying to figure out. Let's say I have a rectangle of length <math_exp> and width <math_exp>. Is there a simple equation that can be used to show me how many circles of radius <math_exp> can be packed into the rectangle, in the optimal way?  So that no circles overlap. (<math_exp> is less than both <math_exp> and <math_exp>) I'm rather in the dark as to what the optimum method of packing circles together in the least amount of space is, for a given shape. An equation with a non-integer output is useful to me as long as the truncated (rounded down) value is the true answer. (I'm not that interested in how the circles would be packed, as I am going to go into business and only want to know how much I can demand from the packers I hire to pack my product) \"}\n",
      "702 {'Id': '702', 'Type': 'answer', 'ParentId': '603', 'urls': [], 'exp': [], 'Body': \"Couldn't all transformation which send each point (x,y) to another point (x',y') which can be computed from the first one by performing only the four operations and extraction of square root? \"}\n",
      "704 {'Id': '704', 'Type': 'question', 'Title': '<span class=\"math-container\" id=\"7300\">n</span> out of <span class=\"math-container\" id=\"7301\"> m</span> theorems (some imply the rest)', 'Tags': ['terminology', 'logic', 'notation'], 'AcceptedAnswerId': '9229', 'urls': [], 'exp': ['A \\\\wedge B \\\\Rightarrow C, A \\\\wedge C \\\\Rightarrow B, B \\\\wedge C \\\\Rightarrow A'], 'Body': 'Is there symbolism (or even a name) for groups of statements in which any fixed-number of them imply all the rest? For example, in linear algebra, a basis is sometimes defined as a set of n-dimensional vectors which: However, it is then shown that any TWO of these statements being true implies the third is true.  How would you write this symbolically? (other than <math_exp>, which does not scale well...) '}\n",
      "705 {'Id': '705', 'Type': 'question', 'Title': 'Compass-and-straightedge construction of the square root of a given line?', 'Tags': ['geometry', 'euclidean-geometry', 'geometric-construction'], 'urls': [], 'exp': [], 'Body': \"Given Is there a way to geometrically construct (using only a compass and straightedge) the a line with the length of the square root of the arbitrary-lengthed line?  What is the mathematical basis? Also, why can't this be done without the unit line length? \"}\n",
      "706 {'Id': '706', 'Type': 'answer', 'ParentId': '705', 'urls': [], 'exp': [], 'Body': \"Without the unit-length segment--that is, without something to compare the first segment to--its length is entirely arbitrary, so can't be valued, so there's no value of which to take the square root. Let the given segment (with length x) be AB and let point C be on ray AB such that BC = 1.  Construct the midpoint M of segment AC, construct the circle with center M passing through A, construct the line perpendicular to AB through B, and let D be one of the intersections of that line with the circle centered at M (call the other intersection E).  BD = sqrt(x). AC and DE are chords of the circle intersecting at B, so by the power of a point theorem, AB * BC = DB * BE, so x * 1 = x = DB * BE.  Since DE is perpendicular to AC and AC is a diameter of the circle, AC bisects DE and DB = BE, so x = DB^2 or DB = sqrt(x). edit: this is a special case of the more general geometric-mean construction.  Given two lengths AB and BC (arranged as above), the above construction produces the length BD = sqrt(AB * BC), which is the geometric mean of AB and BC. \"}\n",
      "707 {'Id': '707', 'Type': 'question', 'Title': 'The limiting case of a discrete probability problem', 'Tags': ['probability-theory', 'probability'], 'AcceptedAnswerId': '789', 'urls': [], 'exp': ['j_1, j_2, j_3', 'p_i^k(1-p_i)^{n-k}', 'p_i = \\\\frac{i}{m + 1}', 'm', 'i', 'k ', \"$'s and \", ' is the length of the string. So for three jars we have ', ', and ', ' for ', ' respectively. Here are the sequences and their probabilities for ', ' with ', ': \\\\begin{align*} P(00) = 9 / 16 \\\\\\\\ P(10) = 3 / 16 \\\\\\\\   P(01) = 3 / 16 \\\\\\\\   P(11) = 1 / 16. \\\\end{align*} If I tell you that I have selected a binary sequence and the first element is ', ' what is the E(', ')? Well, this can be calculated by looking at each of the jars and adding up the probability of candidate sequences times the value of ', \" I wasn't normalizing this conditionally space properly. I'm skipping a step which I'll explain, someone wants. \\\\begin{equation*} E(p_i) = (4/24 * 1/4) + (8/24 * 1/2) + (12/24 * 3/4) = 14 / 24 = 0.58. \\\\end{equation*} So the question is ... what is \", ' when the numbers of jars goes to infinity (or alternatively, when ', ' can take on values between ', ' and ', ')? Also what happens when the size of the binary strings goes to infinity? Does it have an effect on the outcome? If it does, does the order we take the limits change the answer? And most importantly what is the general case for when I have ', \" 1's and \", ' ', \"'s?, with a continuous \", ' from ', ' to '], 'Body': 'Say there are three jars, <math_exp> filled with different binary sequences of length two. The distribution of the binary sequences in each of the jars is given by the <math_exp>, where  <math_exp> where <math_exp> is the number of jars, <math_exp> is the jar index, <math_exp>is number of 1<math_exp>n<math_exp>p_1 = 0.25, p_2 = 0.5<math_exp>p_3 = 0.75<math_exp>j_1, j_2, j_3<math_exp>j_1<math_exp>p_1 = 0.25<math_exp>1<math_exp>p_i<math_exp>p_i<math_exp>E(p_i)<math_exp>p<math_exp>0<math_exp>1<math_exp>s<math_exp>r<math_exp>0<math_exp>p<math_exp>0<math_exp>1$ and infinite sequences? '}\n",
      "708 {'Id': '708', 'Type': 'answer', 'ParentId': '705', 'urls': [], 'exp': ['AB', 'AB', 'A', 'B', 'C', 'BC', 'A', 'D', 'AD', 'AB', '\\\\triangle BCD', '\\\\triangle ACD', '\\\\triangle ABD', 'AC/AD = AD/AB', 'AC=1', 'AD = \\\\sqrt{AB}'], 'Body': 'If you have a segment <math_exp>, place the unit length segment on the line where <math_exp> lies, starting with <math_exp> and in the direction opposite to <math_exp>; let <math_exp> be the other point of the segment. Now draw a semicircle with diameter <math_exp> and the perpendicular to <math_exp>; this line crosses the semicircle in a point <math_exp>. Now <math_exp> is the square root of <math_exp>. <math_exp> is a right triangle, like <math_exp> and <math_exp>; all of these are similar, so you find out that <math_exp>. But <math_exp>, so <math_exp>. See the drawing below: <img src=\"https://i.stack.imgur.com/bAuQy.png\" alt=\"constructing square root of a line segment\"> '}\n",
      "709 {'Id': '709', 'Type': 'question', 'Title': 'Derivation of the formula for the vertex of a parabola', 'Tags': ['algebra-precalculus', 'analytic-geometry', 'conic-sections', 'plane-curves'], 'AcceptedAnswerId': '6842', 'urls': [], 'exp': ['y = a x^2 + b x + c', 'x = -\\\\frac{b}{2a}', 'x', 'x'], 'Body': \"I'm taking a course on Basic Conic Sections, and one of the ones we are discussing is of a parabola of the form <math_exp> My teacher gave me the formula: <math_exp> as the <math_exp> coordinate of the vertex. I asked her why, and she told me not to tell her how to do her job. My smart friend mumbled something about it involving calculus, but I've always found him a rather odd fellow and I doubt I'd be able to understand a solution involving calculus, because I have no background in it.  If you use something you know from calculus, explain it to someone who has no background in it.  Because I sure don't. Is there a purely algebraic or geometrical yet elegant derivation for the <math_exp> coordinate of a parabola of the above form? \"}\n",
      "710 {'Id': '710', 'Type': 'answer', 'ParentId': '709', 'urls': ['http://en.wikipedia.org/wiki/Completing_the_square'], 'exp': ['y = ax^2 + bx + c', 'y = a[x^2 + bx/a + c/a]', 'y = a[(x + b/2a)^2 - (b/2a)^2 + c/a]', '- (b/2a)^2 + c/a', 'y = a[(x + b/2a)^2 + k]', '(x + b/2a)^2', '(x + b/2a)^2', 'x = -b/2a'], 'Body': \"By the vertex I assume you mean the minimum/maximum point of the parabola. Indeed, this result can be discovered easily through a bit of calculus, but there is also a simple purely algebraic way, which I will present here. Let's consider a generic quadratic expression: <math_exp> We now complete the square on this formula. <math_exp> <math_exp> The expression <math_exp> is a constant (it does not depend on x), so we can replace it with k for the matter of discussion. <math_exp> Now, depending on whether a is positive or negative, the parabola given by y will either have a maximum or minimum. Since a and k are fixed, this must occur when <math_exp> is zero (we know it cannot be less than zero, and it can extend to infinity). Hence, we know that for <math_exp> to be zero, <math_exp>. This in turn implies that the function y is at a minimum or a maximum when this is true. Q.E.D. \"}\n",
      "711 {'Id': '711', 'Type': 'answer', 'ParentId': '709', 'urls': [], 'exp': ['ax^2+bx+c=0', '\\\\frac{-b\\\\pm\\\\sqrt{b^2-4ac}}{2a}', '-\\\\frac{b}{a}', 'k=-\\\\frac{b}{2a}', '-\\\\frac{b}{2a}', \"y=ax^2+bx+c'\", \"c'\", 'x=-\\\\frac{b}{2a}', 'x=-\\\\frac{b}{2a}', '-\\\\frac{b}{2a}'], 'Body': \"Parabolas of the form you described (y = ...) are symmetric over a vertical line through their vertex.  Let's call that line x = k.  This means that if the graph crosses the x-axis (meaning that <math_exp> has real solution(s)), they must be equidistant from x = k, so (k,0) must be the midpoint of the segment with endpoints at the zeros of the quadratic or k is the average of the zeros.  From the quadratic formula, the two zeros of the quadratic are <math_exp>, so their sum is <math_exp> and their average is <math_exp>.  So, the x-coordinate of the vertex must be <math_exp>. If the parabola does not cross the x-axis (no real solutions), there is another parabola with equation <math_exp> for some <math_exp> for which the graph is a vertical translation of the graph of the original quadratic, but crosses the x-axis.  Its axis of symmetry is <math_exp> and because it is a vertical translation of the original, the axis of symmetry of the original is also <math_exp>, so the vertices of both have x-coordinate <math_exp>. \"}\n",
      "712 {'Id': '712', 'Type': 'question', 'Title': 'Why does area differentiate to perimeter for circles and not for squares?', 'Tags': ['calculus', 'geometry', 'circles', 'area'], 'AcceptedAnswerId': '715', 'urls': ['https://math.stackexchange.com/questions/625/why-is-the-derivative-of-a-circles-area-its-perimeter-and-similarly-for-spheres'], 'exp': ['\\\\pi r^2', '2 \\\\pi r', 'l^2', '2l'], 'Body': \"I read this question the other day and it got me thinking: the area of a circle is <math_exp>, which differentiates to <math_exp>, which is just the perimeter of the circle. Why doesn't the same thing happen for squares? If we start with the area formula for squares, <math_exp>, this differentiates to <math_exp> which is sort of right but only half the perimeter. I asked my calculus teacher and he couldn't tell me why. Can anyone explain??? \"}\n",
      "713 {'Id': '713', 'Type': 'question', 'Title': 'How to visualize a rank-2 tensor?', 'Tags': ['geometry', 'linear-algebra', 'intuition', 'tensors'], 'urls': [], 'exp': ['3 \\\\times 3'], 'Body': 'The notion (rank-2) \"tensor\" appears in many different parts of physics, e.g. stress tensor, moment of inertia tensor, etc. I know mathematically a tensor can be represented by a <math_exp> matrix. But I can\\'t grasp its geometrical picture — unlike scalar (a number) and vector (an arrow with direction and magnitude) which I can easily see what\\'s going on. How to visualize a tensor? '}\n",
      "714 {'Id': '714', 'Type': 'answer', 'ParentId': '712', 'urls': [], 'exp': [], 'Body': 'If you use the formula you describe for squares, your measurements are coming from one corner of the square.  Imagine those measurements growing slowly.  The square will grow, but only along the two sides opposite the corner from which you measured, so the derivative of the area formula is only the perimeter on those two sides. Alternately, consider measuring the size of a square by the distance d from its center to the midpoint of a side.  This would make the side length 2d, the perimeter 8d and the area 4d^2.  Now, the derivative of the area is the perimeter.  (Also, if you imagine growing the square slowly with this measurement, it grows from the center outward, growing on all four sides.) '}\n",
      "715 {'Id': '715', 'Type': 'answer', 'ParentId': '712', 'urls': ['http://en.wikipedia.org/wiki/Apothem'], 'exp': ['A = (2r)^2 = 4 r^2', 'P = 4 (2r) = 8 r', 'r'], 'Body': 'Actually, it is also true for squares (and for regular polygons in general!). The problem you ran into is what the equivalent of \"r\" is. The side length of a square is actually more comparable to the circle\\'s diameter. Instead, the correct analogue of the circle\\'s radius is the distance from the center of the square to the midpoint of one side, which is only half as long as the square\\'s side. <img src=\"https://i.imgur.com/FIoOK2h.png\" alt=\"alt text\"> Here, we have <math_exp> and <math_exp>. The perimeter is the derivative of the area with respect to <math_exp>, just as in the case of a circle. '}\n",
      "716 {'Id': '716', 'Type': 'question', 'Title': 'Sum of the alternating harmonic series <span class=\"math-container\" id=\"7574\">\\\\sum_{k=1}^{\\\\infty}\\\\frac{(-1)^{k+1}}{k} = \\\\frac{1}{1} - \\\\frac{1}{2} + \\\\cdots </span>', 'Tags': ['sequences-and-series', 'analysis', 'convergence-divergence', 'logarithms', 'closed-form'], 'AcceptedAnswerId': '718', 'urls': [], 'exp': ['\\\\sum_{k=1}^{\\\\infty}\\\\frac{1}{k}  = \\\\frac{1}{1} + \\\\frac{1}{2} + \\\\frac{1}{3} + \\\\frac{1}{4} + \\\\frac{1}{5} + \\\\frac{1}{6} + \\\\cdots + \\\\frac{1}{n} + \\\\cdots \\\\tag{I}', '\\\\sum_{k=1}^{\\\\infty}\\\\frac{(-1)^{k+1}}{k}  = \\\\frac{1}{1} - \\\\frac{1}{2} + \\\\frac{1}{3} - \\\\frac{1}{4} + \\\\frac{1}{5} - \\\\frac{1}{6} + \\\\cdots + \\\\frac{(-1)^{n+1}}{n} + \\\\cdots \\\\text{?} \\\\tag{II}'], 'Body': 'I know that the harmonic series <math_exp> diverges, but what about the alternating harmonic series <math_exp> Does it converge?   If so, what is its sum? '}\n",
      "717 {'Id': '717', 'Type': 'answer', 'ParentId': '716', 'urls': ['http://en.wikipedia.org/wiki/Mercator_series'], 'exp': ['\\\\ln(2)'], 'Body': 'it is not absolutely convergent (that is, if you are allowed to reorder terms you may end up with whatever number you fancy). If you consider the associated series formed by summing the terms from 1 to n of the original one, that is you fix the order of summation of the original series, that series (which is not the original one...) converges to <math_exp> See Wikipedia. '}\n",
      "718 {'Id': '718', 'Type': 'answer', 'ParentId': '716', 'urls': ['http://en.wikipedia.org/wiki/Riemann_series_theorem', 'http://en.wikipedia.org/wiki/Radius_of_convergence'], 'exp': ['a_n', '\\\\sum|a_n|', 'a_n', 'a_n', '\\\\ln 2', '\\\\ln(1-x)'], 'Body': \"Complementary to Mau's answer: Call a series <math_exp> absolutely convergent if <math_exp> converges. If <math_exp> converges but is not absolutely convergent we call <math_exp> conditionally convergent The Riemann series theorem states that any conditionally convergent series can be reordered to converge to any real number. Morally this is because both the positive and negative parts of your series diverge but the divergences cancel each other out, one or other's canceling the other can be staggered by adding on, say,  the negative bits every third term in stead of every other term. This means that in the race for the two divergences to cancel each other out, we give the positive bit something of a head-start and will get a larger positive outcome. Notice how, even in this rearranged version of the series, every term will still come up exactly once. It is also worth noting, on the Wikipedia link Mau provided, that the convergence to <math_exp> of your series is at the edge of the radius of convergence for the series expansion of <math_exp>- this is a fairly typical occurrence: at the boundary of a domain of convergence of a Taylor series, the series is only just converging- which is why you see this conditional convergence type behavior. \"}\n",
      "719 {'Id': '719', 'Type': 'answer', 'ParentId': '704', 'urls': [], 'exp': [], 'Body': 'Often the most obvious solution is the best solution. I would write T(s) be 1 if a statement is true and 0 if it is false. I would then call the statements A, B and C and let S={A, B, C}. Then is would write sum T(s) over S >=2 => all s are true. You can make these statement more symbolic if you really like '}\n",
      "720 {'Id': '720', 'Type': 'question', 'Title': 'How to calculate a heading on the earths surface?', 'Tags': ['geometry', 'trigonometry', 'spherical-geometry'], 'AcceptedAnswerId': '770', 'urls': [], 'exp': [], 'Body': 'Given an initial position and a subsequent position, each given by latitude and longitude in the WGS-84 system.  How do you determine the heading in degrees clockwise from true north of movement? '}\n",
      "721 {'Id': '721', 'Type': 'question', 'Title': 'Algorithm for calculating <span class=\"math-container\" id=\"7675\">A^n</span> with as few multiplications as possible', 'Tags': ['algorithms', 'abstract-algebra'], 'AcceptedAnswerId': '722', 'urls': [], 'exp': ['A^n', 'A', 'A^9', '4', 'A^2 = A \\\\cdot A', 'A^3 = A^2 \\\\cdot A', ' A^6 = A^3  \\\\cdot A^3 ', ' A^9 = A^6 \\\\cdot A^3 ', 'A^{2^i}', 'n', 'n=23', '6', ' A^2 = A \\\\cdot A ', ' A^3 = A^2 \\\\cdot A ', ' A^5 = A^3 \\\\cdot A^2 ', ' A^{10} = A^5 \\\\cdot A^5 ', ' A^{20} = A^{10} \\\\cdot A^{10} ', ' A^{23} = A^{20} \\\\cdot A^3 ', '7', ' A^2 = A\\\\cdot A ', ' A^4 = A^2 \\\\cdot A^2 ', ' A^8 = A^4 \\\\cdot A^4 ', ' A^{16} = A^8 \\\\cdot A^8 ', ' A^{20} = A^{16} \\\\cdot A^4 ', ' A^{22} = A^{20} \\\\cdot A^2 ', ' A^{23} = A^{22} \\\\cdot A '], 'Body': 'Is there an algorithm for working out the best way (i.e. fewest multiplications) of calculating <math_exp> in a structure where multiplication is associative? For example, suppose <math_exp> is a square matrix. Matrix multiplication is associative, and I can compute <math_exp> with <math_exp> multiplications: <math_exp> <math_exp> <math_exp> <math_exp> One method which works is to compute <math_exp> and use the binary representation of <math_exp>, but this is not always optimal, e.g. with <math_exp>, we can do it in <math_exp> multiplications: <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> rather than <math_exp>: <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> Is there an algorithm which gives the quickest way? '}\n",
      "722 {'Id': '722', 'Type': 'answer', 'ParentId': '721', 'urls': ['http://en.wikipedia.org/wiki/Addition-chain_exponentiation', 'http://eref.uqu.edu.sa/files/a_survey_of_fast_exponentiation_methods__64651.pdf'], 'exp': [], 'Body': \"There seems to be no efficient algorithm for this. Quoting Wikipedia on : … the addition-chain method is much more complicated, since the determination of a shortest addition chain seems quite difficult: no efficient optimal methods are currently known for arbitrary exponents, and the related problem of finding a shortest addition chain for a given set of exponents has been proven NP-complete.  Even given a shortest chain, addition-chain exponentiation requires more memory than the binary method, because it must potentially store many previous exponents from the chain simultaneously. In practice, therefore, shortest addition-chain exponentiation is primarily used for small fixed exponents for which a shortest chain can be precomputed and is not too large. On the same page, however, it does link to a reference* of some methods better than binary exponentiation. One simple example is to use base-N number instead of base-2. e.g. for A510, (I don't know how to choose the optimal N.) *:  (1998), pp 129–146 \"}\n",
      "723 {'Id': '723', 'Type': 'answer', 'ParentId': '716', 'urls': ['http://en.wikipedia.org/wiki/Alternating_series_test'], 'exp': ['a_1 \\\\geq a_2 \\\\geq \\\\dots', '\\\\sum (-1)^i a_i', '\\\\log(2)', '\\\\log(1+x)', 'x \\\\to 1'], 'Body': \"Let's  say you have a sequence of nonnegative numbers <math_exp> tending to zero. Then it is a theorem that the alternating sum <math_exp> converges (not necessarily absolutely, of course). This in particular applies to your series. Incidentally, if you're curious why it converges to <math_exp> (which seems somewhat random), it's because of the Taylor series of <math_exp> while letting <math_exp>. \"}\n",
      "724 {'Id': '724', 'Type': 'answer', 'ParentId': '713', 'urls': [], 'exp': [], 'Body': \"While tensors are generalizations of vectors, I don't think you can really generalize the way you visualize them.  This is because you really want to think of tensors as multi-linear functions and you usually don't think of a vector as a linear function from the dual space to the real numbers. So even if you can't get as nice a geometric picture of a tensor, you do get a nice grasp on what they are if you view them as multi-linear functions (as opposed to just a collection of numbers) from some copies of your vector space V (and/or its dual V*) into R.  For example, a metric is a type of rank 2 tensor and has a nice geometric meaning-- applying it to two copies of a vector gives the vectors squared length, you can apply to to two vectors to get the angle between them, etc.  The moment of inertia tensor is a 2-tensor I such that I(u,u) is the moment of inertia about the u-axis. There are also usually different ways of looking at the same tensor, which may make it easier to get a grasp on it.  For example, a (1,1) tensor is a multilinear map from V x V* into R.  However, this can naturally (i.e. basis independently) be identified with a linear map from V to itself: if T is a (1,1) tensor then contracting it with a vector gives a linear map V* to R, which is just another vector. \"}\n",
      "725 {'Id': '725', 'Type': 'answer', 'ParentId': '701', 'urls': ['http://en.wikipedia.org/wiki/Circle_packing_in_a_square', 'http://hydra.nat.uni-magdeburg.de/packing/csq/csq.html'], 'exp': [], 'Body': 'I had an answer before, but I looked into it a bit more and my answer was incorrect so I removed it. This link may be of interest: Circle Packing in a Square (wikipedia) It was suggested by KennyTM that there may not be an optimal solution yet to this problem in general. Further digging into this has shown me that this is probably correct. Check out this page: Circle Packing - Best Known Packings. As you can see, solutions up to only 30 circles have been found and proven optimal. (Other higher numbers of circles have been proven optimal, but 31 hasn\\'t) Note that although problem defined on the wikipedia page and the other link is superficially different than the question asked here, the same fundamental question is being asked, which is \"what is the most efficient way to pack circles in a square/rectangle container?\". ...And it seems the answer is \"we don\\'t really know\" :) '}\n",
      "726 {'Id': '726', 'Type': 'question', 'Title': 'What are NP-complete problems and why are they so important?', 'Tags': ['computer-science', 'np-complete'], 'AcceptedAnswerId': '727', 'urls': [], 'exp': [], 'Body': 'I keep hearing questions about whether something is NP-complete, but they never really mention what it is. Why do people care so much about NP-complete problems? '}\n",
      "727 {'Id': '727', 'Type': 'answer', 'ParentId': '726', 'urls': [], 'exp': [], 'Body': 'A problem is in class NP if its solution may be verified in polynomial time, that is if the dimension of the problem is n you may be sure that for large enough n you need less than r&middot;nk operations to verify the solution. A problem is in class P if its solution may be found in polynomial time, instead. A problem in P is in NP by definition, but the converse may not be the case; probably the most important open question in computer science is whether classes P and NP are the same, that is P=NP. NP-complete is a family of NP problems for which you know that if one of them had a polynomial solution then everyone of them has. (EDITED) For the time being, only known algorithms for NP-complete problems are exponential in number of operations, so they are not practically solvable for n large. '}\n",
      "728 {'Id': '728', 'Type': 'answer', 'ParentId': '726', 'urls': ['http://en.wikipedia.org/wiki/List_of_NP-complete_problems'], 'exp': [], 'Body': \"To expand on Mau's answer, you should care about NP-complete problems because there is an entire family of them that spans a large number of seemingly basic algorithms across a wide range of disciplines. These aren't obscure problems, but extremely important and highly practical questions. For examples, consider the following: Although many of these problems may seem abstract, many more complicated problems can't be efficiently solved with current techniques, as they are equivalent to one of these. The problem of NP completeness has received a huge amount of attention. Once you've reduced a problem to NP-complete, you know to give up on an efficient fast algorithm and to start looking at approximations. \"}\n",
      "729 {'Id': '729', 'Type': 'answer', 'ParentId': '726', 'urls': ['http://en.wikipedia.org/wiki/P%3DNP', 'http://en.wikipedia.org/wiki/Millennium_Prize_Problems'], 'exp': [], 'Body': 'Any problem for which a solution (once found) can be quickly verified as a solution is said to be \"in NP\"  (Here, \"quickly\" means in polynomial-time).  Any problem for which a solution can be found quickly is said to be \"in P.\"  P is a subset of NP - that is, any problem for which a solution can be quickly found can also be quickly verified. A problem is NP-complete if it is the hardest problem in NP.  Surprisingly, there are many NP-complete problems, which are all equivalent - here, equivalent means that a quick (polynomial-time) solution to any one of them would give you a quick solution to all the rest.  Also somewhat surprisingly, a quick solution to any NP-complete problem would also give you a quick solution to any problem in NP. So is there a quick (polynomial-time) algorithm to solve NP-complete problems?  That is the P=NP problem, one of the greatest unsolved problems of our time.  However, most sane mathemeticians believe (and hope!) that P≠NP, because proving math-theorems is NP-Complete; so if P=NP, we\\'d be out of a job! :) '}\n",
      "730 {'Id': '730', 'Type': 'question', 'Title': 'An elegant description for graded-module morphisms with non-zero zero component', 'Tags': ['modules', 'graded-modules', 'graded-rings'], 'urls': [], 'exp': ['R', 'R', '\\\\Sigma=\\\\left\\\\lbrace f\\\\in \\\\hom_{\\\\text{gr}R\\\\text{-mod}}\\\\left(A,B\\\\right) \\\\ | \\\\ \\\\ker\\\\left(f\\\\right)_0\\\\neq 0, \\\\ \\\\mathrm{coker}\\\\left(f\\\\right)_0\\\\neq 0\\\\right\\\\rbrace'], 'Body': 'In an example I have worked out for my work, I have constructed a category whose objects are graded <math_exp>-modules (where <math_exp> is a graded ring), and with morphisms the usual morphisms quotient the following class of morphisms: <math_exp> (by quotient I mean simply that this class of morphisms are isomorphisms, thus creating an equivalence relation) I am wondering if this category has a better (more canonical) description, or if I can show it is equivalent to some other interesting category. Thanks! '}\n",
      "731 {'Id': '731', 'Type': 'answer', 'ParentId': '721', 'urls': ['http://en.wikipedia.org/wiki/Matrix_decomposition#Eigendecomposition'], 'exp': [], 'Body': 'Another option would be to use eigendecomposition . It allows you to raise the eigenvalues in the diagonal of the decomposition A = VDV^-1 to a power. It changes the problem from matrix multiplication to the multiplication of the eigenvalues. Once in eigendecomposition form you could perform the same addition-chain exponentiation technique but it would be with scalars, not matrices. Much more efficient, because each matrix multiplication has n^3 multiplies, but with ed you would only have n. More is explained here: http://en.wikipedia.org/wiki/Matrix_decomposition#Eigendecomposition '}\n",
      "732 {'Id': '732', 'Type': 'question', 'Title': 'Proof that the irrational numbers are uncountable', 'Tags': ['elementary-set-theory', 'irrational-numbers'], 'AcceptedAnswerId': '733', 'urls': [], 'exp': ['\\\\mathbb{Q}'], 'Body': 'Can someone point me to a proof that the set of irrational numbers is uncountable? I know how to show that the set <math_exp> of rational numbers is countable, but how would you show that the irrationals are uncountable? '}\n",
      "733 {'Id': '733', 'Type': 'answer', 'ParentId': '732', 'urls': ['http://en.wikipedia.org/wiki/Cantor_diagonalization'], 'exp': [], 'Body': 'Given that the reals are uncountable (which can be shown via Cantor diagonalization) and the rationals are countable, the irrationals are the reals with the rationals removed, which is uncountable. (Or, since the reals are the union of the rationals and the irrationals, if the irrationals were countable, the reals would be the union of two countable sets and would have to be countable, so the irrationals must be uncountable.) '}\n",
      "734 {'Id': '734', 'Type': 'question', 'Title': 'Online Math Degree Programs', 'Tags': ['soft-question', 'education'], 'urls': [], 'exp': [], 'Body': \"Are there any real online mathematics (applied math, statistics, ...) degree programs out there? I'm full-time employed, thus not having the flexibility of attending an on campus program. I also already have a MSc in Computer Science. My motivation for a math degree is that I like learning and am interested in the subject. I've studied through number of OCW courses on my own, but it would be nice if I could actually be able to have my studying count towards something. I've done my share of Googling for this, but searching for online degrees seems to bring up a lot of institutions that (at least superficially) seem a bit shady (diploma mills?). \"}\n",
      "735 {'Id': '735', 'Type': 'question', 'Title': 'What does it mean to be going 40 mph (or 64 kph, etc.) at a given moment?', 'Tags': ['calculus', 'intuition'], 'AcceptedAnswerId': '739', 'urls': [], 'exp': ['20', '\\\\frac{10 mi}{25 h}', '0', '0', '0', '0', '0', '0', '\\\\frac00', '40'], 'Body': 'I was coming back from my Driver\\'s Education class, and something mathsy really stuck out to me. One of the essential properties of a car is its current speed.  Or speed at a current time.  For example, at a given point in time in my drive, I could be traveling 40 mph.  But what does that mean? From my basic algebra classes, I\\'ve learned that speed = distance/time.  So if I travel ten miles in half an hour, my average speed would be <math_exp> mph (<math_exp>). But instantaneous velocity...you aren\\'t measuring average speed for a given amount of time.  You\\'re measuring instantaneous speed over an...instantaneous amount of time. That would be something like (miles) / (time), where time = <math_exp>?  Isn\\'t that infinite? And perhaps, in a difference of time = <math_exp>, then I\\'d be travelling <math_exp> miles.  So would I be said to be going <math_exp> mph at an instantaneous moment in time?  I\\'d like to be able to tell that to any cops pull me over for \"speeding\"! But then if miles = <math_exp> and time = <math_exp>, then you have <math_exp>? This is all rather confusing.  What does it mean to be going <math_exp> mph at a given moment in time, exactly? I\\'ve heard this explained using this strange art called \"calculus\" before, and it\\'s all gone over my head.  Can anyone explain this using terms I (a High School Algebra and Geometry and Driving student) will understand? (I figured that my problem had numbers in it, and therefore has to do with Maths.) '}\n",
      "737 {'Id': '737', 'Type': 'answer', 'ParentId': '735', 'urls': [], 'exp': ['t=t_0', 't=t_0+\\\\epsilon'], 'Body': 'If you were to try to measure instantaneous speed as you described, you would in fact have traveled 0 miles in 0 time and 0/0 is undefined  If, however, you look at your average speed over smaller and smaller periods of time around the instant you care about--that is, (distance traveled from <math_exp> to <math_exp> for various small values of &epsilon;--and these average speeds \"converge\" (they all get closer to a single value as &epsilon; gets closer to 0), then we say that the instantaneous speed is that single value upon which the average speeds around that point converge.  This is, of course, a somewhat informal explanation; to be more precise requires getting into differential calculus. '}\n",
      "738 {'Id': '738', 'Type': 'answer', 'ParentId': '735', 'urls': [], 'exp': [], 'Body': \"in the real world you need a bit of time to brake or accelerate. This means that in a small amount of time (let's say 1/10 of a second) your speed is more or less constant. So, instead of dividing by 0 seconds, just take a small elapsed time and do the division. In calculus you will learn how to be more precise, noticing that using a smaller and smaller elapsed time the ratio between distance and time changes less and less; at that point you postulate that in this scenario only you may say that the limit (which is 0/0 and therefore indefinite) is the instantaneous speed. But the odometers don't care for instantaneous speed, so for the moment you shouldn't worry. \"}\n",
      "739 {'Id': '739', 'Type': 'answer', 'ParentId': '735', 'urls': [], 'exp': [], 'Body': 'I think there is a very clear meaning in the physical world: If, at some moment, you were going 40 mph, if you were to stop de/accelerating and just hold that velocity, you would cover 40 miles in 1 hour. '}\n",
      "740 {'Id': '740', 'Type': 'question', 'Title': 'Useful examples of pathological functions', 'Tags': ['soft-question', 'big-list', 'calculus', 'real-analysis'], 'AcceptedAnswerId': '1144', 'urls': [], 'exp': ['f&#39;(a) = b', 'f(a)', 'f', 'a', 'f', 'a', 'f&#39;', 'a', 'f(x) = x^2 \\\\sin(1/x)', 'x=0'], 'Body': 'What are some particularly well-known functions that exhibit pathological behavior at or near at least one value and are particularly useful as examples? For instance, if <math_exp>, then <math_exp> exists, <math_exp> is continuous at <math_exp>, <math_exp> is differentiable at <math_exp>, but <math_exp> need not be continuous at <math_exp>.  A function for which this is true is <math_exp> at <math_exp>. '}\n",
      "741 {'Id': '741', 'Type': 'question', 'Title': 'What is linear programming?', 'Tags': ['linear-algebra', 'algorithms', 'computer-science', 'optimization', 'linear-programming'], 'AcceptedAnswerId': '743', 'urls': ['http://en.wikipedia.org/wiki/Linear_programming'], 'exp': [], 'Body': 'I asked this question on Stack Overflow but it was closed as \"not programming related\". So I think this is probably the best place for it... I read over the wikipedia article, but it seems to be beyond my comprehension. It says it\\'s for optimization, but how is it different than any other method for optimizing things? An answer that introduces me to linear programming so I can begin diving into some less beginner-accessible material would be most helpful. '}\n",
      "742 {'Id': '742', 'Type': 'question', 'Title': 'Least wasteful use of stamps to achieve a given postage', 'Tags': ['combinatorics', 'optimization', 'algorithms', 'contest-math'], 'AcceptedAnswerId': '746', 'urls': [], 'exp': ['42', '29', '\\\\', ' to mail a package.  What is the   least amount you can make with the ', '-   and ', '-cent stamps that is sufficient   to mail the package? A contest problem such as this is probably most easily solved by tabulating the possible combinations, using ', ' through ceiling(total/greater value) of the greater-value stamp and computing the necessary number of the smaller stamp and the total postage involved.  The particular example above would be solved with a ', '-row table, showing the minimum to be ', '3.23', '42', '29'], 'Body': 'You have sheets of <math_exp>-cent stamps and   <math_exp>-cent stamps, but you need at least   <math_exp>3.20<math_exp>42<math_exp>29<math_exp>0<math_exp>9<math_exp>\\\\<math_exp>, made with seven <math_exp>-cent stamps and one <math_exp>-cent stamp. Is there a better algorithm for solving this kind of problem?  What if you have more than two values of stamps? '}\n",
      "743 {'Id': '743', 'Type': 'answer', 'ParentId': '741', 'urls': ['http://en.wikipedia.org/wiki/Linear_programming#Standard_form', 'http://en.wikipedia.org/wiki/Linear_programming#Example', 'http://en.wikipedia.org/wiki/Linear_programming_relaxation', 'http://en.wikipedia.org/wiki/Flow_network', 'http://en.wikipedia.org/wiki/Linear_programming#Integer_unknowns', 'https://math.stackexchange.com/questions/726/what-are-np-complete-problems-and-why-are-they-so-important/729#729'], 'exp': [], 'Body': \"The standard form (and example) sections pretty well describe what it is. How is it different than any other method for optimizing things? It's, well, just another method.  However, it is somewhat special in that many other optimization algorithms either use linear programming as part of their solution, or are in reality a specialized solution to a linear programming problem.  In fact, integer linear programming is NP-complete, meaning that any problem in NP can be stated as an (integer) linear programming problem. (this also means solving your typical integer linear programming problem is much more difficult than if we didn't restrict ourselves to integers..) \"}\n",
      "744 {'Id': '744', 'Type': 'answer', 'ParentId': '735', 'urls': ['http://en.wikipedia.org/wiki/Newton%27s_laws_of_motion', 'http://en.wikipedia.org/wiki/Miles_per_hour#Conversions', 'http://en.wikipedia.org/wiki/Integral', 'http://en.wikipedia.org/wiki/Simpson%27s_rule'], 'exp': [], 'Body': 'A state-variable kind of approach: (represents the physical states of a system) Newton\\'s first law of motion says that without external forces, masses will move at constant velocity. Every change in velocity (for a car, whether a change in speed due to braking, or a change in direction due to friction of the tires and the steering mechanism) is due to an external force. So the \"state\" of the car at any instant in time is its velocity; distance traveled can be derived from velocity: While you are going 60mi/hr = 26.8m/sec (see conversion factors) in a particular direction at constant velocity, you can multiply a change in time times velocity and get distance: 26.8m/sec = 26.8 meters in one second. Or 26.8mm in one millisecond. Or 26.8 micrometers in one microsecond. The process of going from velocity to (directed) distance is called integration, and for those two variables (velocity and directed distance) is just the summing up of distance increments (= velocity * time increments) over infinitesimally short periods of time. For smoothly varying velocities, coarse time increments are a good approximation to reality -- differential equation solvers use this or similar approaches (e.g. Simpson\\'s Rule) to perform integration. But instantaneous velocity...you aren\\'t measuring average speed for a given amount of time Well, instantaneous velocity is just the limit of average velocity as the time period becomes infinitely small. '}\n",
      "745 {'Id': '745', 'Type': 'answer', 'ParentId': '741', 'urls': [], 'exp': [], 'Body': 'BlueRaja\\'s answer is certainly more complete than this one (and gives good references), but here\\'s a rough overview of linear programming.  Suppose that you have a linear function (in high school courses, it\\'s typically a function of two variables) that you want to optimize on a convex \"feasible\" region bounded by linear equations (again, in high school, the bounds are typically described using linear inequalities in two variables). Because the function to optimize is linear, the set of points for which the function has a particular value, say c, is a line (and all such lines are parallel) and the value of the function anywhere to one side of the line is greater than c and anywhere to the other side of the line is less than c.  So, you can think about moving through this set of constant-value lines to increase/decrease the value of the target function as appropriate to optimize it. Since the feasible region is bounded by linear equations, as the constant-value line moves through and out of the feasible region, it last touches the feasible region at a vertex (or possibly at all points on an edge connecting two vertices), so the optimal solution must occur at a vertex of the feasible region. Given all that, linear programming comes down to evaluating the target function at all the vertices of the feasible region to find the optimal value. '}\n",
      "746 {'Id': '746', 'Type': 'answer', 'ParentId': '742', 'urls': ['http://en.wikipedia.org/wiki/Integer_linear_programming#Integer_unknowns'], 'exp': [], 'Body': 'With two stamps, you can do it in linear pseudo-linear time - O(totalCost/costOfLargerStamp) - by simply enumerating every possibility (there is only one possible count of the smaller stamp for each count of the larger stamp). In general, however, solving this is equivalent to solving a general integer linear programming problem written in standard form, which is NP-complete. '}\n",
      "747 {'Id': '747', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://en.wikipedia.org/wiki/Sinc_function'], 'exp': ['\\\\displaystyle\\\\frac{\\\\sin(x)}{x}', 'x=0', '\\\\displaystyle y=\\\\frac{\\\\sin(x)}{x}', '(x=0,y=1)', '\\\\text{sinc}(x/\\\\pi)'], 'Body': '<math_exp> is useful; it has a singularity at <math_exp>, but if you take the union of <math_exp> with the point <math_exp> then you get <math_exp>. The  function has a lot of applications in signal processing and diffraction. '}\n",
      "748 {'Id': '748', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://en.wikipedia.org/wiki/Dirac_delta_function'], 'exp': [], 'Body': 'The Dirac delta \"function.\" It\\'s not a \"function,\" strictly speaking, but rather a very simple example of a distribution that isn\\'t a function. '}\n",
      "749 {'Id': '749', 'Type': 'answer', 'ParentId': '742', 'urls': ['http://en.wikipedia.org/wiki/Knapsack_problem', 'http://en.wikipedia.org/wiki/Binary_search', 'http://en.wikipedia.org/wiki/Backtracking'], 'exp': ['\\\\lceil \\\\frac{n}{t_i}\\\\rceil', 't_i'], 'Body': \"This is a simple variation of the Knapsack problem. Which is a NP-Complete problem. Let n to be the target value, it can be solve using knapsack's dynamic programming solutions. If you have a finite amount of stamps, then you can add them all up, subtract the target value. Run 0-1 knapsack on the stamps with the resulting number. The stamps outside the knapsack is the solution. If you have a infinite amount of stamps. You can either make them finite: <math_exp> stamps for stamps with value <math_exp> and run the above process, or you can compute the result by running knapsacks O(log n) times similar to a binary search. It's no better than a naive backtracking solution when n is large. I assume you are not going to pay $100 for stamps. ;) \"}\n",
      "750 {'Id': '750', 'Type': 'question', 'Title': 'Determinants and volume of parallelotopes', 'Tags': ['linear-algebra', 'geometry', 'determinant'], 'AcceptedAnswerId': '754', 'urls': [], 'exp': ['2 \\\\times 2', '2', '3 \\\\times 3', '3', 'n-D', 'n \\\\times n', 'n-'], 'Body': 'The absolute value of a <math_exp> matrix determinant is the area of a corresponding parallelogram with the <math_exp> row vectors as sides. The absolute value of a <math_exp> matrix determinant is the volume of a corresponding parallelepiped with the <math_exp> row vectors as sides. Can it be generalized to <math_exp>? The absolute value of an <math_exp> matrix determinant is the volume of a corresponding <math_exp>parallelotope? '}\n",
      "751 {'Id': '751', 'Type': 'answer', 'ParentId': '735', 'urls': [], 'exp': [], 'Body': 'This is similar to other answers. Imagine another car beside yours. That car is covering a distance of 40 miles over the next hour at a constant velocity. At the point at which you keep pace with that car (relative velocity = 0), you are traveling at 40 mph. '}\n",
      "752 {'Id': '752', 'Type': 'answer', 'ParentId': '750', 'urls': ['http://en.wikipedia.org/wiki/Determinant'], 'exp': [], 'Body': \"Yes--see the Wikipedia article. This follows from the change of variables formula with the Jacobian. However, it is probably more accurate to say that the change-of-variables formula is a (nontrivial) consequence of this.  One argument for this fact, which is given in Rudin's Real and Complex Analysis, is the fact that both the determinant and the volume function up to sign behave very nicely satisfying certain specific properties (i.e., multilinear, alternating, normalized) hence are the same. \"}\n",
      "753 {'Id': '753', 'Type': 'answer', 'ParentId': '750', 'urls': [], 'exp': [], 'Body': 'Yes, that is because in its most general form the determinate is defined that way, except without the absolute value part. The determinant is a way to compare volumes of subspaces that have the same grade, which is the multi-vector equivalent of the size of matrix. '}\n",
      "754 {'Id': '754', 'Type': 'answer', 'ParentId': '750', 'urls': ['https://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant/669#669'], 'exp': [], 'Body': 'Yes it can. In fact, as Jamie Banks noted, a determinant is an intuitive way of thinking about volumes. To summarise the argument, if we consider the vectors as a matrix, switching two rows, multiplying one by a constant or adding a linear combination will have the same effect on the volume as on the determinate. We can use these operations to transform any n-parallelotope to cube and note that the determinate matches the signed volume here, so it will match it everywhere as well. '}\n",
      "755 {'Id': '755', 'Type': 'answer', 'ParentId': '742', 'urls': [], 'exp': [], 'Body': 'It is important to understand that NP-completeness is defined in terms of input size, not in terms of either cost to be paid, C, or the number of types of stamps, S, when we are dealing with integers rather than real numbers. This algorithm can actually be solved in time CS. We take the first stamp value an mark all multiple of it as obtainable (up to the least number greater than the cost). Then, for each subsequent stamp value, we mark as obtainable all numbers that can be obtained by adding a multiple of its value with an obtainable number (up to the least number greater than the cost). We can keep track of the lowest cost postage amount that we have found so far. '}\n",
      "756 {'Id': '756', 'Type': 'question', 'Title': 'Twenty questions against a liar', 'Tags': ['algorithms', 'computer-science', 'discrete-mathematics', 'searching'], 'AcceptedAnswerId': '1268', 'urls': [], 'exp': [], 'Body': 'Here\\'s one that popped into my mind when I was thinking about binary search. I\\'m thinking of an integer between 1 and n. You have to guess my number. You win as soon as you guess the correct number. If your guess is not correct, I\\'ll give you a hint by saying \"too high\" or \"too low\". What\\'s your best strategy? This is an easy problem if I always tell the truth: by guessing the (rounded) mean of the lower bound and the upper bound, you can find my number in roughly log2 n guesses at most. But what if I\\'m allowed to cheat once? What is your best strategy then? To clarify: if you guess my number, you always win instantly. But I\\'m allowed, at most once, to tell you \"too high\" when your guess is actually too low, or the opposite. I can also decide not to lie. Here\\'s a rough upper bound: you can ask each number twice to make sure I\\'m not cheating, and if I ever give two different answers, just ask a third time and from then on, play the regular game. In this way, you can win with about 2 log2 n guesses at most. I\\'m pretty sure that bound can be improved. Any ideas? '}\n",
      "757 {'Id': '757', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://en.wikipedia.org/wiki/Weierstrass_function', 'http://en.wikipedia.org/wiki/Nowhere_continuous_function', 'http://en.wikipedia.org/wiki/Thomae%27s_function', 'http://en.wikipedia.org/wiki/Cantor_function'], 'exp': [], 'Body': \"The Weierstrass function is continuous everywhere and differentiable nowhere. The Dirichlet function (the indicator function for the rationals) is continuous nowhere. A modification of the Dirichlet function is continuous at all irrational values and discontinuous at rational values. The Devil's Staircase is uniformly continuous but not absolutely. It increases from 0 to 1, but the derivative is 0 almost everywhere. \"}\n",
      "758 {'Id': '758', 'Type': 'answer', 'ParentId': '734', 'urls': [], 'exp': [], 'Body': \"I would recommend against getting a degree from any online-only university. Even if you happen to find one that's not shady, everyone else who hasn't heard of it will assume it is some kind of diploma mill without bothering to do much research. Instead, I think you'd be better off going to a nearby university you're interested in, and ask them if they would be willing to make some kind of special arrangement for you. Many universities allow reduced course loads for students that have families or work full time, and aren't very good about advertising it. \"}\n",
      "759 {'Id': '759', 'Type': 'answer', 'ParentId': '740', 'urls': [], 'exp': ['\\\\left(\\\\frac{1}{x}\\\\right)^{\\\\frac{1}{x}}'], 'Body': '<math_exp>. Try graphing it if you dare (including over the negatives) '}\n",
      "760 {'Id': '760', 'Type': 'question', 'Title': 'How do I measure distance on a globe?', 'Tags': ['geometry', 'calculus', 'trigonometry'], 'AcceptedAnswerId': '762', 'urls': ['http://en.wikipedia.org/wiki/Arc_length', 'https://math.stackexchange.com/questions/720/how-to-calculate-a-heading-vector-on-the-earths-surface'], 'exp': ['3', 'R', '(x_1,y_1,z_1)', '(x_2,y_2,z_2)'], 'Body': 'I have a <math_exp>-D sphere of radius <math_exp>, centered at the origin. <math_exp> and <math_exp> are two points on the sphere. The Euclidean distance is easy to calculate, but what if I were to restrict myself to traveling on the surface of the sphere? Two approaches come to mind: use arc-length in some way, or simply use trigonometry: calculate the angle between the two points and get a distance from that. Will both/either of these methods work? Which would be easier? Somewhat related to this question. Maybe it will inspire someone to go answer it! '}\n",
      "761 {'Id': '761', 'Type': 'question', 'Title': 'Using differential equations to graph velocity over time of a falling object subject to wind resistance', 'Tags': ['calculus', 'physics', 'ordinary-differential-equations'], 'AcceptedAnswerId': '763', 'urls': [], 'exp': [], 'Body': \"Wind resistance -- upwards acceleration, typically varies either linearly or quadratically by the current velocity. There is a constant downward acceleration due to gravity. How can we model the velocity over time of a falling object, subject only to wind resistance and downwards gravity? I don't have much experience with differential equations, but I do know that this answer necessarily involves it, so could you possibly explain every step? Thank you. \"}\n",
      "762 {'Id': '762', 'Type': 'answer', 'ParentId': '760', 'urls': ['http://en.wikipedia.org/wiki/Geodesic'], 'exp': ['R', 'v', 'w', 'v\\\\cdot w', '\\\\cos^{-1}\\\\left(\\\\large\\\\frac{v\\\\cdot w}{R^2}\\\\right)', 'R^2', 'v', 'w', 'R', 'R', 'R\\\\cos^{-1}\\\\left(\\\\large\\\\frac{v\\\\cdot w}{R^2}\\\\right)'], 'Body': 'What you are looking for is what is called the great circle distance: the shortest curve connecting two points on the surface of a sphere is given by traveling along the (there will be exactly one unless the points are polar opposites)  arc of the great circle (that is, the circle of radius <math_exp>) connecting them. So, you do need to find an arc length but it is easy to do this without invoking calculus if you know about the dot product.  Suppose the two points on the circle are represented by the vectors <math_exp> and <math_exp>.  If <math_exp> denotes the dot product of these two vectors, then the angle between them will be: <math_exp> (we divide by <math_exp> since <math_exp> and <math_exp> have length <math_exp>). Assuming this is in radians, to get the length of the arc connecting them we just multiply this angle by <math_exp> to get: <math_exp>. We are quite lucky that there is such a simple formula.  For most manifolds, the curves that minimize distances are not very easy to find since it involves solving a non-linear differential equation (the geodesic equation).  The fact that the sphere is so symmetric helps in this case, and you can maybe convince yourself that an arc of a great circle minimizes distance. You may find this wikipedia article interesting: http://en.wikipedia.org/wiki/Geodesic. '}\n",
      "763 {'Id': '763', 'Type': 'answer', 'ParentId': '761', 'urls': [], 'exp': ['\\\\begin{align*} \\\\sum F &amp;= ma\\\\\\\\ \\\\frac{dv}{dt} &amp;= a\\\\\\\\ &amp;= \\\\frac{\\\\sum F}{m}\\\\\\\\ \\\\sum F &amp;= mg - kv\\\\\\\\ \\\\frac{dv}{dt} &amp;= g - \\\\frac{k}{m} v\\\\end{align*}', '\\\\begin{align*} v &amp;= A + B \\\\space exp\\\\left(\\\\frac{-k}{m} t\\\\right)\\\\\\\\ \\\\frac{dv}{dt} &amp;= - B \\\\cdot \\\\frac{k}{m} \\\\cdot \\\\exp\\\\left(\\\\frac{-k}{m} t\\\\right) \\\\end{align*}', 'v = 0', 't = 0', '\\\\begin{align*} &amp;g - \\\\frac{k}{m}A - \\\\frac{k}{m} B\\\\cdot \\\\exp\\\\left(\\\\frac{-k}{m} t\\\\right)\\\\\\\\ \\\\implies&amp;-B  \\\\frac{k}{m} \\\\cdot \\\\exp\\\\left(\\\\frac{-k}{m} t\\\\right)\\\\\\\\  \\\\implies&amp; A = g \\\\cdot  \\\\frac{m}{k} \\\\space \\\\text{and} \\\\space  B = -g \\\\cdot \\\\frac{m}{k}\\\\\\\\  \\\\implies&amp; v = \\\\frac{mg}{k} \\\\cdot  \\\\left(1 - \\\\exp\\\\left(\\\\frac{-k}{m} t\\\\right)\\\\right)\\\\end{align*}', '\\\\frac{dv}{dt}', 'v', '\\\\sum F = -kv^2'], 'Body': \"<math_exp> This is a differential equation with a solution of <math_exp> Match terms and initial conditions (<math_exp> at time <math_exp>) and you get  <math_exp> That's a linear differential equation (<math_exp> is a linear function of <math_exp>); the <math_exp> is a nonlinear differential equation (can't remember off the top of my head how to deal with that one; it may not have a closed form solution). It's a bit difficult to summarize the techniques in general, but any good book on differential equations would cover them. \"}\n",
      "764 {'Id': '764', 'Type': 'question', 'Title': 'How is the Riemann integral a special case of the Stieltjes integral?', 'Tags': ['real-analysis', 'riemann-integration', 'stieltjes-integral'], 'AcceptedAnswerId': '765', 'urls': [], 'exp': ['\\\\alpha', '[a,b]', 'P', '[a,b]', '\\\\Delta \\\\alpha_i = \\\\alpha(x_i) - \\\\alpha(x_{i-1}).', 'f', '\\\\alpha', '[a,b]', '\\\\alpha(x)=x', '\\\\alpha(x)=x', '\\\\Delta x = x_i - x_{i-1}', 'f', '\\\\alpha(x) \\\\not\\\\equiv x'], 'Body': 'From Rudin\\'s Principles of mathematical analysis, 6.2 Definition Let <math_exp> be a monotonically increasing function on <math_exp>. ... Corresponding to each partition <math_exp> of <math_exp>, we write <math_exp> He then goes on to define the Riemann Stieltjes integral of <math_exp> with respect to <math_exp>, over the interval <math_exp>. The Riemann integral is then pointed out to be a special case of this when <math_exp>. With <math_exp>, I understand <math_exp> to represent the directed magnitude of the \"base of the approximating rectangle\" that we then multiply by the value of <math_exp> taken somewhere within this interval, thus obtaining the area of an approximating rectangle. I don\\'t know where to begin to interpret the case where <math_exp>. '}\n",
      "765 {'Id': '765', 'Type': 'answer', 'ParentId': '764', 'urls': ['http://en.wikipedia.org/wiki/Lebesgue_measure'], 'exp': ['\\\\alpha', '(x_i-x_{i-1})', '\\\\alpha(x_i)-\\\\alpha(x_{i-1})', '\\\\alpha', '\\\\alpha(x_i) -\\\\alpha(x_{i-1})', \"\\\\alpha'(x) (x_i - x_{i-1})\", 'x \\\\in [x_{i-1}, x_i]', '\\\\alpha', \"\\\\alpha'\"], 'Body': 'The idea is that we define a different way of measuring the width of the rectangle. Instead of using the difference of the two coordinates, we take the difference of the images of those two coordinates under the map <math_exp>.  Basically, the idea is that the <math_exp> is to be interpreted as the width or measure of the interval, a construction which leads more generally to the Lebesgue measure of a subset of the reals.  The <math_exp> leads to a different measure, starting with a different assignment of \"size\" to an interval. Note that when <math_exp> is continuously differentiable, then <math_exp> is equal to <math_exp> for <math_exp> up to higher order terms. So in this case the Stieltjes integral with respect to <math_exp> is the Riemann integral of the same function times <math_exp>. '}\n",
      "766 {'Id': '766', 'Type': 'question', 'Title': 'RHS Congruency test - What makes 90 degrees different?', 'Tags': ['geometry', 'trigonometry', 'euclidean-geometry', 'triangles'], 'AcceptedAnswerId': '767', 'urls': ['http://en.wikipedia.org/wiki/Congruence_%28geometry%29#Determining_congruence'], 'exp': [], 'Body': \"RHS is a well known test for determining the congruency of triangles. It is easy enough to prove it works, simply use Pythagorus' theorem to reduce to SSS. I thought that it seems strange that this only works for an angle being 90 degrees - or does it? What if I tried changed the given angle to 89 degrees or 91 degrees, would it still be uniquely identified up to congruence? \"}\n",
      "767 {'Id': '767', 'Type': 'answer', 'ParentId': '766', 'urls': [], 'exp': ['\\\\frac{\\\\sin A}{BC}=\\\\frac{\\\\sin C}{AB}', '\\\\sin C=\\\\frac{AB\\\\cdot\\\\sin A}{BC}'], 'Body': 'Supposing that we knew two triangles had one angle congruent, a side adjacent to the angle congruent, and the side opposite the angle congruent.  This is sometimes referred to as SSA, which is not a congruence theorem (and I\\'ve heard it said that it is \"ass-backwards\").  With a little more information, it is possible to determine congruence in some instances. As you\\'d said about RHS, in that case, you can use right-triangle trigonometry to determine that the unknown sides are congruent, then use SSS to establish congruence.  Without the right angle, the technique for determining the length of the third side of the triangle is to use the Law of Sines to determine the measure of the unknown angle opposite the known side, use that to find the measure of the third angle, then use the Law of Cosines to determine the length of the unknown side. Let\\'s call one of the triangles ABC with &ang;A and AB and BC being known.  From the Law of Sines, <math_exp> or <math_exp>.  There will be two values of C in the range 0&deg; to 180&deg; that satisfy this equation, unless sin C = 1.  So: So, to your specific question, if the angle were 91&deg; (case 2), congruence would follow; if the angle were 89&deg;, congruence may or may not follow, depending on what you can determine about the other sides. As an aside, RHS is also commonly referred to (at least in the midwestern U.S. in contemporary high school geometry) as hypotenuse-leg or HL. '}\n",
      "768 {'Id': '768', 'Type': 'question', 'Title': \"Pick's Theorem on a triangular (or hex) grid\", 'Tags': ['geometry', 'discrete-geometry', 'integer-lattices'], 'AcceptedAnswerId': '773', 'urls': ['http://en.wikipedia.org/wiki/Pick%27s_theorem'], 'exp': ['i + \\\\frac{b}{2} - 1', 'i', 'b'], 'Body': \"Pick's theorem says that given a square grid consisting of all points in the plane with integer coordinates, and a polygon without holes and non selt-intersecting whose vertices are grid points, its area is given by: <math_exp> where <math_exp> is the number of interior lattice points and <math_exp> is the number of points on its boundary. Theorem and proof may be found on Wikipedia. Let us suppose that the grid is not square but triangular (or hexagonal). Does a similar theorem hold? \"}\n",
      "769 {'Id': '769', 'Type': 'answer', 'ParentId': '664', 'urls': ['http://en.wikipedia.org/wiki/Cylindrical_coordinate_system'], 'exp': ['dm', 'dm = \\\\rho(\\\\vec r) d^3 \\\\mathbf r', '\\\\rho = \\\\frac{m}{V}\\\\cdot\\\\theta(R-r)\\\\theta(a^2-z^2)', '\\\\theta(x) = \\\\begin{cases} 0 \\\\;\\\\text{ for } x&lt;0 \\\\text{ and}\\\\\\\\ 1 \\\\;\\\\text{ for } x&gt;0 \\\\end{cases}', 'r = \\\\sqrt{x^2+y^2}', 'r', '0', 'R', 'z', '-a', 'a', 'V=\\\\pi R^2 \\\\cdot 2a', 'm', 'd^3\\\\mathbf r = r\\\\, dr \\\\,d\\\\phi\\\\, dz', 'I = \\\\int_{r=0}^{R} \\\\int_{z=-a}^a \\\\int_{\\\\phi=0}^{2\\\\pi} r^2 \\\\frac{m}{V} \\\\cdot r\\\\, d\\\\phi\\\\, dz\\\\, dr = 4a\\\\pi \\\\frac{m}{V} \\\\int_{r=0}^R r^3 dr = \\\\frac{am\\\\pi}{V}R^4 = \\\\frac{1}{2}mR^2.'], 'Body': \"<math_exp> takes density fluctuations into account, it's just <math_exp>. E.g. for a homogeneous cylinder with the rotational axis align parallel to the z-axis it is <math_exp>, where <math_exp> and <math_exp> is the radial coordinate in cylindrical coordinates. The result is that your integration over <math_exp> is from <math_exp> to <math_exp> and for <math_exp> from <math_exp> to <math_exp>. <math_exp> is the volume of the cylinder, <math_exp> its weight. In cylindrical coordinates, <math_exp>, so you got <math_exp> \"}\n",
      "770 {'Id': '770', 'Type': 'answer', 'ParentId': '720', 'urls': [], 'exp': [], 'Body': 'First, some background.  A \"line\" (the object containing the path of least distance between two points) on a sphere is a great circle; a great circle is the intersection of a sphere with a plane passing through the center of the sphere.  A spherical triangle is the triangle formed by the spherical line segments connecting three non-collinear points on a sphere.  Below is the Spherical Law of Cosines as it appears in UCSMP Functions, Statistics, and Trigonometry, 3rd ed., copied here because the diagram is good and helps with clarity. <img src=\"https://i.stack.imgur.com/KAOhC.png\" alt=\"Spherical Law of Cosines (from UCSMP Functions, Statistics, and Trigonometry, 3rd ed.)\"> If ABC is a spherical triangle with arcs a, b, and c (meaning the measures of the arcs, not the lengths), then cos c = cos a * cos b + sin a * sin b * cos C. Now, to the specific problem at hand.  Let\\'s use the diagram below, also from UCSMP Functions, Statistics, and Trigonometry, 3rd ed., for reference. <img src=\"https://i.stack.imgur.com/DRkJ3.png\" alt=\"globe (from UCSMP Functions, Statistics, and Trigonometry, 3rd ed.)\"> Let B be the initial point and A be the final point of the movement (and for simplicity, I\\'ll assume they are both in the northern hemisphere, and leave extending the solution to any points as an exercise); N and S are the north and south poles, respectively; C and D are the points on the equator that are on the same line of longitude as A and B, respectively.  Consider spherical triangle ABN.  a = (90&deg; - latitude of point A); b = (90&deg; - latitude of point B). N = positive difference in longitude between points A and B.  Use the Spherical Law of Cosines (cos n = ... form) to determine n, then use the Spherical Law of Cosines (cos b = ... form) to determine B.  If the points are arranged as pictured, the heading of travel from B to A is 360&deg; - B; for other arrangements of the points, the exact method of determining the heading based on the measure of angle B may vary. (graphics from Lesson 5-10 of UCSMP Functions, Statistics, and Trigonometry, 3rd ed., © 2010 Wright Group/McGraw Hill) '}\n",
      "771 {'Id': '771', 'Type': 'question', 'Title': 'Expected value of a function of a random variable: help!', 'Tags': ['probability-theory', 'taylor-expansion', 'probability'], 'AcceptedAnswerId': '775', 'urls': ['http://www.princeton.edu/~markus/research/papers/liquidity.pdf', 'http://en.wikipedia.org/wiki/Moment-generating_function#Examples'], 'exp': ['e^{-\\\\gamma W}'], 'Body': \"I am trying to show the following: \\\\begin{equation*} E[e^{-\\\\gamma W}]=e^{-\\\\gamma(E[W]-\\\\frac{\\\\gamma}{2}Var [W])} \\\\end{equation*} but I really can't remember what I am supposed to do to get from the LHS to the RHS. I have tried using integration this way \\\\begin{equation*} \\\\int We^{-\\\\gamma W}dW \\\\end{equation*} and then use integration by parts, but even though what I get resembles it, it can't be correct (because <math_exp> is not the distribution of W). I have also tried using Taylor series expansion, but I think I am way off, and I don't think an approximation here is what I need, because the equality above is exact. FYI, this is not homework, I am working through a paper (page 10) and I would really like to know how every step was derived. Can anyone at least point me to the right direction? EDIT: This expectation on the RHS is very similar to the moment generating function formula (with a negative exponent). If you check here, you will see that the moment generating function for the normal distribution is like the LHS (but with a positive sign). So in a way I have my answer, but I still would like to know how to derive it, if there is a way. I know little if anything at all about moment generating functions, so maybe I shouldn't try and derive it but rather just use the result? Does it even make sense to try and derive it? \"}\n",
      "772 {'Id': '772', 'Type': 'answer', 'ParentId': '768', 'urls': [], 'exp': ['P(i,b)=i+\\\\frac{b}{2}-1', 'i', 'P', 'P(A)=P(i_A,b_A)', 'A', 'P(C)=P(A)+P(T)', 'T', 'A', 'C', 'A', 'T', 'P(C)=sum P(t)', 't', 'C', 'P(t)', 'P(T)'], 'Body': \"This is a very interesting question. I don't have a complete solution yet, but I did get some results. Consider an arbitrary distribution of points. Let <math_exp> where <math_exp> is the number of internal points and <math_exp> is the number of boundary points. Let <math_exp>, where <math_exp> is a simple polygon with all its vertices on the points. Wikipedia shows that <math_exp> where <math_exp> is a triangle that shares a single edge with <math_exp> and <math_exp> a simple polygon formed by the union of <math_exp> and <math_exp>. Since all simple polygons can be triangulated, <math_exp> for all <math_exp> in the triangulation of <math_exp>. The proof then has a second part that shows <math_exp> equals the area for any triangle. So if we want to generalize it for other grids, we have to find a property equal to <math_exp> for any triangle. \"}\n",
      "773 {'Id': '773', 'Type': 'answer', 'ParentId': '768', 'urls': ['http://www.jstor.org/pss/2323889'], 'exp': ['\\\\frac{1}{2}'], 'Body': 'The short answer is that, no, there can be no formula for polygons with vertices in the hexagonal lattice in terms of just boundary and interior points. This is based on the fact that primitive triangles on this lattice--ones with no lattice points on their boundary (besides the vertices) or in the interior--can have different areas, whereas for the square lattice all primitive triangles have area <math_exp>. However, as Casebash has partly gotten at in his answer, you can approximate things well if you compute what, in the below paper, is called the \"boundary characteristic\" of the polygon, a number that is somewhat complicated to think to compute, but which gives a decent proxy for how many of each type of primitive triangle the polygon contains. Kolodziejczyk has been the main one doing work on hexagonal lattice results of this type that I know; he\\'s worth looking up for similar results. Ding Ren is another, and the older work of Grunbaum, etc., still bears on the problem. \"A Fast Pick-Type Approximation for the Area of H-Polygons,\" Ren, Kolodziejczyk, et al., American Mathematical Monthly, 1993. '}\n",
      "774 {'Id': '774', 'Type': 'answer', 'ParentId': '764', 'urls': [], 'exp': [], 'Body': \"To add to Akhil's answer, the Stieltjes integral made a lot more sense to me when I started thinking about it in terms of physics. Think of taking something like the integral you get to determine moment of inertia, where it's mass--a function of position--you're integrating over, and not position itself. \"}\n",
      "775 {'Id': '775', 'Type': 'answer', 'ParentId': '771', 'urls': ['http://mathcache.appspot.com/?tex=%5cpng%5c%5bE%5Be%5E%7B-%5Cgamma%20W%7D%5D%3D%5Cint_%7B-%5Cinfty%7D%5E%5Cinfty%20P%28x%29%20e%5E%7B-%5Cgamma%20x%7D%20dx%5c%5d'], 'exp': [], 'Body': 'If W is randomly chosen with the PDF P(x), then the expectation value should be  http://mathcache.appspot.com/?tex=%5cpng%5c%5bE%5Be%5E%7B-%5Cgamma%20W%7D%5D%3D%5Cint_%7B-%5Cinfty%7D%5E%5Cinfty%20P%28x%29%20e%5E%7B-%5Cgamma%20x%7D%20dx%5c%5d And I think that equation (E[e-&gamma;W] = e-&gamma;(E[W] - &frac12;&gamma;Var[W])) is correct only when W is a normal distribution. '}\n",
      "776 {'Id': '776', 'Type': 'question', 'Title': 'Proof of Angle in a Semi-Circle is of <span class=\"math-container\" id=\"7928\">90</span> degrees', 'Tags': ['geometry', 'euclidean-geometry', 'circles'], 'AcceptedAnswerId': '777', 'urls': [], 'exp': ['90', '90'], 'Body': 'There is a well known theorem often stated as the angle in a semi-circle being <math_exp> degrees. To be more accurate, any triangle with one of its sides being a diameter and all vertices on the circle has its angle opposite the diameter being <math_exp> degrees. The standard proof uses isosceles triangles and is worth having as an answer, but there is also a much more intuitive proof as well (this proof is more complicated though). '}\n",
      "777 {'Id': '777', 'Type': 'answer', 'ParentId': '776', 'urls': ['http://www.imgftw.net/img/762828246.png', 'http://www.imgftw.net/img/319527897.png', 'http://en.wikipedia.org/wiki/Stewart&#39;s_theorem'], 'exp': ['(AB)^2(OC) + (BC)^2(AO) = (AC)((BO)^2 + (AO)(OC))', '(AB)^2r + (BC)^2r = 2r(r^2 + r^2)=4r^3', '(AB)^2+(BC)^2=(2r)^2=(AC)^2'], 'Body': \"Nonstandard proof Consider the semi-circle with endpoints A and C and center O and the inscribed angle &ang;ABC (B on the semi-circle) together with the rotation image of both about O by 180&deg;.  The image of A is C and vice versa; let B' be the image of B.  The image of a line under a 180&deg; rotation is parallel to the original line, AB is parallel to CB' and BC is parallel to B'A, so ABCB' is a parallelogram.  BO and its image must be parallel, but the image of O is itself, since it is the center of rotation, and if BO and B'O are parallel and contain a point in common, they must lie on the same line, so BB' passes through O.  AC and BB' (the diagonals of ABCB') are both diameters of the circle, so they are congruent.  A parallelogram with congruent diagonals is a rectangle.  Thus, &ang;ABC is a right angle (and has measure 90&deg;). diagram http://www.imgftw.net/img/762828246.png Standard proof (or, at least, my guess at it based on the description in the question) As above, consider the semi-circle with endpoints A and C and center O and the inscribed angle &ang;ABC (B on the semi-circle).  Draw in radius OB.  OA = OB, so &#x25B3;AOB is isosceles and &ang;OAB&cong;&ang;OBA.  OB = OC, so &#x25B3;BOC is isosceles and &ang;OBC&cong;&ang;OCB.  Let &alpha;=m&ang;OAB=m&ang;OBA and &beta;=m&ang;OBC=m&ang;OCB.  In  &#x25B3;ABC, the measures of the angles are &alpha;, &alpha;+&beta;, and &beta;, so &alpha;+(&alpha;+&beta;)+&beta;=180&deg; or 2(&alpha;+&beta;)=180&deg; or &alpha;+&beta;=90&deg;, so &ang;ABC has measure 90&deg; and is a right angle. diagram http://www.imgftw.net/img/319527897.png edit: Another Nonstandard proof Use the labeling as above and apply Stewart's Theorem to &#x25B3;ABC: <math_exp>  Substituting the length r of the radius of the semicircle as appropriate: <math_exp>  Dividing both sides by r: <math_exp> So, by the converse of the Pythagorean Theorem, &ang;ABC is a right angle. \"}\n",
      "778 {'Id': '778', 'Type': 'answer', 'ParentId': '756', 'urls': ['https://mathoverflow.net/questions/32269', 'http://en.wikipedia.org/wiki/Hamming_code', 'https://mathoverflow.net/users/7079/falagar'], 'exp': [], 'Body': 'Look at this answer on MathOverflow: Yes, there is a way to guess a number asking 14 questions in worst case. To do it you need a linear code with length 14, dimension 10 and distance at least 3. One such code can be built based on Hamming code (see http://en.wikipedia.org/wiki/Hamming_code). Here is the strategy. Let us denote bits of first player\\'s number as ai, i ∈ [1..10]. We start with asking values of all those bits. That is we ask the following questions: \"is it true that i-th bit of your number is zero?\" Let us denote answers on those questions as bi, i ∈ [1..10]. Now we ask 4 additional questions: Is it true that a1 ⊗ a2 ⊗ a4 ⊗ a5 ⊗ a7 ⊗ a9 is equal to zero? (⊗ is sumation modulo 2). Is it true that a1 ⊗ a3 ⊗ a4 ⊗ a6 ⊗ a7 ⊗ a10 is equal to zero? Is it true that a2 ⊗ a3 ⊗ a4 ⊗ a8 ⊗ a9 ⊗ a10 is equal to zero? Is it true that a5 ⊗ a6 ⊗ a7 ⊗ a8 ⊗ a9 ⊗ a10 is equal to zero? Let q1, q2, q3 and q4 be answers on those additional questions. Now second player calculates ti (i ∈ [1..4]) --- answers on those questions based on bits bj which he previously got from first player. Now there are 16 ways how bits qi can differ from ti. Let di = qi ⊗ ti (hence di = 1 iff qi ≠ ti ). Let us make table of all possible errors and corresponding values of di:   position of error -> (d1, d2, d3, d4) no error -> (0, 0, 0, 0) error in b1 -> (1, 1, 0, 0) error in b2 -> (1, 0, 1, 0) error in b3 -> (0, 1, 1, 0) error in b4 -> (1, 1, 1, 0) error in b5 -> (1, 0, 0, 1) error in b6 -> (0, 1, 0, 1) error in b7 -> (1, 1, 0, 1) error in b8 -> (0, 0, 1, 1) error in b9 -> (1, 0, 1, 1) error in b10 -> (0, 1, 1, 1) error in q1 -> (1, 0, 0, 0) error in q2 -> (0, 1, 0, 0) error in q3 -> (0, 0, 1, 0) error in q4 -> (0, 0, 0, 1) All the values of (d1, d2, d3, d4) are different. Hence we can find where were an error and hence find all ai. Answered by falagar. '}\n",
      "779 {'Id': '779', 'Type': 'answer', 'ParentId': '742', 'urls': [], 'exp': [], 'Body': \"Let t be your target and a and b &lt; a be the values of the stamps you have. Let n = ⌈t/a⌉ be the maximum number of a stamps it makes sense to use. For each number i from n to 0, consider the cost of the solution with: The cost of each solution would obviously be i*a + j*b. Your solution is the pair (i, ⌈(t - i*a) / b⌉ ) that minimizes said cost. (Note there's only one unknown in that pair, i.) An additional optimization would be to stop immediately as soon as a division without rest is performed, either when determining n or j. A division without rest means that we have found a solution that perfectly pays the due amount and cannot thus be improved on. (There may be others like it, but none that is actually better, so there's no need to keep searching). \"}\n",
      "780 {'Id': '780', 'Type': 'answer', 'ParentId': '631', 'urls': [], 'exp': [], 'Body': 'This is still WIP. There are a few missing details, still I think it\\'s better than nothing. Feel free to edit in the missing details. Given a problem of SUBSET-SUM. We have a set of A={a1,a2,...,an} numbers, and another number s. The question we\\'re seeking answer to is, whether or not there\\'s a subset of A whose sum is s. I\\'m assuming that the 24-game allows you to use rational numbers. Even if it doesn\\'t, I think that it is possible to emulate rational numbers up to denominator of size p with integers. We know that SUBSET-SUM is NP-complete even for integers only. I think the SUBSET-SUM problem is NP-hard even if you allow treating each ai as a negative number. That is even if A is of the form A={a1,-a1,a2,-a2,...,an,-an}. This is still a wrinkle I need to iron out in this reduction. Obviously, if there\\'s a subset of A with sum s, then there\\'s a solution to the 24-problem for how to reach using A to s. The solution is only using the + sign. The problem is, what happens if there\\'s no solution which only uses the + sign, but there is a solution which uses other arithmetic operations. Let us consider the following problem. Let\\'s take a prime p which is larger than n, the total number of elements in A. Given an oracle which solves the 24-problem, and a SUBSET-SUM problem of A={a1,a2,...,an} and s. We\\'ll ask the oracle to solve the 24-problem on A={a1+(1/p),a2+(1/p),...,an+(1/p)} for the following values: s1=s+1/p,s2=s+2/p,...,sn=s+n/p. If the solution includes multiplication, we will have a denominator larger than p in the end result, and thus we will not be able to reach any si. Given an arithmetric expression that contains aiaj=x+(1/p2), It is impossible that the denominator p2 would \"cancel\" out, since there are at most n elements in the summation, and thus the numerator would never reach p, since p&gt;n. THIS IS NOT QUIT RIGHT! The expression aiaj-akal will be an integer, and therefor our oracle might return answer which includes two multiplications one negative and one positive. What about division? How can we be sure no division will occur. Find another prime q which is different than p, and larger than the largest ai times n. Multiply all answers by q. The set A will be A={qa1+(1/p),qa2+(1/p),...,qan+(1/p)} We will look for the following values: s1=qs+1/p,s2=qs+2/p,...,sn=qs+n/p. In that case, ai/aj will be smaller than the minimal element in A, and therefor the end result which will contains ai/aj will never be one of the si we\\'re looking for. '}\n",
      "781 {'Id': '781', 'Type': 'question', 'Title': \"Why does Benford's Law (or Zipf's Law) hold?\", 'Tags': ['statistics', 'soft-question', 'applications'], 'AcceptedAnswerId': '863', 'urls': ['http://en.wikipedia.org/wiki/Benford%27s_law', 'http://en.wikipedia.org/wiki/Zipf%27s_law', 'https://math.stackexchange.com/questions/58'], 'exp': [], 'Body': \"Both Benford's Law (if you take a list of values, the distribution of the most significant digit is rougly proportional to the logarithm of the digit) and Zipf's Law (given a corpus of natural language utterances, the frequency of any word is roughly inversely proportional to its rank in the frequency table) are not theorems in a mathematical sense, but they work quite good in the real life. Does anyone have an idea why this happens? (see also this question) \"}\n",
      "782 {'Id': '782', 'Type': 'answer', 'ParentId': '781', 'urls': [], 'exp': [], 'Body': \"For the case of Bendford's Law, of course scale invariance is a necessary condition; it the law must be true either if we measure things in meters or in feet or in furlongs, thus multiplying given data for a constant, the only distribution which allows this is the logarithmic one. But its being necessary does not mean that it is the answer, of course. Scale invariance is not relevant for Zipf's Law, however, since we have an absolute rank. \"}\n",
      "783 {'Id': '783', 'Type': 'question', 'Title': 'Isomorphism and <span class=\"math-container\" id=\"8022\">\\\\mathrm{id}</span>', 'Tags': ['category-theory'], 'AcceptedAnswerId': '784', 'urls': [], 'exp': ['a', 'b', 'm', 'a', 'b', 'n', 'b', 'a', 'm \\\\circ n = \\\\mathrm{id}_b', 'n \\\\circ m = \\\\mathrm{id}_a', 'm', 'n'], 'Body': 'In a category I have two objects <math_exp> and <math_exp> and a morphism <math_exp> from <math_exp> to <math_exp> and one <math_exp> from <math_exp> to <math_exp>. Is this always an isomorphism? Why is it emphasized that this has to be true, too: <math_exp> and <math_exp>? I am looking for an example in which the id-part is not true and therefore <math_exp> and <math_exp> are not isomorphic. '}\n",
      "784 {'Id': '784', 'Type': 'answer', 'ParentId': '783', 'urls': ['https://math.stackexchange.com/questions/309/if-a-is-a-subobject-of-b-and-b-a-subobject-of-a-are-they-isomorphic/327#327'], 'exp': [], 'Body': 'If you have no restrictions on m and n, then clearly they cannot be isomorphisms in general.  For instance, take any two groups G and H and let m: G --> H, n: H --> G be the zero homomorphisms. Even if you say that n and m are monomorphisms, then it is still not true in general that they are isomorphisms.  I believe it is true however if your category is one whose objects are sets with additional structure. See this question: , are they isomorphic?. '}\n",
      "785 {'Id': '785', 'Type': 'question', 'Title': 'Is there a general formula for solving 4th degree equations (quartic)?', 'Tags': ['algebra-precalculus', 'polynomials', 'quartic-equations'], 'AcceptedAnswerId': '786', 'urls': [], 'exp': ['ax^3+bx^2+cx+d=0', 'ax^4+bx^3+cx^2+dx+e=0'], 'Body': 'There is a general formula for solving quadratic equations, namely the Quadratic Formula. For third degree equations of the form <math_exp>, there is a set of three equations: one for each root. Is there a general formula for solving equations of the form <math_exp> ? How about for higher degrees? If not, why not? '}\n",
      "786 {'Id': '786', 'Type': 'answer', 'ParentId': '785', 'urls': ['http://en.wikipedia.org/wiki/Quartic_formula#Solving_a_quartic_equation', 'http://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem', 'http://library.wolfram.com/examples/quintic/'], 'exp': ['ax^4+bx^3+cx^2+d+e=0', 'w=\\\\sqrt{z}', 'w^2=z', '\\\\arg(w)\\\\in(-\\\\frac{\\\\pi}{2},\\\\frac{\\\\pi}{2}]', 'w=\\\\sqrt[3]{z}', 'w^3=z', '\\\\arg(w)\\\\in(-\\\\frac{\\\\pi}{3},\\\\frac{\\\\pi}{3}]', 'p_k', '\\\\quad\\\\quad\\\\quad\\\\quad', '\\\\begin{align} x&amp;=-\\\\frac{b}{4a}-\\\\frac{p_4}{2}-\\\\frac{\\\\sqrt{p_5-p_6}}{2} \\\\\\\\\\\\\\\\ \\\\mathrm{or\\\\ }x&amp;=-\\\\frac{b}{4a}-\\\\frac{p_4}{2}+\\\\frac{\\\\sqrt{p_5-p_6}}{2} \\\\\\\\\\\\\\\\ \\\\mathrm{or\\\\ }x&amp;=-\\\\frac{b}{4a}+\\\\frac{p_4}{2}-\\\\frac{\\\\sqrt{p_5+p_6}}{2} \\\\\\\\\\\\\\\\ \\\\mathrm{or\\\\ }x&amp;=-\\\\frac{b}{4a}+\\\\frac{p_4}{2}+\\\\frac{\\\\sqrt{p_5+p_6}}{2} \\\\end{align}'], 'Body': \"There is, in fact, a general formula for solving quartic (4th degree polynomial) equations.  As the cubic formula is significantly more complex than the quadratic formula, the quartic formula is significantly more complex than the cubic formula.  Wikipedia's article on quartic functions has a lengthy process by which to get the solutions, but does not give an explicit formula. Beware that in the cubic and quartic formulas, depending on how the formula is expressed, the correctness of the answers likely depends on a particular choice of definition of principal roots for nonreal complex numbers and there are two different ways to define such a principal root. There cannot be explicit algebraic formulas for the general solutions to higher-degree polynomials, but proving this requires mathematics beyond precalculus (it is typically proved with Galois Theory now, though it was originally proved with other methods).  This fact is known as the Abel-Ruffini theorem. Also of note, Wolfram sells a poster that discusses the solvability of polynomial equations, focusing particularly on techniques to solve a quintic (5th degree polynomial) equation.  This poster gives explicit formulas for the solutions to quadratic, cubic, and quartic equations. edit:  I believe that the formula given below gives the correct solutions for x to <math_exp> for all complex a, b, c, d, and e, under the assumption that <math_exp> is the complex number such that <math_exp> and <math_exp> and <math_exp> is the complex number such that <math_exp> and <math_exp> (these are typically how computer algebra systems and calculators define the principal roots).  Some intermediate parameters <math_exp> are defined to keep the formula simple and to help in keeping the choices of roots consistent. Let: \\\\begin{align*} p_1&amp;=2c^3-9bcd+27ad^2+27b^2e-72ace \\\\\\\\\\\\\\\\ p_2&amp;=p_1+\\\\sqrt{-4(c^2-3bd+12ae)^3+p_1^2} \\\\\\\\\\\\\\\\ p_3&amp;=\\\\frac{c^2-3bd+12ae}{3a\\\\sqrt[3]{\\\\frac{p_2}{2}}}+\\\\frac{\\\\sqrt[3]{\\\\frac{p_2}{2}}}{3a} \\\\end{align*} <math_exp> \\\\begin{align*} p_4&amp;=\\\\sqrt{\\\\frac{b^2}{4a^2}-\\\\frac{2c}{3a}+p_3} \\\\\\\\\\\\\\\\ p_5&amp;=\\\\frac{b^2}{2a^2}-\\\\frac{4c}{3a}-p_3 \\\\\\\\\\\\\\\\ p_6&amp;=\\\\frac{-\\\\frac{b^3}{a^3}+\\\\frac{4bc}{a^2}-\\\\frac{8d}{a}}{4p_4} \\\\end{align*} Then: <math_exp> (These came from having Mathematica explicitly solve the quartic, then seeing what common bits could be pulled from the horrifically-messy formula into parameters to make it readable/useable.) \"}\n",
      "787 {'Id': '787', 'Type': 'answer', 'ParentId': '785', 'urls': ['http://en.wikipedia.org/wiki/Quartic_formula#The_general_case.2C_along_Ferrari.27s_lines', 'http://en.wikipedia.org/wiki/Insolubility_of_the_quintic'], 'exp': ['S_5', '\\\\mathbb{Q}'], 'Body': \"Yes, there is a quartic formula. There is no such solution by radicals for higher degrees. This is a result of Galois theory, and follows from the fact that the symmetric group <math_exp> is not solvable. It is called Abel's theorem. In fact, there are specific fifth-degree polynomials whose roots cannot be obtained by using radicals from <math_exp>. \"}\n",
      "788 {'Id': '788', 'Type': 'question', 'Title': 'Why is it so hard to find the roots of polynomial equations?', 'Tags': ['intuition', 'polynomials', 'roots'], 'AcceptedAnswerId': '96329', 'urls': ['https://math.stackexchange.com/questions/785/is-there-a-general-formula-for-solving-4th-degree-equations'], 'exp': [], 'Body': 'The question that follows was inspired by this question: When trying to solve for the roots of a polynomial equation, the quadratic formula is much more simple than the cubic formula and the cubic formula is much more simple than the quartic formula. '}\n",
      "789 {'Id': '789', 'Type': 'answer', 'ParentId': '707', 'urls': [], 'exp': ['P(10)=p_i (1-p_i)=\\\\frac{i}{m+1}\\\\left (1-\\\\frac{i}{m+1}  \\\\right )=\\\\frac{i(m+1-i)}{(m+1)^2}', 'P(11)=p_i^2=\\\\left (\\\\frac{i}{m+1}  \\\\right )^2=\\\\frac{i^2}{(m+1)^2}', 'E(p_i)=\\\\sum_{i=1}^{m}p_iP(p_i)=\\\\sum_{i=1}^{m}\\\\frac{i}{m+1}\\\\left(\\\\frac{P(10)+P(11)}{\\\\sum_{i=1}^{m}(P(10)+P(11))} \\\\right )', '=\\\\frac{\\\\sum_{i=1}^{m}\\\\frac{i}{m+1}\\\\left(\\\\frac{i(m+1-i)}{(m+1)^2}+\\\\frac{i^2}{(m+1)^2} \\\\right )}{\\\\sum_{i=1}^{m}\\\\left(\\\\frac{i(m+1-i)}{(m+1)^2}+\\\\frac{i^2}{(m+1)^2} \\\\right )}', '=\\\\frac{\\\\sum_{i=1}^{m}\\\\frac{i}{m+1}\\\\left(i(m+1-i)+i^2\\\\right )}{\\\\sum_{i=1}^{m}\\\\left(i(m+1-i)+i^2\\\\right )}', '=\\\\frac{\\\\sum_{i=1}^{m}i^2}{(m+1)\\\\sum_{i=1}^{m}i}', '=\\\\frac{\\\\left(\\\\frac{m(m+1)(2m+1)}{6} \\\\right )}{(m+1)\\\\frac{m(m+1)}{2}}', '=\\\\frac{2m+1}{3(m+1)}', '\\\\lim_{m\\\\to\\\\infty}E(p_i)=\\\\lim_{m\\\\to\\\\infty}\\\\frac{2m+1}{3(m+1)}=\\\\frac{2}{3}', 'P(1...)=p_i\\\\sum_{k=0}^{n-1}\\\\left({n-1 \\\\choose k}p_i^k(1-p_i)^{n-1-k} \\\\right )=p_i', 'E(p_i)=\\\\sum_{i=1}^{m}p_iP(p_i)=\\\\sum_{i=1}^{m}p_i\\\\left(\\\\frac{P(1...)}{\\\\sum_{i=1}^{m}P(1...)} \\\\right )=\\\\sum_{i=1}^{m}p_i\\\\left(\\\\frac{p_i}{\\\\sum_{i=1}^{m}p_i} \\\\right )', '=\\\\frac{\\\\sum_{i=1}^{m}p_i^2}{\\\\sum_{i=1}^{m}p_i}=\\\\frac{\\\\sum_{i=1}^{m}\\\\left(\\\\frac{i}{m+1} \\\\right )^2}{\\\\sum_{i=1}^{m}\\\\frac{i}{m+1}}', '=\\\\frac{\\\\sum_{i=1}^{m}i^2}{(m+1)\\\\sum_{i=1}^{m}i}=\\\\frac{2m+1}{3(m+1)}', 'P(1...)', '\\\\lim_{m\\\\to\\\\infty}E(p_i)=\\\\lim_{m\\\\to\\\\infty}\\\\frac{2m+1}{3(m+1)}=\\\\frac{2}{3}'], 'Body': \"First, sticking to strings of length n=2: <math_exp> and <math_exp>; <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> <math_exp>; <math_exp> For general n: <math_exp> (yes, P(1...) does not depend on n at all!); <math_exp> <math_exp> <math_exp> (no surprise there, given that <math_exp> doesn't depend on n); <math_exp> \"}\n",
      "790 {'Id': '790', 'Type': 'question', 'Title': 'Volume of the sphere in a Banach space', 'Tags': ['real-analysis'], 'AcceptedAnswerId': '824', 'urls': [], 'exp': [' 1'], 'Body': 'For <math_exp> an integer, there are well-known formulas for volume of the balls. What is the analogous statement in a Banach space/Hilbert Space? '}\n",
      "791 {'Id': '791', 'Type': 'question', 'Title': 'How to compute the volume of this object via integration?', 'Tags': ['calculus'], 'AcceptedAnswerId': '913', 'urls': [], 'exp': ['1', 'x, y, z', '1'], 'Body': 'What is the volume of intersection of the three cylinders with axes of length <math_exp> in <math_exp> directions starting from the origin, and with radius <math_exp>? '}\n",
      "792 {'Id': '792', 'Type': 'answer', 'ParentId': '788', 'urls': [], 'exp': ['n', 'n', 'n', 'n', 'n = 2', 'n = 3', '4', 'n = 5', 'n'], 'Body': 'When you try to solve a degree <math_exp> equation, there are <math_exp> roots you have to find (in principle) and none of them is favoured over any of the others, which (in some metaphorical sense) means that you have to break an <math_exp>-fold symmetry in order to write down the roots. Now the symmetry group of the n roots becomes more and more complicated the larger <math_exp> is. For <math_exp>, it is abelian (and very small!); for <math_exp> and <math_exp> it is still solvable (in the technical sense of group theory), which explains the existence of an explicit formula involving radicals (this is due to Galois, and is a part of so-called Galois theory); for <math_exp> or more this group is non-solvable (in the technical sense of group theory), and this corresponds to the fact that there is no explicit formula involving radicals. Summary: The complexity of the symmetry group of the <math_exp> roots leads to a corresponding complexity in explicitly solving the equation. '}\n",
      "793 {'Id': '793', 'Type': 'question', 'Title': 'Exponential and log functions compose to identity', 'Tags': ['sequences-and-series', 'logarithms', 'exponentiation'], 'AcceptedAnswerId': '813', 'urls': [], 'exp': ['z', '1', 'x^n', 'n', 'n'], 'Body': 'How to prove that the exponential function and the logarithm function are the inverses of each other? I want it the following way. We must use the definition as power series, and must verify that all the terms of the composition except the coefficent of <math_exp> vanish, and that the first degree term is <math_exp>. I can write down the proof for the coefficient of <math_exp> for arbitrary but fixed <math_exp> by explicit verification. But how to settle this for all <math_exp> at one go? '}\n",
      "794 {'Id': '794', 'Type': 'answer', 'ParentId': '462', 'urls': [], 'exp': [], 'Body': \"I don't think the people working in higher-order theorem proving really care about Henkin semantics or models in general, they mostly work with their proof calculi. As long as there are no contradictions or other counterintuitive theorems they are happy. The most important and most difficult theorem they prove is usually that their proof terms terminate, which IIRC can be viewed as a form of soundness. Henkin semantics is most interesting for people trying to extend their first-order methods to higher-order logic, because it behaves essentially like models of first-order logic. Henkin semantics is somewhat weaker than what you would get with standard set-theoretic semantics, which by Gödels incompleteness theorem can't have a complete proof calculus. I think type theories should lie somewhere in between Henkin and standard semantics. Where does typed lambda calculus come into proof verification? To prove some implication P(x) --&gt; Q(x) with some free variables x you need to map any proof of P(x) to a proof of Q(x). Syntactically a map (a function) can be represented as a lambda term. Are there any other approaches than higher order logic to proof verification? You can also verify proofs in first-order or any other logic, but then you would loose much of the power of the logic. First-order logic is mostly interesting because it is possible to automatically find proofs, if they are not too complicated. The same applies even more to propositional logic. What are the limitations/shortcomings of existing proof verification systems (see below)? The more powerful the logic becomes the harder it becomes to construct proofs. Since the systems are freely available I suggest you play with them, e.g. Isabelle and Coq for a start. \"}\n",
      "795 {'Id': '795', 'Type': 'question', 'Title': 'How to prove that a conditionally convergent series can be rearranged to sum to any real number?', 'Tags': ['calculus'], 'AcceptedAnswerId': '867', 'urls': [], 'exp': [], 'Body': 'There is a  theorem of Riemann to that effect. How to prove it? Note: This was asked by Kenny in the beta for \"calculus\". '}\n",
      "796 {'Id': '796', 'Type': 'question', 'Title': 'Separation of variables for partial differential equations', 'Tags': ['calculus', 'pde'], 'urls': [], 'exp': [], 'Body': 'What class of Partial Differential Equations can be solved using the method of separation of variables? '}\n",
      "797 {'Id': '797', 'Type': 'question', 'Title': 'What\\'s so \"natural\" about the base of natural logarithms?', 'Tags': ['calculus', 'logarithms', 'exponential-function'], 'AcceptedAnswerId': '817', 'urls': [], 'exp': ['e', '\\\\frac{1}x'], 'Body': 'There are so many available bases. Why is the strange number <math_exp> preferred over all else? Of course one could integrate <math_exp> and see this. But is there more to the story? '}\n",
      "798 {'Id': '798', 'Type': 'question', 'Title': 'Why is the integral expression for the length of a curve more complicated than the expression for area or volume of a figure rotated?', 'Tags': ['calculus'], 'urls': [], 'exp': [], 'Body': 'There are quite simple, intuitive and straightforward expressions for evaluating the area or volume of a figure. But why is the expression for the length of a curve so complicated? '}\n",
      "799 {'Id': '799', 'Type': 'question', 'Title': 'Why does the discriminant of an integral polynomial have integral coefficients?', 'Tags': ['abstract-algebra', 'polynomials'], 'AcceptedAnswerId': '821', 'urls': [], 'exp': [], 'Body': 'Ok, Ok, I know that in fact the discriminant is defined (up to sign) as a product of differences of the roots of the polynomial. But why does it then have integral coefficients, if the polynomial you started with had integer coefficients? '}\n",
      "800 {'Id': '800', 'Type': 'answer', 'ParentId': '797', 'urls': [], 'exp': ['\\\\displaystyle\\\\int \\\\frac{1}{x} dx=\\\\ln x', 'e^x', ' \\\\frac{d}{dx} e^x=e^x ', ' \\\\int e^x dx=e^x+c ', 'a^x', '\\\\frac {d} {dx} a^x= \\\\frac{d}{dx} e^{x\\\\ln(a)}=\\\\ln(a) \\\\cdot a^x', '\\\\int a^x dx= \\\\int e^{x\\\\ln(a)} dx=\\\\frac{a^x}{\\\\ln(a)}+c', 'e'], 'Body': 'Differentiation and integration is precisely why it is considered natural, but not just because  <math_exp> <math_exp> has the two following nice properties <math_exp> <math_exp> If we looked at <math_exp> instead, we would get: <math_exp> <math_exp> So <math_exp> is vital to the integration and differentiation of exponentials. '}\n",
      "801 {'Id': '801', 'Type': 'answer', 'ParentId': '797', 'urls': ['http://en.wikipedia.org/wiki/E_%28mathematical_constant%29'], 'exp': [' \\\\frac{d}{dt} e^t =e^t ', ' \\\\frac{d}{dt} a^t = c \\\\cdot a^t', 'c'], 'Body': 'The Wikipedia article about e lists many properties of the constant that make it naturally occurring. I think the biggest reason it is natural when it comes to exponentiation/logarithms is that it is the only number that satisfies <math_exp> while every other number satisfies <math_exp> where <math_exp> is some constant, different than 1.  This makes it \"normalized\" in a sense. '}\n",
      "803 {'Id': '803', 'Type': 'question', 'Title': 'What is the most elegant proof of the Pythagorean theorem?', 'Tags': ['geometry', 'big-list', 'algebra-precalculus', 'euclidean-geometry'], 'urls': ['http://en.wikipedia.org/wiki/Pythagorean_theorem', 'http://www.cut-the-knot.org/pythagoras/', 'http://en.wikipedia.org/wiki/James_A._Garfield'], 'exp': [], 'Body': 'The Pythagorean Theorem is one of the most popular to prove by mathematicians, and there are many proofs available (including one from James Garfield). What\\'s the most elegant proof? My favorite is this graphical one: <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Pythagorean_Proof_%283%29.PNG/220px-Pythagorean_Proof_%283%29.PNG\" alt=\"alt text\"> According to cut-the-knot: Loomis (pp. 49-50) mentions that the   proof \"was devised by Maurice Laisnez,   a high school boy, in the   Junior-Senior High School of South   Bend, Ind., and sent to me, May 16,   1939, by his class teacher, Wilson   Thornton.\" The proof has been published by Rufus   Isaac in Mathematics Magazine, Vol. 48   (1975), p. 198. '}\n",
      "804 {'Id': '804', 'Type': 'question', 'Title': 'Applications for the class equation from group theory', 'Tags': ['group-theory', 'big-list'], 'urls': [], 'exp': [], 'Body': \"I have seen and studied this class equation for a finite group acting on itself by conjugations. The only applications I know are Cauchys' theorm and Sylow's theorem. Are there more? \"}\n",
      "805 {'Id': '805', 'Type': 'answer', 'ParentId': '797', 'urls': ['http://bit.ly/FtPFd'], 'exp': [' = 2.613035…. Bernoulli noticed that this sequence approaches a limit (the force of interest) for more and smaller compounding intervals. Compounding weekly yields 2.692597…, while compounding daily yields 2.714567…, just two cents more. Using n as the number of compounding intervals, with interest of 100%/n in each interval, the limit for large n is the number that came to be known as e; with continuous compounding, the account value will reach 2.7182818…. More generally, an account that starts at '], 'Body': 'The wikipedia article on e tells a bit of the story. One example is an account that starts with 1.00 and pays 100% interest per year. If the interest is credited once, at the end of the year, the value is 2.00; but if the interest is computed and added twice in the year, the 1 is multiplied by 1.5 twice, yielding 1.00×1.5² = <math_exp>1, and yields (1+R) dollars at simple interest, will yield eR dollars with continuous compounding. Additionally, it is the base of the exponential function y = k^x, finding a specific value for k where d/dx k^x = k^x.  That is, the rate of change of the exponential curve at any point is equal to the y value of the curve at that point. '}\n",
      "806 {'Id': '806', 'Type': 'question', 'Title': 'Applications of class number', 'Tags': ['algebraic-number-theory'], 'AcceptedAnswerId': '933', 'urls': [], 'exp': ['1'], 'Body': 'There is the notion of class number from algebraic number theory. Why is such a notion defined and what good comes out of it? It is nice if it is <math_exp>; we have unique factorization of all ideals; but otherwise? '}\n",
      "807 {'Id': '807', 'Type': 'answer', 'ParentId': '799', 'urls': ['http://mathworld.wolfram.com/FundamentalTheoremofSymmetricFunctions.html'], 'exp': ['x^2 + bx + c', 'r_1, r_2', 'x^2 + bx + c = (x - r_1)(x - r_2)', 'b = - r_1 - r_2', 'c = r_1 r_2', 'r_1', 'r_2', 'b', 'c', '(r_1 - r_2)^2 = r_1^2 - 2r_1 r_2 + r_2^2 = (r_1 + r_2)^2 - 4r_1 r_2 = b^2 - 4c.'], 'Body': 'The square of the discriminant is symmetric in the roots of the polynomial; if you permute the roots, it stays the same.  By the fundamental theorem of symmetric functions, that means it can be expressed as a polynomial, with integer coefficients, in the coefficients of the original polynomial.  (I am using both \"coefficients\" and \"polynomial\" in two senses here, so let me know if this doesn\\'t make sense.)  This is a basic observation which will become important if you ever study Galois theory. An example will probably make this clearer.  Suppose I have a quadratic polynomial <math_exp> with two roots <math_exp>.  Then <math_exp>, so <math_exp> and <math_exp>.  These are the elementary symmetric functions in two variables, and the theorem above implies that every polynomial function of <math_exp> and <math_exp> which is invariant under switching the two is actually a polynomial in <math_exp> and <math_exp>.  For example, the discriminant is <math_exp> '}\n",
      "808 {'Id': '808', 'Type': 'answer', 'ParentId': '795', 'urls': [], 'exp': [], 'Body': 'Let the number be r.  The idea is that you first add all the positive elements of the sequence in order, until you get over r, then you add all the negative elements until you get under r, and then all the positive until you get over r, and so forth. '}\n",
      "809 {'Id': '809', 'Type': 'answer', 'ParentId': '798', 'urls': [], 'exp': ['x'], 'Body': 'Obviously this depends on the shape, as you implied. I would say for a circle the circumference formula is simpler. Also the length of the curve is a volume in some sense, its just 1 dimensional. Usually in calculus we are calculating the area relative to the the x-axis, and your formula is in terms of <math_exp>, so that makes things simpler. I imagine that if the curve was parameterized in terms of its arc length finding its length would be quite trivial, and finding the area between the curve and the x-axis would seem very complicated. '}\n",
      "811 {'Id': '811', 'Type': 'answer', 'ParentId': '803', 'urls': ['http://www.arseweb.com/rupe/pythag/proof.html'], 'exp': [], 'Body': 'The proof that uses the fact that shearing a parallelogram parallel to one of its sides preserves area is my favorite.  Here\\'s an animation from this site: <img src=\"https://i.stack.imgur.com/W9r8D.gif\" alt=\"animated proof\"> '}\n",
      "812 {'Id': '812', 'Type': 'answer', 'ParentId': '798', 'urls': [], 'exp': [], 'Body': \"The length of a curve doesn't transform in a nice way under scaling of one of the variables the same way that the area of a 2d figure or the volume of a 3d figure does.  That puts a hard lower bound on how complicated it has to be. \"}\n",
      "813 {'Id': '813', 'Type': 'answer', 'ParentId': '793', 'urls': ['http://qchu.wordpress.com/2009/06/24/gila-vi-the-cycle-index-polynomials-of-the-symmetric-groups/'], 'exp': ['A(x) = \\\\sum_{n \\\\ge 0} a_n \\\\frac{x^n}{n!}', 'a_n', 'n', 'a_0 = 0', '\\\\exp A(x) = \\\\sum_{n \\\\ge 0} b_n \\\\frac{x^n}{n!}', 'b_n', 'n', 'A', 'A(x) = e^x - 1', '\\\\exp A(x)', '\\\\log \\\\frac{1}{1 - x} = \\\\sum_{n \\\\ge 1} \\\\frac{x^n}{n}', '(n-1)!', 'n', 's_1 \\\\to s_2 \\\\to ... \\\\to s_n \\\\to s_1', '\\\\exp \\\\log \\\\frac{1}{1 - x}', 'n', 'n', '\\\\exp \\\\log \\\\frac{1}{1 - x} = \\\\frac{1}{1 - x}.', '\\\\mathbb{Z}'], 'Body': 'There is a combinatorial proof.  If <math_exp> is an exponential generating function for the number of ways <math_exp> to put a certain structure on a set of size <math_exp>, where <math_exp>, then <math_exp> has coefficients <math_exp> which count the number of ways to split a set of size <math_exp> into subsets, each of which is given an <math_exp>-structure.  For example, <math_exp> is the structure of \"being a nonempty set,\" and <math_exp> counts the number of ways to partition a set into nonempty subsets. Now <math_exp> can be thought of as the structure of \"being a cycle,\" which is to say there are <math_exp> ways to arrange <math_exp> objects into a cycle <math_exp>.  So <math_exp> counts the number of ways to split <math_exp> objects up into disjoint cycles, which is the same as the number of permutations of <math_exp> by cycle decomposition.  So <math_exp> A generalization of this argument lets you compute the cycle index polynomials of the symmetric groups.  Note that all of the above manipulations take place in the ring of formal power series over <math_exp>. '}\n",
      "814 {'Id': '814', 'Type': 'answer', 'ParentId': '804', 'urls': [], 'exp': ['F', 'Z', 'Z', '\\\\mathbb{F}_q', 'q = p^k', 'F', 'n', '\\\\mathbb{F}_q', 'q^n', 'F', 'q^n - 1 = q - 1 + \\\\sum_i \\\\frac{q^n - 1}{q^{t_i} - 1}', 'q - 1', 'q^{t_i} - 1', 'q^n - 1', 't_i', 'n', 'q', 'q^{t_i} - 1', 't_i', 'q^n = 1 \\\\pmod{q^{t_i} - 1}', 'f_n', 'n', 'f_n(q)', 'q^n - 1', 'f_n(q)', 'q - 1', 'n = 1', 'F', 'f_n(q)', 'q - \\\\omega', '\\\\omega', 'q - 1', ' 1'], 'Body': \"A nice application is Wedderburn's theorem: every finite skewfield is necessarily commutative. Here a skewfield is something which satisfies the same axioms as field, except that multiplication is not required to be commutative; the typical example are quaternions. To see this, let <math_exp> be a finite skewfield, <math_exp> be its center. It is easy to see that <math_exp> is a field, hence it must be <math_exp> for some prime power <math_exp>. <math_exp> will then be a vector space of finite dimension <math_exp> over <math_exp>, hence it will have <math_exp> elements. Now write the class equation for the multiplicative group of <math_exp>: <math_exp> Here <math_exp> appears as the cardinality of the center, while the sum extends over a set of representatives of the non-trivial conjugacy classes. Note that for <math_exp> to divide <math_exp>, <math_exp> must divide <math_exp>. Indeed the order of <math_exp> modulo <math_exp> is <math_exp>, and <math_exp>. Now let <math_exp> be the <math_exp>-th cyclotomic polynomial. Then <math_exp> divides <math_exp> and it also divides each term in the sum, so <math_exp> divides <math_exp>. But this is impossible unless <math_exp> and the sum is empty, in which case <math_exp> is commutative. Indeed <math_exp> is a product of terms of the form <math_exp>, where <math_exp> is a root of unity, and this product will have bigger absolute value than <math_exp> as soon as <math_exp>. \"}\n",
      "815 {'Id': '815', 'Type': 'question', 'Title': 'Which mathematicians have influenced you the most?', 'Tags': ['soft-question', 'math-history', 'big-list'], 'urls': ['https://mathoverflow.net/questions/5499/'], 'exp': [], 'Body': \"This question is lifted from Mathoverflow.. I feel it belongs here too. There are mathematicians whose creativity, insight and taste have the power of driving anyone into a world of beautiful ideas, which can inspire the desire, even the need for doing mathematics, or can make one to confront some kind of problems, dedicate his life to a branch of math, or choose an specific research topic. I think that this kind of force must not be underestimated; on the contrary, we have the duty to take advantage of it in order to improve the mathematical education of those who may come after us, using the work of those gifted mathematicians (and even their own words) to inspire them as they inspired ourselves. So, I'm interested on knowing who (the mathematician), when (in which moment of your career), where (which specific work) and why this person had an impact on your way of looking at math. Like this, we will have an statistic about which mathematicians are more akin to appeal to our students at any moment of their development. Please, keep one mathematician for post, so that votes are really representative. \"}\n",
      "816 {'Id': '816', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://en.wikipedia.org/wiki/Cauchy_distribution'], 'exp': [], 'Body': \"The Cauchy distribution is another example.  This distribution pops up frequently in statistical reasoning. For example, the ratio of two independent standard normal random variables is a standard Cauchy variable.  We calculate ratios all the time, but often forget to consider that their distribution does not follow a simple normal distribution. This is interesting for several reasons: The mean and variance for the Cauchy distribution are undefined (--> infinity).  If one is trying to estimate these parameters for the ratio of two normal variables, the results may blow up to rather large values that are hard to interpret. In such a situation, one would therefore prefer to use more robust estimators of the central tendency (such as the median) and scale (median absolute deviation).  When designing and testing robust estimators (en.wikipedia.org/wiki/Robust_statistics), one way to test them is to try them out on a Cauchy distribution, and make sure that they don't blow up like the usual formulas for mean and variance. \"}\n",
      "817 {'Id': '817', 'Type': 'answer', 'ParentId': '797', 'urls': [], 'exp': ['e^x', '1', 'D', '\\\\mathbb{R}', 'D', '\\\\sum a_k D^k', 'D', 'D', 'e', 'e^{\\\\lambda x}', '\\\\lambda'], 'Body': \"If you know some linear algebra, then here is an abstract reason: <math_exp> is the unique eigenvector of eigenvalue <math_exp> of the derivative <math_exp> acting on, say, the space of smooth functions on <math_exp>.  Why is this important?  The study of solutions of linear differential equations with constant coefficients is equivalent to the study of nullspaces of operators which are polynomials in <math_exp>, e.g. operators of the form <math_exp>.  Any such operator automatically commutes with <math_exp>, so this nullspace splits up into eigenspaces of <math_exp>.  That's why solutions to linear differential equations with constant coefficients can be expressed as sums of complex exponentials.  The choice of <math_exp> makes it particularly easy to see what the eigenvalue is: the eigenvalue of the eigenvector <math_exp> is <math_exp>. \"}\n",
      "818 {'Id': '818', 'Type': 'answer', 'ParentId': '806', 'urls': ['http://www.jmilne.org/math/CourseNotes/ant.html'], 'exp': [], 'Body': \"We say that a prime p is regular if it does not divide the class number of the p-th cyclotomic field. For regular primes, it is easy to prove Fermat's last theorem, as outlined for instance in Milne's notes. Basically, everything would be easy if the class number was 1, in which case one could use unique factorization. If the class number is prime to p, you use the fact that every ideal whose p-th power is principal must itself be principal. This, together with unique factorization for ideals, turns out to be all that you need in the naif proof. \"}\n",
      "819 {'Id': '819', 'Type': 'answer', 'ParentId': '803', 'urls': ['http://rads.stackoverflow.com/amzn/click/0691140200'], 'exp': ['A, B, C', 'C', \"A', B', C'\", 'a, b, c', \"A A'\", 'c^2 - b^2 - a^2'], 'Body': \"This is not really a proof, in the sense that it depends on certain physical assumptions, but it forces you to think very hard about what those physical assumptions are.  I learned it from Mark Levi's The Mathematical Mechanic, although unfortunately I don't have a diagram. Consider a fish tank in the shape of a triangular prism, where the triangle on top has vertices <math_exp> (<math_exp> is the right angle) and the triangle on the bottom has opposite vertices <math_exp>.  Let <math_exp> denote the lengths of the opposite sides.  Run a rod through the edge <math_exp> on which the fish tank is allowed to pivot, and fill it with water.  By conservation of energy, this system is in mechanical equilibrium. On the other hand, I claim (and it is not hard to see) that the total torque about the rod is proportional to <math_exp>, each contribution coming from the pressure of the water in the tank against the corresponding side. \"}\n",
      "820 {'Id': '820', 'Type': 'answer', 'ParentId': '640', 'urls': [], 'exp': ['f : \\\\mathbb{R} \\\\to \\\\mathbb{R}', 'f(x) = ax + b', 'f : \\\\mathbb{C} \\\\to \\\\mathbb{C}', 'f(z) = az + b', 'z, a, b'], 'Body': \"As Akhil mentions, the keyword is elliptic regularity.  Since I don't know anything about this, let me just say some low-level things and maybe they'll make sense to you. A differentiable function <math_exp> can be thought of as a function which behaves locally like a linear function <math_exp>.  So, very roughly, it is a collection of tiny vectors which fit together.  These tiny vectors can, however, fit together in a very erratic manner.  That's because since you only have to fit one vector to the two vectors that are its neighbors, there is a lot of room for bad behavior. A differentiable function <math_exp> has to satisfy a much more stringent requirement: locally, it has to behave like a linear function <math_exp> where <math_exp> are complex, which is a rotation (and scale, and translation).  So, very roughly, it is a collection of tiny rotations which fit together.  Now one rotation has a continuum of neighbors to worry about, and it becomes much harder for erratic behavior to persist. \"}\n",
      "821 {'Id': '821', 'Type': 'answer', 'ParentId': '799', 'urls': [], 'exp': [], 'Body': \"Another definition of discriminant is as the resultant of the polynomial with it's first derivative (up to a scalar), and the resultant of two polynomials vanishes if and only if they have a common root.  So when does a polynomial have a common root with it's derivative? That's when a zero is also a local max or min, precisely a multiple root. \"}\n",
      "822 {'Id': '822', 'Type': 'answer', 'ParentId': '785', 'urls': ['http://mathworld.wolfram.com/TridentofDescartes.html'], 'exp': ['+', '-', '\\\\times', '\\\\div', '\\\\sqrt[n]{}'], 'Body': 'Regarding the inability to solve the quintic, this is sort-of true and sort-of false.  No, there is no general solution in terms of <math_exp>, <math_exp>, <math_exp> and <math_exp>, along with <math_exp>.  However, if you allow special theta values (a new operation, not among the standard ones!) then yes, you can actually write down the solutions of arbitrary polynomials this way.  Also, you can do construct lengths equal to the solutions by intersecting lower degree curves (for a quintic, you can do so with a trident and a circle.) '}\n",
      "823 {'Id': '823', 'Type': 'answer', 'ParentId': '640', 'urls': [], 'exp': ['\\\\frac {(f(z+h) - f(z))}{h}', 'f(x, y) = (x, -y)'], 'Body': 'The existence of a complex derivative means that locally a function can only rotate and expand.  That is, in the limit, disks are mapped to disks.  This rigidity is what makes a complex differentiable function infinitely differentiable, and even more, analytic. For a complex derivative to exist, it has to exist and have the same value for all ways the \"h\" term can go to zero in <math_exp>.  In particular, h could approach 0 along any radial path, and that\\'s why an analytic function must map disks to disks in the limits. By contrast, an infinitely differentiable function of two real variables could map a disk to an ellipse, stretching more in one direction than another.  An analytic function can\\'t do that. A smooth function of two variables could also flip a disk over, such as <math_exp>.  An analytic function can\\'t do that either.  That\\'s why complex conjugation is not an analytic function. '}\n",
      "824 {'Id': '824', 'Type': 'answer', 'ParentId': '790', 'urls': ['http://www.johndcook.com/blog/2010/07/02/volumes-of-generalized-unit-balls/'], 'exp': [], 'Body': 'Maybe this is helpful, a formula for the volume of Lp balls in R^n. '}\n",
      "825 {'Id': '825', 'Type': 'answer', 'ParentId': '600', 'urls': ['http://qchu.wordpress.com/2009/06/21/gila-v-the-polya-enumeration-theorem-and-applications/'], 'exp': ['\\\\frac{1}{n} \\\\sum_{d | n} (x_1^{n/d} + ... + x_k^{n/d})^d \\\\varphi \\\\left( \\\\frac{n}{d} \\\\right)', 'x_1^{r_1} ... x_k^{r_k}'], 'Body': 'I wrote a series of blog posts which explains how to solve questions like this; the relevant one is here.  The generating function you want is <math_exp> where the coefficient of <math_exp> is the number you want. '}\n",
      "827 {'Id': '827', 'Type': 'answer', 'ParentId': '815', 'urls': [], 'exp': [], 'Body': \"I'm not going to give a complete biography, but for those who don't know... \"}\n",
      "828 {'Id': '828', 'Type': 'answer', 'ParentId': '535', 'urls': [], 'exp': ['p_c(x) = 4x^4 + 8x^3 - 3x^2 - 9x + c=0', 'c=-4x^4-8x^3+3x^2+9x', 'xc', 'c', 'c'], 'Body': \"Well, you're looking at <math_exp>, which is just <math_exp>, so if you graph in the <math_exp>-plane, you'll get a quartic that opens down with the horizontal cross sections the zeros for given <math_exp>.  In fact, because we can solve for <math_exp> like this to make it a graph in the affine plane, it is a rational quartic curve, so you might want to start looking there. \"}\n",
      "829 {'Id': '829', 'Type': 'answer', 'ParentId': '756', 'urls': [], 'exp': [], 'Body': \"Falagar's answer doesn't seem to be for this question. In this question we only can ask higher/lower, not questions about arbitrary bits. I would think only checking if he lied after a certain number of questions would be best. When playing the game, if he lied at one point, you'd start to get all the answers being either higher or lower (because you're searching in the wrong area) So the point is to maximize the number of times he must change his answer to be able to detect his lie more easily. So I think it would skew your guesses to either the upper or lower end of range. [Sorry for this long post, I can't comment yet and I didn't want to just say people aren't reading the question correctly] \"}\n",
      "830 {'Id': '830', 'Type': 'answer', 'ParentId': '575', 'urls': ['http://rigtriv.wordpress.com/2008/08/04/dual-curves/'], 'exp': ['F(x_0,x_1,x_2)=0', '(y_0,y_1,y_2)', '(\\\\mathbb{P}^2)^*', 'F', 'y_0 x_0+y_1 x_1+y_2 x_2=0', '\\\\mathbb{P}^2', 'y_0,y_1,y_2', 'x_i', '(\\\\mathbb{P}^2)^*', 'y_2\\\\neq 0', 'x_2', 'g(x_0,x_1)=y_2^n F(x_0,x_1,-\\\\frac{1}{y_2}(y_0x_0+y_1x_1))', 'n', 'x_0,x_1', 'y_i', 'C', 'L', '2n^2-n', 'y_i'], 'Body': \"Answer edited, in response to the comment and a second wind for explaining mathematics: Let <math_exp> be the equation for your curve, and take <math_exp> to be coordinates on <math_exp>.  Also, assume that <math_exp> is irreducible and has no linear factors. Then <math_exp> is the equation of a general line in <math_exp> (recall, here <math_exp> are fixed, and the <math_exp> are the coordinates on the plane) and we look at the open set of <math_exp> where <math_exp>.  On this open set, we can solve the equation of the line for <math_exp>, and look at <math_exp>, a homogeneous polynomial of degree <math_exp> in <math_exp> with coefficients homogeneous polynomials in the <math_exp>.  This polynomial has zeros the intersections of our curve <math_exp> with the line <math_exp> we're looking at. So we want to find points of multiplicity at least two.  So how do we find multiple roots of a polynomial? We take the discriminant! Specifically, we do it for an affinization, and we get a homogeneous polynomial of degree <math_exp> in the <math_exp>. All that's left is to factor the polynomial, and kill all the linear factors, just throw them away, the reasons are explained in more computational detail on my blogpost, and there I also do explicit examples, but the method for calculating the equation of the dual curve is as above. \"}\n",
      "831 {'Id': '831', 'Type': 'answer', 'ParentId': '815', 'urls': ['https://upload.wikimedia.org/wikipedia/commons/3/37/Mandel_zoom_14_satellite_julia_island.jpg'], 'exp': [], 'Body': 'Although he provided many valuable contributions to the field, I am most in love with his work on Fractals. I find math to be quite beautiful, and the Mandelbrot Set (magnified portion shown below) is a perfect example:  '}\n",
      "832 {'Id': '832', 'Type': 'answer', 'ParentId': '540', 'urls': ['https://docs.google.com/viewer?url=http://www.math.uconn.edu/~kconrad/blurbs/analysis/entropypost.pdf', 'http://en.wikipedia.org/wiki/Primon_gas'], 'exp': ['p', 'q', 'p', 'k', '(1 - r_p) r_p^k', 'r_p', 'n = p_1^{e_1} ... p_k^{e_k}', '\\\\displaystyle C \\\\prod_{i=1}^{k} r_p^{e_i}', 'C = \\\\prod_p (1 - r_p)', 'r_p', 'n', 'n', 'r_p = p^{-s}', 's &gt; 1', 'C', 'n', '\\\\frac{ \\\\frac{1}{n^s} }{ \\\\zeta(s)}', '\\\\zeta(s)', '\\\\zeta(s)', 's', '1', 's', '1', 's \\\\to 1', 's \\\\to 1', 'k', 'p', '(1 - p^{-1}) p^{-k}', '\\\\bmod p^n', 'k &lt; n', 'p'], 'Body': 'Suppose you want to put a probability distribution on the natural numbers for the purpose of doing number theory.  What properties might you want such a distribution to have?  Well, if you\\'re doing number theory then you want to think of the prime numbers as acting \"independently\": knowing that a number is divisible by <math_exp> should give you no information about whether it\\'s divisible by <math_exp>. That quickly leads you to the following realization: you should choose the exponent of each prime in the prime factorization independently.  So how should you choose these?  It turns out that the probability distribution on the non-negative integers with maximum entropy and a given mean is a geometric distribution, as explained for example by Keith Conrad here.  So let\\'s take the probability that the exponent of <math_exp> is <math_exp> to be equal to <math_exp> for some constant <math_exp>. This gives the probability that a positive integer <math_exp> occurs as <math_exp> where <math_exp>.  So we need to choose <math_exp> such that this product converges.  Now, we\\'d like the probability that <math_exp> occurs to be monotonically decreasing as a function of <math_exp>.  It turns out (and this is a nice exercise) that this is true if and only if <math_exp> for some <math_exp> (since <math_exp> has to converge), which gives the probability that <math_exp> occurs as <math_exp> where <math_exp> is the zeta function. One way of thinking about this argument is that <math_exp> is the partition function of a statistical-mechanical system called the Riemann gas.  As <math_exp> gets closer to <math_exp>, the temperature of this system increases until it would require infinite energy to make <math_exp> equal to <math_exp>.  But this limit is extremely important to understand: it is the limit in which the probability distribution above gets closer and closer to uniform.  So it\\'s not surprising that you can deduce statistical information about the primes by studying the behavior as <math_exp> of this distribution. Let me mention two other reasons to care about the limit as <math_exp> of the above distribution.  First, the basic reason to think of the primes as acting independently is the Chinese Remainder Theorem.  Second, a natural reason to look at a distribution where the probability that a number has exactly <math_exp> factors of <math_exp> is <math_exp> is that this is precisely the distribution you get on the residues <math_exp> for <math_exp>.  In fact, I believe this can be upgraded to the corresponding statement about Haar measure on the <math_exp>-adic integers. '}\n",
      "833 {'Id': '833', 'Type': 'answer', 'ParentId': '519', 'urls': ['http://en.wikipedia.org/wiki/Coxeter_group#Finite_Coxeter_groups'], 'exp': ['\\\\langle x, y, z | x^2 = y^2 = z^2 = (xy)^2 = (yz)^3 = (zx)^7 = 1 \\\\rangle', '2', '3'], 'Body': 'Your group is the alternating subgroup of a Coxeter group <math_exp> which is not on the (well-understood) list of finite Coxeter groups, and the alternating subgroup always has index <math_exp>.  (The connection to the hyperbolic plane is that Coxeter groups of rank <math_exp> always act as symmetries of a tiling of either the sphere, the Euclidean plane, or the hyperbolic plane by triangles.) '}\n",
      "834 {'Id': '834', 'Type': 'answer', 'ParentId': '815', 'urls': ['http://en.wikipedia.org/wiki/List_of_topics_named_after_Leonhard_Euler', 'http://www.math.dartmouth.edu/~euler/tour/tour_08.html'], 'exp': [], 'Body': 'He made important discoveries in pretty much every mathematical field there was at his time. He discovered graph theory. He is responsible for much of the current mathematical notation we use today, including Σ, i, e, f(x), π, and sin/cos.  is named after him His combined works fill 80 (!) volumes. And last but not least, <img src=\"https://upload.wikimedia.org/math/c/6/6/c669a6c5e0faf3a8ba0befed0f517ae5.png\" alt=\"(Euler&#39;s formula)\"> '}\n",
      "835 {'Id': '835', 'Type': 'question', 'Title': 'Optimal Strategy for Deal or No Deal', 'Tags': ['probability', 'recreational-mathematics'], 'urls': [], 'exp': [], 'Body': 'When I have watched Deal or No Deal (I try not to make a habit of it) I always do little sums in my head to work out if the banker is offering a good deal. Where odds drop below \"evens\" it\\'s easy to see it\\'s a bad deal, but what would be the correct mathematical way to decide if you\\'re getting a good deal? '}\n",
      "836 {'Id': '836', 'Type': 'answer', 'ParentId': '806', 'urls': [], 'exp': ['L/K', 'G(L/K)', 'K', 'L'], 'Body': 'Well, class field theory states that the class number is the degree of the largest everywhere-unramified abelian extension of a number field (namely, the Hilbert class field).  But class field theory really says a lot more: it says that there\\'s an isomorphism between the Galois group and the ideal class group. And in general, for any abelian extension <math_exp>, there\\'s an isomorphism between <math_exp> and a certain \"generalized ideal class group\" of <math_exp> where you quotient by ideals that are norms from <math_exp>.  This can be stated somewhat more elegantly using ideles. But, you asked about the class number. In class field theory, it\\'s difficult (to my knowledge) to prove directly that the Artin reciprocity map defined from ideals to the Galois group is an isomorphism. You can show that it is surjective and injective, however; to do this, you have to estimate the order of these generalized ideal class groups, which you can do using either analytic (L-function) methods or algebraic (e.g., using the Herbrand quotient) methods. So this is where the size and seemingly \"decategorified\" properties (like the order) become more important than the groups themselves: the proofs. '}\n",
      "837 {'Id': '837', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': [], 'Body': 'The rational numbers are both a continuum (between any two rationals you can find another rational) and countable (they can be lined up in correspondence with the positive integers). Mathematician missed it for hundreds (thousands?) of years, until Cantor. Of course, the proof of that works both ways, and is equally surprising the other way -- there is a way to order the integers (or any countable set) that makes it into a continuum. '}\n",
      "838 {'Id': '838', 'Type': 'answer', 'ParentId': '640', 'urls': [], 'exp': ['\\\\mathbb R^2', '2', '1'], 'Body': 'On an intuition level: Complex numbers are somewhat equivalent to <math_exp> (homeomorphic to a <math_exp>-dimensional real space), on the other hand they are also a <math_exp>-dimensional complex space. This gives them an extra structure with consequences like this theorem. Another view on this is that differentiable complex functions must also satisfy the Cauchy-Riemann equations and this extra hypothesis is what makes them holomorphic. '}\n",
      "839 {'Id': '839', 'Type': 'answer', 'ParentId': '835', 'urls': ['http://en.wikipedia.org/wiki/Utility', 'http://en.wikipedia.org/wiki/Deal_or_No_Deal'], 'exp': [], 'Body': \"In maths, the expected value is the average of how much you'd win if you used the same strategy from the same position a large (approaching infinity) number of times. We should first note that expected value alone doesn't decide the best option. We also have to take into account risk. Most people (other than gamblers) would prefer a certain dollar rather than a fifty percent chance of two and fifty percent of nothing. To deal with this, we typically define utility instead. Utility varies between individuals and is determined by their risk profile. Since the dollar amount are quite large for contestants, it would be logical (in theory) to find someone who can afford to take the risk to insure you. This would allow you to receive the gains from a risker strategy. For deal or no deal, they encourage you to play by making their offers worse than the expected value at the start of the game. Later, (according to Wikipedia) the offers may even exceed the expected value. Without knowing how exactly the offers are calculated (or forming a model), we can't answer this question accurately, but only make general statements. If we ignore risk and offers being greater than expected utility, you'd always want to go as far as possible in the game. But, as is, the game is extremely difficult to analyse mathematically. \"}\n",
      "840 {'Id': '840', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': ['n'], 'Body': \"Suppose you have a large collection of books, all of the same size.  Balance one of them on the edge of a table so that one end of the book is as far from the table as possible.  Balance another book on top of that one, and again try to get as far from the table as possible.  Take <math_exp> of them and try to balance them on top of each other so that the top book is as far as possible away from the edge of the table horizontally. Theorem:  With enough books, you can get arbitrarily far from the table.  If you are really careful.  This is a consequence of the divergence of the harmonic series.  I think if you haven't heard this one before it's very hard to tell whether it's true or false. \"}\n",
      "841 {'Id': '841', 'Type': 'question', 'Title': 'Integral via complex analysis. Integral via hypercomplex analysis', 'Tags': ['integration', 'complex-analysis', 'analysis'], 'urls': [], 'exp': [], 'Body': 'If I remember rightly there are some integrals of real functions which are easier to compute by using complex analysis. Is this because of properties of the particular function or because of a lack of a known real analysis technique? Are there functions which would require hypercomplex analysis to integrate? '}\n",
      "842 {'Id': '842', 'Type': 'question', 'Title': 'Why is compactness in logic called compactness?', 'Tags': ['general-topology', 'logic', 'terminology', 'compactness'], 'AcceptedAnswerId': '864', 'urls': [], 'exp': [], 'Body': \"In logic, a semantics is said to be compact iff if every finite subset of a set of sentences has a model, then so to does the entire set. Most logic texts either don't explain the terminology, or allude to the topological property of compactness. I see an analogy as, given a topological space X and a subset of it S, S is compact iff for every open cover of S, there is a finite subcover of S. But, it doesn't seem strong enough to justify the terminology. Is there more to the choice of the terminology in logic than this analogy? \"}\n",
      "843 {'Id': '843', 'Type': 'answer', 'ParentId': '841', 'urls': ['http://en.wikipedia.org/wiki/Residue_theorem'], 'exp': ['e^{-x^2}', '\\\\sqrt{\\\\pi}', '-R', 'R', 'R', '-R', 'R', '\\\\infty'], 'Body': \"I'm pretty sure you mean the evaluation of definite integrals. For instance, <math_exp> has no elementary antiderivative, but it's definite integral over the real line can be computed explicitly (it's <math_exp>). You ask what complex analysis has to do with this. Well, the idea is that complex integrals around closed curves (of holomorphic functions) are usually comparatively easy to evaluate: it's because we have the residue theorem as a tool. However, these real integrals are usually over the whole real line, which doesn't satisfy the hypothesis of a path under the residue theorem. For instance, it's not a finite closed path.  However, we can replace the real number by a large semicircle (to pick one common example) that goes from <math_exp> to <math_exp>, then around the upper-half-plane from <math_exp> to <math_exp>. This is a closed path and the residue theorem can apply to this. When you let <math_exp> to <math_exp>, the integral around semicircular path often tends to zero (by direct bounding arguments). So what you're left with is the integral along the real line, and it's equal to the residues. Examples of this will be in any complex analysis textbook, e.g. Ahlfors's.  The Wikipedia article on the residue theorem (link above) also has examples. \"}\n",
      "844 {'Id': '844', 'Type': 'answer', 'ParentId': '842', 'urls': [], 'exp': [], 'Body': 'This isn\\'t a complete answer, partly because I\\'ve had discussions with other grad students and we weren\\'t able to work it out satisfactorily, but I\\'ve been told that you can actually put a topology on logical statements such that the compactness theorem translates to \"The set of true statements is a compact subset of the set of all statements\" or something similar.  But as I said, a few of my friends and I weren\\'t able to work out the topology. '}\n",
      "845 {'Id': '845', 'Type': 'answer', 'ParentId': '842', 'urls': [], 'exp': [], 'Body': \"As far as I know, the link comes from the syntactic theory. You are given a set of symbols of sentence F = {f_i} and you are allowed allowed to form complex statements using them. You can combine the elementary statements with AND, OR, NOT operators and parentheses, in the usual way. So you get a set X of composed sentences, like (f AND g) OR (NOT h) or something like that. A syntactic version of the compactness theorem states the following. Assume that for every finite subset Y of X you can assign truth values to the f_i in such a way that all sentences in Y are true. Than you can do the same for X. Proof Consider the topological space A obtained by taking the product of {0, 1} over the set F. The topology on A is the product topology. By Tychonoff's theorem, A is compact. For every composed statement s, the set of truth values which make s true is a finite intersection of cylinders, hence it is a closed set of A. The hypothesis say that every finite intersection of such closed sets, for s ranging in X, is not empty. Hence the intersection of all such closed set is not empty, which means one can make all statements in X true. \"}\n",
      "846 {'Id': '846', 'Type': 'answer', 'ParentId': '842', 'urls': [], 'exp': ['p_i ', '2^{\\\\mathbb{N}}', 'S_t', 'F', '2^{\\\\mathbb{N}}', 'S_t', 't \\\\in F', 'V_F', 'F', 'V_F'], 'Body': \"The analogy for the compactness theorem for propositional calculus is as follows. Let <math_exp> be propositional variables; together, they take values in the product space <math_exp>.  Suppose we have a collection of statements <math_exp> in these boolean variables such that every finite subset is satisfiable. Then I claim that we can prove that they are all simultaneously satisfiable by using a compactness argument. Let <math_exp> be a finite set. Then the set of all truth assignments (this is a subset of <math_exp>) which satisfy <math_exp> for <math_exp> is a closed set <math_exp> of assignments satisfying the sentences in <math_exp>.  The intersection of any finitely many of the <math_exp> is nonempty, so by the finite intersection property, the intersection of all of them is nonempty (since the product space is compact), whence any truth in this intersection satisfies all the statements. I don't know how this works in predicate logic. \"}\n",
      "847 {'Id': '847', 'Type': 'question', 'Title': 'Law of cosines with impossible triangles', 'Tags': ['geometry', 'trigonometry'], 'AcceptedAnswerId': '1570', 'urls': [], 'exp': [' \\\\cos(\\\\textrm{angle between }a\\\\textrm{ and }b) = \\\\frac{a^2 + b^2 - c^2}{2ab} ', '&lt; -1', 'c &gt; a+b', '&gt; 1', 'c &lt; \\\\left|a-b\\\\right|', 'a = 3', 'b = 4', 'c = 8', '\\\\cos(\\\\textrm{angle }ab) = -39/24', 'a = 3', 'b = 5', 'c = 1', '\\\\cos(\\\\textrm{angle }ab) = 33/30'], 'Body': 'Is there any mathematical significance to the fact that the law of cosines... <math_exp> ... for an impossible triangle yields a cosine <math_exp> (when <math_exp>), or <math_exp> (when <math_exp>) For example, <math_exp>, <math_exp>, <math_exp> yields <math_exp>. Or <math_exp>, <math_exp>, <math_exp> yields <math_exp>. Something to do with hyperbolic geometry/cosines? '}\n",
      "848 {'Id': '848', 'Type': 'answer', 'ParentId': '311', 'urls': [], 'exp': [], 'Body': 'A further sense in which Higher Order Logics with standard or saturated semantics (HOL, hereafter) are less well-behaved than First Order Logic (FOL, hereafter) is a direct consequences of the failure of Completeness (and thus, as explained in other answers, of Compactness). The set of logical truths and the set of correct claims of semantic consequence for these logics are not recursively enumerable. FOL is Complete, yet not Decidable. So, we determine of an arbitrary sentences and sets of sentences of the language of FOL if those sentences are logical truths or if a set has a given sentence as a consequence. But, since FOL is Complete and proofs are finitely long, we can (in the mathematicians sense of \"can\") enumerate the proofs and inspect one by one, checking what sentence the proof shows as a theorem or what sentence the proof derives from what set of assumptions. This gets us a recursive enumeration of the truths and the sentence/set pairs that stand in the consequence relation. (This does not contradict the failure of decidability as we cannot conclude that since we\\'ve yet to come across a proof in out enumeration, there isn\\'t one if only we kept looking.) Since HOL\\'s are not Complete, this means of showing them recursively enumerable is not available. Indeed, there can be no means; were there such a means, it could be exploited to induce a Complete proof system, and there cannot be such a Complete proof system as the HOL\\'s are not Compact. '}\n",
      "849 {'Id': '849', 'Type': 'answer', 'ParentId': '841', 'urls': [], 'exp': ['\\\\mathbb{R}', '\\\\mathbb{C}', '\\\\mathbb{R}'], 'Body': \"A generic real-valued function carries no additional information other than its values.  But many of the common functions encountered in analysis are just the restriction to <math_exp> of holomorphic functions, and these functions carry a lot of additional information, namely, their values in <math_exp>.  Unlike real-analytic functions, holomorphic functions are extremely rigid: for example, if two holomorphic functions agree on a set of complex numbers with a limit point, they must be equal everywhere.  That means it's possible to deduce information about how a holomorphic function behaves on part of its domain (say, <math_exp>) from information about how it behaves on other parts of its domain, since the two have to determine each other.  This information is extracted using contour integrals. \"}\n",
      "851 {'Id': '851', 'Type': 'answer', 'ParentId': '847', 'urls': ['http://en.wikipedia.org/wiki/Minkowski_space', 'http://en.wikipedia.org/wiki/Hyperboloid_model'], 'exp': ['\\\\langle a, b \\\\rangle^2 \\\\le ||a||^2 ||b||^2', '1', '-1'], 'Body': 'One can prove the triangle inequality in any abstract inner product space, such as a Hilbert space; it is a consequence of the Cauchy-Schwarz inequality <math_exp>.  In order for the triangle inequality to fail, the Cauchy-Schwarz inequality has to fail, and in order for Cauchy-Schwarz to fail (which corresponds to the \"cosine\" no longer being between <math_exp> and <math_exp>), some axiom of an abstract inner product space has to be given up.  One choice is to give up positive-definiteness; in other words, lengths of vectors are no longer always non-negative.  This leads to geometries like Minkowski spacetime which are relevant to relativity.  In Minkowski spacetime, there is a reverse triangle inequality instead. Edit:  I should also mention that the \"unit sphere\" in Minkowski spacetime is a model for hyperbolic geometry called the hyperboloid model. '}\n",
      "852 {'Id': '852', 'Type': 'answer', 'ParentId': '847', 'urls': ['http://en.wikipedia.org/wiki/Hyperbolic_law_of_cosines'], 'exp': [], 'Body': \"For some a, b, and c that form a triangle: increasing the length of c increases the measure of angle C and as m&ang;C approaches 180&deg;, cos C approaches -1; decreasing the length of c increases the measure of angle C and as m&ang;C approaches 0&deg;, cos C approaches 1.  Extending this pattern, it makes sense that if c > a + b, c has gotten bigger past making a triangle with a and b, so cos C &lt; -1, and if c &lt; |a-b|, c has gotten smaller past making a triangle with a and b, so cos C > 1. In hyperbolic geometry, the definition of lines (and hence triangles) is different and the sum of the measures of the angles in a triangle is less than 180&deg;.  There is a Hyperbolic Law of Cosines, but it's not quite the same. I don't think there's a sensible way to relate hyperbolic cosine to the Law of Cosines in Euclidean geometry. \"}\n",
      "853 {'Id': '853', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': [], 'Body': 'Position-based Cryptography. This is a fun example since it seems very \"out of left field\". The setup: Three servers are positioned in known locations on the globe (their positions can be arbitrary, provided they aren\\'t on top of each other). A single computer wants to prove its location to the servers. In other words, if the computer is actually located where it claims, then the protocol will accept. However, if the computer is located anywhere else, then the protocol will reject, no matter how the computer cheats (it is even allowed to recruit friends to help it cheat). All communication is subject to the laws of physics -- information travels at speed c, and quantum mechanics holds. Theorem 1: This is impossible if all communication is classical. Cheating is always possible. Theorem 2: This is possible if quantum communication is possible. '}\n",
      "854 {'Id': '854', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': [], 'Body': 'Goldbach\\'s Conjecture. Granted this is open, so it may be cheating a bit. However, this seems like a very hard problem to intuit your way to the \"conventional wisdom\". By contrast, P != NP is \"obviously\" true. Indeed, Feynman had trouble believing P != NP was even open. '}\n",
      "855 {'Id': '855', 'Type': 'question', 'Title': 'For any prime <span class=\"math-container\" id=\"8692\">p > 3</span>, why is <span class=\"math-container\" id=\"8693\">p^2-1</span> always divisible by 24?', 'Tags': ['elementary-number-theory', 'prime-numbers', 'divisibility'], 'AcceptedAnswerId': '860', 'urls': [], 'exp': ['$p&gt;3$', '$24 \\\\mid p^2-1$'], 'Body': 'Let <math_exp> be a prime. Prove that <math_exp>. I know this is very basic and old hat to many, but I love this question and I am interested in seeing whether there are any proofs beyond the two I already know. '}\n",
      "856 {'Id': '856', 'Type': 'answer', 'ParentId': '855', 'urls': [], 'exp': ['$p^2-1  = (p+1)(p-1)$', '$p$', '1', '$2 \\\\pmod 3$', '3', '$p$', '1', '3 \\\\mod 4', '$2|(p+1)$', '$4|(p-1)$', '$2|(p-1)$', '$4|(p+1)$', '$8\\\\times3= 24$'], 'Body': '<math_exp>. <math_exp> must be either <math_exp> or <math_exp>, so we have a factor of <math_exp> in the product. And <math_exp> is also either <math_exp> or <math_exp>. Hence either <math_exp> and <math_exp> or <math_exp> and <math_exp>. Thus <math_exp> divides the product. '}\n",
      "857 {'Id': '857', 'Type': 'answer', 'ParentId': '855', 'urls': [], 'exp': ['p', 'p^2', '1', '8', '8', 'p^2-1', 'p', 'p-1', 'p+1', '3', 'p^2-1', 'p^2 -1 '], 'Body': '<math_exp> must be congruent either to 1,3,5,7 modulo 8.  Then <math_exp> is congruent to <math_exp> modulo <math_exp> in either case.  So <math_exp> divides <math_exp>. Now, <math_exp> is not a multiple of 3, so either <math_exp> or <math_exp> is a multiple of three. So <math_exp> divides <math_exp>. Together, it follows that 24 divides <math_exp>. '}\n",
      "858 {'Id': '858', 'Type': 'answer', 'ParentId': '742', 'urls': [], 'exp': [], 'Body': 'A similar problem is known in mathematics.  It is called the \"Postage-Stamp\" problem, and usually asks which postal values can be realized and which cannot.  Dynamic programming is a common but not polynomial-time solution for this.  A polynomial time solution using continued fractions in the case of two stamp denominations exists, and more complex algorithms exist for three or more stamp denominations.  Look up \"Frobenius Number\" for more technical information. For the given problem, I would roughly estimate the Frobenius number, and based on the estimate make an easy guess as to a near-solution and then use a dynamic programming/tabular solution, depending on the amount of time I had to solve the problem. '}\n",
      "859 {'Id': '859', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://sbseminar.wordpress.com/'], 'exp': [], 'Body': 'Secret Blogging Seminar '}\n",
      "860 {'Id': '860', 'Type': 'answer', 'ParentId': '855', 'urls': [], 'exp': ['$p - 1$', 'p', 'p + 1', '3', '2', '4', '3 · 2 · 4 = 24', 'p'], 'Body': \"The most elementary proof I can think of, without explicitly mentioning any number theory: out of the three consecutive numbers <math_exp>, <math_exp>, <math_exp>, one of them must be divisible by <math_exp>; also, since the neighbours of p are consecutive even numbers, one of them must be divisible by <math_exp> and the other by <math_exp>, so their product is divisible by <math_exp> — and of course, we can throw <math_exp> out since it's prime, and those factors cannot come from it. \"}\n",
      "861 {'Id': '861', 'Type': 'question', 'Title': 'Cotangent bundle', 'Tags': ['geometry', 'general-topology'], 'AcceptedAnswerId': '865', 'urls': [], 'exp': [], 'Body': 'This may be a poorly phrased question - please let me know of it - but what is the correct way to think of the cotangent bundle? It seems odd to think of it as the dual of the tangent bundle (I am finding it odd to reconcile the notions of \"maps to the ground field\" with this object). '}\n",
      "862 {'Id': '862', 'Type': 'answer', 'ParentId': '842', 'urls': [], 'exp': [], 'Body': 'A motto which is related (and sometimes true) is \"proofs are finite\".  In most systems of logic under consideration, the statement and a proof of the statement are finite strings of symbols.  One can think of them as \"compactly\" represented.  How nice then that certain systems will always allow a proof to be found if there is one.  While this does not give a tight connection to topology, it suggests (and you need to work this through on your own to be convinced) that certain infinite abnormalities (like an infinite proof) will not occur.  A similar topological claim is that, for compact sets, an infinite inclusive chain of certain subsets will have nonempty intersection, such a claim easily seen to be false for some non-compact sets, such as the collection { C_n | C_n = {x | x >= n and x is a real number } for n a non-negative integer } . '}\n",
      "863 {'Id': '863', 'Type': 'answer', 'ParentId': '781', 'urls': [], 'exp': ['1000 to \\\\', '8000 to \\\\', '1000 to \\\\', '2000 to \\\\', '4000 to \\\\'], 'Body': \"As a rough/somewhat-intuitive explanation of why Benford's Law makes sense, consider it with respect to amounts of money.  The amount of time(/effort/work) needed to get from \\\\<math_exp>2000 (100% increase) is a lot greater than the amount of time needed to get from \\\\<math_exp>9000 (12.5% increase)--increasing money is usually done in proportion to the money one has.  Thought about in the other direction, it should take a fixed amount of time to, say, double one's money, so going from \\\\<math_exp>2000 takes as long as from \\\\<math_exp>4000 and \\\\<math_exp>8000, so the leading digit spends more time at lower values than at higher values.  Because the value growth is exponential, the time spent at each leading digit is roughly logarithmic. \"}\n",
      "864 {'Id': '864', 'Type': 'answer', 'ParentId': '842', 'urls': ['http://en.wikipedia.org/wiki/Stone%27s_representation_theorem_for_Boolean_algebras', 'http://en.wikipedia.org/wiki/Lindenbaum%E2%80%93Tarski_algebra', 'http://en.wikipedia.org/wiki/Type_%28model_theory%29'], 'exp': [], 'Body': \"The Compactness Theorem is equivalent to the compactness of the Stone space of the Lindenbaum&ndash;Tarski algebra of the first-order language L. (This is also the space of 0-types over the empty theory.) A point in the Stone space SL is a complete theory T in the language L. That is, T is a set of sentences of L which is closed under logical deduction and contains exactly one of &sigma; or &not;&sigma; for every sentence &sigma; of the language. The topology on the set of types has for basis the open sets U(&sigma;) = {T: &sigma; &isin; T} for every sentence &sigma; of L. Note that these are all clopen sets since U(&not;&sigma;) is complementary to U(&sigma;). To see how the Compactness Theorem implies the compactness of SL, suppose the basic open sets U(&sigma;i), i &isin; I, form a cover of SL. This means that every complete theory T contains at least one of the sentences &sigma;i. I claim that this cover has a finite subcover. If not, then the set {&not;&sigma;i: i &isin; I} is finitely consistent. By the Compactness Theorem, the set consistent and hence (by Zorn's Lemma) is contained in a maximally consistent set T. This theory T is a point of the Stone space which is not contained in any U(&sigma;i), which contradicts our hypothesis that the U(&sigma;i), i &isin; I, form a cover of the space. To see how the compactness of SL implies the Compactness Theorem, suppose that {&sigma;i: i &isin; I} is an inconsistent set of sentences in L. Then U(&not;&sigma;i), i &isin; I, forms a cover of SL. This cover has a finite subcover, which corresponds to a finite inconsistent subset of {&sigma;i: i &isin; I}. Therefore, every inconsistent set has a finite inconsistent subset, which is the contrapositive of the Compactness Theorem. \"}\n",
      "865 {'Id': '865', 'Type': 'answer', 'ParentId': '861', 'urls': [], 'exp': ['T^*M'], 'Body': 'One fruitful way to think about it, if you have physics background, is as phase space.  Your manifold is the configuration space for some system of particles, and the cotangent bundle is then the phases, so the cotangent directions are velocities.  This is helpful also with the symplectic structure on <math_exp>. '}\n",
      "866 {'Id': '866', 'Type': 'answer', 'ParentId': '815', 'urls': ['http://en.wikipedia.org/wiki/Nicolas_Bourbaki', 'http://rads.stackoverflow.com/amzn/click/3764326506', 'http://en.wikipedia.org/wiki/Jean_Dieudonn%C3%A9', 'http://www.fermentmagazine.org/home5.html'], 'exp': [], 'Body': ' The story had me from the moment I realized it is true. And as I come to understand more mathematics, I find that I frequently ask myself \"What would Bourbaki do?\" Also, the individuals have done much to expose the human side of mathematics. '}\n",
      "867 {'Id': '867', 'Type': 'answer', 'ParentId': '795', 'urls': [], 'exp': ['\\\\sum\\\\limits_{i=1}^{\\\\infty} c_n', '\\\\mathbb{C}', 'A', 'B', 'a,b', 'A', 'a', 'B', 'A', 'b', 'ta + (1 - t)b', 't \\\\in \\\\mathbb{R}', 'c', 'a', 'b', 'a', 'b', 'c', 'd', 'd', 'a', 'b', '\\\\mathbb{C}'], 'Body': 'More generally, if <math_exp> is a conditionally convergent series of complex numbers, then the collection of sums of all convergent rearrangements is an affine subspace of the plane <math_exp>. Here\\'s a sketch of why this is true. Suppose <math_exp> and <math_exp> are two rearrangements that converge to the distinct complex numbers <math_exp>, respectively. Focus on a partial sum of <math_exp> that\\'s pretty close to <math_exp>. A sufficiently long partial sum of <math_exp> will include every term of the former partial sum of <math_exp>, but it will also tend to <math_exp> as you take more and more terms. It\\'s \"clear\" then that it\\'s possible to obtain any combination <math_exp> for <math_exp>. If in addition there is a point <math_exp> which is the sum of some rearrangement but is not on the line through <math_exp> and <math_exp>, then the previous result can be reapplied, replacing the numbers <math_exp> and <math_exp> with the numbers <math_exp> and <math_exp>, where <math_exp> is any point on the line through <math_exp> and <math_exp>. It follows that in this case the entire plane <math_exp> is obtained by rearranging the original series. '}\n",
      "868 {'Id': '868', 'Type': 'question', 'Title': 'What is the best way to factor arbitrary polynomials?', 'Tags': ['polynomials', 'algorithms', 'roots', 'factoring', 'symbolic-computation'], 'AcceptedAnswerId': '904', 'urls': [], 'exp': [], 'Body': 'I am currently working on a Computer Algebra System and was wondering for suggestions on methods of finding roots/factors of polynomials. I am currently using the Numerical Durand-Kerner method but was wondering if there are any good non-numerical methods (primarily for simplifying fractions etc). Ideally this should work for equations in multiple variables. '}\n",
      "869 {'Id': '869', 'Type': 'answer', 'ParentId': '861', 'urls': ['https://mathoverflow.net/questions/17325/why-is-cotangent-more-canonical-than-tangent'], 'exp': [], 'Body': \"You might be interested in this MathOverflow post: https://mathoverflow.net/questions/17325/why-is-cotangent-more-canonical-than-tangent (Sorry, I'd leave this as a comment but I just joined this site and don't have enough reputation.) \"}\n",
      "870 {'Id': '870', 'Type': 'answer', 'ParentId': '868', 'urls': [], 'exp': [], 'Body': 'If you\\'re looking to factor exactly, then you\\'ll need to use something that\\'s not one of the fundamental operations of addition, subtraction, multiplication, division and extraction of roots.  The Abel-Ruffini theorem says so for degree five and above.  However, there are numerous other methods to find roots exactly, using more general functions, my favorite being theta functions, as explained in the appendix to Mumford\\'s \"Tata Lectures on Theta II\" '}\n",
      "871 {'Id': '871', 'Type': 'answer', 'ParentId': '392', 'urls': [], 'exp': [], 'Body': \"If you look carefully and geometrically at the quotient limit that defines sin'(x) in the unit circle, and take the chord and tangent as approximations to the arc (that is the angle; this is the essence of sin(x)/x approaches 1), you will see that limit of the derivative quotient tends exactly to cos(x), that is, it's adjacent/hypotenuse. In other words, it's built into right triangle geometry, like so many phenomena in mathematics. Also, in that geometry, you'll see lurking the proof for the sin(x+y) formula, which, along with the limit of sin(x)/x, is how the standard proof that sin'(x) = cos(x) goes.  But skipping that algebra and going directly to the geometry is the most straightforward way I know to answer the question. Sorry I don't have time or tools to draw the pictures. I suspect this saying the same thing as the physics answer above, but perhaps more directly.  I do think all the answers referring to series expansions miss the point. \"}\n",
      "872 {'Id': '872', 'Type': 'question', 'Title': 'What is the relationship between the Hodge dual of p-vectors and the dual space of an ordinary vector space?', 'Tags': ['linear-algebra', 'exterior-algebra'], 'AcceptedAnswerId': '881', 'urls': [], 'exp': ['\\\\mathbb{R}^3', 'a \\\\wedge b'], 'Body': \"I understand what the Hodge dual is, but I can't quite wrap my head around the dual space of vector space. They seem very similar, almost the same, but perhaps they are unrelated. For instance, in <math_exp>, the blade <math_exp> gives you a subspace that's like a plane, and the dual is roughly the normal to the plane. Is there a similarly simple example for the dual space of a vector space, or is there a way to describe the vector space dual in terms of the Hodge dual? \"}\n",
      "873 {'Id': '873', 'Type': 'answer', 'ParentId': '806', 'urls': [], 'exp': [], 'Body': \"Frequently in studying diophantine equations, or related problems, one is forced to look at rings of integers of algebraic number fields, and it may be that the class number of the ring your are forced to deal with is > 1.  In this case, there is nothing you can do to change that fact; you have to live with it. As a response, it is natural to try and develop a theory of the class number and the class group, as a means of finding ways to deal with the failure of unique factorization.  Kummer was the one who invented the notion of class number, and he did in the course of his work on Fermat's Last Theorem; this is what is described in Andrea Ferretti's answer, and is an excellent example of what I am talking about. Another example, quite different in nature, occurs in the proof of Dirichlet's theorem on primes in arithmetic progressions.    This is the theorem that if a and d are coprime natural numbers, then there are infinitely many primes p such that p is congruent to a mod d. In his proof, he reduces everything to showing that a certain number (the value at 1 of a certain so-called L-function) is non-zero.  He then is able to give a precise formula for this L-function, and shows that it is equal to some non-zero constants times a certain class number.  Since the class number is positive (and so in particular, is non-zero!), he is able to complete his proof. \"}\n",
      "874 {'Id': '874', 'Type': 'question', 'Title': 'How to get an equation that output the end point of an angle line in rectangle?', 'Tags': ['geometry', 'algebra-precalculus'], 'urls': [], 'exp': ['p = (x,y)', 'p2', 'p2', 'x, y, w, h', '(0,0)'], 'Body': 'When drawing an angle line (45 degrees) in a rectangle from a general point <math_exp> that located on the right or the top line of the rectangle. How can I find the intersection point <math_exp> of this line with the rectangle? In other words, I want to write the target point, <math_exp>, with my current information: <math_exp>. (This variables are described in the picture below). The point <math_exp> is in the top-right corner. <img src=\"https://i.stack.imgur.com/vpcRR.jpg\" alt=\"rect\"> '}\n",
      "875 {'Id': '875', 'Type': 'answer', 'ParentId': '574', 'urls': [], 'exp': [], 'Body': 'Generic has different meanings in different branches of geometry.  In algebraic geometry, it usually means that the property in question holds on a Zariski dense open set.  In other geometric contexts, it could also that the property holds on a dense open set (in whatever is the natural topology under consideration), but can also mean that it holds on a countable intersection of such sets.  (The reason for considering such intersections as being big is motivated by the Baire category theorem, which says that in reasonable contexts the complement of such a set will be very \"thin\", in some sense.) '}\n",
      "876 {'Id': '876', 'Type': 'answer', 'ParentId': '735', 'urls': [], 'exp': [], 'Body': \"Hmm, apparently the other answerers' algebra classes were a lot more intense than mine...My answer's based on what I would have been comfortable with after basic algebra and geometry. Basically, if you think graphically, instantaneous velocity is the slope of a line at a single point rather than over an interval. At least that's what you want. So, if you happen to be going at a constant velocity (any straight line on a position graph), you can just use the slope formula. Where calculus comes in is if you're dealing with an inconstant velocity (a curved line). If that's not making sense right off, just try to realistically graph the movement of a car as it accelerates/decelerates, where the y-axis is position and the x-axis is time. Using algebra, you can't take the slope of a curved line. What you can do is take the slope between two points on the curve. The closer these points get to each other, the closer the slope between them will approximate the actual slope of the curve. So, if the slope between t=5.001 min. and t=5.01 min. is 40, then that approximates the actual slope, and instantaneous velocity, at t=5.0055 min. I don't think I can get much more specific/accurate without going into (pre-)calculus. \"}\n",
      "877 {'Id': '877', 'Type': 'answer', 'ParentId': '861', 'urls': [], 'exp': [], 'Body': 'I\\'m not completely sure what you mean by this: \"It seems odd to think of it as the dual of the tangent bundle (I am finding it odd to reconcile the notions of \"maps to the ground field\" with this object),\" but maybe the following will help you see why it is natural to consider the dual space of the tangent bundle. Given a function f on our manifold, we want to associate something like the gradient of f.  Well, in calculus, what characterized the gradient of a function?  Its the vector field such that when we take its dot product with a vector v at some point p, we get the directional derivative, at p, of f along v.  In a general manifold we don\\'t have a dot product (which is a metric) but we can form a covector field (something which gives an element of the cotangent bundle at any point) such that, when applied to a vector v, we get the directional derivative of f along v.  This covector field is denoted df and is called the exterior derivative of f. '}\n",
      "878 {'Id': '878', 'Type': 'answer', 'ParentId': '574', 'urls': ['http://en.wikipedia.org/wiki/Filter_%28mathematics%29#Filter_on_a_set', 'http://en.wikipedia.org/wiki/Baire_category_theorem', 'http://en.wikipedia.org/wiki/Sober_space'], 'exp': [], 'Body': 'The term generic usually applies to something which happens in an open dense set of some space. The idea is that open dense sets are large subsets of the space. Indeed, they are closed under finite intersections and thus form a base for a filter of subsets of the space. Sometimes, the term is applied more generally to something which happens in a countable intersection of open dense sets (a dense G&delta; set) of some space. Usually, this is in the context of a complete metric space or a locally compact Hausdorff space for which the Baire Category Theorem applies. The generic point is a (sometimes fictitious) point which lies in every open dense set of the space. Irreducible sober spaces always have a generic point, it is the unique point whose closure is the whole space. The only Hausdorff space with a generic point is the one-point space. Fictitious generic points have a variety of uses. Usually one means a point which lies in all open dense sets which are considered in the current argument, without specifying the open dense sets in question. This is fine because the intersection of finitely many (or even countably many in the case of Baire spaces) open dense sets is guaranteed to be nonempty. '}\n",
      "881 {'Id': '881', 'Type': 'answer', 'ParentId': '872', 'urls': ['http://www-users.math.umd.edu/~toni/hodge.pdf'], 'exp': ['V', 'V', 'V', 'n', '\\\\Lambda^2 V, \\\\Lambda^3 V, ... \\\\Lambda^n V', '\\\\Lambda^k V', '\\\\Lambda^{n-k} V', '\\\\Lambda^k V \\\\times \\\\Lambda^{n-k} V \\\\to \\\\Lambda^n V', 'V, W', 'V \\\\times W \\\\to F', 'F', 'V \\\\simeq W^{\\\\ast}', '\\\\Lambda^k V \\\\simeq \\\\Lambda^{n-k} V^{\\\\ast}', '\\\\Lambda^n V \\\\simeq F', '\\\\Lambda^n V', '\\\\Lambda^{n-k} V^{\\\\ast}', '\\\\Lambda^{n-k} V', '\\\\Lambda^n V', 'b_1, ... b_n', 'b_i', '\\\\Lambda^n V', '1', '-1', 'V', '\\\\Lambda^n V', '\\\\Lambda^k V \\\\simeq \\\\Lambda^{n-k} V^{\\\\ast}', '\\\\Lambda^{n-k} V^{\\\\ast} \\\\simeq \\\\Lambda^{n-k} V'], 'Body': '(Edit:  I have edited this answer several times because my understanding of the situation has been improving.) It is always profitable to understand these kind of constructions by understanding exactly what information they depend on.  The Hodge dual depends on a surprising amount of information: you need a vector space <math_exp> which is equipped with both an inner product and an orientation, which is essentially a choice of which bases of <math_exp> are \"right-handed.\"  So let\\'s see what we can say ignoring all this information first. Any abstract vector space <math_exp> of finite dimension <math_exp> has exterior powers <math_exp>, the last of which is one-dimensional.  The vector spaces <math_exp> and <math_exp> always have the same dimension, so we would like to be able to define some sort of \"canonical\" map between them.  What can we say?  Well, they are always dual: the wedge product defines a natural bilinear map <math_exp>, and since the latter is one-dimensional this means (once you\\'ve proven nondegeneracy) that the two vector spaces are in fact dual. But duality does not give you a map between them.  When two vector spaces <math_exp> are dual, meaning there is a nondegenerate bilinear map <math_exp> (where <math_exp> is the ground field), all you get is an isomorphism <math_exp>.  Here you get an isomorphism <math_exp>, once you have specified an isomorphism <math_exp>.  This is equivalent to picking out a distinguished vector in <math_exp>, which there is no way to do in general. So the answer is to introduce extra data.  To identify <math_exp> with <math_exp>, we need an inner product.  An inner product gives you two distinguished vectors in <math_exp>, as follows: take any orthonormal basis <math_exp>.  Then wedging together the <math_exp> in any order gets you one of two elements of <math_exp>, depending on whether the corresponding permutation is even or odd.  But without any extra data, there is no way to identify one of these elements with <math_exp> and one of these elements with <math_exp>. The extra data that does this is an orientation on <math_exp>, which tells you which bases are \"right-handed\" and which are \"left-handed.\"  So an oriented orthonormal basis gives you a distinguished element of <math_exp>, which gives you a distinguished isomorphism <math_exp>, which composed with the isomorphism <math_exp> is the Hodge dual. Phew. This is explained in these notes I just found on Google. '}\n",
      "882 {'Id': '882', 'Type': 'answer', 'ParentId': '835', 'urls': [], 'exp': [], 'Body': \"From what I hear about game-shows in general, if your performance does not make it to air, then you don't get anything.  Hence you cannot just accept the first amount offered (if it turns out to be a better choice) and expect to get it, since it won't make an interesting show. \"}\n",
      "886 {'Id': '886', 'Type': 'question', 'Title': 'Deal or no deal: does one switch (to avoid a goat)?/ Should deal or no deal be 10 minutes shorter?', 'Tags': ['probability', 'game-theory'], 'AcceptedAnswerId': '888', 'urls': ['https://math.stackexchange.com/questions/835/optimal-strategy-for-deal-or-no-deal', 'http://en.wikipedia.org/wiki/Deal_or_no_deal', 'http://en.wikipedia.org/wiki/Monty_Hall_problem'], 'exp': [], 'Body': \"Okay so this question reminded me of one my brother asked me a while back about the hit day-time novelty-worn-off-now snoozathon Deal or no deal. In playing deal or no deal, the player is presented with one of 22 boxes (randomly selected) each containing different sums of money, he then asks in turn for each of the 21 remaining boxes to be opened, occasionally receiving an offer (from a wholly unconvincing 'banker' figure) for the mystery amount in his box. If he rejects all of the offers along the way, the player is allowed to work his way through several (for some unfathomable reason, emotionally charged) box openings until there remain only two unopened boxes: one of which is his own, the other not. He is then given a choice to stick or switch (take the contents of his own box or the other), something he then agonises pointlessly over for the next 10 minutes. [If you have not seen the monty hall 'paradox' check out this wikipedia link and prepare to be baffled, then enlightened, then disappointed that the whole thing is so trivial. After which feel free to read on.] There is a certain similarity, you will agree, between the situation a deal or no deal player finds himself in having rejected all offers and the dilemma of Monty's contestant in the classic problem: several 'bad choices' have been eliminated and he is left with a choice between a better and worse choice with no way of knowing between them. Question: The solution to the monty hall problem is that it is, in fact, better to switch- does the same apply here? Does this depend upon the money in the boxes? Should every player opt for 'switch', cutting the 10 minutes of agonising away??? \"}\n",
      "887 {'Id': '887', 'Type': 'answer', 'ParentId': '22', 'urls': [], 'exp': ['{e_1, e_2, e_3}', '\\\\mathbb{R}^3', 'a = a_1 e_1 + a_2 e_2 + a_3 e_3', 'b = b_1 e_1 + b_2 e_2 + b_3 e_3', 'a\\\\times b = (a_2 b_3 - a_3 b_2) e_1 + (a_3 b_1 - a_1 b_3) e_2 + (a_1 b_2 - a_2 b_1) e_3', 'a \\\\wedge b = (a_1 b_2 - a_2 b_1) e_1 \\\\wedge e_2 + (a_2 b_3 - a_3 b_2) e_2 \\\\wedge e_3 + (a_3 b_1 - a_1 b_3) e_3 \\\\wedge e_1', '\\\\wedge', '(e_1 \\\\wedge e_2)', '(e_3 \\\\wedge e_2 \\\\wedge e_1)', 'e_3', 'a \\\\times b = (a \\\\wedge b)^*', 'a', 'b', 'a', 'b', '(a_1 e_1 + a_2 e_2 + a_3 e_3) \\\\wedge (b_1 e_1 + b_2 e_2 + b_3 e_3)'], 'Body': \"Here's an explanation in terms of the Hodge dual and the exterior (wedge) product. Let <math_exp> be the standard orthonormal basis for <math_exp>. Consider the two vectors <math_exp> and <math_exp>. From the matrix computation we obtain the familiar formula <math_exp>. But (see note at the bottom) <math_exp>, where the wedge <math_exp> represents the exterior product. One can now compute the dual of this latter expression using that the left contraction of <math_exp> onto <math_exp> is <math_exp> (and similar relations). The result is that <math_exp>, that is, the cross product of <math_exp> and <math_exp> is the dual of their exterior product. Geometrically, this is an incredible picture. The exterior product is the plane element spanned by both <math_exp> and <math_exp>, and the dual is the vector orthogonal to that plane. This is my favorite interpretation of the cross product, but it's only helpful, of course, if you're familiar with exterior algebra and the Hodge dual. Note: The wedge product can be found by formally computing <math_exp> using the distributivity and anticommutation relations of the exterior product. \"}\n",
      "888 {'Id': '888', 'Type': 'answer', 'ParentId': '886', 'urls': ['https://math.stackexchange.com/questions/15055/in-a-family-with-two-children-what-are-the-chances-if-one-of-the-children-is-a/15085#15085'], 'exp': [], 'Body': \"I would ceratinly plump for saying they were opened at random Then no, the Monty Hall solution doesn't apply.  The whole point is that the door isn't randomly opened, it's always a goat. An easy way of seeing this is imagining there are 100 doors, with 99 goats.  If, after you pick a door, the host always opens 98 doors of goats, then switching is very intuitively favorable.  However, if he had just opened 98 doors at random, then most of the time (98 out of 100) he would open the door with the car behind it; and even on the rare occasions he didn't, you still wouldn't be any better off switching than staying. See also this answer, in which I try to intuitively explain probability fallacies. \"}\n",
      "889 {'Id': '889', 'Type': 'answer', 'ParentId': '874', 'urls': [], 'exp': ['45', '(q,0)', 'q&lt;w', '45', 'y=w-q', 'q=w-y', '(w-y, 0)\\\\dots'], 'Body': 'If it is just <math_exp> degrees, then the answer is not very difficult. Center a coordinate system at the bottom left hand corner of the rectangle. Hence the coordinates of the (???) point are <math_exp> for some <math_exp>. Note that the because theta is <math_exp> degrees, <math_exp> (Isosceles right triangle). Hence <math_exp>, and our point is simply <math_exp> '}\n",
      "890 {'Id': '890', 'Type': 'answer', 'ParentId': '118', 'urls': ['http://betterexplained.com/articles/a-gentle-introduction-to-learning-calculus/'], 'exp': [], 'Body': 'Have a look at this explanation http://betterexplained.com/articles/a-gentle-introduction-to-learning-calculus/ I hope you love it :) '}\n",
      "891 {'Id': '891', 'Type': 'answer', 'ParentId': '886', 'urls': [], 'exp': [], 'Body': \"This doesn't have the key features of the Monty Hall problem: Without being given extra information, their is no point in changing your choice. \"}\n",
      "892 {'Id': '892', 'Type': 'question', 'Title': 'Projective plane and its dual', 'Tags': ['geometry'], 'AcceptedAnswerId': '893', 'urls': [], 'exp': ['\\\\mathbb{RP}^2', '\\\\mathbb{RP}^2'], 'Body': 'So the projective plane <math_exp> is not a vector space.  Is it still isomorphic to its dual?  If not, is there at least an invertible map that takes <math_exp> to its dual? '}\n",
      "893 {'Id': '893', 'Type': 'answer', 'ParentId': '892', 'urls': ['http://en.wikipedia.org/wiki/Pascal%27s_theorem', 'http://en.wikipedia.org/wiki/Brianchon%27s_theorem'], 'exp': ['\\\\mathbb{P}(\\\\mathbb{R}^3)', '\\\\mathbb{P}((\\\\mathbb{R}^3)^*)'], 'Body': 'Yes, though the word \"dual\" is somewhat questionable.  If you mean is <math_exp>, the standard projective plane, isomorphic to <math_exp>, the projectivization of the dual, then yes, it follows from the isomorphism of vector spaces. Much more interestingly, the duality allows you to switch points and lines in theorems, such as the Mystic Hexagon and Brianchon\\'s Theorem. '}\n",
      "894 {'Id': '894', 'Type': 'question', 'Title': 'Why is the \"finitely many\" quantifier not definable in First Order Logic?', 'Tags': ['logic', 'quantifiers'], 'AcceptedAnswerId': '928', 'urls': [], 'exp': [], 'Body': 'In First Order Logic with Identity (FOL+I), one can express the proposition that there are exactly 3 items that have the property P. Why is it not possible to express the proposition that there is a finite number of items that have the property P (in FOL+I)? '}\n",
      "895 {'Id': '895', 'Type': 'answer', 'ParentId': '894', 'urls': [], 'exp': [], 'Body': 'Any property expressible under first-order logic is closed under ultraproducts. The property of finite sets is not, however, closed under ultraproducts. '}\n",
      "896 {'Id': '896', 'Type': 'answer', 'ParentId': '894', 'urls': [], 'exp': ['P_1\\\\vee P_2\\\\vee\\\\ldots', 'P_i', 'i', 'P'], 'Body': 'Well, it would mean that you have the statement <math_exp> where <math_exp> stands for \"there are <math_exp> objects with property <math_exp>\", and infinite disjunctions aren\\'t allowed.  As for proving that this isn\\'t equivalent to anything else you can write that IS allowed, I don\\'t know how to do that. '}\n",
      "897 {'Id': '897', 'Type': 'answer', 'ParentId': '835', 'urls': ['http://en.wikipedia.org/wiki/Risk_aversion'], 'exp': ['5 is more than 100 times my utility from winning 5 cents. However, my utility from winning $\\\\'], 'Body': \"There are (at least) two factors that mean that simply calculating the average of the remaining options is not enough to describe how someone should play. Risk aversion Someone's utility is not a predictable function of the amount of money that they win. For instance my utility from winning $\\\\<math_exp>100 million is less than 100 times my utility from winning $\\\\$$1 million. \"}\n",
      "898 {'Id': '898', 'Type': 'answer', 'ParentId': '892', 'urls': [], 'exp': [], 'Body': \"This is a comment more than an answer, mainly in response to Qiaochu's question, but I don't have sufficient rep. to comment. The dual to a projective plane is the set of all lines in the plane, which itself is a projective plane (as hinted at in Charles Siegel's answer).  This is an important concept in classical projective geometry.  (Concretely, the equation for a line has the form a x + b y + c z = 0, where a,b, and c are some parameters, not all zero, and x,y,z are homogeneous coords. for the points in the proj. plane.  The set of all such lines can thus be thought of as the set of all (a,b,c) not all zero; but note that simultaneously multiplying a, b, and c by a non-zero scalar doesn't change the solution set, i.e. doesn't change the line, so the line should really be thought of as corresponding to the homogeneous coordinates (a:b:c); thus the set of all lines is again a projective plane.) \"}\n",
      "899 {'Id': '899', 'Type': 'answer', 'ParentId': '154', 'urls': [], 'exp': ['(R, \\\\theta)', 'R', '\\\\theta', 'x', '(x,0)', 'x', '-1', 'P', '(1,\\\\pi/2)', '\\\\pi/2', 'P', '-1', 'P^2=-1', \"x'\", 'a_0 x^n + a_1 x^{n-1} + \\\\cdots + a_n =0', \"(x',0)\"], 'Body': 'In my opinion, the most natural way to view complex number is as a class of maps from the plane to itself. Specifically, lets define <math_exp> to be the map which multiplies every point in the plane by the number <math_exp>, and then rotates it by the angle <math_exp>. We may call these maps \"dilations with rotations.\" Such maps can be added and composed (multiplied) in the obvious way, and its not hard to work out that the sum and product of two such mappings is another dilation with rotation. We can also identify the real number <math_exp> with the map <math_exp>, i.e. the map which multiplies every point in the plane by <math_exp>.  Then we see that these maps have the magical property that <math_exp> has a square root! Namely, if <math_exp> is the mapping <math_exp> (i.e. rotate every point by angle <math_exp>), then applying <math_exp> twice is the same as multiplying every number by <math_exp>, i.e. <math_exp>! As should be obvious by now, these maps are just complex numbers in disguise. Unsurprisingly, they are  singularly useful for solving polynomial equations. Indeed, the real number <math_exp> is a root of the polynomial equations <math_exp> if and only if the mapping <math_exp> satisfies the same equation. So viewing polynomial equations over the set of these mappings loses no solutions, while at the same time giving us additional freedom to do operations such as taking the square roots of negative numbers. '}\n",
      "900 {'Id': '900', 'Type': 'answer', 'ParentId': '667', 'urls': [], 'exp': [], 'Body': 'What you are referring to is the \"stacks as sheaves of groupoids\" point of view. To illustrate where it comes from, imagine for example that we are talking about the moduli stack of elliptic curves (on the category of schemes).  To give an elliptic curve over a scheme, it is not just enough to specify the elliptic curve over the members of the open cover; we have to explain how we glue the restrictions of the curves on the various opens on their overlaps, and this gluing has to be coherent over triple overlaps. The reason for this is that elliptic curves can have non-trivial automorphisms, so that there is no a priori determined way to make the identifications on the overlaps (because having non-trivial automorphisms is the same as saying that when two curves are isomorphic, they can be isomorphic in more than one way), so it is your job to choose these identifications, and to make sure that you do it in a coherent way. (Here elliptic curves can be replaced by any other moduli problem you can think of, of course.) '}\n",
      "901 {'Id': '901', 'Type': 'answer', 'ParentId': '631', 'urls': [], 'exp': [], 'Body': 'There are a few subtleties that will probably effect the final answer. Are we required to find the solution, or merely establish existence? By analogy, determining if a number has a prime factorization is trivial, but finding its prime factorization is hard. Is the runtime being measured in terms of {a_1,...,a_n,s} or {log(a_i),...,log(a_n),log(s)}? By analogy, SUBSET-SUM is in P in the first case, but NP-complete in the second case. '}\n",
      "902 {'Id': '902', 'Type': 'answer', 'ParentId': '518', 'urls': [], 'exp': ['A_P', 'A_P', 'A =\\\\mathbb C[[x^{1/2},x^{1/3},\\\\dots]]/(x)', 'A', 'x^{1/n}', '(a_i)', '(0,1)', 'x^{a_i}A', 'A', 'PA_P', 'A_P', 'PA_P', 'P'], 'Body': \"This is a comment on the proof sketch in the question: the localization <math_exp> is local of dimension zero (its unique maximal ideal is also a minimal prime ideal, and hence is the unique prime ideal of <math_exp>), but need not be Artinian, as far as I can tell.  E.g. if <math_exp>, then <math_exp> is local with a unique prime ideal (namely the ideal generated by all the <math_exp>), but is not Artinian (equivalently, not Noetherian), since if <math_exp> is any strictly descending sequence of rational numbers in the interval <math_exp>, then the ideals <math_exp> form a strictly descending sequence of ideals in <math_exp>. (Hopefully I'm not blundering here; if I am, someone please let me know!) (Also, I should add that it is still the case that since <math_exp> is the unique minimal prime of <math_exp>, every element of <math_exp> is nilpotent, and hence every element of <math_exp> is a zero divisor, so my comment is very nitpicky: it is just about the use of the terminology Artinian.) \"}\n",
      "903 {'Id': '903', 'Type': 'question', 'Title': 'Choosing a text for a First Course in Topology', 'Tags': ['general-topology', 'reference-request', 'soft-question', 'book-recommendation'], 'urls': [], 'exp': [], 'Body': \"Which is a better textbook - Dugundji or Munkres? I'm concerned with clarity of exposition and explanation of motivation, etc. \"}\n",
      "904 {'Id': '904', 'Type': 'answer', 'ParentId': '868', 'urls': ['http://www4.ncsu.edu/~kaltofen/bibliography/kaltofen.html', 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.7916&amp;rep=rep1&amp;type=pdf', 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.7916&amp;rep=rep1&amp;type=pdf', 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.7916&amp;rep=rep1&amp;type=pdf'], 'exp': [], 'Body': 'If you are interested in the factorization algorithms employed in modern computer algebra systems such as Macsyma, Maple, or Mathematica, then see any of the standard introductions to computer algebra , e.g. Geddes et.al. \"Algorithms for Computer Algebra\"; Knuth, \"TAOCP\" v.2; von zur Gathen \"Modern Computer Algebra\"; Zippel \"Effective Polynomial Computation\". See also Kaltofen\\'s surveys on polynomial factorization [116,68,56,7] in his publications list, which contains plenty of theory, history and literature references. Note: Kaltofen\\'s home page appears to be temporarily down so instead see his paper [1] to get started (see comments) 1 Kaltofen, E. Factorization of Polynomials, pp. 95-113 in: Computer Algebra, B. Buchberger, R. Loos, G. Collins, editors, Vienna, Austria,  (1982). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.7916&amp;rep=rep1&amp;type=pdf '}\n",
      "905 {'Id': '905', 'Type': 'answer', 'ParentId': '29', 'urls': [], 'exp': [], 'Body': \"Try sage.  It's like python (it is actually written in python, and uses something similar to a python shell), but with a vastly larger set of built-in mathematical operations. \"}\n",
      "906 {'Id': '906', 'Type': 'answer', 'ParentId': '29', 'urls': [], 'exp': [], 'Body': \"Some people have suggested Sage or Python. Sage can do the symbolic manipulation that TI calculators can do using Maxima. Python with the right packages can do this as well (SymPy package I believe). Plus these things will be programmable. Sage however is fairly heavy at a few GB if I recall correctly. I'm not sure if there is anyway to interface with Maxima through Python like in Sage without actually having Sage. \"}\n",
      "907 {'Id': '907', 'Type': 'question', 'Title': 'Correct usage of the phrase \"In the sequel\"? History? Alternatives?', 'Tags': ['soft-question', 'terminology'], 'AcceptedAnswerId': '924', 'urls': [], 'exp': [], 'Body': 'While I feel quite confident that I\\'ve inferred the correct meaning of \"In the sequel\" from context, I\\'ve never heard anyone explicitly tell me, so first off, to remove my niggling doubts: What does this phrase mean? (Someone recently argued to me that \"sequel\" was actually supposed to refer to a forthcoming second part of a paper, which I found highly unlikely, but I\\'d just like to make sure. ) My main questions: At what points in the text, and for what kinds of X, is it appropriate to use the phrase \"In the sequel, X\" in a paper? In a book? Is it ever acceptable to introduce definitions via \"In the sequel, we introduce the concept of a \"blah\", which is a thing satisfying ...\" at the start of a paper or book without a formal \"Definition. A \"blah\" is a thing, satsifying ...\" in the main text of the paper or book? Finally, out of curiosity, I\\'m wondering how long this phrase has been around, if it\\'s considered out of date or if it\\'s still a popular phrase, and what some good alternatives are. '}\n",
      "908 {'Id': '908', 'Type': 'answer', 'ParentId': '907', 'urls': [], 'exp': [], 'Body': \"I would never write that (mostly because it really sounds like the definition will be in another paper...). I'm pretty sure I'd write «In what follows, ...». \"}\n",
      "910 {'Id': '910', 'Type': 'answer', 'ParentId': '855', 'urls': ['http://planetmath.org/encyclopedia/Exponent.html', 'http://en.wikipedia.org/wiki/Carmichael_function', 'http://groups.google.com/groups?selm=y8zk55tqcie.fsf%40nestle.csail.mit.edu'], 'exp': ['\\\\rm\\\\ \\\\ 24\\\\ |\\\\ M^2 - N^2 \\\\;', '\\\\rm \\\\; M,N \\\\perp 6, \\\\;', '6.\\\\;', '\\\\rm\\\\quad\\\\quad\\\\quad N\\\\perp 2 \\\\;\\\\Rightarrow\\\\; N = \\\\pm 1, \\\\pm 3 \\\\pmod 8 \\\\;\\\\Rightarrow\\\\; N^2 = 1 \\\\pmod 8', '\\\\rm\\\\quad\\\\quad\\\\quad N\\\\perp 3 \\\\;\\\\Rightarrow\\\\; N \\\\;\\\\;= \\\\;\\\\;\\\\;\\\\pm 1\\\\ \\\\pmod 3 \\\\;\\\\Rightarrow\\\\; N^2 = 1 \\\\pmod 3 \\\\;', '\\\\rm \\\\quad\\\\; 3, 8\\\\ |\\\\ N^2 - 1 \\\\;\\\\Rightarrow\\\\;  24\\\\ |\\\\ N^2 - 1 \\\\ ', '\\\\ {\\\\rm lcm}(3,8) = 24.', '\\\\rm\\\\ n = 24\\\\ ', '\\\\ ', '\\\\rm\\\\ a,e,n ', '\\\\rm\\\\ e,n&gt;1 ', '\\\\rm\\\\quad n\\\\ |\\\\ a^e-1', \"\\\\rm a\\\\perp n \\\\ \\\\iff\\\\  \\\\phi'(p^k)\\\\:|\\\\:e\\\\ \", '\\\\rm\\\\ p^k\\\\:|\\\\:n,\\\\ \\\\ p\\\\:', \"\\\\rm \\\\;\\\\;\\\\; \\\\phi'(p^k) = \\\\phi(p^k)\\\\ \", '\\\\rm p\\\\:,\\\\ ', '\\\\phi', \"\\\\rm\\\\ \\\\quad \\\\phi'(2^k) = 2^{k-2}\\\\ \", '\\\\rm  k&gt;2\\\\:,\\\\ ', '\\\\,2', '\\\\rm \\\\mathbb Z/2^k', '\\\\rm C(2) \\\\times C(2^{k-2})', '\\\\rm k&gt;2', '\\\\rm e', \"\\\\rm \\\\;\\\\lambda(n)\\\\; = \\\\;{\\\\rm lcm}\\\\;\\\\{\\\\phi'(\\\\;{p_i}^{k_i})\\\\}\\\\;\", '\\\\rm \\\\; n = \\\\prod {p_i}^{k_i}\\\\;', '\\\\rm\\\\lambda(n)', '\\\\rm \\\\mathbb Z/n^*,\\\\;', \"\\\\rm\\\\ \\\\lambda(24) = lcm(\\\\phi'(2^3),\\\\phi'(3)) = lcm(2,2) = 2\\\\:.\"], 'Body': \"In fact the result holds a bit more generally, namely: Lemma <math_exp> if <math_exp> i.e. coprime to <math_exp> The proof is easy: <math_exp> <math_exp> So <math_exp> since <math_exp> This is a special case <math_exp> of this much more general result Theorem <math_exp>  For naturals <math_exp> with <math_exp> <math_exp> for all <math_exp> for all <math_exp> prime with <math_exp> for odd primes <math_exp> where <math_exp> is Euler's totient function and   <math_exp> if <math_exp> else <math_exp> The latter exception is due to <math_exp> having multiplicative group <math_exp> for <math_exp>. Notice that the least such exponent <math_exp> is given by <math_exp> where <math_exp>. <math_exp> is called the (universal) exponent of the group <math_exp> a.k.a. the Carmichael function. So the case at hand is simply <math_exp> See my post here for proofs and further discussion. \"}\n",
      "911 {'Id': '911', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://scottaaronson.com/blog/?p=152', 'http://scottaaronson.com/blog/?p=153'], 'exp': [], 'Body': 'Scott Aaronson once basically did this for a bunch of theorems in computer science here and here.  I particularly like this one: Suppose a baby is given some random examples of grammatical and ungrammatical sentences, and based on that, it wants to infer the general rule for whether or not a given sentence is grammatical. If the baby can do this with reasonable accuracy and in a reasonable amount of time, for any “regular grammar” (the very simplest type of grammar studied by Noam Chomsky), then that baby can also break the RSA cryptosystem. '}\n",
      "912 {'Id': '912', 'Type': 'answer', 'ParentId': '796', 'urls': ['http://www.ima.umn.edu/~miller/separationofvariables.html', 'http://www.ima.umn.edu/~miller/separationofvariables.html'], 'exp': [], 'Body': \"There is an extremely beautiful Lie-theoretic approach to separation of variables, e.g. see Willard Miller's book [1] (freely downloadable). I quote from his introduction: This book is concerned with the   relationship between symmetries of a    linear second-order partial   differential equation of mathematical   physics,  the coordinate systems in   which the equation admits solutions   via separation  of variables, and the   properties of the special functions   that arise in  this manner. It is an   introduction intended for anyone with   experience in  partial differential   equations, special functions, or Lie   group theory, such  as group   theorists, applied mathematicians,   theoretical physicists and  chemists,   and electrical engineers. We will   exhibit some modem group-theoretic    twists in the ancient method of   separation of variables that can be    used to provide a foundation for much   of special function theory. In    particular, we will show explicitly   that all special functions that arise    via separation of variables in the   equations of mathematical physics can    be studied using group theory. These   include the functions of Lam6, Ince,    Mathieu, and others, as well as those   of hypergeometric type. This is a very critical time in the   history of group-theoretic methods in    special function theory. The basic   relations between Lie groups, special    functions, and the method of   separation of variables have recently   been  clarified. One can now construct   a group-theoretic machine that, when    applied to a given differential   equation of mathematical physics,   describes  in a rational manner the   possible coordinate systems in which   the equation  admits solutions via   separation of variables and the   various expansion  theorems relating   the separable (special function)   solutions in distinct  coordinate   systems. Indeed for the most important   linear equations, the  separated   solutions are characterized as common   eigenfunctions of sets of    second-order commuting elements in the   universal enveloping algebra of  the   Lie symmetry algebra corresponding to   the equation. The problem of    expanding one set of separable   solutions in terms of another reduces   to a  problem in the representation   theory of the Lie symmetry algebra. [1] Willard Miller. Symmetry and Separation of Variables. Addison-Wesley, Reading, Massachusetts, 1977 (out of print) \"}\n",
      "913 {'Id': '913', 'Type': 'answer', 'ParentId': '791', 'urls': ['http://www.math.vt.edu/people/plinnell/Vtregional/solutions.pdf'], 'exp': [], 'Body': 'The answer can be found on pages 27-28 of the following link. http://www.math.vt.edu/people/plinnell/Vtregional/solutions.pdf '}\n",
      "914 {'Id': '914', 'Type': 'answer', 'ParentId': '741', 'urls': ['http://rjlipton.wordpress.com/2010/06/26/stating-pnp-without-turing-machines/'], 'exp': [], 'Body': \"I suggest you this article, which talks about Linear Equations, Linear Programming, Integer Programming and P=NP. It's easy to understand and talks about the differences among these things \"}\n",
      "915 {'Id': '915', 'Type': 'answer', 'ParentId': '362', 'urls': ['http://retro.seals.ch/cntmng?type=pdf&amp;rid=elemat-001:1998:53%3a:22&amp;subp=hires', 'http://dx.doi.org/10.5169/seals-3627', 'http://www.springerlink.com/content/e7p7kxk2y3a71j0d/fulltext.pdf', 'http://dx.doi.org/10.1007/s000170050029'], 'exp': [], 'Body': 'For a nice introduction to the history of ring theory see the following paper I. Kleiner. From numbers to rings: the early history of ring theory. Elemente der Mathematik 53 (1998) 18-35. SEALS: direct link to pdf, persistent link to article Springerlink: direct link to pdf, persistent link to article '}\n",
      "916 {'Id': '916', 'Type': 'answer', 'ParentId': '874', 'urls': [], 'exp': [], 'Body': \"Alright, I'm not 100% sure I'm understanding this correctly. You say that p can be located on the right or top line and that p2 can be located on the bottom or left line. Do you mean the rectangle can be rotated? If that's the case, the question should say that p can be on the right or bottom line of the rectangle. Also, are you looking for two separate answers or one that works both when p2 is on the bottom and on the left? If you do mean that the rectangle can be rotated, and want two different answers, it's pretty simple. First I'll deal with when p2 is on the bottom and p is on the right. Since p2 is on the bottom line we know the y-coordinate is h, according to the diagram. We also know that p is (0,y). Because of the 45 degree angle, we know that the distance between p's y-coordinate and the lower right corner is the same as the distance between the lower right corner and p2's x-coordinate, which in this case is p2's x-coordinate. Therefore, the coordinates of p2 are (h-y, h). If p2 is on the left and p is on the bottom, it's very similar. Since p2 is on the left, it's x-coordinate is h. Because p is on the x-axis, it's (x,0). Because of the 45 degree angle, the distance between the lower left corner and p is the same as the distance between the lower left corner and p2, which this time gives us p2's y-coordinate. Therefore the coordinates of p2 are (h,h-x). Hopefully I understood your intentions correctly. If not, I hope you can use my misunderstandings to further improve your question. \"}\n",
      "917 {'Id': '917', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': ['37', '2', '2', '2', '37 = 2^{(2^2 + 1)} + 2^2 + 1', '2', '3', '3', '3^{(3^3 + 1)} + 3^3 + 1 - 1= 3^{(3^3 + 1)} + 3^3', '2 \\\\times 10^{13}', '3', '4', '4', '4^{(4^4 + 1)} + 4^4 - 1 = 4^{(4^4 + 1)} + 3*4^3 + 3*4^2 + 3*4 + 3', '5 \\\\times 10^{154}', 'n^{th}', 'n+1', 'n+2', '1', 'n+2', '37', '4', '3*2^{(402653211)} - 2', '10^{(100,000,000)}', '1', '0', '37', '0'], 'Body': 'My favorite would probably be Goodstein\\'s theorem: Start with your favorite number (mine is <math_exp>) and express it in hereditary base <math_exp> notation.  That is, write it as a power of <math_exp> with exponents powers of <math_exp>, etc. So, <math_exp>.  This is the first element of the sequence. Next, change all the <math_exp>\\'s to <math_exp>\\'s, and subtract one from what\\'s remaining and express in hereditary base <math_exp> notation. We get <math_exp> (which is roughly <math_exp>).  This is the second element of the sequence. Next, change all <math_exp>\\'s to <math_exp>\\'s, subtract one, and express in hereditary base <math_exp> notation. We get <math_exp> (which is roughly <math_exp>) .  This is the third element of the sequence. Rinse, repeat:  at the <math_exp> stage, change all the \"<math_exp>\" to \"<math_exp>\", subtract <math_exp>, and reexpress in hereditary base <math_exp> notation. The theorem is: no matter which number you start with, eventually, your sequence hits 0, despite the fact that it grows VERY quickly at the start. For example, if instead of starting with <math_exp>, we started with <math_exp>, then (according to the wikipedia page), it takes <math_exp> steps ( VERY roughly <math_exp>, or a <math_exp> followed by a hundred million <math_exp>s).  <math_exp> takes vastly longer to drop to <math_exp>. '}\n",
      "918 {'Id': '918', 'Type': 'question', 'Title': 'How do I count the subsets of a set whose number of elements is divisible by 3? 4?', 'Tags': ['number-theory', 'combinatorics', 'binomial-coefficients'], 'AcceptedAnswerId': '929', 'urls': [], 'exp': ['S', 'n', '\\\\displaystyle \\\\sum_{k=0}^{n} {n \\\\choose k} = (1 + 1)^n', '\\\\displaystyle \\\\sum_{k=0}^{n} (-1)^k {n \\\\choose k} = (1 - 1)^n', '\\\\displaystyle \\\\sum_{k=0}^{n/2} {n \\\\choose 2k} = 2^{n-1}', 's \\\\in S', 's', '3', '4', 'n \\\\bmod 6', 'n \\\\bmod 8', '\\\\omega', '\\\\displaystyle \\\\sum_{k=0}^{n} \\\\omega^k {n \\\\choose k} = (1 + \\\\omega)^n = (-\\\\omega^2)^n', '\\\\displaystyle \\\\sum_{k=0}^{n} \\\\omega^{2k} {n \\\\choose k} = (1 + \\\\omega^2)^n = (-\\\\omega)^n', '1 + \\\\omega^k + \\\\omega^{2k} = 0', 'k', '3', '3', '\\\\displaystyle \\\\sum_{k=0}^{n/3} {n \\\\choose 3k} = \\\\frac{2^n + (-\\\\omega)^n + (-\\\\omega)^{2n}}{3}.', '-\\\\omega', '-\\\\omega^2', '\\\\displaystyle \\\\sum_{k=0}^{n/4} {n \\\\choose 4k} = \\\\frac{2^n + (1+i)^n + (1-i)^n}{4}', '1+i = \\\\sqrt{2} e^{ \\\\frac{\\\\pi i}{4} }'], 'Body': 'Let <math_exp> be a set of size <math_exp>.  There is an easy way to count the number of subsets with an even number of elements.  Algebraically, it comes from the fact that <math_exp> while <math_exp>. It follows that <math_exp>. A direct combinatorial proof is as follows: fix an element <math_exp>.  If a given subset has <math_exp> in it, add it in; otherwise, take it out.  This defines a bijection between the number of subsets with an even number of elements and the number of subsets with an odd number of elements. The analogous formulas for the subsets with a number of elements divisible by <math_exp> or <math_exp> are more complicated, and divide into cases depending on the residue of <math_exp> and <math_exp>, respectively.  The algebraic derivations of these formulas are as follows (with <math_exp> a primitive third root of unity):  observe that <math_exp> while <math_exp> and that <math_exp> if <math_exp> is not divisible by <math_exp> and equals <math_exp> otherwise.  (This is a special case of the discrete Fourier transform.)  It follows that <math_exp> <math_exp> and <math_exp> are sixth roots of unity, so this formula splits into six cases (or maybe three).  Similar observations about fourth roots of unity show that <math_exp> where <math_exp> is a scalar multiple of an eighth root of unity, so this formula splits into eight cases (or maybe four). Question:  Does anyone know a direct combinatorial proof of these identities? '}\n",
      "919 {'Id': '919', 'Type': 'answer', 'ParentId': '636', 'urls': ['http://en.wikipedia.org/wiki/Binary_decision_diagram', 'http://www.combinatorics.org/Volume_3/Abstracts/v3i1r5.html', 'http://www.combinatorics.org/Volume_3/Comments/v3i1r5.html', 'http://www.combinatorics.org/Volume_3/Comments/v3i1r5.01.ps', 'http://books.google.com/books?id=-DZjVz9E4f8C&amp;pg=PA369&amp;dq=532', 'http://www.mayhematics.com/t/8a.htm'], 'exp': [], 'Body': 'I was recently surprised to discover that it\\'s actually not known (edit: see below). The number of closed knight\\'s tours (cyclic) was computed in the 1990s, using binary decision diagrams. There are 26,534,728,821,064 closed directed knight\\'s tours, and the number of undirected ones is half that or 13,267,364,410,532. If you count equivalence classes under rotation and reflection, there are slightly more than 1/8th of that: 1,658,420,855,433. (Loebbing and  Wegener (1996) wrote a paper \"The Number of Knight\\'s Tours Equals 33,439,123,484,294 — Counting with Binary Decision Diagrams\"; the number in the title in the mistake, as they pointed out in a comment to their paper. Brendon McKay independently computed the correct number with another method, and the original authors seem to have later found the same answer.) Finding the exact number of open tours (not cyclic/reentrant) was open, but was estimated to be about 1015 or 2×1016. Edit: Please see and upvote the answer by user rantonse below, pointing out that Alex Chernov appears to have calculated the total number of knight\\'s tours (including non-cyclic ones). '}\n",
      "920 {'Id': '920', 'Type': 'answer', 'ParentId': '20', 'urls': ['http://en.wikipedia.org/wiki/Invariance_of_domain', 'http://en.wikipedia.org/wiki/Dedekind_cut', 'http://en.wikipedia.org/wiki/Real_number#Completeness', 'http://usf.usfca.edu/vca//', 'http://en.wikipedia.org/wiki/Erlangen_program', 'http://en.wikipedia.org/wiki/Group_theory'], 'exp': ['\\\\mathbb{R}^n', '\\\\mathbb{R}^m', 'n', 'm', 'q', 'u, v', 'u + v', '(1, 0)', '(0, 1)', '(1, 0)', '(0, 1)', '(1, 0)', '(1, 0)', '(0, 1)', '(1, 0)', '\\\\theta', 'r', '(1, 0)', '(r \\\\cos \\\\theta, r \\\\sin \\\\theta)', '\\\\phi', 's', '(1, 0)', '(s \\\\cos \\\\phi, s \\\\sin \\\\phi)', '(1, 0)', '(rs \\\\cos (\\\\theta + \\\\phi), rs \\\\sin (\\\\theta + \\\\phi))', '(a, b) * (c, d) = (ac - bd, ad + bc).', '(a, b)', 'a + bi'], 'Body': 'I think you were being a little too hard on Isaac.  The truth is that the real numbers are a sophisticated mathematical construction and that any explanation of what they \"are\" which pretends otherwise is a convenient fiction.  Mathematicians need these kind of sophisticated constructions because they are what is required for rigorous proofs.  Before people explicitly constructed the real numbers and used them to define and prove things about other concepts, it was never totally clear what was true or what was false, and everybody was very confused. For example, Cantor proved that the number of points in the plane is the same as the number of  points on a line.  Many people thought that this was impossible before he did it; they had an intuition that you couldn\\'t possibly \"fit\" the plane into the line.  More generally, people were pretty sure you couldn\\'t fit <math_exp> into <math_exp> if <math_exp> was greater than <math_exp>.  It wasn\\'t until quite a bit later that mathematicians formalized and proved a rigorous mathematical statement which justified this intuition called invariance of domain, which says you can\\'t do this in a continuous way.  One of the many mathematical constructions you need to even state this theorem is the construction of the real numbers.  (Another is a formal definition of what \"continuous\" means, but one thing at a time.) So, what are the real numbers?  They are a formal way to fill \"holes\" in the rational numbers, which is necessary for all sorts of things.  The most basic thing they are necessary for is doing geometry.  You probably know that the square root of 2 is irrational.  What this means is that it is impossible to think about the diagonal of a square as being the same kind of object as the sides of a square while only using rational numbers.  But you can rotate a diagonal, and it looks just like the side of a square, only a bit longer.  So you\\'d like a number system in which you can sensibly talk about any number you can construct geometrically.  You\\'d also like to be able to talk about rotation!  You can\\'t do that with just rational numbers, either. So how do you fill in enough holes to do geometry?  Dedekind came up with a very clever way to do this.  It starts by observing that a rational number <math_exp> is completely determined by the set of rational numbers greater than it and the set of rational numbers less than it.  For example, 1/2 is completely determined by the fact that it\\'s always between 1/2 + 1/n and 1/2 - 1/n.  (For the initiated, this is a special case of the Yoneda lemma.)  But there are \"numbers,\" such as the square root of 2, which aren\\'t rational, and yet have the property that we can always tell what rational numbers are greater than it and what rational numbers are less than it.  For the square root of 2, these are precisely the fractions p/q such that 2q^2 &lt; p^2 and such that 2q^2 > p^2, respectively.  Dedekind\\'s brilliant idea was the following: Define a real number to be a partition of the rational numbers! In Dedekind\\'s construction, the square root of 2 quite literally is the set of rational numbers that are greater than it, and the set of rational numbers that are less than it. You can define all the usual arithmetic operations on these \"numbers,\" called Dedekind cuts, and prove all the wonderful theorems you\\'ll find in a standard book on real analysis.  In particular, the property that guarantees that all the holes are filled is called completeness. Figured I might as well add something about the complex numbers.  The story here is beautiful, and if you\\'re really interested you should check out Tristan Needham\\'s Visual Complex Analysis.  Some people say that the point of the complex numbers is to let you solve polynomials, but this is really selling them short.  The complex numbers are an inherently geometric construction, and should be understood as such.  Their geometry and topology just happens to be responsible for the fact that you can solve polynomials with them, but it\\'s also responsible for much more. Here is a quick sketch.  Now that you\\'ve got the real numbers on your hands, you can rigorously talk about plane geometry.  In plane geometry, an important notion is that of similarity.  Informally, two figures are similar if they have the same shape.  More formally, two figures are similar if you can rotate, translate, and scale one figure so that it matches up with the other.  So similarity is all about a certain collection of transformations of the points in the plane.  It was Klein who first realized that the important features of different flavors of \"geometry\" are captured in what kind of transformations are allowed.  So to do geometry the modern way we should focus our attention on these transformations, which form a group. To make this easier, let\\'s ignore the translations for now.  We\\'ll pick an origin for our plane, and we\\'ll only allow rotations and scalings about this origin.  Rotations and scalings have the property that they are both linear transformations; this means that if you know what the transformation does to two points <math_exp>, you also know what it does to the vector sum <math_exp>.  In particular, a linear transformation is determined by what it does to the point <math_exp> and to the point <math_exp>. However, rotations and scalings satisfy an extra property: they are, in fact, determined by what they do to the point <math_exp>.  This is because <math_exp> can be obtained from <math_exp> by a rotation by 90 degrees, and rotations and scalings commute with each other: if you rotate x degrees then y degrees, that\\'s the same as rotating y degrees then x degrees, which is the same as rotating x+y degrees.  Similarly, if you rotate x degrees then scale by 2, that\\'s the same as scaling by 2, then rotating x degrees.  So if you know what a rotation-and-scaling does to <math_exp>, you just rotate that vector by 90 degrees, and you know what it did to <math_exp>. So to every rotation-and-scaling, we can assign two real numbers: the coordinates of the image of the point <math_exp>.  In general, a rotation by <math_exp> angles followed by a scaling by <math_exp> sends <math_exp> to <math_exp>.  A different transformation, say a rotation by <math_exp> angles followed by a scaling by <math_exp>, sends <math_exp> to <math_exp>.  And their composition sends <math_exp> to <math_exp>.  In other words, composition of rotations-and-scalings defines a multiplication law on pairs of real numbers.  What is this law, exactly?  Well, by the angle addition formulas, it\\'s <math_exp> And this is precisely the rule for multiplication in the complex numbers, where <math_exp> corresponds to <math_exp>.  You get the rule for addition by observing that not only can you compose two rotations-and-scalings, you can also add their results. Together, the real numbers and the complex numbers provide a foundation for much of modern mathematics and physics.  For example, the complex numbers turn out (for reasons which are still not well understood) to be fundamental in the description of quantum mechanics. '}\n",
      "923 {'Id': '923', 'Type': 'answer', 'ParentId': '20', 'urls': ['https://math.stackexchange.com/questions/20/can-you-explain-what-a-real-number-is-also-rational-decimal-integer-natural/79#79'], 'exp': [], 'Body': 'The natural numbers are defined by the Peano axioms, as in the answer of Isaac. You can also view the natural numbers as the cardinalities of finite sets, which implies that zero is a natural number. Now the other number domains arise because mathematicians want to give values for certain operations which otherwise are only defined partially. Alternatively, they want to give solutions to equations of certain forms, which cannot always be solved in the smaller domain. integers arise from subtraction (solutions of a + x = b) rational numbers arise from division (solutions of a * x = b) real numbers arise from taking limits or upper bounds complex numbers arise from taking roots (solutions of polynomial equations) cardinal numbers arise as the sizes of sets. They can be constructed as isomorphism classes of sets, e.g. the number one would then be the class of all sets containing one element. They are useful for talking about the different sizes of infinite sets, which can get extremely large. Some set theorists compete over who can define the largest cardinal. ordinal numbers arise as the sizes of well-orderings. They can be constructed as isomorphism classes of well-orderings, e.g. the number two would then be the class of all well-orderings containing two elements (with the first smaller than the second in the ordering). Interestingly, the ordinals are themselves well-ordered in a single well-ordering, so they come in a well-defined sequence, and an ordinal number can be viewed as describing its position in this sequence. '}\n",
      "924 {'Id': '924', 'Type': 'answer', 'ParentId': '907', 'urls': [], 'exp': [], 'Body': 'Your interpretation of how \"in the sequel\" is used in mathematical literature is correct: it means \"in what follows\", \"in the remainder of the present text\", \"from now on\"...As you can see, there are many other such expressions, and I don\\'t think that \"in the sequel\" has any particular nuance of meaning that these other phrasings lack. On to your question of whether this is acceptable usage in a math paper: acceptable, yes, but not completely recommended.  In more detail: Ths phrase is, as you point out, rather common, so any experienced reader of math will have seen it before.  Also its meaning should be relatively transparent to a literate native speaker of English: this is after all what the dictionary says that \"sequel\" means. However, there are two drawbacks.  First, as Mariano says, nowadays we hear \"sequel\" used most commonly for the next movie in a franchise, so its use in a math paper will inevitably make some readers think that you are referring to [TITLE OF YOUR PAPER] Part II: This Time, It\\'s Personal, or whatever.  Second, in my opinion it is somewhat hackneyed language and an idiomatic usage that doesn\\'t add any meaning.  If you read enough math papers you\\'ll find that there are certain linguistic ticks that people pick up from each other (and also sometimes, from writing in other languages), e.g. beginning a sentence with \"Remark that...\" sounds a little stilted and is probably a semiconscious translation of the french phrase \"Remarquer que\" (which I would translate as \"Notice that\").  Most people agree that good mathematical writing is as stylistically unobstrusive as possible, so using phrases that make people ponder and ask questions about them on websites is, arguably, slightly too distracting. Anyway, it\\'s  no big deal.  If I were a referee or even a copyeditor, I would almost certainly let the author use \"in the sequel\" if she wants. '}\n",
      "928 {'Id': '928', 'Type': 'answer', 'ParentId': '894', 'urls': [], 'exp': ['P_i', 'i', 'P', 'P_i', 'P_i', 'P_i', 'i', 'P', 'i+1'], 'Body': 'We can define formula <math_exp> that says \"there are at most <math_exp> elements satisfying <math_exp>\". Now, if the infinite disjunction of the <math_exp> was definable in FO, it would (by compactness) imply a conjunction of some finite subset of the <math_exp>, hence it would imply <math_exp> for some <math_exp>. That is not true, if <math_exp> can have (say) <math_exp> elements satisfying it. '}\n",
      "929 {'Id': '929', 'Type': 'answer', 'ParentId': '918', 'urls': [], 'exp': [], 'Body': 'Fix two elements s1,s2&isin;S and divide subsets of S into two parts (subsets of S containing only s2)&cup;(subsets of S which contains s1 if they contain s2). The second part contains equal number of sets for all reminders mod 3 (because Z/3 acts there adding s1, then s2, then removing both of them) -- namely, 2n-2. And for the first part we have a bijection with subsets (edit: with 2 mod 3 elements) of a set with (n-2) elements. So we get a recurrence relation that gives an answer 2n-2+2n-4+... -- i.e. (2n-1):3 for even and (2n-2):3 for odd n. Errata. For n=0,1,5 mod 6 one should add \"+1\" to the answer from the previous paragraph (e.g. for n=6 the correct answer is 1+20+1=22 and not 21). Let me try to rephrase the solution to make it obvious. For n=2k divide S on k pairs and consider an action of a group Z/3Z on each pair described in a first paragraph. We get an action of (Z/3Z)k on subsets of S, and after removal of it\\'s only fixed point (k-element set consisting of second points from each pair) we get a bijection between subsets which have 0, 1 or 2 elements mod 3. So there are (2n-1):3 sets with i mod 3 elements excluding the fixed point and to count that point one should add \"+1\" for i=k mod 3. And for n=2k+1 there are 2 fixed points&nbsp;&mdash; including or not (2k+1)-th element of S&nbsp;&mdash; with k+1 and k elements respectively. '}\n",
      "930 {'Id': '930', 'Type': 'answer', 'ParentId': '855', 'urls': ['http://arxiv.org/PS_cache/arxiv/pdf/1104/1104.5052v1.pdf'], 'exp': ['p &gt; 3', 'p^2 \\\\equiv 1 \\\\pmod 3', 'p^2 \\\\equiv 1 \\\\pmod 8', '\\\\varphi(3) = 2', '3', '\\\\varphi(8) = 4', '8', 'n', 'U(n) = (\\\\mathbb{Z}/n\\\\mathbb{Z})^{\\\\times}', 'n', 'G', '[2]: G \\\\rightarrow G', 'g \\\\mapsto g^2', 'G', 'n = 3', 'n = 8', 'U(n)', 'U(3) = \\\\{ \\\\pm 1\\\\}', '2', '(-1)^2 = 1', '1', 'p', '[2]', 'U(p)', 'U(p)', 'p = 3', 'p-1', '1', 'p', 'U(8) = \\\\{1,3,5,7\\\\}', '4', 'U(p)', '1^2 \\\\equiv 3^2 \\\\equiv 5^2 \\\\equiv 7^2 \\\\equiv 1 \\\\pmod 8', '\\\\mathbb{Z}/8\\\\mathbb{Z}', 'U(n)', 'p', 'U(p)', 'p-1', 'U(8)', '4', '4', 'C_2 \\\\times C_2', 'p', 'a', 'U(p^a)', 'p^{a-1}(p-1)', 'C_{p^{a-1}} \\\\times C_{p-1}', 'a \\\\geq 2', 'U(2^a)', 'C_{2^{a-2}} \\\\times C_2', '2', 'N', 'p', '\\\\operatorname{gcd}(p,N) = 1', 'N', '(p^2-1)', 'N = 24'], 'Body': 'This is somewhere between an answer and commentary.  As others have said, the question is equivalent to showing: for any prime <math_exp>, <math_exp> and <math_exp>.  Both of these statements are straightforward to show by just looking at the <math_exp> reduced residue classes modulo <math_exp> and the <math_exp> reduced residue classes modulo <math_exp>.  But what is their significance? For a positive integer <math_exp>, let <math_exp> be the multiplicative group of units (\"reduced residues\") modulo <math_exp>.  Like any abelian group <math_exp>, we have a squaring map <math_exp>, <math_exp>, the image of which is the set of squares in <math_exp>.  So, the question is equivalent to: for <math_exp> and also <math_exp>, the subgroup of squares in <math_exp> is the trivial group. The group <math_exp> has order <math_exp>; since <math_exp>, the fact that the subgroup of squares is equal to <math_exp> is pretty clear.  But more generally, for any odd prime <math_exp>, the squaring map <math_exp> on <math_exp> is two-to-one onto its image -- an element of a field has no more than two square roots -- so that precisely half of the elements of <math_exp> are squares.  It turns out that when <math_exp>, half of <math_exp> is <math_exp>, but of course this is somewhat unusual: it doesn\\'t happen for any other odd prime <math_exp>. The group <math_exp> has order <math_exp>.  By analogy to the case of <math_exp>, one might expect the squaring map to be two-to-one onto its image so that exactly half of the elements are squares.  But that is not what is happening here: indeed <math_exp>, so the subgroup of squares is again trivial.  What\\'s different?  Since <math_exp> is not a field, it is legal for a given element to have more than two square roots, but a more insightful answer comes from the structure of the groups <math_exp>.  For any odd prime <math_exp>, the group <math_exp> is cyclic of order <math_exp> (\"existence of primitive roots\").  It is easy to see that in any cyclic group of even order, exactly half of the elements are squares.  So <math_exp> must not be cyclic, so it must be the other abelian group of order <math_exp>, i.e., isomorphic to the Klein <math_exp>-group <math_exp>. More generally, if <math_exp> is an odd prime number and <math_exp> is a positive integer, then  <math_exp> is cyclic of order <math_exp> hence isomorphic to <math_exp>, whereas for any <math_exp>, the group <math_exp> is isomorphic to <math_exp>.  This is one of the first signs in number theory \"there is something odd about the prime <math_exp>\". Added: Note that the above considerations allow us to answer the more general question: \"What is the largest positive integer <math_exp> such that for all primes <math_exp> with <math_exp>, <math_exp> divides <math_exp>?\"  (Answer: <math_exp>.) Added Later: I just saw this arxiv preprint which is entirely devoted to the observation made in the previous paragraph.  I guess the author does not follow this site... '}\n",
      "931 {'Id': '931', 'Type': 'answer', 'ParentId': '8', 'urls': ['http://en.wikipedia.org/wiki/Fibonacci_number#Matrix_form', 'http://en.wikipedia.org/wiki/Exponentiation_by_squaring'], 'exp': ['   \\\\begin{pmatrix}1&amp;1\\\\\\\\1&amp;0\\\\end{pmatrix}^n   =   \\\\begin{pmatrix}F_{n+1}&amp;F_{n}\\\\\\\\F_{n}&amp;F_{n-1}\\\\end{pmatrix} ', 'n'], 'Body': 'Also you can use the matrix equation for Fibonacci numbers: <math_exp> To calculate <math_exp>-th power of the matrix you can use  exponentiation by squaring algorithm. This approach could also be generalized on the case of arbitrary sequence with linear recurrence relation. '}\n",
      "932 {'Id': '932', 'Type': 'answer', 'ParentId': '59', 'urls': [], 'exp': [], 'Body': 'In case you want to implement it on some programming language. Usually there is a function called like atan2(y, x) which returns oriented angle between points (x, y) and (1, 0). In that case use could use This is usually more stable than using just arccos or arcsin. '}\n",
      "933 {'Id': '933', 'Type': 'answer', 'ParentId': '806', 'urls': ['http://math.uga.edu/~pete/4400MordellEquation.pdf'], 'exp': ['p', 'x^p + y^p = z^p', 'xyz \\\\neq 0', '\\\\mathbb{Q}[e^{\\\\frac{2 \\\\pi i}{p}}]', 'p', 'y^2 + k = x^3', 'k \\\\equiv 1,2 \\\\pmod 4', '\\\\mathbb{Z}[\\\\sqrt{-k}]', '3'], 'Body': \"As others have said, often what you want for a particular Diophantine application is that the class number of a certain number field be relatively prime to a certain number.  The famous example of this (as already noted by others) is Kummer's Theorem that for an odd prime <math_exp>, the Fermat equation <math_exp> has no integer solutions with <math_exp> if the ring of integers of <math_exp> has class number prime to <math_exp>. Another -- simpler -- nice example is the Mordell equation <math_exp>.  If <math_exp> and the ring <math_exp> has class number prime to <math_exp>, then all of the integer solutions to the Mordell equation can be found.  See Section 4 of http://math.uga.edu/~pete/4400MordellEquation.pdf for an exposition of this which is (I hope) reasonably elementary and accessible to undergraduates. \"}\n",
      "934 {'Id': '934', 'Type': 'answer', 'ParentId': '8', 'urls': ['http://livetoad.org/Courses/Documents/132d/Notes/fibonacci_matrix.pdf'], 'exp': [], 'Body': \"To expand on falagar's answer, my favourite proof of Binet's formula: ...Which I was going to post a summary of here, but remembered that everything was awful without Tex, so here is a link to some notes on it I found on google. The basic idea is to treat pairs of fibonnacci numbers, adjacent in the sequence, as vectors. Moving on to the next adjacent pair induces a linear transformation not unlike that of the matrix falagar posted. Calculating eigenvalues and eigenvectors can give a complete prediction of where an initial vector will find itself, predicting the whole sequence. It's quite a lot of work but I think it's rather illuminating. \"}\n",
      "935 {'Id': '935', 'Type': 'answer', 'ParentId': '594', 'urls': [], 'exp': ['z \\\\in \\\\mathbb{N}', 'z^2 + 1 \\\\equiv 0 \\\\pmod p', 'a^2 + b^2 = pm', 'x', 'y', 'x \\\\equiv a \\\\pmod m', 'y \\\\equiv b \\\\pmod m', 'x, y \\\\in [-m/2, m/2)', 'u = ax + by', 'v = ay - bx', 'u^2 + v^2 = (a^2 + b^2)(x^2 + y^2)', 'u', 'v', 'm', '(u/m)^2 + (v/m)^2 = p (x^2 + y^2)/m', '(x^2 + y^2)/m', 'x', 'y', 'a^2 + b^2 = pm', '(x^2 + y^2)/m', 'm/2', 'a', 'u', 'b', 'v', 'm=1', 'p', 'O(\\\\log p)', 'z', 'z^2 + 1', 'p'], 'Body': 'Here is another proof without complex numbers. We start with proving that there exists <math_exp> such that <math_exp>. We do this in the same way as Akhil Mathew. Let we have <math_exp>. Take <math_exp> and <math_exp> such that<br /> <math_exp> and<br /> <math_exp> and<br /> <math_exp>. Consider <math_exp> and <math_exp>. Then <math_exp>. Moreover, <math_exp> and <math_exp> are multiples of <math_exp>. Hence <math_exp>. <math_exp> is an integer because of the definition of <math_exp> and <math_exp> and that <math_exp>.<br /> Also <math_exp> is less than <math_exp>. Now we change <math_exp> by <math_exp> and <math_exp> by <math_exp> and continue this process until we get <math_exp>. Notice that this is quite efficient way to find representation of <math_exp> as a sum of two squares - it takes <math_exp> steps to find it provided we have found <math_exp> such that <math_exp> is multiple of <math_exp>. '}\n",
      "936 {'Id': '936', 'Type': 'question', 'Title': 'Looking for functions <span class=\"math-container\" id=\"9137\">f</span> with <span class=\"math-container\" id=\"9138\">\\\\int_{-\\\\infty}^{\\\\infty}f(x)\\\\,dx = 1</span>.', 'Tags': ['big-list', 'calculus', 'analysis'], 'AcceptedAnswerId': '937', 'urls': [], 'exp': [], 'Body': 'I am looking for functions and/or constants that when being integrated from minus infinity to infinity produce 1. I think the Dirac delta function is one example but perhaps there are some more? References  on useful material is also greatly appreciated. '}\n",
      "937 {'Id': '937', 'Type': 'answer', 'ParentId': '936', 'urls': ['http://en.wikipedia.org/wiki/List_of_probability_distributions#Continuous_distributions'], 'exp': ['\\\\int f(x)dx=A', 'g(x)=f(x)/A', '\\\\int g(x) dx=A/A=1'], 'Body': 'Any integrable functions that gives a finite nonzero answer can be modified to suit your need. Suppose <math_exp>, then let <math_exp>, automatically we have <math_exp>. (Actually, all continuous probability distribution function must have this property.) '}\n",
      "938 {'Id': '938', 'Type': 'answer', 'ParentId': '653', 'urls': ['http://prl.aps.org/abstract/PRL/v51/i1/p51_1'], 'exp': ['p_1, p_2, \\\\ldots, p_n', 'p_{r_1}, p_{r_2}, \\\\ldots, p_{r_m}', 'k_x, and k_y', 'x', 'y', '\\\\mathbb{Z}^2', 'T^2'], 'Body': \"Homotopy theory is useful in quantum mechanics when (for instance) talking about manifolds of Hamiltonians.  You might have a collection of Hamiltonians that depend continuously on some parameters <math_exp> such that the matrix representation of the Hamiltonian is periodic in some subset of the parameters <math_exp>.  From this, the fundamental group of the manifold of Hamiltonians can be computed, which has some physical ramifications. A concrete example of this would be the Hamiltonian that describes the quantum hall effect.  The quantum hall effect is the phenomena that the resistance in a 2-dimensional substrate exposed to a perpendicular electric field at close to zero temperature is quantized.  In condensed matter physics there is a notion called quasimomentum that can be thought of as being related to momentum but is a bit different.  We need something different from the classical definition of momentum because the classical definition depends on translation invariance, and in a crystal there is only discrete translation invariance.  In the quantum hall effect, we have two quasimomenta: <math_exp>, corresponding to quasimomentum in the <math_exp> and <math_exp> directions.  The Hamiltonian is periodic in both of these parameters, leading to a fundamental group of the manifold of Hamiltonians of <math_exp>, i.e. the fundamental group of the torus <math_exp>. There's a lot more to the topic than this.  This paper by Avron, Seiler, and Simon has more details: Homotopy and Quantization in Condensed Matter Physics \"}\n",
      "939 {'Id': '939', 'Type': 'question', 'Title': 'Counting primes', 'Tags': ['number-theory', 'algorithms', 'prime-numbers'], 'AcceptedAnswerId': '956', 'urls': ['http://en.wikipedia.org/w/index.php?title=Prime-counting_function'], 'exp': ['\\\\pi(x)', 'x', '\\\\pi(10^{23}) = 1,925,320,391,606,803,968,923', '\\\\pi(x)', 'x'], 'Body': 'Let <math_exp> be the number of primes not greater than <math_exp>. Wikipedia article says that <math_exp>. The question is how to calculate <math_exp> for large <math_exp> in a reasonable time? What algorithms do exist for that? '}\n",
      "940 {'Id': '940', 'Type': 'answer', 'ParentId': '939', 'urls': ['http://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle', 'http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes'], 'exp': [], 'Body': 'You can use inclusion exclusion principle to get a boost over the Eratosthenes sieve '}\n",
      "941 {'Id': '941', 'Type': 'question', 'Title': 'Approximation symbol: Is <span class=\"math-container\" id=\"9189\">\\\\pi \\\\approx 3.14\\\\dots</span> equivalent to <span class=\"math-container\" id=\"9190\">\\\\pi \\\\fallingdotseq 3.14\\\\dots</span>?', 'Tags': ['definition', 'approximation'], 'AcceptedAnswerId': '945', 'urls': ['http://en.wikipedia.org/wiki/Approximation#Mathematics'], 'exp': ['\\\\pi', '\\\\pi', '\\\\fallingdotseq', '\\\\pi', '\\\\approx', '\\\\pi \\\\approx 3.14\\\\dots', '\\\\pi \\\\fallingdotseq 3.14\\\\dots'], 'Body': \"This could be a trivial question, but what is exactly the difference of between these two expressions? Am I correct to state the both interchangeably whenever I need to express the approximation of <math_exp>? I'm bit confused as here, it states <math_exp> can be express by <math_exp> as it's not a rational number, but <math_exp> can also be expressed by a series (asymptotic), so it should be <math_exp> as well. <math_exp> <math_exp> \"}\n",
      "942 {'Id': '942', 'Type': 'question', 'Title': 'Meaning of closed points of a scheme', 'Tags': ['intuition', 'algebraic-geometry'], 'AcceptedAnswerId': '984', 'urls': [], 'exp': ['X', 'X'], 'Body': \"This is a question in Liu's book. Let <math_exp> be a quasi-compact scheme. Show that <math_exp> contains a closed point. Well I'm unable to do this question, so any help would be appreciated. This question also makes me curious to know about the meaning/use of closed points of a scheme in general - by that I mean a scheme which is not an algebraic variety/local scheme over a field, which has a geometric meaning. Thanks! \"}\n",
      "944 {'Id': '944', 'Type': 'question', 'Title': 'Proving the Riemann Hypothesis without revealing anything other than you proved it', 'Tags': ['computer-science', 'riemann-hypothesis'], 'AcceptedAnswerId': '950', 'urls': ['http://scottaaronson.com/blog/?p=152#'], 'exp': [], 'Body': \"Consider the following assertion from Scott Aaronson's blog: Supposing you do prove the Riemann   Hypothesis, it’s possible to convince   someone of that fact, without   revealing anything other than the fact   that you proved it. It’s also possible   to write the proof down in such a way   that someone else could verify it,   with very high confidence, having only   seen 10 or 20 bits of the proof. Can anyone explain where this result comes from? \"}\n",
      "945 {'Id': '945', 'Type': 'answer', 'ParentId': '941', 'urls': [], 'exp': ['\\\\approx', '\\\\approx'], 'Body': 'Any mathematical notation is ok as long as it is common knowledge in your community. For instance, I believe I fully understand the meaning of the <math_exp> symbol. However, I haven\\'t ever seen the second symbol you provided. To be on the sure side you should provide a definition of any relation symbol you don\\'t consider to be common knowledge. This may happen as a short remark (\"..., where <math_exp> denotes ...\") or maybe as a table of the used symbols in the front matter of your work. As with any definition in mathematics, there is no right or wrong in the symbol/notion/etc. you use, only proper or unsound definitions. Also: When in doubt, use the symbol that is used more commonly in the standard textbooks of your field. There is no benefit in being avant-garde at notation. '}\n",
      "946 {'Id': '946', 'Type': 'question', 'Title': 'Computation with a memory wiped computer', 'Tags': ['computer-science'], 'urls': ['http://scottaaronson.com/blog/?p=152'], 'exp': [], 'Body': \"Here is another result from Scott Aaronson's blog: If every second or so your computer’s   memory were wiped completely clean,   except for the input data; the clock;   a static, unchanging program; and a   counter that could only be set to 1,   2, 3, 4, or 5, it would still be   possible (given enough time) to carry   out an arbitrarily long computation —   just as if the memory weren’t being   wiped clean each second. This is   almost certainly not true if the   counter could only be set to 1, 2, 3,   or 4. The reason 5 is special here is   pretty much the same reason it’s   special in Galois’ proof of the   unsolvability of the quintic equation. Does anyone have idea of how to show this? \"}\n",
      "947 {'Id': '947', 'Type': 'answer', 'ParentId': '944', 'urls': ['http://en.wikipedia.org/wiki/Zero-knowledge_proof', 'http://euler.nmt.edu/~brian/students/pope.pdf'], 'exp': [], 'Body': 'The concept behind it is Zero knowledge proof. Wikipedia has a good article about it. The similar question was asked in paper \"How to Prove a Theorem So No One Else Can Claim It\" by M. Blum. Also this article discusses the question. '}\n",
      "948 {'Id': '948', 'Type': 'answer', 'ParentId': '942', 'urls': [], 'exp': ['F \\\\subset X', 'F', 'F', 'U=\\\\mathrm{Spec}\\\\, A', 'A', 'F', 'A', 'F', 'X'], 'Body': \"Zorn's lemma implies there is a minimal nonempty closed set <math_exp> with no proper closed subsets (because the closed sets have the finite intersection property in view of quasi-compactness).  It is sufficient to find a closed point in <math_exp>. Now <math_exp> is in itself a scheme, and it has an open subset <math_exp> for <math_exp> a ring. This must be all of <math_exp> by minimality.  A maximal ideal in <math_exp> gives a closed point in <math_exp>, hence in <math_exp>. There are people here who could give a much better answer to your other question, so I'll leave it. \"}\n",
      "949 {'Id': '949', 'Type': 'question', 'Title': 'Probability to find connected pixels', 'Tags': ['probability', 'graph-theory'], 'AcceptedAnswerId': '963', 'urls': [], 'exp': ['0', '1', '2D', '3D', '8', '4', '1', 'p', '1', 'k', 'n\\\\times n', 'k', 'k=3'], 'Body': \"Say I have an image, with pixels that can be either <math_exp> or <math_exp>. For simplicity, assume it's a <math_exp> image (though I'd be interested in a <math_exp> solution as well). A pixel has <math_exp> neighbors (if that's too complicated, we can drop to <math_exp>-connectedness). Two neighboring pixels with value <math_exp> are considered to be connected. If I know the probability <math_exp> that an individual pixel is <math_exp>, and if I can assume that all pixels are independent, how many groups of at least <math_exp> connected pixels should I expect to find in an image of size <math_exp>? What I really need is a good way of calculating the probability of <math_exp> pixels being connected given the individual pixel probabilities. I have started to write down a tree to cover all the possibilities up to <math_exp>, but even then, it becomes really ugly really fast. Is there a more clever way to go about this? \"}\n",
      "950 {'Id': '950', 'Type': 'answer', 'ParentId': '944', 'urls': ['http://en.wikipedia.org/wiki/Interactive_proof_system', 'http://en.wikipedia.org/wiki/Probabilistically_checkable_proof'], 'exp': [], 'Body': 'The correct answer has already been given by Akhil Mathew in the comments above. The topic belongs to the field of complexity theory in computer science. In complexity theory, there exists an intriguing concept for the problem of deciding whether a given word belongs to a given formal language or not: interactive proof systems. These systems model the interaction between resource-limited verifier (say, you or me) and an almighty prover (say, your much, much smarter older sister). The goal of the interaction is that the prover convinces the verifier from the fact that the given word is or is not an element of the language such that: There are a large number of theoretical results with respect to these interactive systems. These resuts include the follwoing two statements (given informally): Both of these concepts and results are highly non-trivial and beyond the scope of this forum. Of course, in the quote above Scott Aaronson is just using some everyday\\'s problem to illustrate these concepts. To use these results formally, would have to convert the task of \"proving the Riemann Hypothesis\" to a decision problem of formal languages, as is standard in complexity theory. EDIT: There is in fact a small modification in the model between both results stated above which I omitted. First, the interaction between one prover and the verifier can be generalized to multiple provers and a verifier. Next, there is a result that finds that the case of multiple provers can be equivalently reformulated as follows: The provers are removed from the protocol, rather there is a single (possibly very long) string which acts as a written proof for the word problem. Interaction is now looking at an arbitrarily chosen bit of this proof, and the verifier may choose the location of these bits randomly. This is called a Probabilistic Checkable Proof (hence, PCP). So ,in this scenario, the \"written proof\" of the Riemann Hypothesis would be interpreted as the proof string. '}\n",
      "951 {'Id': '951', 'Type': 'question', 'Title': 'Visualising functions from complex numbers to complex numbers', 'Tags': ['big-list', 'math-software', 'complex-analysis'], 'AcceptedAnswerId': '952', 'urls': [], 'exp': [], 'Body': 'I think that complex analysis is hard because graphs of even basic functions are 4 dimensional. Does anyone have any good visual representations of basic complex functions or know of any tools for generating them? '}\n",
      "952 {'Id': '952', 'Type': 'answer', 'ParentId': '951', 'urls': ['http://en.wikipedia.org/wiki/Domain_coloring', 'http://www.mai.liu.se/~halun/complex/domain_coloring-unicode.html'], 'exp': [], 'Body': 'One way that functions from C to C can be represented is to show the image of a grid.  That is, plot the images of the lines x = constant and y = constant under your function, where z = x + yi. Another is what Wikipedia calls domain coloring (see this article by Hans Lundmark for a more detailed exposition.  The idea here is to color the range of the complex function -- since color space is three-dimensional there are multiple ways to do this -- and then color each point in the domain by the color of the corresponding point in the range. '}\n",
      "953 {'Id': '953', 'Type': 'answer', 'ParentId': '936', 'urls': ['http://en.wikipedia.org/wiki/Normal_distribution'], 'exp': ['\\\\phi(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi}} e^{-x^2/2}'], 'Body': 'One good example is the standard Gaussian distribution, <math_exp>.  This is the most straightforward example of a continuous probability distribution function as mentioned by KennyTM above. '}\n",
      "954 {'Id': '954', 'Type': 'question', 'Title': 'What is the easiest known expression for inverse of Laplace transform?', 'Tags': ['analysis', 'integral-transforms'], 'urls': [], 'exp': [], 'Body': 'There is a very simple expression for the inverse of Fourier transform. What is the easiest known expression for the inverse Laplace transform? Moreover, what is the easiest way to prove it? '}\n",
      "955 {'Id': '955', 'Type': 'question', 'Title': 'Exact differential eqns and exact differential forms', 'Tags': ['ordinary-differential-equations'], 'AcceptedAnswerId': '958', 'urls': [], 'exp': [], 'Body': 'Is there any connection between exact differential equations and forms, or is the similarity in name just an accident? '}\n",
      "956 {'Id': '956', 'Type': 'answer', 'ParentId': '939', 'urls': ['http://primes.utm.edu/howmany.shtml'], 'exp': [], 'Body': 'The most efficient prime counting algorithms currently known are all essentially optimizations of the method developed by Meissel in 1870, e.g. see the discussion here http://primes.utm.edu/howmany.shtml '}\n",
      "957 {'Id': '957', 'Type': 'question', 'Title': 'Why are Hopf algebras called quantum groups?', 'Tags': ['terminology', 'noncommutative-geometry', 'quantum-groups'], 'AcceptedAnswerId': '960', 'urls': [], 'exp': [], 'Body': 'Why are noncommutative nonassociative Hopf algebras called quantum groups? This seems to be a purely mathematical notion and there is no quantum anywhere in it prima facie. '}\n",
      "958 {'Id': '958', 'Type': 'answer', 'ParentId': '955', 'urls': [], 'exp': [], 'Body': 'The differential equation \\\\begin{equation*} I(x,y)dx + J(x,y)dy = 0 \\\\end{equation*} is exact is by definition the same as this differential being an exact form. '}\n",
      "960 {'Id': '960', 'Type': 'answer', 'ParentId': '957', 'urls': [], 'exp': [], 'Body': 'One way that Hopf algebras come up is as the algebra of (real or complex) functions on a topological group.  The multiplication is commutative since it is just pointwise multiplication of functions.  However, in non-commutative geometry you want to replace the algebra of functions on a space with a non-commutative algebra, giving a non-commutative Hopf algebra. This relates to quantum mechanics because there the analog of the classical coordinate functions of position and momentum do not commute.  Therefore we think of the algebra of functions on a quantum \"space\" as being non-commutative. '}\n",
      "961 {'Id': '961', 'Type': 'answer', 'ParentId': '951', 'urls': ['http://www.wolframalpha.com/', 'http://bit.ly/bEZgsA', 'http://bit.ly/bfeLyS'], 'exp': [], 'Body': 'Open two browser windows side-by-side and use wolfram alpha. Recast your function f(z) as f(x+iy) and plot the real part in one window and the imaginary part in another. My example links provide plots for z3. '}\n",
      "963 {'Id': '963', 'Type': 'answer', 'ParentId': '949', 'urls': [], 'exp': [], 'Body': \"This looks a bit like percolation theory to me. In the 4-neighbour case, if you look at the dual of the image, the chance that an edge is connected (runs between two pixels of the same colour) is 1-2p+2p^2. I don't think you can get nice closed-form answer for your question, but maybe a computer can help with some Monte Carlo simulation? \"}\n",
      "964 {'Id': '964', 'Type': 'answer', 'ParentId': '939', 'urls': ['http://en.wikipedia.org/wiki/Sieve_of_Atkin', 'http://numbers.computation.free.fr/Constants/Primes/Pix/pixproject.html'], 'exp': ['pi(x)', 'pi(4\\\\times 10^{22})'], 'Body': 'The Sieve of Atkin is one of the fastest algorithm used to calculate <math_exp>. The Wikipedia page says that its complexity is O(N/ log log N). (edit) I found a distributed computation project which was able to calculate <math_exp>, maybe it could be useful. '}\n",
      "965 {'Id': '965', 'Type': 'answer', 'ParentId': '936', 'urls': [], 'exp': [], 'Body': \"The function which is 1 on the interval [0;1], and 0 elsewhere, is a non-continuous probability distribution function. The function which is 3 on [0;1] and -1 on (1;3], and so on and on. What kind of answer do you want? What kind of properties do you want your functions to have? There really are too many functions to list, since multiplying any function by a C^oo function with compact support and then applying Kenny's trick gives you an answer. \"}\n",
      "966 {'Id': '966', 'Type': 'answer', 'ParentId': '957', 'urls': [], 'exp': [], 'Body': 'I cannot comment, and this should be a comment... Observe that the question in your title and the question in the body of your question are quite different! A non-commutative non-cocommutative Hopf algebra is not the same thing as a non-commutative group, and quantum groups are usually associative. '}\n",
      "967 {'Id': '967', 'Type': 'question', 'Title': 'Learning Lambda Calculus', 'Tags': ['logic', 'learning', 'online-resources', 'lambda-calculus', 'type-theory'], 'urls': [], 'exp': [], 'Body': 'What are some good online/free resources (tutorials, guides, exercises, and the like) for learning Lambda Calculus? Specifically, I am interested in the following areas: (As I understand, this should provide a solid basis for the understanding of type theory.) Any advice and suggestions would be appreciated. '}\n",
      "968 {'Id': '968', 'Type': 'answer', 'ParentId': '936', 'urls': [], 'exp': ['F', '\\\\mathbb{R}', 'F(x)\\\\to l', 'l', 'x\\\\to \\\\infty', 'f(x) = \\\\frac{1}{2l}F&#39;(x)', 'F(x) = \\\\operatorname{arctan}(x)'], 'Body': 'If you take any odd function <math_exp> differentiable on <math_exp> and such that <math_exp> (with <math_exp> a nonzero real) for <math_exp>, then <math_exp> statisfies your request. For example <math_exp> '}\n",
      "969 {'Id': '969', 'Type': 'answer', 'ParentId': '936', 'urls': [], 'exp': [], 'Body': 'This should be a comment but I cannot comment... The Dirac delta function is not a function! '}\n",
      "970 {'Id': '970', 'Type': 'answer', 'ParentId': '936', 'urls': ['http://en.wikipedia.org/wiki/Wavelet'], 'exp': ['\\\\int_{-\\\\infty}^\\\\infty |\\\\psi(t)|^2 dt = 1'], 'Body': 'Most practical mother wavelet have square norm 1. <math_exp> '}\n",
      "971 {'Id': '971', 'Type': 'answer', 'ParentId': '936', 'urls': ['http://en.wikipedia.org/wiki/List_of_probability_distributions#Supported_on_the_whole_real_line'], 'exp': ['f(x)', '1', '[a,b]', 'g(x)=f(x)', '[a,b]', '0', 'f(x)', 'f(a)=f(b)=0', '&gt;0', '[-\\\\infty,\\\\infty]'], 'Body': 'Any function <math_exp> which integrates to <math_exp> over any range <math_exp> fits this bill, since we can define <math_exp> on <math_exp>, and <math_exp> everywhere else. Even if you only want continuous functions, restricting ourselves above to <math_exp> where <math_exp> still satisfies this. If you want continuous functions strictly <math_exp> everywhere, these are known as probability distributions (continuous on <math_exp>).  A large list of such functions can be found here.  A few more notable examples are: '}\n",
      "972 {'Id': '972', 'Type': 'answer', 'ParentId': '946', 'urls': ['http://scottaaronson.com/blog/?p=152'], 'exp': [], 'Body': \"As Scott himself states in comments section of the post in question (comment #9): (4) Width-5 branching programs can compute NC1 (Barrington 1986); corollary pointed out by Ogihara 1994 that width-5 bottleneck Turing machines can compute PSPACE Unfortunately, I don't have any ideas how this is proved. \"}\n",
      "973 {'Id': '973', 'Type': 'answer', 'ParentId': '967', 'urls': ['http://mitpress.mit.edu/sicp/'], 'exp': [], 'Body': 'It might be nice to work through Structure and Interpretation of Computer Programs, which is available online for free. This book is an introduction to computer science and the programming language Scheme, which is a flavor of the programming language Lisp, which is based on the lambda calculus. Although it is not strictly a book about the lambda calculus, it might be fun or useful to gain some hands-on and \"practical\" experience with the lambda calculus by reading some of this book and working through some of its exercises. '}\n",
      "974 {'Id': '974', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': \"I would read a book about Perelman's proof of the Poincaré conjecture (or even the papers themselves). Oh, you mean the book had to be written when I was starting? \"}\n",
      "976 {'Id': '976', 'Type': 'answer', 'ParentId': '262', 'urls': ['http://books.google.com/books?id=R6E0AAAAMAAJ&amp;dq=flatland&amp;printsec=frontcover&amp;source=bn&amp;hl=en&amp;ei=eE9QTLKVF5G5rAex79zVDQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=11&amp;ved=0CEQQ6AEwCg#v=onepage&amp;q&amp;f=false'], 'exp': [], 'Body': 'I am not a mathematician but Flatland: A Romance of Many Dimensions blew my mind. I read it when I was a college student in a class on Special Relativity and wish I had read it way earlier. '}\n",
      "977 {'Id': '977', 'Type': 'answer', 'ParentId': '942', 'urls': ['http://www.neverendingbooks.org/index.php/mumfords-treasure-map.html', 'http://books.google.com/books?id=ZhzXJHUgcRUC&amp;lpg=PA67&amp;ots=aVQoeMkBwc&amp;dq=analytification&amp;pg=PA61#v=onepage&amp;q=analytification&amp;f=false'], 'exp': ['\\\\operatorname{Spec} \\\\mathbb{Z}[x]', '\\\\mathbb{C}'], 'Body': 'Closed points should be thought of as being \"actual points\", whereas non-closed points can correspond to all sorts of different things: subvarieties, \"fat\" or \"fuzzy\" points, generic points, etc. You might be interested in reading this blog post about Mumford\\'s drawing of <math_exp>. One possible way to justify the claim that closed points are the \"actual points\" is the fact that if we have, for instance, a smooth variety over <math_exp>, then its analytification will be a complex manifold. The closed points of the former will then correspond exactly to the points of the latter. '}\n",
      "978 {'Id': '978', 'Type': 'question', 'Title': 'How to prove and interpret <span class=\"math-container\" id=\"9281\">\\\\operatorname{rank}(AB) \\\\leq \\\\operatorname{min}(\\\\operatorname{rank}(A), \\\\operatorname{rank}(B))</span>?', 'Tags': ['linear-algebra', 'matrices', 'inequality', 'matrix-rank'], 'AcceptedAnswerId': '981', 'urls': [], 'exp': ['A', 'B', '\\\\operatorname{rank}(AB) \\\\leq \\\\operatorname{min}(\\\\operatorname{rank}(A), \\\\operatorname{rank}(B)).', '\\\\operatorname{rank}(AB) \\\\leq \\\\operatorname{rank}(B)', 'AB', '\\\\operatorname{ker}(B) \\\\subseteq \\\\operatorname{ker}(AB)', '\\\\operatorname{rank}(AB) \\\\leq \\\\operatorname{rank}(A)'], 'Body': 'Let <math_exp> and <math_exp> be two matrices which can be multiplied. Then <math_exp> I proved <math_exp> interpreting <math_exp> as a composition of linear maps, observing that <math_exp> and using the kernel-image dimension formula. This also provides, in my opinion, a nice interpretation: if non stable, under subsequent compositions the kernel can only get bigger, and the image can only get smaller, in a sort of loss of information. How to manage <math_exp>? Is there a nice interpretation like the previous one? '}\n",
      "979 {'Id': '979', 'Type': 'answer', 'ParentId': '951', 'urls': ['http://www.youtube.com/watch?v=JX3VmDgiFnY'], 'exp': [], 'Body': 'For Moebius transformations, check out this nice YouTube video. '}\n",
      "980 {'Id': '980', 'Type': 'question', 'Title': \"What's the difference between open and closed sets?\", 'Tags': ['general-topology', 'terminology', 'intuition'], 'urls': [], 'exp': [], 'Body': \"What's the difference between open and closed sets? Especially with relation to topology - rigorous definitions are appreciated, but just as important is the intuition! \"}\n",
      "981 {'Id': '981', 'Type': 'answer', 'ParentId': '978', 'urls': [], 'exp': [], 'Body': 'Yes. If you think of A and B as linear maps, then the domain of A is certainly at least as big as the image of B. Thus when we apply A to either of these things, we should get \"more stuff\" in the former case, as the former is bigger than the latter. '}\n",
      "982 {'Id': '982', 'Type': 'answer', 'ParentId': '978', 'urls': [], 'exp': ['f:X\\\\to Y', 'g:Y\\\\to Z', '|g(f(X))| \\\\leq \\\\min \\\\{ |f(X)|, |g(Y)| \\\\}.'], 'Body': 'Prove first that if <math_exp> and <math_exp> are functions between finite sets, then  <math_exp> Then use the same idea. '}\n",
      "983 {'Id': '983', 'Type': 'answer', 'ParentId': '980', 'urls': [], 'exp': [], 'Body': \"The rigorous definition of open and closed sets is fundamental to topology: you define a topology by saying what its open sets are.  From this perspective, open and closed sets are axiomatic, like points and lines in geometry.  In any case, closed sets are the complements of open sets and vice versa. The most familiar example of open sets would be open intervals on the real line, intervals of the form {x : a &lt; x &lt; b}. Such sets and their arbitrary unions define the standard topology on the real line. Note that the paragraph above describes the standard topology, but not the only one.  You could put a very different topology on the real line.  That's because a topology is determined by what you call open sets and not by the underlying space per se.  For example, another topology on the real line defines a set to be open if its complement has only a finite number of points. \"}\n",
      "984 {'Id': '984', 'Type': 'answer', 'ParentId': '942', 'urls': [], 'exp': ['$x$', '$X$', '$y$', '$x$', '$y$', '$y$', '$x$', '$\\\\mathfrak p$', '$\\\\mathfrak q$', '$A$', '$\\\\mathfrak q$', '$\\\\mathfrak p$', '$\\\\text{Spec} A$', '$\\\\mathfrak q$', '$\\\\mathfrak p$', '$A/\\\\mathfrak p$', '$A/\\\\mathfrak q$', '$A$', '$\\\\mathbb C[x,y]$', '$\\\\mathfrak p$', '$(x-1)$', '$\\\\mathfrak q$', '$(x-1,y)$', '$A/\\\\mathfrak p$', '$x$', '$1$', '$x-1 = 0$', '$y$', '$A/\\\\mathfrak q$', '$x$', '$y$', '$x$', '$1$', '$y$', '$0$', '$\\\\mathfrak q$', '$A$', '$\\\\text{Spec} A$', '$x$', '$y$', '$B = \\\\mathbb Z[x,y]$', '$\\\\mathfrak p$', '$\\\\mathfrak q$', '$x-1$', '$(x-1,y)$', '$\\\\mathfrak q$', '$x$', '$y$', '$\\\\mathbb Z$', '$\\\\mathbb C$', '$x$', '$y$', '$5$', '$\\\\mathfrak r = (x-1,y,5)$', '$B$', '$\\\\mathfrak q$', '$\\\\mathfrak r$', '$\\\\mathbb Z$'], 'Body': 'Kevin Lin\\'s answer regarding the meaning of closed points is quite reasonable, especialy in the case when the scheme in question underlies a classical variety.  I want to add some additional remarks and examples for thinking about more general schemes. Here are some tautological remarks: recall that a point <math_exp> in a scheme <math_exp> is called a specialization of <math_exp> if <math_exp> lies in the Zariski closure of <math_exp> (and <math_exp> is called a generalization of <math_exp>).  So tautologically, a closed points is one that cannot be specialized any further (just as a generic point cannot be generalized any further).  What does specialization really mean: ring theoretically, it means taking the image under a homomorphism; so if <math_exp> and <math_exp> are prime ideals of a ring <math_exp>, then <math_exp> is a specialization of <math_exp> in <math_exp> if and only if <math_exp> contains <math_exp>, i.e. if <math_exp> surjects onto <math_exp>.  It is perhaps best to think of an example: say <math_exp> is <math_exp>, <math_exp> is the prime ideal gen\\'d by <math_exp> and <math_exp> is the prime (actually maximal ideal) gen\\'d by <math_exp>.  Then in <math_exp>, we have \"specialized\" the value of <math_exp> to equal <math_exp> (because we have declared <math_exp>) but <math_exp> is still a free variable.  When we pass to the further quotient <math_exp>, we have specialized both <math_exp> and <math_exp>: <math_exp> is specialized to <math_exp> and <math_exp> is specialized to <math_exp>.   At this point, we can\\'t specialize any more; technically, this is because <math_exp> is a maximal ideal of <math_exp>, so a closed point of <math_exp>; intuitively, it is because both <math_exp> and <math_exp> have now both been \"specialized\" to actual numbers, and so we can\\'t specialize any further. But suppose now we set <math_exp>, and take <math_exp> and <math_exp> to be the same, i.e. gen\\'d by <math_exp> and by <math_exp> respectively.  Then <math_exp> is not maximal; there is more capacity for specialization. How is this?  Well, <math_exp> and <math_exp> are now taking values in <math_exp> (rather than the field <math_exp>) and so we can also reduce both <math_exp> and <math_exp> modulo some prime, say <math_exp>; this gives a prime ideal <math_exp> in <math_exp> containing <math_exp>.  Now <math_exp> is maximal, and so we are done specializing. So if you have a scheme that is finite type over <math_exp>, the closed points will correspond to  \"actual points\", in Kevin\\'s terminology, but defined over finite fields.  The points of the scheme whose coordinates are integers, say, will not be closed.  One has the choice of thinking them of them as \"actual points\" which nevertheless can be specialized further by reducing modulo primes, or as subvarieties rather than \"actual points\", by identifying them with their Zariski closures (for a picture of this, see the drawing of Mumford that Kevin links to). '}\n",
      "986 {'Id': '986', 'Type': 'answer', 'ParentId': '980', 'urls': [], 'exp': [], 'Body': 'Intuitively speaking, an open set is a set without a border: every element of the set has, in its neighborhood, other elements of the set. If, starting from a point of the open set, you move away a little, you never exit the set. A closed set is the complement of an open set (i.e. what stays \"outside\" from the open set). Note that some set exists, that are neither open nor closed. '}\n",
      "987 {'Id': '987', 'Type': 'answer', 'ParentId': '980', 'urls': [], 'exp': [], 'Body': 'A set X is open if for every point p in X, there exists a neighborhood (open ball) N of p such that N is a subset of X. We call the point p of the set X a limit point if every neighborhood of p has another point q which is also in X. The set X is closed if every limit point of X is a point of X. A set can be both open and closed, and such sets are occasionally termed \"clopen.\" Trivial examples of clopen sets are the empty set (since it has no points, both the above definitions are vacuously true) and the set of all real numbers. You can in turn visualize an open set in R as an open interval on the real line, and a closed set as a closed interval on the real line. '}\n",
      "988 {'Id': '988', 'Type': 'answer', 'ParentId': '951', 'urls': ['http://mathworld.wolfram.com/ConformalMapping.html'], 'exp': [], 'Body': \"The graphs in the middle of the MathWorld page on Conformal Mapping show examples of the first method in Michael Lugo's answer as well as something somewhat similar to the second method in that answer. \"}\n",
      "989 {'Id': '989', 'Type': 'answer', 'ParentId': '250', 'urls': ['http://en.wikipedia.org/wiki/Kakeya_set'], 'exp': [], 'Body': \"What is the smallest area of a parking lot in which a car (that is, a segment of) can perform a complete turn (that is, rotate 360 degrees)? (This is obviously the Kakeya Needle Problem. Fairly easy to explain, models an almost reasonable real-life scenario, and has a very surprising answer as you probably know - the lot can have as small an area as you'd like). Wikipedia entry: Kakeya Set. \"}\n",
      "990 {'Id': '990', 'Type': 'answer', 'ParentId': '980', 'urls': [], 'exp': [], 'Body': 'An open set is a set S for which, given any of its element A, you can find a ball centered in A and whose points are all in S. A closed set is a set S for which, if you have a sequence of points in S who tend to a limit point B, B is also in S. Intuitively, a closed set is a set which contains its own boundary, while an open set is a set where you are able not to leave it if you move just a little bit. '}\n",
      "992 {'Id': '992', 'Type': 'answer', 'ParentId': '3', 'urls': ['http://www.scienceofbetter.org/podcast/'], 'exp': [], 'Body': 'The Science of Better is a good applied math podcast.  Features applications of operations research to practical problems. '}\n",
      "993 {'Id': '993', 'Type': 'answer', 'ParentId': '3', 'urls': ['http://www.travelsinamathematicalworld.co.uk/', 'http://www.pulse-project.org/pulsemathsmaths/', 'http://itunes.apple.com/us/podcast/strongly-connected-components/id334982081', 'http://cppodcast.libsyn.com/'], 'exp': [], 'Body': \"Peter Rowlett has a couple mathematical podcasts.  Travels in a Mathematical World produced 64 episodes, but recently stopped.  He has a new podcast Math / Maths that he co-hosts with Samuel Hansen. Samuel Hansen also has a couple other podcasts: Strongly Connected Components and Permutations and Combinations. Strongly Connected Components is more mathematical and often features interviews.  I've only listened to Permutations and Combinations once or twice.  I believe it's more of a comedy show. \"}\n",
      "994 {'Id': '994', 'Type': 'answer', 'ParentId': '903', 'urls': [], 'exp': [], 'Body': 'Try Simmons, Introduction to Topology and Modern Analysis. '}\n",
      "995 {'Id': '995', 'Type': 'answer', 'ParentId': '370', 'urls': [], 'exp': [], 'Body': \"First Chapter of Jacobson's Basic Algebra -II. \"}\n",
      "996 {'Id': '996', 'Type': 'question', 'Title': 'What is the difference between matrix theory and linear algebra?', 'Tags': ['linear-algebra'], 'urls': ['https://mathoverflow.net/questions/11669/'], 'exp': [], 'Body': \"I have lifted this from Mathoverflow since it belongs here. Hi, Currently, I'm taking matrix theory, and our textbook is Strang's Linear Algebra. Besides matrix theory, which all engineers must take, there exists linear algebra I and II for math majors. What is the difference, if any, between matrix theory and linear algebra? Thanks! kolistivra \"}\n",
      "997 {'Id': '997', 'Type': 'answer', 'ParentId': '329', 'urls': [], 'exp': [], 'Body': \"Problems in Algebraic Number Theory is written in a style I'd like to see in more textbooks \"}\n",
      "998 {'Id': '998', 'Type': 'question', 'Title': 'Best Algebraic Geometry text book? (other than Hartshorne)', 'Tags': ['big-list', 'reference-request', 'algebraic-geometry'], 'urls': ['https://mathoverflow.net/questions/2446'], 'exp': [], 'Body': \"Lifted from Mathoverflow: I think (almost) everyone agrees that Hartshorne's Algebraic Geometry is still the best. Then what might be the 2nd best? It can be a book, preprint, online lecture note, webpage, etc. One suggestion per answer please. Also, please include an explanation of why you like the book, or what makes it unique or useful. \"}\n",
      "1002 {'Id': '1002', 'Type': 'question', 'Title': 'Fourier transform for dummies', 'Tags': ['fourier-analysis', 'fourier-transform'], 'urls': ['https://mathoverflow.net/q/446'], 'exp': [], 'Body': \"What is the Fourier transform? What does it do? Why is it useful (in math, in engineering, physics, etc)? This question is based on the question of Kevin Lin, which didn't quite fit in Mathoverflow. Answers at any level of sophistication are welcome. \"}\n",
      "1008 {'Id': '1008', 'Type': 'answer', 'ParentId': '996', 'urls': [], 'exp': [], 'Body': \"My answer from the MO thread: A matrix is just a list of numbers, and you're allowed to add and multiply matrices by combining those numbers in a certain way. When you talk about matrices, you're allowed to talk about things like the entry in the 3rd row and 4th column, and so forth. In this setting, matrices are useful for representing things like transition probabilities in a Markov chain, where each entry indicates the probability of transitioning from one state to another. You can do lots of interesting numerical things with matrices, and these interesting numerical things are very important because matrices show up a lot in engineering and the sciences. In linear algebra, however, you instead talk about linear transformations, which are not (I cannot emphasize this enough) a list of numbers, although sometimes it is convenient to use a particular matrix to write down a linear transformation. The difference between a linear transformation and a matrix is not easy to grasp the first time you see it, and most people would be fine with conflating the two points of view. However, when you're given a linear transformation, you're not allowed to ask for things like the entry in its 3rd row and 4th column because questions like these depend on a choice of basis. Instead, you're only allowed to ask for things that don't depend on the basis, such as the rank, the trace, the determinant, or the set of eigenvalues. This point of view may seem unnecessarily restrictive, but it is fundamental to a deeper understanding of pure mathematics. \"}\n",
      "1012 {'Id': '1012', 'Type': 'question', 'Title': 'Resources for getting maths on to the web', 'Tags': ['soft-question'], 'urls': ['https://mathoverflow.net/questions/5095/', 'http://terrytao.wordpress.com/2009/10/29/displaying-mathematics-on-the-web/', 'http://terrytao.wordpress.com/2009/11/04/displaying-maths-online-ii/'], 'exp': [], 'Body': \"An off-topic question posed at Mathoverflow by Andrew Stacey, but one which fits here: One thing that came out of Terry Tao's recent blog posts on this matter (first post and follow up) is that it's hard to get an overview of all the different ways of getting one's amazing mathematics onto the web.  I thought it'd be useful to gather together a list of such.  This meant to be a list of ways to do it, not examples of where it's already being done. Standard community wiki rules: one thing per answer and feel free to edit other's answers. Additional rules: it'd be useful to have a little more than just links.  A brief description, pros and cons (be objective), platforms (does it only work on Linux, sort of thing) - things that might help someone decide which things to examine further. \"}\n",
      "1016 {'Id': '1016', 'Type': 'answer', 'ParentId': '1002', 'urls': [], 'exp': [], 'Body': 'I\\'ll give an engineering answer. If you have a time series that you think is the result of a additive collection of periodic function, the Fourier transform will help you determine what the dominant frequencies are. This is the way guitar tuners work. The perform and FFT on the sound data and pick out the frequency with the greatest power (squares of the real and imaginary parts) and consider that the \"note.\" This is called the fundamental frequency. There are many other uses, so you might want to add big list as a tag. '}\n",
      "1017 {'Id': '1017', 'Type': 'question', 'Title': 'Proving the Shoelace Method at the Precalculus Level', 'Tags': ['geometry', 'algebra-precalculus', 'contest-math', 'analytic-geometry'], 'AcceptedAnswerId': '1262', 'urls': ['http://en.wikipedia.org/wiki/Shoelace_Method'], 'exp': ['(x_1,y_1)', '(x_2,y_2)', '2\\\\times 2', '(x_1,y_1)', '(x_2,y_2)', '\\\\frac{1}{2}\\\\cdot\\\\left|x_1\\\\cdot y_2 - x_2\\\\cdot y_1\\\\right|'], 'Body': 'Using only precalculus mathematics (including that the area of the triangle with vertices at the origin, <math_exp>, and <math_exp> is half of the absolute value of the determinant of the <math_exp> matrix of the vertices <math_exp> and <math_exp>, <math_exp>) how can one prove that the shoelace method works for all non-self-intersecting polygons? '}\n",
      "1018 {'Id': '1018', 'Type': 'question', 'Title': 'Examples of well-displayed mathematics on the internet', 'Tags': ['soft-question', 'online-resources'], 'urls': ['https://mathoverflow.net/questions/3036/'], 'exp': [], 'Body': \"An off-topic question asked at Mathoverflow by Andrew Stacey; but one which fits here: I'm interested in hearing of examples of mathematical (or, at a pinch, scientific) websites with serious content where the design of the website actually makes it easy to read and absorb the material. To be absolutely clear, the mathematical content of the website should be on the website itself and not in an electronic article (so meta-sites that make it easy to find material, like MathSciNet or the arXiv, don't count). Edit: I'm extending this to non-internet material. I want examples where the design of the document/website/whatever actually helped when reading the material. As a little background, I know that LaTeX is meant to help us separate content from context and concentrate on each one in turn, but I often feel when reading an article that the author has concentrated solely on the content and left all of the context to TeX. This is most obvious with websites where there are some really well-designed websites to compare with, but holds as well with articles. \"}\n",
      "1019 {'Id': '1019', 'Type': 'answer', 'ParentId': '1002', 'urls': [], 'exp': [], 'Body': 'You could think of a Fourier series expanding a function as a sum of sines and cosines analogous to the way a Taylor series expands a function as a sum of powers. Or you could think of the Fourier series as a change of variables.  A fundamental skill in engineering and physics is to pick the coordinate system that makes your problem simplest.  Since the derivatives of sines and cosines are more sines and cosines, Fourier series are the right \"coordinate system\" for many problems involving derivatives. '}\n",
      "1022 {'Id': '1022', 'Type': 'answer', 'ParentId': '1012', 'urls': ['http://mathurl.com'], 'exp': [], 'Body': 'http://mathurl.com is handy if you want to send someone a quick link to a mathematical expression. '}\n",
      "1024 {'Id': '1024', 'Type': 'answer', 'ParentId': '1018', 'urls': ['http://tutorial.math.lamar.edu/'], 'exp': [], 'Body': \"For students in lower level university courses, there are two amazing resources. Paul's online math notes. My linear algebra book was complete garbage. I pretty much used these notes to get through the class. It explains everything very well and does not assume that you already know graduate level mathematics. Presentation wise, its very simple. A plain HTML website, however content is king. And ofcourse, Khan Academy. I think everyone knows about Khan. Last but not least, I would like to add the MIT online courses. I didn't like them that much but they did help me. Maybe its just me but I can not learn anything while staring at a computer screen. \"}\n",
      "1025 {'Id': '1025', 'Type': 'question', 'Title': 'Usefulness of Conic Sections', 'Tags': ['algebra-precalculus', 'conic-sections', 'education'], 'AcceptedAnswerId': '1027', 'urls': [], 'exp': [], 'Body': \"Conic sections are a frequent target for dropping when attempting to make room for other topics in advanced algebra and precalculus courses.  A common argument in favor of dropping them is that typical first-year calculus doesn't use conic sections at all.  Do conic sections come up in typical intro-level undergraduate courses?  In typical prelim grad-level courses?  If so, where? \"}\n",
      "1026 {'Id': '1026', 'Type': 'question', 'Title': \"Number of colorings of cube's faces\", 'Tags': ['combinatorics'], 'AcceptedAnswerId': '1028', 'urls': [], 'exp': [], 'Body': \"How many ways are there to color faces of a cube with N colors if  two colorings are the same if it's possible to rotate the cube such that one coloring goes to another? \"}\n",
      "1027 {'Id': '1027', 'Type': 'answer', 'ParentId': '1025', 'urls': ['http://www-math.mit.edu/~poonen/papers/millennial.pdf', 'http://www.fen.bilkent.edu.tr/~franz/publ/conics.pdf'], 'exp': [], 'Body': 'Conic sections are basic examples in algebraic geometry, since they are (the real forms of) curves of genus zero.  As such, they are also basic examples in number theory, since it is easy to determine the rational points on a conic section, and this is a good warm-up for studying more complicated Diophantine equations.  In fact, curves of genus zero are the only class of variety for which an algorithm provably exists to determine the rational points!  Even for the next hardest case, elliptic curves, there are no algorithms which provably always work. A great survey of these topics is Bjorn Poonen\\'s Computing rational points on curves. Edit:  There is also Franz Lemmermeyer\\'s Conics - a poor man\\'s elliptic curves, which explains how certain conics can be thought of as \"degenerate\" elliptic curves. '}\n",
      "1028 {'Id': '1028', 'Type': 'answer', 'ParentId': '1026', 'urls': ['http://en.wikipedia.org/wiki/Burnside_lemma'], 'exp': [], 'Body': 'The number of different colorings is equal to \\\\begin{equation*} \\\\frac{n^6 + 3n^4 + 12n^3 + 8n^2}{24}. \\\\end{equation*} You can get this number using Burnside lemma. The wikipedia article contains solution of your problem as well. '}\n",
      "1029 {'Id': '1029', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://www.johndcook.com/modes_of_convergence.html'], 'exp': [], 'Body': \"Three examples suffice to show why some modes of convergence don't imply other modes of convergence: pointwise convergence, Lp norm convergence, convergence in measure, etc.  See the counterexamples section here. \"}\n",
      "1030 {'Id': '1030', 'Type': 'answer', 'ParentId': '740', 'urls': [], 'exp': ['sin(1/x)', '(0,\\\\infty)'], 'Body': 'When I was first learning calculus, the fact that <math_exp> is continuous on the set <math_exp> gave me a headache. '}\n",
      "1031 {'Id': '1031', 'Type': 'answer', 'ParentId': '544', 'urls': [], 'exp': [], 'Body': \"Markov chains are used in Markov Chain Monte Carlo (MCMC).  This computational technique is extremely common in Bayesian statistics. In Bayesian statistics, you want to compute properties of a posterior distribution.  You'd like to draw independent samples from this distribution, but often this is impractical.  So you construct a Markov chain that has as its limiting distribution the the distribution you want.  So, for example, to get the mean of your posterior distribution you could take the mean of the states of your Markov chain.  (Ergodic theory blesses this process.) \"}\n",
      "1032 {'Id': '1032', 'Type': 'answer', 'ParentId': '668', 'urls': [], 'exp': [], 'Body': \"You could think of a determinant as a volume.  Think of the columns of the matrix as vectors at the origin forming the edges of a skewed box.  The determinant gives the volume of that box.  For example, in 2 dimensions, the columns of the matrix are the edges of a rhombus. You can derive the algebraic properties from this geometrical interpretation.  For example, if two of the columns are linearly dependent, your box is missing a dimension and so it's been flattened to have zero volume. \"}\n",
      "1033 {'Id': '1033', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://www.johndcook.com/blog/2008/04/23/fibonacci-numbers-at-work/'], 'exp': [], 'Body': \"Here's an example of Fibonacci numbers applied to numerical integration. \"}\n",
      "1034 {'Id': '1034', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://www.johndcook.com/blog/2009/05/19/golden-ratio-rational-approximation/'], 'exp': [], 'Body': 'Here\\'s a humorous application of Fibonacci numbers to breastfeeding twins.  Even though the post is somewhat of a joke, it makes a serious point about what are called \"almost periodic functions.\" '}\n",
      "1035 {'Id': '1035', 'Type': 'answer', 'ParentId': '213', 'urls': ['http://rads.stackoverflow.com/amzn/click/0521358809', 'http://rads.stackoverflow.com/amzn/click/052154677X'], 'exp': [], 'Body': \"I wish I'd understood the importance of inequalities earlier.  I wish I'd carefully gone through the classic book Inequalities by Hardy, Littlewood, and Poyla early on.  Another good book is The Cauchy-Schwarz Masterclass. You can study inequalities as a subject in their own right, often without using advanced math.  But they're critical techniques for advanced math. \"}\n",
      "1036 {'Id': '1036', 'Type': 'answer', 'ParentId': '588', 'urls': ['http://en.wikipedia.org/wiki/Radius_of_convergence'], 'exp': [], 'Body': 'To your question regarding radius of convergence, Wikipedia gives a good answer. '}\n",
      "1037 {'Id': '1037', 'Type': 'question', 'Title': 'Why are derivatives specified as d/dx?', 'Tags': ['calculus', 'reference-request', 'notation', 'math-history'], 'AcceptedAnswerId': '1047', 'urls': [], 'exp': [], 'Body': \"Is the purpose of the derivative notation d/dx strictly for symbolic manipulation purposes? I remember being confused when I first saw the notation for derivatives - it looks vaguely like there's some division going on and there are some fancy 'd' characters that are added in... I recall thinking that it was a lot of characters to represent an action with respect to one variable. Of course, once you start moving the dx around it makes a little more sense as to why they exist - but is this the only reason? Any history lesson or examples  where this notation is helpful or unhelpful is appreciated. \"}\n",
      "1038 {'Id': '1038', 'Type': 'answer', 'ParentId': '1037', 'urls': [], 'exp': [], 'Body': 'If you have access to it, the book A History of Mathematical Notations, by Florian Cajori, has a pretty detailed description of the history of notations for derivatives in its second volume. '}\n",
      "1039 {'Id': '1039', 'Type': 'answer', 'ParentId': '1037', 'urls': ['http://www.vendian.org/mncharity/dir3/dxdoc/'], 'exp': [], 'Body': \"Adding to Mariano's answer, its worthwhile to look at the modern notation and compare it with the older one. Here is a link that explains it very well.  http://www.vendian.org/mncharity/dir3/dxdoc/ \"}\n",
      "1040 {'Id': '1040', 'Type': 'answer', 'ParentId': '1002', 'urls': [], 'exp': [], 'Body': \"A more complicated answer (yet it's going to be imprecise, because I haven't touched this in 15 years...) is the following. In a 3-dimentional space (for example) you can represent a vector v by its end point coordinates, x, y, z, in a very simple way. You choose three vectors which are of unit length and orthogonal with each other (a base), say i, j and k, and calculate the coordinates as such: x = v ∙ i y = v ∙ j z = v ∙ k In multidimentional space, the equations still hold. In a discrete infinite space, the coordinates and the base vectors become a sequence. The dot product becomes an infinite sum. In a continuous infinite space (like the space of good functions) the coordinates and the bases become functions and the dot product an infinite integral. Now, the Fourier transform is exactly this kind of operation (based on a set of base functions which are basically a set of sines and cosines). In other words, it is a different representation of the same function in relation to a particular set of base functions. As a consequence, for example, functions of time, represented against functions of time and space (in other words integrated over time multiplied by functions of space and time), become functions of space, and so on. Hope it helps! \"}\n",
      "1041 {'Id': '1041', 'Type': 'answer', 'ParentId': '998', 'urls': ['http://rads.stackoverflow.com/amzn/click/0387977163'], 'exp': [], 'Body': \"Algebraic Geometry: A First Course by Joe Harris is a very good book that sits in that region between undergraduate treatments and the prerequisites of Hartshorne. In particular, one does not need to know much commutative algebra to get a lot out of Harris's book. Harris himself recommends reading Hartshorne after his book for the theory of schemes. \"}\n",
      "1042 {'Id': '1042', 'Type': 'answer', 'ParentId': '980', 'urls': [], 'exp': [], 'Body': 'I will not reiterate the very nice definitions found in the other answers, however I think that these \"practical\" definitions might help you as well on an intuitive level. Open sets are typically used as domains for functions, as they are more useful for analysing \"continuous\" properties like differentiability. Also they don\\'t have nasty borders (hence you don\\'t have to deal with functions which are well behaved only on one side of the edge). Closed sets are useful because, if they are limited, they are compact. '}\n",
      "1043 {'Id': '1043', 'Type': 'answer', 'ParentId': '1025', 'urls': [], 'exp': [], 'Body': 'The study of parabolas (with axis parallel to y-axis) is useful when you have to solve 2nd degree inequations. '}\n",
      "1044 {'Id': '1044', 'Type': 'answer', 'ParentId': '1037', 'urls': [], 'exp': ['dy', 'dx', 'y', 'x', '\\\\dot y', 'D', 'D(\\\\sin(x))=\\\\cos(x)'], 'Body': 'This is the Leibniz notation, which is based on the ratio of \"infinitesimals\". <math_exp> and <math_exp> are, respectively, the infinitesimal increment of the dependent variable <math_exp> and the infinitesimal increment of the variable <math_exp>. There are other notations: Newton notation, which puts a dot over the variable name, as in <math_exp>, and Cauchy notation, which uses the operator <math_exp>, as in <math_exp>. '}\n",
      "1045 {'Id': '1045', 'Type': 'question', 'Title': 'How do I convert from Cartesian to conical coordinates?', 'Tags': ['geometry', 'coordinate-systems'], 'AcceptedAnswerId': '1055', 'urls': ['http://www.math.montana.edu/frankw/ccp/multiworld/multipleIVP/cylindrical/body.htm#skip3'], 'exp': [], 'Body': 'I have some polygons I would like to map onto the face of a cone. I can see from this page that I can convert  the points of the polygon to cylindrical coordinates, which is almost what I want. How do I go about modifying the formulas to work for conical coordinates? '}\n",
      "1046 {'Id': '1046', 'Type': 'answer', 'ParentId': '1037', 'urls': [], 'exp': ['f(x)', '\\\\frac{d}{dx} f(x)', 'x'], 'Body': \"If you're a physics kind of person, then a good reason to like this notation is that it gives the correct units for the derivative: whatever units <math_exp> is in, the units for <math_exp> are obtained by dividing by the units for <math_exp>. \"}\n",
      "1047 {'Id': '1047', 'Type': 'answer', 'ParentId': '1037', 'urls': [], 'exp': ['\\\\frac{f\\\\left(x_2\\\\right)-f\\\\left(x_1\\\\right)}{x_2-x_1}', '\\\\frac{\\\\Delta f\\\\left(x\\\\right)}{\\\\Delta x}', '\\\\Delta x', 'd', '\\\\Delta', '\\\\frac{df\\\\left(x\\\\right)}{dx}', '\\\\frac{d}{dx}f\\\\left(x\\\\right)', '\\\\delta', '\\\\partial ', 'D'], 'Body': 'Because of their definition: Start with a function, calculate the difference in value between two points and divide by the size of the interval between the two. You can represent this as such: <math_exp> or <math_exp> Where ∆, delta, is the Greek capital D and indicates an interval. Now, take the limit as <math_exp> goes to zero, and you have the differential. This is indicated by using a lower case <math_exp> instead of the <math_exp>. <math_exp> Now, if this operation is treated as an operator applied to a function, it is usually represented as <math_exp> Note that (typically in physics), you can also use the letter <math_exp> to indicate very small intervals and in general you would use the symbol <math_exp> to represent partial differentials. They are all variations of the letter <math_exp>. '}\n",
      "1048 {'Id': '1048', 'Type': 'question', 'Title': 'Different definitions of trigonometric functions', 'Tags': ['geometry', 'calculus', 'trigonometry'], 'AcceptedAnswerId': '1052', 'urls': [], 'exp': [], 'Body': 'In school, we learn that sin is \"opposite over hypotenuse\" and cos is \"adjacent over hypotenuse\". Later on, we learn the power series definitions of sin and cos. How can one prove that these two definitions are equivalent? '}\n",
      "1049 {'Id': '1049', 'Type': 'answer', 'ParentId': '1002', 'urls': ['http://www.gnu.org/software/octave/', 'http://rads.stackoverflow.com/amzn/click/0961408804'], 'exp': [], 'Body': \"Here's some simple Matlab code to play around with if you like. As written you will have the first N=20 terms of the Fourier approximation to the cosine on the interval [a,b]=[0,2*pi].  Not very interesting as is... Reference: Gilbert Strang. \"}\n",
      "1050 {'Id': '1050', 'Type': 'answer', 'ParentId': '687', 'urls': ['http://en.wikipedia.org/wiki/Braess_paradox'], 'exp': [], 'Body': \"The form this question is usually asked is whether adding a route can increase the average traveling time, and this is known as Braess's paradox.  The Wiki article gives an explicit example in which the travel time on some of the routes depends on the traffic. \"}\n",
      "1051 {'Id': '1051', 'Type': 'answer', 'ParentId': '1025', 'urls': [], 'exp': [], 'Body': 'If you\\'re a physics sort of person, conic sections clearly come up when you study how Kepler figured out what the shapes of orbits are, and some of their synthetic properties give useful shortcuts to things like proving \"equal area swept out in equal time\" that need not involve calculus. The other skills you typically learn while studying conic sections in analytic geometry - polar parametrization of curves, basic facts about various invariants related to triangles and conics, rotations and changing coordinate systems (so as to recognize the equation of a conic in general form as some sort of transformation of a standard done), are all extremely useful in physics. I\\'d say that plane analytic geometry was the single most useful math tool for me in solving physics problems until I got to fluid dynamics stuff (where that is replaced by complex analysis). Relatedly, independent of their use in physics, I think they\\'re a great way to show the connections between analytic and synthetic thinking in math, which will come up over and over again for people who go on to study math (coordinate-based versus intrinsic perspectives, respectively). '}\n",
      "1052 {'Id': '1052', 'Type': 'answer', 'ParentId': '1048', 'urls': [], 'exp': ['\\\\sin x', '\\\\sin x', '\\\\cos x', '\\\\lim_{x \\\\to 0} \\\\frac{ \\\\sin x}{x} = 1', '\\\\sin x', '\\\\cos x', \"y'' = -y\", '\\\\sin x', ' \\\\sin x', '\\\\cos x', '\\\\sin x', '\\\\cos x'], 'Body': \"Most of the proofs in elementary calculus textbooks use the definition of <math_exp> via geometry to prove that the derivative of <math_exp> is <math_exp> (namely, the fact that <math_exp>). Consequently, it follows that <math_exp> and <math_exp> are the two linearly independent solutions of <math_exp>.  The power series equations are also two linearly independent solutions of this differential equation.  Moreover, <math_exp> and its derivative coincide with the derivative of the power series for <math_exp> at zero (no surprise, it's a Taylor series).  Same for <math_exp>. By uniqueness of solutions to ordinary differential equations, this proves that <math_exp> and <math_exp> as defined in school are equal to their power series.  (This is an expansion of Qiaochu's comment.) \"}\n",
      "1053 {'Id': '1053', 'Type': 'answer', 'ParentId': '1048', 'urls': ['http://en.wikipedia.org/wiki/Taylor%27s_theorem'], 'exp': [], 'Body': \"As a rough outline, the circular definitions of sine and cosine (the y- and x-coordinates of the image of (1,0) under a rotation about the origin) lead to being able to differentiate sine and cosine, and once you know how to differentiate them (infinitely), Taylor's Theorem justifies that the power series is equal to the function. \"}\n",
      "1054 {'Id': '1054', 'Type': 'answer', 'ParentId': '1002', 'urls': ['http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29'], 'exp': [], 'Body': 'Let me partially steal from the accepted answer on MO, and illustrate it with examples I understand: The Fourier transform is a different representation that makes convolutions easy. Or, to quote directly from there: \"the Fourier transform is a unitary change of basis for functions (or distributions) that diagonalizes all convolution operators.\" This often involves expressing an arbitrary function as a superposition of \"symmetric\" functions of some sort, say functions of the form eitx —\\xa0in the common signal-processing applications, an arbitrary \"signal\" is decomposed as a superposition of \"waves\" (or \"frequencies\"). This is the use of the discrete Fourier transform I\\'m most familiar with. Suppose you want to multiply two polynomials of degree n, given by their coefficients (a0, …, an) and (b0, …, bn). In their product, the coefficient of xk is ck = &sum;aibk-i. This is a convolution, and doing it naively would take O(n2) time. Instead, suppose we represent the polynomials by their values at 2n points. Then the value of the product polynomial (the one we want) at any point is simply the product of the values of our original two polynomials. Thus we have reduced convolution to pointwise multiplication. The Fourier transform and its inverse correspond to polynomial evaluation and interpolation respectively, for certain well-chosen points (roots of unity). The Fast Fourier Transform (FFT) is a way of doing both of these in O(n log n) time. Suppose we have two independent (continuous) random variables X and Y, with probability densities f and g respectively. In other words, P(X ≤ x) = ∫x-∞ f(t)dt and P(Y ≤ y) = ∫y-∞ f(t)dt. We often want the distribution of their sum X+Y, and this is given by a convolution: P(X+Y ≤ z) = ∫f(t)g(z-t)dt. This integration may be hard. But instead of representing the random variables by their densities, we can also represent them by their characteristic functions &phi;X(t) =  E[eitX] and &phi;Y(t) = E[eitY]. Then the characteristic function of X+Y is just: &phi;X+Y(t) = E[eit(X+Y)] = &phi;X(t)&phi;Y(t) since they\\'re independent. The characteristic function is the continuous Fourier transform of the density function; it is a change of representation in which convolution becomes pointwise multiplication. To quote again the answer on MO, many transformations we want to study (translation, differentiation, integration, …) are actually convolutions, so the Fourier transform helps in a wide number of instances. '}\n",
      "1055 {'Id': '1055', 'Type': 'answer', 'ParentId': '1045', 'urls': ['http://en.wikipedia.org/wiki/Conical_coordinates'], 'exp': [], 'Body': \"Presumably you already have the formulas for converting from conical to rectangular coordinates as listed on the Wikipedia page for conical coordinates. You'll need to solve for r, μ, and ν in terms of x, y, and z to get your answer. I can't see offhand the easiest way to find a general formula, but if you're trying to find it for particular values of r, μ, and ν, it shouldn't be too hard. \"}\n",
      "1056 {'Id': '1056', 'Type': 'answer', 'ParentId': '1048', 'urls': [], 'exp': ['\\\\sin_h', '\\\\cos_h', '\\\\sin_p', '\\\\cos_p', '0', '2\\\\pi', '\\\\sin_p^2(\\\\theta)+\\\\cos_p^2(\\\\theta)=1', '\\\\sin_h = \\\\sin_p \\\\circ \\\\gamma', '\\\\cos_h = \\\\cos_p \\\\circ \\\\gamma', '\\\\gamma', '\\\\gamma'], 'Body': 'Call the highschool functions (defined by the right triangle inscribed in a unit circle, the angle being equal to the length of the arc of the circle) <math_exp> and <math_exp>, and let <math_exp> and <math_exp> be the power series definitions. (Note that these functions are continuous and agree at the end points <math_exp> and <math_exp>). Since <math_exp> the power series definitions also form a right triangle. Hence <math_exp> and <math_exp> for some parameterization <math_exp>. We know the power series definitions satisfy the arc length criteria so <math_exp> must be the identity function. '}\n",
      "1057 {'Id': '1057', 'Type': 'question', 'Title': 'Group With an Endomorphism That is \"Almost\" Abelian is Abelian.', 'Tags': ['group-theory'], 'AcceptedAnswerId': '1165', 'urls': [], 'exp': ['x, y'], 'Body': 'Suppose a finite group has the property that for every <math_exp>, it follows that \\\\begin{equation*} (xy)^3 = x^3 y^3. \\\\end{equation*} How do you prove that it is abelian? Edit: I recall that the correct exercise needed in addition that the order of the group is not divisible by 3. '}\n",
      "1058 {'Id': '1058', 'Type': 'answer', 'ParentId': '1018', 'urls': ['http://local.wasp.uwa.edu.au/~pbourke/', 'http://local.wasp.uwa.edu.au/~pbourke/geometry/'], 'exp': [], 'Body': \"Paul Bourke's website, in particular his geometry section fueled my passion for geometry at a young age, infact I learned the idea of analytic geometry from it. For this reason it holds a special place in my heart. It is a very rich site full of graphics and derivations, very easy to get sucked in and lose a few hours! \"}\n",
      "1059 {'Id': '1059', 'Type': 'answer', 'ParentId': '967', 'urls': ['http://lambda-the-ultimate.org/node/492'], 'exp': [], 'Body': 'Recommendations: All of these are mentioned in the LtU Getting Started thread. '}\n",
      "1061 {'Id': '1061', 'Type': 'answer', 'ParentId': '1037', 'urls': ['http://www.maths.tcd.ie/pub/HistMath/People/Berkeley/AnalCont.html', 'http://www.math.wisc.edu/~keisler/calc.html', 'http://publish.uwo.ca/~jbell/invitation%20to%20SIA.pdf'], 'exp': [], 'Body': 'hysterical raisins. Calculus used to be done with infinitesimals (Archimedes The Method of Mechanical Theorems, Newtons Fluxions, ...) but there was some controversy about these ghostly quantities eventually the whole foundation of analysis was rebuilt using limits but the old notations have been kept. So there is (as you noticed) a strange gap between reality (epsilonics) and intuition (infinitesimal quantities) but there are a few more recent redevelopments of the foundation of analysis for example Keisler or Bell. '}\n",
      "1062 {'Id': '1062', 'Type': 'answer', 'ParentId': '275', 'urls': ['http://press.princeton.edu/titles/8350.html'], 'exp': [], 'Body': 'As a computer scientist with an interest in mathematics I liked the , though it is a heavy book and not always light reading. <img src=\"https://i.stack.imgur.com/HfOdm.png\" alt=\"\"> '}\n",
      "1063 {'Id': '1063', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://ncatlab.org/'], 'exp': [], 'Body': 'The nLab is a great resource for category theory. '}\n",
      "1064 {'Id': '1064', 'Type': 'question', 'Title': 'Perfect set without rationals', 'Tags': ['real-analysis', 'general-topology', 'examples-counterexamples'], 'urls': [], 'exp': ['\\\\mathbb R^n'], 'Body': 'Give an example of a perfect set in <math_exp> that does not contain any of the rationals. (Or prove that it does not exist). '}\n",
      "1065 {'Id': '1065', 'Type': 'answer', 'ParentId': '1025', 'urls': [], 'exp': [], 'Body': 'Conic sections should definitely be retained. If you don\\'t cover conic sections, then what other examples can you cover? Lines? Too simple. General curves? Insufficiently concrete. Examples are very important for illustrating the general theory and techniques. Also, in a multivariable calculus course, typical examples will involve quadric surfaces. Here conic sections will come into play, since hyperplane sections (or \"level curves\") of quadric surfaces are conic sections. '}\n",
      "1066 {'Id': '1066', 'Type': 'question', 'Title': 'Packing boxes and proof of Riemann Hypothesis', 'Tags': ['computer-science', 'proof-theory', 'packing-problem', 'riemann-hypothesis'], 'AcceptedAnswerId': '1071', 'urls': ['http://scottaaronson.com/blog/?p=152', 'https://math.stackexchange.com/questions/946/computation-with-a-memory-wiped-computer'], 'exp': [], 'Body': 'From Scott Aaronson\\'s blog: There’s a finite (and not   unimaginably-large) set of boxes, such   that if we knew how to pack those   boxes into the trunk of your car, then   we’d also know a proof of the Riemann   Hypothesis. Indeed, every formal proof   of the Riemann Hypothesis with at most   (say) a million symbols corresponds to   some way of packing the boxes into   your trunk, and vice versa.   Furthermore, a list of the boxes and   their dimensions can be feasibly   written down. His later commented to explain where he get this from: \"3-dimensional bin-packing is NP-complete.\" I don\\'t see how these two are related. Another question inspired by the same article is here. '}\n",
      "1067 {'Id': '1067', 'Type': 'answer', 'ParentId': '1064', 'urls': ['http://en.wikipedia.org/wiki/Continued_fraction'], 'exp': [], 'Body': \"An easy example comes from the fact that a number with an infinite continued fraction expansion is irrational (and conversely). The set of all irrationals with continued fractions consisting only of 1's and 2's in any arrangement is a perfect set of irrational numbers. \"}\n",
      "1068 {'Id': '1068', 'Type': 'question', 'Title': 'Usage of dx in Integrals', 'Tags': ['calculus', 'notation'], 'AcceptedAnswerId': '2280', 'urls': [], 'exp': ['\\\\int f(x)\\\\mathrm{d}x', 'f(x_i)\\\\cdot\\\\mathrm{d}x', '\\\\mathrm{d}x', '\\\\int 1', '\\\\int e^{\\\\mathrm{d}x}', 'e^0 = 1', '\\\\int (e^{\\\\mathrm{d}x} - 1)'], 'Body': \"All the integrals I'm familiar with have the form: <math_exp>. And I understand these as the sum of infinite tiny rectangles with an area of: <math_exp>. Is it valid to have integrals that do not have a differential, such as <math_exp>, or that have the differential elsewhere than as a factor ? Let me give couple of examples on what I'm thinking of: <math_exp> If this is valid notation, I'd expect it to sum infinite ones together, thus to go inifinity. <math_exp> Again, I'd expect this to go to infinity as <math_exp>, assuming the notation is valid. <math_exp> This I could potentially imagine to have a finite value. Are any such integrals valid? If so, are there any interesting / enlightening examples of such integrals? \"}\n",
      "1069 {'Id': '1069', 'Type': 'question', 'Title': 'Intuitive explanation of covariant, contravariant and Lie derivatives', 'Tags': ['differential-geometry', 'intuition', 'lie-derivative'], 'AcceptedAnswerId': '1074', 'urls': [], 'exp': [], 'Body': 'I would be glad if someone could explain in intuitive terms what these different derivatives are, and possibly give some practical, understandable examples of how they would produce different results. To be clear, I would like to understand the geometrical or physical meaning of these operators more than the mathematical or topological subtleties that lead to them! Thanks! '}\n",
      "1070 {'Id': '1070', 'Type': 'answer', 'ParentId': '1068', 'urls': [], 'exp': ['\\\\int f(x) dx', '\\\\int ... dx', 'd/dx', 'df/dx', '\\\\int e^{dx}'], 'Body': 'When you write <math_exp>, the whole of <math_exp> is an indivisible symbol, just as the <math_exp> is an indivisible symbol when you write <math_exp>. Of course, there are reasons why the notation is as it is, but trying to manipulate it like you suggest in <math_exp>, for example, is simply meaningless. '}\n",
      "1071 {'Id': '1071', 'Type': 'answer', 'ParentId': '1066', 'urls': [], 'exp': [], 'Body': 'The question of whether a formal proof of the Riemann Hypothesis exists (with at most a million symbols) is a problem in NP: given such a proof, it can be verified to be correct in polynomial time. Bin-packing is NP-complete: this means that every problem in NP can be reduced to bin packing. In particular, the problem mentioned in the previous paragraph can. (This is a reduction that can be made explicit, so once we specify the proof verifier etc., we can carry out the steps of the reduction to get an instance of bin packing. We also need the reduction to be \"parsimonious\" i.e. solutions correspond one-to-one; I believe it is.) '}\n",
      "1072 {'Id': '1072', 'Type': 'answer', 'ParentId': '1068', 'urls': [], 'exp': [], 'Body': 'No, it\\'s not valid. The dx in the integral is a representation of the fact that the integral is obtained as an area, so multiplying the \"average\" of the function value at each point by an infinitesimal interval. As the manner in which we don\\'t calculate the area does not change, the notation does not change. There are different notations that are used when the integral is over a curve, or over more than variable (thus leading for example to volumes). The d(variable) notation is also used as a reminder that the integral is against a specific variable and not another, e.g. that int x/y dx differs from int x/y dy. '}\n",
      "1073 {'Id': '1073', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://scrummethodology.com/scrum-effort-estimation-and-story-points/'], 'exp': ['\\\\forall n \\\\gt 1 \\\\in \\\\mathbb N, F_{n+1} \\\\ne \\\\frac{F_{n+2} + F_n}{2}'], 'Body': \"It is useful when making estimates (for examples for programming tasks) as <math_exp> and therefore it discourages bad practices like assuming that two equal-sized tasks necessarily take twice as much as one of them (it's false because the larger the scope, the less accurate the estimate). Example \"}\n",
      "1074 {'Id': '1074', 'Type': 'answer', 'ParentId': '1069', 'urls': [], 'exp': ['V_p', 't \\\\to 0', 'V_{p+tW} - V_p'], 'Body': \"The Lie derivative is a derivative of a vector field V along another vector field W.  It is defined at a point p as follows: flow the point p along W for some time t and look at the value of V at this point.  Then push this forward along the flow of W to a vector at p.  Subtract <math_exp> from this, divide by t, and take the limit as <math_exp>.  So this is a measure of how V changes as it gets pushed around by the flow of W. The covariant derivative is a derivative of a vector field V along a vector W.  Unlike the Lie derivative, this does not come for free: we need a connection, which is a way of identifying tangent spaces.  The reason we need this extra data is because if we wanted to take the directional derivative of V along the vector W how we do in Euclidean space, we would be taking something like <math_exp>, which is the difference of vectors living in different tangent spaces.  If we have a metric, then we can impose reasonable conditions that give us a unique connection (the Levi-Civita connection). I have no idea what a contravariant derivative is.  I'd guess it has to do with applying a covariant derivative and lowering indices. \"}\n",
      "1075 {'Id': '1075', 'Type': 'answer', 'ParentId': '1068', 'urls': ['https://i.stack.imgur.com/8Odsc.png', 'http://mathurl.com/27gla3n.png'], 'exp': [], 'Body': 'I think your question here shows that, while you have been using these symbols, you haven\\'t really been given a proper motivation for where they came from. Let\\'s go back and consider how we came up with the idea of an integral. In a typical class, you will see a lot of pictures like this: <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Integral_approximations.svg/420px-Integral_approximations.svg.png\" alt=\"alt text\"> We find the area under the curve by summing up the area of all these little rectangles. If we wanted to write an expression for the area, it would look like:  (source: mathurl.com) The Σ means that we are computing a sum. We are adding the areas of the rectangles, which we have numbered 1 through n, to get the complete area under the curve. The area of each rectangle is given by multiplying the height by the width. The height is given by f(xi) because the base of the rectangle is at 0, and the top of the rectangle is where it meets the function f. The Δx represents the width of each rectangle. When we find the integral, we are taking the limit of this sum as the number of rectangles goes to infinity, and each individual rectangle becomes infinitesimally tiny. You can think of the dx as the equivalent of Δx: it represents the infinitesimally small width of each rectangle that we added up to get the area. Once you realize this, we can see why integrals only make sense when written ∫f(x)dx. Because we are adding up the areas of rectangles that have height f(x) and width dx. If you try to interpret the expressions you wrote in this way, you will see that they do not really make sense as integrals: you are not summing up rectangles, so you are not finding an area under a curve. You could, of course, define your own notation in which those expressions behave the way you expect them to, but all mathematical notation is driven based on what people find useful, and what people can agree on and easily understand. Your reuse of the integral sign and dx that people are used to seeing in a particular context will probably result in few people adopting your definition. '}\n",
      "1076 {'Id': '1076', 'Type': 'answer', 'ParentId': '1048', 'urls': [], 'exp': [], 'Body': \"There is another proof that the derivative of sine is cosine that doesn't use the sandwich theorem mentioned by Qiaochu and Akhil above.  Instead, one can use the definition of arcsine and the standard calculus formula for arc length in terms of an integral to show that arcsine = the integral of (1 - x^2)^(-.5).  It follows that the derivative of arcsine is (1 - x^2)^(-.5), and (by the chain rule) one can use this fact to prove that the derivative of sine is cosine. In fact, I'm not sure why this proof is presented less frequently then the one via the sandwich theorem.  The unit circle definition of sine is based on arc length, and in calculus we learn a formula for arc length based on integration.  Why not connect these two concepts for a natural proof that the derivative of sine is cosine? \"}\n",
      "1077 {'Id': '1077', 'Type': 'answer', 'ParentId': '713', 'urls': [], 'exp': ['X', 'D', 'X\\\\dot D'], 'Body': 'One way to \"visualize\" a 2-tensor (in the presence of an inner product) is as follows: a vector <math_exp> can be regarded as a function mapping each direction <math_exp> to a scalar in a linear and homogeneous fashion, namely, the inner product <math_exp>. In the same vein, a 2-tensor is a function mapping directions to vectors linearly and homogeneously. '}\n",
      "1078 {'Id': '1078', 'Type': 'answer', 'ParentId': '946', 'urls': [], 'exp': ['\\\\{1, \\\\ldots, 8\\\\}', '\\\\{1, \\\\ldots, 5\\\\}', '(R_1,R_2,R_3)', 'R_i \\\\gets R_a + x_b R_c', 'R_i \\\\gets R_a + x_b R_c + R_d', 'x_1\\\\ \\\\ldots x_n', '(R_1,R_2,R_3) = (1,0,0)', 'R_3 + f R_1', 'R_3 \\\\gets R_3 + x_i R_1', '\\\\neg f', 'f', '(R_1,R_2,R_3 + f R_1)', 'R_3 \\\\gets R_3 + R_1', 'f_1 \\\\wedge f_2', '(R_1,R_2,R_3+f_1f_2R_1)', 'f_1 \\\\vee f_2', 'f R_1', 'f', '(R_1,R_2,R_3)', '(R_1,R_2,R_3 + f R_1)', 'f_1 \\\\wedge f_2', 'R_2', 'R_3', '(1,0,0)', '(1,0,f)'], 'Body': 'Regardless of whether the constant is 5 or 500, its still very surprising. Thankfully, it\\'s fairly straightforward to prove this if you allow the counter to be <math_exp> instead of <math_exp>. [This proof is by Ben-Or and Cleve.]. Start by representing the computation as a circuit, and ignore the whole wiping-clean thing. Define a register machine as follows: It has 3 registers <math_exp>, each of which holds a single bit. At each step, the program performs some computation on the registers of the form <math_exp> or <math_exp> (where <math_exp> is the input). Initially, set <math_exp>. The machine should end in the state <math_exp>. We\\'ll simulate the circuit using a register machine. We now proceed by induction on the depth of the circuit. If the circuit has depth 0, then we just copy the appropriate bit: <math_exp>. For the induction, we have 3 cases, according to whether the final gate is NOT, AND, or OR. Suppose that the circuit is <math_exp>. By induction, we can compute <math_exp>, yielding the state <math_exp>. We can therefore perform the instruction <math_exp> to get the desired output. If the circuit is <math_exp>, then life is a tad more complicated. By induction, we then execute the following 4 instructions: \\\\begin{align*} R_2 &amp;\\\\gets R_2 + f_1 R_1 \\\\\\\\ R_3 &amp;\\\\gets R_3 + f_2 R_2 \\\\\\\\ R_2 &amp;\\\\gets R_2 + f_1 R_1 \\\\\\\\ R_3 &amp;\\\\gets R_3 + f_2 R_2 \\\\end{align*} Assuming I haven\\'t made any typos, we are left with the state <math_exp>, as desired. <math_exp> works similarly. QED. Take a moment to process what just happened. It\\'s a slick proof that you have to read 2 or 3 times before it begins to sink in. What we\\'ve shown is that we can simulate a circuit by applying a fixed program that stores only 3 bits of information at any time. To convert this into Aaronson\\'s version, we encode the three registers into the counter (that\\'s why we needed the extra 3 spaces). The simple program uses the input and the clock to determine how far we\\'ve made it through the computation and then applies the appropriate change to the counter. To get from 8 states down to 5, you use a similar argument, but are much more careful about exactly how much information needs to be propagated between stages and how it can be encoded. A formal proof requires lots of advanced group theory. Edit to answer Casebash\\'s questions: 1) Correct. Any computation can be expressed as a circuit composed solely of \"NOT\", binary-\"AND\", and binary-\"OR\" gates. 2) The notation <math_exp> means (boolean) multiplication. 3) The program for computing <math_exp> should take input <math_exp> to <math_exp>. We insist that the first two registers are unchanged since we use those as temporary storage in the induction. For example, when computing <math_exp>, we compute the first branch and store the result in <math_exp> while computing the second branch. 4) The single bit of output is the final value of <math_exp>. Since we started with <math_exp>, we end with <math_exp>. '}\n",
      "1079 {'Id': '1079', 'Type': 'question', 'Title': 'Sum of two periodic functions', 'Tags': ['analysis'], 'AcceptedAnswerId': '1083', 'urls': [], 'exp': ['f', 'g', '\\\\Bbb{R}', 'T', 'f', 'S', 'g', 'T/S', 'f+g'], 'Body': 'Let <math_exp> and <math_exp> be two periodic functions over <math_exp> with the following property: If <math_exp> is a period of <math_exp>, and <math_exp> is a period of <math_exp>, then <math_exp> is irrational. Conjecture: <math_exp> is not periodic. Could you give a proof or a counter example?  It is easier if we assume continuity.  But is it true for arbitrary real valued functions? '}\n",
      "1081 {'Id': '1081', 'Type': 'answer', 'ParentId': '1079', 'urls': [], 'exp': ['B', '\\\\mathbb R', '\\\\mathbb Q', 'B_1', 'B_2', '\\\\mathbb Q', 'f,g:\\\\mathbb R\\\\to\\\\mathbb R', 'f(x)=x', 'g(x)=0', 'x\\\\in B_1', 'f(x)=0', 'g(x)=x', 'x\\\\in B_2', 'f(x)+g(x)=x', 'x\\\\in B', 'f+g=\\\\operatorname{id}_{\\\\mathbb R}', 'f', 'g', 'B_1', 'B_2', 'B_1\\\\cup B_2', '\\\\mathbb Q', 'x/y\\\\not\\\\in\\\\mathbb Q', 'x\\\\in B_1', 'y\\\\in B_2'], 'Body': 'Pick a basis <math_exp> of <math_exp> as a <math_exp> vector space, and split it into two non-empty disjoint parts <math_exp> and <math_exp>. Define <math_exp>-linear maps <math_exp> such that <math_exp> and <math_exp> if <math_exp>, <math_exp> and <math_exp> if <math_exp>. Then <math_exp> for all <math_exp>, so that in fact <math_exp>, which is not a periodic function. Morever <math_exp> and <math_exp> are periodic, and their sets of periods are precisely <math_exp> and <math_exp>. Since <math_exp> is linearly independent over <math_exp>, it is easy to see that <math_exp> whenever <math_exp> and <math_exp>. This is then an example where the sum is not periodic. '}\n",
      "1083 {'Id': '1083', 'Type': 'answer', 'ParentId': '1079', 'urls': [], 'exp': ['a, b, c \\\\in \\\\mathbb{R}', '\\\\mathbb{Q}', '\\\\text{span}(x, y, z, ...)', '\\\\mathbb{Q}', '\\\\mathbb{R}', 'x, y, z, ...', 'AB = \\\\text{span}(a, b), BC = \\\\text{span}(b, c), AC = \\\\text{span}(a, c)', 'S', '\\\\mathbb{R}', '\\\\chi_S', 'S', '\\\\displaystyle f(x) = \\\\chi_{AB} - 2 \\\\chi_{BC}', '\\\\displaystyle g(x) = 3 \\\\chi_{AC} + 2 \\\\chi_{BC}.', 'f', '\\\\text{span}(b)', 'g', '\\\\text{span}(c)', 'f + g', '\\\\text{span}(a)', 'h(x) = \\\\sin x + \\\\sin \\\\pi x', '\\\\sin x + \\\\sin \\\\pi x = \\\\sin (x+T) + \\\\sin \\\\pi (x+T)', 'x', ' 0', 'x', '\\\\sin x + \\\\pi^2 \\\\sin \\\\pi x = \\\\sin (x+T) + \\\\pi^2 \\\\sin \\\\pi(x+T).', '\\\\sin x = \\\\sin (x+T)', '\\\\sin \\\\pi x = \\\\sin \\\\pi(x+T)'], 'Body': \"Here is a counterexample.  Let <math_exp> be linearly independent over <math_exp>.  Let <math_exp> be the <math_exp>-vector space in <math_exp> spanned by <math_exp>.  Let <math_exp>.  And for a subset <math_exp> of <math_exp>, let <math_exp> denote the characteristic function of <math_exp>.  Now define <math_exp> and <math_exp> Then <math_exp> has period set <math_exp>, <math_exp> has period set <math_exp>, and <math_exp> has period set <math_exp>.  (I am not sure if the coefficients are necessary; they're just precautions.) Are you still interested in the continuous case? (Old answer below.  I slightly misunderstood the question when I wrote this.) Here is a simpler example.  I claim that the function <math_exp> cannot possibly be periodic.  Why?  Suppose an equation of the form <math_exp> held for all <math_exp> and some <math_exp>.  Take the second derivative of both sides with respect to <math_exp> to get <math_exp> This implies that <math_exp> and that <math_exp>, which is impossible. (Or is the question whether the sum can be periodic?) \"}\n",
      "1084 {'Id': '1084', 'Type': 'answer', 'ParentId': '668', 'urls': [], 'exp': ['n', 'V', 'n', 'f', 'V', 'f(\\\\alpha) = \\\\alpha', '\\\\alpha', 'f(A \\\\wedge B) = f(A) \\\\wedge f(B), f(A + B) = f(A) + f(B)', 'A', 'B', 'A', 'm', 'f(A)', 'm', 'V'], 'Body': 'The top exterior power of an <math_exp>-dimensional vector space <math_exp> is one-dimensional. Its elements are sometimes called pseudoscalars, and they represent oriented <math_exp>-dimensional volume elements. A linear operator <math_exp> on <math_exp> can be extended to a linear map on the exterior algebra according to the rules <math_exp> for <math_exp> a scalar and <math_exp> for <math_exp> and <math_exp> blades of arbitrary grade. Trivia: some authors call this extension an outermorphism. The extended map will be grade-preserving; that is, if <math_exp> is a homogeneous element of the exterior algebra of grade <math_exp>, then <math_exp> will also have grade <math_exp>. (This can be verified from the properties of the extended map I just listed.) All this implies that a linear map on the exterior algebra of <math_exp> once restricted to the top exterior power reduces to multiplication by a constant: the determinant of the original linear transformation. Since pseudoscalars represent oriented volume elements, this means that the determinant is precisely the factor by which the map scales oriented volumes. '}\n",
      "1085 {'Id': '1085', 'Type': 'answer', 'ParentId': '998', 'urls': ['http://rads.stackoverflow.com/amzn/click/354063293X'], 'exp': [], 'Body': \"Before Hartshorne's book there was Mumford's Red Book of Varieties.  I think it is a great introductory textbook to modern algebraic geometry (scheme theory). I found that Mumford is quite good at motivating new concepts; in particular I really enjoy his development of nonsingularity and the sheaf of differentials.  I think another great aspect about this book is that it emphasizes how to define things intrinsically (i.e. without reference to a closed or open immersion into affine space) but also explains how to make local arguments (i.e. using immersion into affine space).  A classic example of the above: (non intrinsic tangent space): Say X is a variety and p is a point of X.  Choose an affine neighborhood so that p corresponds to the origin.  Then this affine neighborhood is spec k[x1, ..., xn]/I for some ideal.  Let I' be all the linear terms of I (i.e. if I = (x,y^2), then I' = (x)).  Then the tangent space at p is spec k[x1,...,xn]/I'. (intrinsic tangent space): Let m be the maximal ideal of the local ring of the structure sheaf at p, then the tangent space is the dual of the vector space m/m^2. Taking spec of the symmetric algebra of the latter gives you the former. Some drawbacks.  This book doesn't cover nearly as much as Hartshorne's book.  It doesn't have that many exercises.  The notation is slightly different; integral finite type schemes are called pre-varieties and you can remove the `pre' if it's also separated.  Nevertheless I think its a great compliment to reading Hartshorne. \"}\n",
      "1086 {'Id': '1086', 'Type': 'answer', 'ParentId': '998', 'urls': ['http://rads.stackoverflow.com/amzn/click/0521423538'], 'exp': [], 'Body': \"Another book I wish I had known about when I was first reading Hartshorne is Miranda's Complex Algebraic Curves. Again this book covers much less then Hartshorne and only discusses curves over the complex numbers (and their Jacobians).  But it gives a lot more details and examples of concepts which I found particularly difficult when I first started learning algebraic geometry (sheafs, divisors, cohomology).  It also has a bunch of exercises which I think are often not as challenging as the the exercises in Hartshorne. It also covers a lot more of the 'classical' theory of curves than Hartshrone does; e.g.  Weierstrass points. \"}\n",
      "1087 {'Id': '1087', 'Type': 'answer', 'ParentId': '381', 'urls': [], 'exp': ['f(x) = \\\\dfrac{\\\\varphi^x -(-\\\\varphi)^x}{\\\\sqrt{5}}', '\\\\varphi', 'x'], 'Body': \"They're a much easier way to evaluate the function <math_exp> by hand, where <math_exp> is the golden ratio and <math_exp> is an integer :) \"}\n",
      "1088 {'Id': '1088', 'Type': 'answer', 'ParentId': '998', 'urls': ['http://math.stanford.edu/~vakil/0910-216/'], 'exp': [], 'Body': \"My last suggestion would be Ravi Vakil's online notes on the foundations of algebraic geometry. I think these notes might be made into a full on textbook someday.  I haven't looked through all of them but these notes seem to cover as much as Hartshorne does (if not more).  Only rarely do Hartshorne and Vakil define things differently (`projective morphisms' is the only example that comes to mind). I've heard it said that Hartshorne's book is a `baby' version of EGA.  I think Vakil's notes are somewhere between Hartshorne and EGA (probably not the midpoint though).  At least Vakil discusses much more the theory of representable functors, and Noetherian hypothesis are less prevalent in Vakil's notes.  Also Vakil's notes are more complete in that they also include proofs of many of the commutative algebra results that are just stated in Hartshorne. I think Vakil spends a lot more time motivating the material and often the notes are a bit conversational.  Also there are tons of exercises and most of the them are appended with useful qualifiers like (easy but important exercises, unimportant exercise, tedious but useful exercise, etc). One drawback is that they are very long and they are online notes so there are many typos.  But most of them are grammatical and easy to spot. [Edit: By now there are only a few typos (because these are online notes)] \"}\n",
      "1090 {'Id': '1090', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': [], 'Body': \"For any five points on the globe, there is an angle in outer space from which you could see at least 4 of the 5 points (assuming the moon or anything isn't in the way).  The proof is pretty simple, too... \"}\n",
      "1091 {'Id': '1091', 'Type': 'question', 'Title': 'Is there a geometrical interpretation to the notion of eigenvector and eigenvalues?', 'Tags': ['linear-algebra', 'eigenvalues-eigenvectors', 'intuition'], 'AcceptedAnswerId': '1095', 'urls': ['http://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace'], 'exp': [], 'Body': 'The wiki article on eigenvectors offers the following geometrical interpretation: Each application of the matrix to an arbitrary vector yields a result which will have rotated towards the eigenvector with the largest eigenvalue. Qn 1: If there is any other geometrical interpretation particularly in the context of a covariance matrix? The wiki also discusses the difference between left and right eigenvectors. Qn 2: Do the above geometrical interpretations hold irrespective of whether they are left or right eigenvectors? '}\n",
      "1092 {'Id': '1092', 'Type': 'answer', 'ParentId': '1091', 'urls': ['http://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace'], 'exp': [], 'Body': \"Of course! Consider a coordinate transformation of rotation and/or scaling (but not translation): where v and u are vectors, and A is a transformation matrix. Then the eigenvectors, if they have real components, are the axes which are left unrotated (scaling only) by the transformation. (see wikipedia) A covariance matrix is a symmetric, positive definite matrix, so it has orthonormal eigenvectors, and these form a tuple of axes; I am fairly sure the eigenvectors form a new basis of linear combinations of the input variables where the basis variables are uncorrelated, but I can't remember how to show this. For example, if w1 = [x;y] is a pair of independent unit-variance zero-mean Gaussian random variables, consider w2 = [u;v] = [1 1; 2 1][x;y] = (x+y,2x+y), so that w1 = [-1 1;2 -1][u;v] = [v-u;2u-v]. Then cov(w2) = [2 3; 3 5]. This has eigenvectors which have sqrt(5) in them, hmmmm... As for question 2, I'm not sure. \"}\n",
      "1093 {'Id': '1093', 'Type': 'answer', 'ParentId': '392', 'urls': ['http://daylateanddollarshort.com/math/pdfs/dsindcos.pdf', 'http://demonstrations.wolfram.com/CalculusFreeDerivativesOfSineAndCosine/'], 'exp': [], 'Body': 'Perhaps the following diagram will provide insight: <img src=\"https://i.imgur.com/LvSyCVH.jpg\" alt=\"(Non)Proof without Words: Derivatives of Sine and Cosine\"> The idea is to look at the sine and cosine curves as projections of a helix drawn on a cylinder. If you look at the cylinder itself as a curled planar square of length 2pi, then helix is a curled version of the square\\'s diagonal. A tangent vector along the flat square\\'s diagonal always lies at 45 degrees to the square\\'s sides, say with length-\"1\" shadows in each direction; after smoothly curling the square into the cylinder, the tangent vector lies at 45 degrees to the cylinder\\'s (z-)axis and the perpendicular (xy-)plane. Projecting the helix into the zy- and zx-planes gives graphs of sine and cosine. Projecting the helix\\'s tangent vector gives tangent vectors to those graphs. The \"dz\"s for these projected tangents are always 1 (the \"vertical\" shadow of the helix\\'s tangent vector). To get at \"dy\" and \"dx\" (\"v_x\" and \"v_y\" in the diagram) we project down into the xy-plane where we see a circle, and yet another projected tangent vector. Basic geometry tells us that a tangent to a circle is perpendicular to the radius at the point of tangency. In our circle, the point of tangency --and the radius vector it-- is parameterized as \"&lt; cos, sin, 0 >\". The perpendicular tangent line must therefore have a \"negative-reciprocal\" direction vector: \"&lt; -sin, cos, 0 >\", which gives us our \"dx\" and \"dy\" for the helix tangent ... and the projected graph tangents as well, so that we may make the following conclusions: The derivative of cosine --by its conceptual definition as \"slope of the tangent line\"-- is change-in-x-over-change-in-z = dx/dz = -sin/1 = -sin. Likewise, the derivative of sine is dy/dz = cos/1 = cos. I like this approach because the conceptual \"slope of tangent line\" definition of the derivative is used throughout; there are no (obvious) appeals to digressive computational tricks involving trig identities and limits of difference quotients. I also like that the curious negative sign in the derivative of cosine traces back to an elementary property of circle geometry. Of course, this approach doesn\\'t constitute proof of the formulas. The process of curling the planar square into a cylinder and claiming that the tangent vector behaves as claimed actually assumes the computational machinery covered by the traditional limit arguments. Nevertheless, on an intuitive level, I think this argument explains the \"why\" of the derivatives quite beautifully. Then, knowing what the formulas are (or \"should be\") helps motivate the investigation of the computational tricks needed to provide a rigorous proof. Here\\'s a PDF with a variant of the above discussion (but the same image). Here\\'s a Mathematica Demonstration that animates the various elements, including the square curling into the cylinder. '}\n",
      "1094 {'Id': '1094', 'Type': 'answer', 'ParentId': '998', 'urls': [], 'exp': [], 'Body': \"for Undergraduate algebraic geometry (significantly below the level of Hartshorne), Cox, Little and O'Shea's Ideals, Varieties, and Algorithms is a pleasant treatment. \"}\n",
      "1095 {'Id': '1095', 'Type': 'answer', 'ParentId': '1091', 'urls': [], 'exp': ['R^n', 'R^n', 'v^T v = 1', 'v^T (M^T M) v = 1'], 'Body': 'Here is a partial answer in the case where M is a real symmetric matrix.  This is to ensure, by the real spectral theorem, that M has real eigenvectors with real eigenvalues, so there is a chance for a genuine geometric interpretation which stays in <math_exp>. M acts on the unit sphere in <math_exp> in the following way: it sends the unit sphere <math_exp> to <math_exp> .  This modified shape is not generally a sphere, but is generally an ellipsoid.  The axes of this ellipsoid are the eigenvectors of M, and the sizes of each axis are given by the squares of the corresponding eigenvalues. '}\n",
      "1096 {'Id': '1096', 'Type': 'question', 'Title': 'Where is the flaw in this \"proof\" that 1=2? (Derivative of repeated addition)', 'Tags': ['calculus', 'recreational-mathematics', 'fake-proofs', 'faq'], 'AcceptedAnswerId': '1097', 'urls': [], 'exp': [], 'Body': 'Consider the following: Therefore, Take the derivative of lhs and rhs and we get: Which simplifies to: and hence Clearly something is wrong but I am unable pinpoint my mistake. '}\n",
      "1097 {'Id': '1097', 'Type': 'answer', 'ParentId': '1096', 'urls': [], 'exp': ['\\\\underbrace{x + x + x + \\\\dots + x}_{\\\\text{repeated $x$ times}}', 'x', 'x', 'x^2', '\\\\underbrace{x + x + x + \\\\dots + x}_{\\\\text{repeated $x$ times}}', 'x', 'x', '\\\\underset{\\\\lfloor x\\\\rfloor\\\\mathrm{\\\\ addends}}{\\\\underbrace{x+x+\\\\cdots+x}}+(x-\\\\lfloor x\\\\rfloor)\\\\cdot x'], 'Body': 'You cannot take the derivative of <math_exp> with respect to <math_exp> one term at a time because the number of terms depends on <math_exp>. Even beyond that, if you can express <math_exp> as <math_exp>, then <math_exp> must be an integer, and if the domain of the expression is the integers, (continuous) differentiation does not make sense and/or the derivatives do not exist. (edit: I gave my first reason first because the second reason can be smoothed over by taking \"repeated <math_exp> times\" to mean something like <math_exp>.) '}\n",
      "1098 {'Id': '1098', 'Type': 'answer', 'ParentId': '1096', 'urls': [], 'exp': ['x + x + x + \\\\cdots', 'x', 'x^2', 'x', 'x'], 'Body': 'You cannot differentiate the LHS of your equation <math_exp> (repeated <math_exp> times) = <math_exp> This is because the LHS is not a continuous function; the number of terms depends on <math_exp> so the LHS is not well defined when <math_exp> is not an integer. We can only differentiate continuous functions, so this is not valid. '}\n",
      "1099 {'Id': '1099', 'Type': 'question', 'Title': 'Mandelbrot-like sets for functions other than <span class=\"math-container\" id=\"9895\">f(z)=z^2+c</span>?', 'Tags': ['fractals', 'dynamical-systems'], 'AcceptedAnswerId': '1110', 'urls': [], 'exp': ['f(z)= z^2+c', '\\\\mathbb{C}'], 'Body': 'Are there any well-studied analogs to the Mandelbrot set using functions other than <math_exp> in <math_exp>? '}\n",
      "1100 {'Id': '1100', 'Type': 'answer', 'ParentId': '381', 'urls': [], 'exp': ['2 \\\\times n', '1 \\\\times 2'], 'Body': 'How many ways are there to tile a <math_exp> grid with <math_exp> dominos? This problem can be solved using Fibonacci numbers. Let Sn be the number of valid tilings of the 2 x n grid. Each such tiling has either a vertical 1 x 2 domino or two horizontal dominos on the right. Therefore, each tiling of a 2 x (n-1) grid or a 2 x (n-2) grid generates a tiling of the 2 x n grid, and hence we have a recurrence relation Sn = Sn-1 + Sn-2. This is precisely the recurrence relation of the Fibonacci numbers. Checking our base cases, we see that there is one way to tile a 1 x 2 grid and two ways to tile a 2 x 2 grid, so S1 = 1 and S2 = 2. Therefore, the number of tilings is precisely the Fibonacci sequence. '}\n",
      "1101 {'Id': '1101', 'Type': 'answer', 'ParentId': '514', 'urls': ['http://en.wikipedia.org/wiki/Euler%27s_sum_of_powers_conjecture'], 'exp': ['\\\\sum_{i=1}^kx_i^n=z^n', '$n \\\\leq\\xa0k$', '$k=1$', '$k=2$', '$n=5$', '$$ 61917364224=27^5+84^5+110^5+133^5=144^5 $$', '$n=4$', '$$ 31858749840007945920321 = 95800^4+217519^4+414560^4=422481^4 $$'], 'Body': 'Another example: Euler\\'s sum of powers conjecture, a generalization of Fermat\\'s Last Theorem. It states: If the equation <math_exp> has a solution in positive integers, then <math_exp> (unless <math_exp>). Fermat\\'s Last Theorem is the <math_exp> case of this conjecture. A counterexample for <math_exp> was found in 1966: it\\'s <math_exp> The smallest counterexample for <math_exp> was found in 1988: <math_exp> This example used to be even more useful in the days before FLT was proved, as an answer to the question \"Why do we need to prove FLT if it has been verified for thousands of numbers?\" :-) '}\n",
      "1102 {'Id': '1102', 'Type': 'answer', 'ParentId': '1099', 'urls': ['http://en.wikipedia.org/wiki/Julia_set'], 'exp': [], 'Body': 'There is the Julia set, which can be defined for any complex rational map f(z) = P(z)/Q(z) where P(z) and Q(z) are polynomials. The Julia set for the map fc(z) = z^2 + c is related to the Mandelbrot set in that a point z is in the Mandelbrot set if the Julia set of fc(z) is connected. '}\n",
      "1103 {'Id': '1103', 'Type': 'answer', 'ParentId': '1048', 'urls': ['http://en.wikipedia.org/wiki/Involute', 'https://daylateanddollarshort.com/math/pdfs/sectan.pdf'], 'exp': ['\\\\sin x / x \\\\to 1', 'x \\\\to 0', 'P_0', 'P', '|P_0 P| = 1', 'P_{2n} P_{2n+1}', 'P_{2n+1} P_{2n+2}', '\\\\cos \\\\theta = \\\\sum_{n=0}^{\\\\infty}(-1)^n | P_{2n} P_{2n+1} |', '\\\\sin \\\\theta = \\\\sum_{n=0}^{\\\\infty} (-1)^n | P_{2n+1} P_{2n+2} |', '|P_{k} P_{k+1}|', '|I_k|', 'I_0', 'I_1', '|I_k| = \\\\theta^k / k!', '\\\\sec \\\\theta = \\\\sum_{n=0}^{\\\\infty} | P_{2n} P_{2n+1} | = \\\\sum_{n=0}^{\\\\infty} | I_{2n} |', '\\\\tan \\\\theta = \\\\sum_{n=0}^{\\\\infty} | P_{2n+1} P_{2n+2} | = \\\\sum_{n=0}^{\\\\infty} | I_{2n+1} |', '|I_k|', '\\\\theta^k'], 'Body': 'If you allow yourself a tiny bit of calculus ( \"<math_exp>\" as \"<math_exp>\" ) and apply some combinatorics, there\\'s a really nice geometric interpretation of the terms of the power series for the functions. Consider this diagram and the polygonal \"spiral\" that starts at <math_exp> and closes in on the point <math_exp> (where <math_exp>). <img src=\"https://daylateanddollarshort.com/misc/sincosinvolutes.jpg\" alt=\"The Sine and Cosine Involute Pinwheel\"> The horizontal segments <math_exp> alternately overshoot and undershoot the length of the cosine segment; the vertical segments <math_exp> do the same for the sine segment. So, <math_exp> <br /><br />   <math_exp> Now, the lengths <math_exp> are equal to the lengths of the curves <math_exp>, which constitute a series of successive  (with <math_exp> defined to be a segment, and <math_exp> defined to be an arc of the unit circle). Combinatorics and the calculus result I mentioned show that the involute lengths satisfy ... <math_exp> ... so that the above are, in fact, power series. Interestingly, the same thing can be done with secant and tangent, using an involute zig-zag: <img src=\"https://daylateanddollarshort.com/misc/sectaninvolutes.jpg\" alt=\"The Secant and Tangent Involute Zig-Zag\"> where <math_exp><br /><br />   <math_exp> and the lengths <math_exp> turn out to be the appropriate multiples of powers of <math_exp>. Reference to the argument for sine and cosine (attributed to Y. S. Chaikovsky, as reported by Leo Gurin), a complete discussion of the trickier argument for secant and tangent, and then a refinement of the argument for sine and cosine, are in my note \"Zig-Zag Involutes, Up-Down Permutations, and Secant and Tangent\" (PDF). BTW: I have not (yet) cracked the case for cosecant and cotangent. '}\n",
      "1104 {'Id': '1104', 'Type': 'answer', 'ParentId': '1096', 'urls': [], 'exp': [' ', ' ', '\\\\rm\\\\:f(x) + x\\\\, Df(x)\\\\:', '\\\\rm\\\\: x \\\\ f(x)\\\\:', '\\\\rm\\\\ S:\\\\, n \\\\to n+1\\\\ ', '\\\\rm\\\\:p(n)\\\\:', '\\\\rm\\\\:S p(n) = p(n+1).\\\\:', '\\\\rm\\\\ S[n^2] = (n+1)^2.\\\\:', '\\\\rm\\\\ S[fg] = S[f]\\\\, S[g],\\\\ ', '\\\\rm\\\\: S[f] g\\\\:', '\\\\rm\\\\:f(x),\\\\:', '\\\\rm\\\\ x:\\\\, f(x) \\\\to x f(x).\\\\:', '\\\\rm\\\\:D\\\\:', '\\\\rm\\\\:x\\\\:', '\\\\rm\\\\:p(x,D)\\\\:', '\\\\rm\\\\:x,D\\\\:', '\\\\rm\\\\:x,D\\\\:', '\\\\rm\\\\:n,S.'], 'Body': 'Here\\'s my explanation from an old sci.math post: Zachary Turner  wrote on 26 Jul 2002: Let D = d/dx = derivative wrt x. Then An obvious analogous fallacious argument proves both <math_exp> D[x f(x)]  =  Df(x) (x  times) = x Df(x) <math_exp> D[x f(x)]  =   Dx (f(x) times) = f(x), via  Dx = 1 vs.  the correct result: their sum  <math_exp> as given by the Leibniz product rule (= chain rule for times). The error arises from overlooking the dependence upon x in both arguments of the product  <math_exp>  when applying the chain rule. The source of the error becomes clearer if we consider a discrete analog. This will also eliminate any tangential concerns on the meaning of \"(x times)\" for non-integer x. Namely, we consider the shift operator  <math_exp>  on polynomials  <math_exp> with integer coefficients, where  <math_exp> Here is a similar fallacy But correct is  <math_exp> Here the \"product rule\" is  <math_exp> not  <math_exp>  as above. The fallacy actually boils down to operator noncommutativity. On the space of functions <math_exp> consider \"x\" as the linear operator of multiplication by  x, so  <math_exp> Then the linear operators  <math_exp>  and  <math_exp>  generate an operator algebra  of polynomials  <math_exp> in NON-commutative indeterminates <math_exp> since we have This view reveals the error as mistakenly assuming commutativity of the operators  <math_exp>  or  <math_exp> Perhaps something to ponder on boring commutes ! '}\n",
      "1105 {'Id': '1105', 'Type': 'answer', 'ParentId': '677', 'urls': [], 'exp': ['n'], 'Body': \"I like your problem. But your opposite parity conjecture doesn't seem to handle the case N=3 and S=2 (even when corrected as in Mau's comment by gcd), with a triangle labeled as follows: This position appears to be invariant under any move, so you cannot reach all 0s. More generally, a similar obstacle works for any 3n-gon, with S=2n. If you repeat the pattern 1 1 0 all the way around, it is stable under any move. If <math_exp> is odd, these will have opposite parities. \"}\n",
      "1106 {'Id': '1106', 'Type': 'answer', 'ParentId': '639', 'urls': ['https://mathoverflow.net/questions/59939/identifying-poisoned-wines/60312#60312'], 'exp': ['k = 2', '{\\\\lceil \\\\log_2 N \\\\rceil + 2 \\\\choose 2} - 1', 'N = 1000', '65', 'n = \\\\lceil \\\\log_2 N \\\\rceil', 'k = 1', 'n', 'n', 'i', 'i^{th}', '1', 'k = 2', 'n', 'n', '{n \\\\choose 2}', '(i, j)', '1 \\\\le i &lt; j \\\\le n', 'i', 'i^{th}', '1', 'i', 'i^{th}', '0', '(i, j)', 'i^{th}', 'j^{th}', '1', 'i_1 &lt; ... &lt; i_m', '(i_1)^{th}', '1', '(i_1)^{th}', '(i_2)^{th}', '(i_1)^{th}', '(i_2)^{th}', '(i_1, i_2)', '(i_2)^{th}', '(i_j, i_{j+1})', 'k', 'N', 'k = N-1', 'N-1'], 'Body': \"I asked this question on MathOverflow and got a great answer there. For <math_exp> I can do it with <math_exp> servants.  In particular for <math_exp> I can do it with <math_exp> servants.  The proof is somewhat long, so I don't want to post it until I've thought about the problem more. I haven't been able to improve on the above result.  Here's how it works.  Let <math_exp>.  Let me go through the algorithm for <math_exp> so we're all on the same page.  Number the wines and assign each of them the binary expansion of their number, which consists of <math_exp> bits.  Find <math_exp> servants, and have servant <math_exp> drink all the wines whose <math_exp> bit is <math_exp>.  Then the set of servants that die tells you the binary expansion of the poisoned wine. For <math_exp> we need to find <math_exp> butlers, <math_exp> maids, and <math_exp> cooks.  The cooks will be named <math_exp> for some positive integers <math_exp>.  Have butler <math_exp> drink all the wines whose <math_exp> bit is <math_exp>, have maid <math_exp> drink all the wines whose <math_exp> bit is <math_exp>, and have cook <math_exp> drink all the wines such that the sum of the <math_exp> bit through the <math_exp> bit, inclusive, mod 2, is <math_exp>.  This is how the casework breaks down for butlers and maids. The second two cases are great.  The problem with case 1 is that if it occurs more than once, there's still ambiguity about which wine has which bit.  (The worst scenario is if all the butlers and maids die.)  To fix the issue with case 1, we use the cooks. Let <math_exp> be the set of bits where case 1 occurs.  We'll say that the poisoned wine whose <math_exp> bit is <math_exp> is wine A, and the other one is wine B.  Notice that the sum of the <math_exp> through <math_exp> bits of wine A mod 2 is the same as the sum of the <math_exp> through <math_exp> bits of wine B mod 2, and we can determine what this sum is by looking at whether cook <math_exp> died.  The value of this sum determines whether the <math_exp> bit of wine A is 1 or 0 (and the same for wine B).  Similarly, looking at whether cook <math_exp> died tells us the remaining bits of wine A, hence of wine B. One last comment for now.  The lower bound is not best possible when <math_exp> is large compared to <math_exp>; for example, when <math_exp> it takes <math_exp> servants.  The reason is that any servant who drinks more than one wine automatically dies, hence gives you no information. \"}\n",
      "1107 {'Id': '1107', 'Type': 'answer', 'ParentId': '1099', 'urls': ['http://gist.github.com/497210'], 'exp': ['\\\\boldsymbol{f_c(z)=\\\\cos(z)+c}', '\\\\cos(z)', '10\\\\pi', '\\\\boldsymbol{f_c(z)=\\\\sin(z)+c}', '\\\\boldsymbol{f_c(z)=e^z+c}', '2\\\\pi', '\\\\boldsymbol{f_c(z)=cz(1-z)}'], 'Body': 'I have no idea about well-studied, but here\\'s what I\\'ve gotten from a bit of playing around (here\\'s the relevant Mathematica code). <math_exp> where <math_exp> is defined however Mathematica defines it, using the escape time algorithm with escape radius <math_exp> and 100 iterations maximum: <img src=\"https://i.stack.imgur.com/2Ddw7.png\" alt=\"cosine1\"> <img src=\"https://i.stack.imgur.com/pL8nx.png\" alt=\"cosine2\"> <math_exp> appears to be the same as cosine, but horizontally translated. <math_exp>, using the escape time algorithm with escape radius 50 and 100 iterations maximum (it\\'s vertically periodic with period <math_exp>): <img src=\"https://i.stack.imgur.com/1cJU6.png\" alt=\"exp1\"> <img src=\"https://i.stack.imgur.com/dq0cl.png\" alt=\"exp2\"> edit (Mathematica code edited, too): <math_exp> (since camccann\\'s answer mentions logistic maps) using the escape time algorithm with escape radius 50 and 200 iterations maximum: <img src=\"https://i.stack.imgur.com/V3csJ.png\" alt=\"logistic1\"> <img src=\"https://i.stack.imgur.com/21gSR.png\" alt=\"logistic2\"> '}\n",
      "1108 {'Id': '1108', 'Type': 'question', 'Title': 'Why should one expect valuations to be related to primes? How to treat an infinite place algebraically?', 'Tags': ['number-theory', 'algebraic-geometry'], 'AcceptedAnswerId': '1114', 'urls': [], 'exp': [], 'Body': 'I understand the mechanics of the proof of Ostrowski\\'s Theorem, but I\\'m a little unclear on why one should expect valuations to be related to primes. Is this a special property of number fields and function fields, or do primes of K[x,y] correspond to valuations on K(x,y) in the same way? I\\'m hoping for an answer that can explain what exactly are the algebraic analogs of archimedian valuations, and how to use them - for example, I\\'ve heard that the infinite place on K(x) corresponds to the \"prime (1/x)\" - how does one take a polynomial in K[x] \"mod (1/x)\" rigorously? Thanks in advance. '}\n",
      "1109 {'Id': '1109', 'Type': 'answer', 'ParentId': '1064', 'urls': [], 'exp': ['I_1 = [\\\\sqrt{2},\\\\sqrt{2}+1/3] \\\\cup [\\\\sqrt{2}+2/3,\\\\sqrt{2}+1]', 'I_2 = [\\\\sqrt{2},\\\\sqrt{2}+1/9] \\\\cup [\\\\sqrt{2}+2/9,\\\\sqrt{2}+1/3]\\\\cup[\\\\sqrt{2}+2/3,\\\\sqrt{2}+7/9]\\\\cup[\\\\sqrt{2}+8/9,\\\\sqrt{2}+1]', 'P = \\\\cap_{i=1}^\\\\infty I_i', 'P', 'P', 'P'], 'Body': 'It can be proven that the Cantor set is perfect. Certainly, this contains infinitely many rationals. How about modifying the construction of the Cantor set by defining: <math_exp>, <math_exp>, etc and setting <math_exp>? Each of end points of any interval that appears in the construction is a member of <math_exp> and is irrational. However, is it true that all the members of <math_exp> must be an end point of a certain interval? I am tempted to think so because we can prove that <math_exp> does not contain any interval. '}\n",
      "1110 {'Id': '1110', 'Type': 'answer', 'ParentId': '1099', 'urls': ['http://en.wikipedia.org/wiki/Tricorn_%28mathematics%29', 'http://en.wikipedia.org/wiki/Burning_Ship_fractal', 'http://en.wikipedia.org/wiki/Newton%27s_method', 'http://www.chiark.greenend.org.uk/~sgtatham/newton/', 'http://en.wikipedia.org/wiki/Logistic_map'], 'exp': [], 'Body': 'A variant on the M-set can be defined in straightforward fashion for any iterated function in the complex plane parameterized by a single initial value. For instance, slight modifications produce the tricorn and burning ship fractals. However, most such variations tend to be either boring, incoherent, or obviously derived from the Mandelbrot set--nothing particularly novel. Some obvious patterns also emerge quickly from many variations: Real exponents alter symmetry, imaginary exponents cause asymmetric twisting, disreputable functions produce misshapen lumps like the burning ship, and so on. On the other hand, it\\'s more difficult than you might expect to avoid the M-set in the first place: For a well known example, consider the Newton-Raphson method for approximating roots, which can be generalized to the complex plane in straightforward fashion. For some polynomial, a point in the complex plane may or may not converge to a particular root after some number of Newton-Raphson iterations. In most cases, plotting the regions of divergence and convergence per root produces a fractal. Modifying the polynomial and the iteration formula produces effects analogous to modifying the constant and iteration function, respectively, of a standard Julia set, and in fact it turns out that Julia fractals can be considered a special case of Newton-Raphson fractals. Analogous structure can also be found elsewhere than the complex plane. The period-doubling behavior of the logistic map relates to the behavior of points on the real line of the M-set, and islands of stability for the logistic map correspond to the positions of mini-Mandelbrots. Elaborate Julia-like structures can also be found in the quaternions, although unfortunately visualization of 4-dimensional fractals is somewhat challenging. My suspicion is that Julia-like structure will arise for any fractal defined by similar means, e.g., classifying points into sets based on their behavior under repeated iteration of a position-sensitive transformation, but I\\'m not sure how to define \"similar means\" precisely enough to formalize that. '}\n",
      "1111 {'Id': '1111', 'Type': 'answer', 'ParentId': '1091', 'urls': [], 'exp': ['M', 'X_1,...,X_n', 'X', 'i', 'X_i', 'M', 'v_1, v_2, ..., v_n', 'M', 'Y_i', 'v_i', 'X', 'Y_1, ..., Y_n', ' E[ Y_i Y_j] = E[ v_i^T X X^T v_j] = v_i^T M v_j = \\\\lambda_j v_i^T v_j = 0 '], 'Body': 'If you are interested in covariance matrices, then the eigenvectors of the covariance matrix tell you how to change variables to make your random variables uncorrelated. Specifically, let <math_exp> be a covariance matrix of the random variables <math_exp>. For simplicity, lets assume that all of these random variables are zero mean. Lets also define <math_exp> to be the random vector whose <math_exp>\"th component is the random variable <math_exp>. Let the eigenvectors of <math_exp> be <math_exp>. We can assume that these are orthogonal since <math_exp> is symmetric. Consider the random variables <math_exp> obtained by taking the dot product of <math_exp> and <math_exp>. Then the random variables <math_exp> are uncorrelated! Indeed: <math_exp> '}\n",
      "1112 {'Id': '1112', 'Type': 'answer', 'ParentId': '941', 'urls': [], 'exp': [], 'Body': \"While it is certainly true that with the proper definition there is now 'wrong' notation, perhaps it should be mentioned that some notation is more suggestive and/or easier to work with than others, e.g. Arabic numeral vs. Roman numerals, the various symbols for the derivative, and countless others. The actual symbols are arbitrary, but good notation can certainly promote the flow of ideas more easily. Also, do I remember correctly that Feynman gave up trying to invent more efficient notation for simple math when he was quite young because nobody could understand what he was doing? A good notation has a subtlety and suggestiveness which at times make it almost seem like a live teacher.     --Bertrand Russell \"}\n",
      "1113 {'Id': '1113', 'Type': 'question', 'Title': 'Nonprimes with <span class=\"math-container\" id=\"9980\">3^{n-1} \\\\equiv 2^{n-1} \\\\pmod n</span>', 'Tags': ['number-theory'], 'AcceptedAnswerId': '1387', 'urls': [], 'exp': ['n', '3^{n-1} - 2^{n-1}', 'n'], 'Body': 'Is it true that there are infinitely many nonprime integers <math_exp> such that <math_exp> is a multiple of <math_exp>? '}\n",
      "1114 {'Id': '1114', 'Type': 'answer', 'ParentId': '1108', 'urls': ['http://math.uga.edu/~pete/8410Chapter1.pdf'], 'exp': ['K', 'R', '\\\\mathfrak{p}', 'R', 'K', '\\\\mathfrak{p}', 'K', 'R', '\\\\mathfrak{p}', '\\\\mathfrak{p}', 'K', 'R', 'K', 'R', '\\\\mathbb{Q}', 'F(t)', 'F', 'F', 'F'], 'Body': 'I couldn\\'t divine much information on your background (e.g. undergraduate, master\\'s level, PhD student...) from the question, but I recently taught an intermediate level graduate course which had a unit on valuation theory.  Sections 1.6 through 1.8 of http://math.uga.edu/~pete/8410Chapter1.pdf address your questions.  In particular, if your field <math_exp> is the fraction field of a Dedekind domain <math_exp>, then you can always use each prime ideal <math_exp> of <math_exp> to define a valuation on <math_exp>, essentially the \"order at <math_exp>\".  There is also a converse result, Theorem 13: if you have a valuation on <math_exp> which has the additional property that it is non-negative at every element of the Dedekind domain <math_exp>, then it has to be (up to equivalence) the <math_exp>-adic valuation for some <math_exp>.   I felt the need to give this additional condition a name, so I called such a valuation R-regular. The point is that (as Qiaochu says in his comments), in case <math_exp> is a number field and <math_exp> is its ring of integers, every valuation on <math_exp> is <math_exp>-regular.  However, in the function field setting this is not true and this leads to a discussion of \"infinite places\".  Note that I do describe the analogues of Ostrowski\\'s Theorem for finite extensions both of <math_exp> and of <math_exp> for any field <math_exp> (in the latter case, one restricts to valuations which are trivial on <math_exp>; when <math_exp> is finite, this condition is automatic). I would be interested to know whether you find the notes helpful.  If not, I or someone else can probably recommend an alternative reference. '}\n",
      "1115 {'Id': '1115', 'Type': 'answer', 'ParentId': '1108', 'urls': [], 'exp': ['k[x_1,\\\\cdots,x_n]', 'P = (a_1,\\\\cdots,a_n)', '(x_1-a_1,\\\\cdots,x_n-a_n)', '\\\\mathbb{Z}', '\\\\mathbb{Q}', '\\\\mathbb{Z}', 'K(x)', 'K\\\\mathbb{P}^1', 'K[x]', '[x,1]', '[1,0]', '[x,1] \\\\to [1,1/x]', 'K[1/x]', '(1/x - 0) = (1/x)', 'x'], 'Body': 'Discrete valuations &lt;-> points on a curve For a nonsingular projective curve over an algebraically closed field, there is a one-one correspondence between the points on it, and the discrete valuations of the function field (i.e. all the meromorphic functions of the curve). The correspondence is point P -> the valuation that sends a function f, to the order of zero/pole of f at P. Maximal ideals &lt;-> points on a curve At least for varieties (common zeros of several polynomials) over an algebraically closed field, there is a one-one correspondence between points on it, and the maximal ideal in <math_exp>. The correspondence is point <math_exp> -> the polynomials vanishing at P, which turns out to be <math_exp>. This is something true not only for curves, but for varieties. (Hilbert\\'s Nullstellensatz) So putting these together, for nonsingular projective curves over an algebraically closed field, you know that there is a one-one correspondence between the maximal ideals (think them as points) and the discrete valuations of the function field. Now the situation here is analogous. You consider a \"curve\", whose coordinate ring is <math_exp>, with function field <math_exp>. The nonarchimedean valuations correspond to discrete valuations in this case. So they should capture order of zeros/poles at some \"points\". What are the points? They should correspond to the maximal ideals of <math_exp>, which are exactly the primes here. As for <math_exp>, look at it as the function field of <math_exp>. Just like the usual real/complex projective spaces, you should have two pieces here. Let\\'s say <math_exp> corresponds to the piece where the second coordinate is nonzero. So the corresponding homogeneous coordinates here is like <math_exp>. We know there is one point missing, which is <math_exp>. For this, we change our coordinates <math_exp>, so the piece where the first coordinate is nonzero should be <math_exp>. The missing point corresponds to the ideal <math_exp>, so this is why the infinite place corresponds to (1/x). Of course, a more straight forward interpretation is that for a rational function, you divide both numerator and denominator sufficiently high power of <math_exp> so that they both become polynomials in 1/x, have nonzero constant term, with an extra term (x to the some power). The infinite place measures this power. '}\n",
      "1116 {'Id': '1116', 'Type': 'question', 'Title': 'Number of ways to partition a rectangle into n sub-rectangles', 'Tags': ['combinatorics', 'computer-science', 'recurrence-relations'], 'AcceptedAnswerId': '8447', 'urls': [], 'exp': [], 'Body': \"How many ways can a rectangle be partitioned by either vertical or horizontal lines into n sub-rectangles? At first I thought it would be: but the recurrence relation only counts the cases in which at least one side (either top, bottom, left or right of the original rectangle) is not split into sub-rectangles. There are many other partitions that don't belong to those simple cases like [EDIT ImageShack has removed the picture. One of the cases is the sixth partition when n = 4 in the picture in the accepted answer below.] Any other related problem suggestions are welcome. Also it is nice to know how to traverse this partition efficiently. \"}\n",
      "1117 {'Id': '1117', 'Type': 'question', 'Title': 'Find all <span class=\"math-container\" id=\"10006\">x</span> for that <span class=\"math-container\" id=\"10007\">x^2 + (x+1)^2</span> is a square', 'Tags': ['number-theory'], 'AcceptedAnswerId': '1119', 'urls': [], 'exp': ['x', 'x^2 + (x+1)^2'], 'Body': 'How to find all natural <math_exp> for that <math_exp> is a perfect square? '}\n",
      "1118 {'Id': '1118', 'Type': 'question', 'Title': 'Characterising functions <span class=\"math-container\" id=\"10015\">f</span> that can be written as <span class=\"math-container\" id=\"10016\">f = g \\\\circ g</span>?', 'Tags': ['functions', 'elementary-set-theory', 'function-and-relation-composition'], 'AcceptedAnswerId': '1122', 'urls': [], 'exp': ['f', 'f = g \\\\circ g', '\\\\circ', 'f(x) = x+10', 'g(x) = x+5', 'f(x) = 9x', 'g(x) = 3x', 'f(x) = x^2 + 1', 'f: \\\\mathbb R \\\\to \\\\mathbb R'], 'Body': \"I'd like to characterise the functions that ‘have square roots’ in the function composition sense. That is, can a given function <math_exp> be written as <math_exp> (where <math_exp> is function composition)? For instance, the function <math_exp> has a square root <math_exp>. Similarly, the function <math_exp> has a square root <math_exp>. I don't know if the function <math_exp> has a square root, but I couldn't think of any. Is there a way to determine which functions have square roots? To keep things simpler, I'd be happy just to consider functions <math_exp>. \"}\n",
      "1119 {'Id': '1119', 'Type': 'answer', 'ParentId': '1117', 'urls': ['http://en.wikipedia.org/wiki/Pell_equation'], 'exp': ['x^2 + (x+1)^2 = y^2', '(2x+1)^2 + 1 = 2y^2', '(2x+1)^2 - 2y^2 = -1', 'z=2x+1', 'z^2 - 2y^2 = -1'], 'Body': \"Suppose <math_exp>. We can rewrite it as <math_exp> or <math_exp>. If <math_exp> then we have <math_exp>. This is Pell's equation. Wikipedia article shows how to solve it. \"}\n",
      "1120 {'Id': '1120', 'Type': 'question', 'Title': 'Summing <span class=\"math-container\" id=\"10077\">{\\\\frac{1}{n^2}}</span> over subsets of <span class=\"math-container\" id=\"10078\">N</span>.', 'Tags': ['sequences-and-series'], 'AcceptedAnswerId': '1124', 'urls': [], 'exp': ['2', 'A', 'B', '\\\\sum_{n\\\\in A}   f(n)   =  \\\\sum_{n\\\\in B}   f(n)', 'f(n)={\\\\frac{1}{n^2}}', 'f(n)=\\\\frac{1}{n}', '\\\\frac23 = \\\\frac12 + \\\\frac16 = \\\\frac14+\\\\frac13+\\\\frac1{12}', 'f(n)=b^{-n}', 'b', 'M', '\\\\sum_{n&gt;M} f(n) &lt; f(M)', 'b=2', '2^{N} \\\\to [0,1])', 'A', 'B'], 'Body': 'Are there <math_exp> subsets, say, <math_exp> and <math_exp>, of the naturals such that <math_exp> where <math_exp>? If <math_exp> then there are many counterexamples, which is probably a consequence of the fact that the harmonic series diverges: <math_exp> And if <math_exp> for some base <math_exp> then it is true because for all <math_exp>, <math_exp>. (This is just the base-b representation of a real number.The case <math_exp> gives a bijection surjection <math_exp>. So we have sort of an in-between case here. Also, what if <math_exp>,<math_exp>: -are required to be finite sets? -are required to be infinite and disjoint? '}\n",
      "1121 {'Id': '1121', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://www.youtube.com/TheCatsters#p/u/68/xqLgGB7Hv7g'], 'exp': [], 'Body': 'And when you get bored of reading, let the Catsters take over. (78 videos on Category theory!) '}\n",
      "1122 {'Id': '1122', 'Type': 'answer', 'ParentId': '1118', 'urls': [], 'exp': ['f', 'f : S \\\\to S', 'S', 'f', 'n', 'x, f(x), ... f^{n-1}(x)', 'f^n(x) = x', 'x, f(x), f^2(x), ...', 'S', 'f', 'f : S \\\\to S', 'f', 'g : S \\\\to S', 'g(g(x)) = f(x)', 'g', 'f', 'f', '1 \\\\to 2 \\\\to 3 \\\\to 1', 'g', '1 \\\\to 3 \\\\to 2 \\\\to 1', 'f', 'f', '1 \\\\to 2 \\\\to 1', 'g', '1 \\\\to 1', '2 \\\\to 2', '1 \\\\to 2 \\\\to 3 \\\\to ... ', '1 \\\\to 3 \\\\to ... ', '2 \\\\to 4 \\\\to ...', 'f', 'g', 'f', 'g', 'f', '2k+1', 'f^{k+1}', 'f^{2k+2} = f', 'g', 'f', 'g', 'f', 'S'], 'Body': 'I showed you the link to the MO question mostly to convince you that this is a hard question.  I will \"answer\" it in the special case that <math_exp> is a bijection. Recall that given a bijection <math_exp>, where <math_exp> is a set, a cycle of <math_exp> length <math_exp> is a set of distinct points <math_exp> such that <math_exp>.  A cycle of infinite length is a set of distinct points <math_exp>.  It is not hard to see that <math_exp> is a disjoint union of cycles of <math_exp>. Claim:  A bijection <math_exp> has a square root if and only if there are an even number of cycles of <math_exp> of any given even length.  (For the purposes of this result, infinity is an even number; so there can be an infinite number of cycles, and you need to consider cycles of infinite length.) Proof.  First we show that any bijection with a square root has this property.  Let <math_exp> be a bijection such that <math_exp>.  Then each cycle of <math_exp> corresponds to either one or two cycles of <math_exp>, as follows.  If the cycle has odd length, it corresponds to one cycle of <math_exp>.  For example, the cycle <math_exp> of <math_exp> would correspond to the cycle <math_exp> of <math_exp>.  If the cycle has even length, it corresponds to two cycles of <math_exp>.  For example, the cycle <math_exp> of <math_exp> would correspond to the pair of cycles <math_exp> and <math_exp>, and the cycle <math_exp> would correspond to the pair of cycles <math_exp> and <math_exp>.  In particular, cycles of <math_exp> of odd length can come from cycles of <math_exp> one at a time or two at a time, but cycles of <math_exp> of even length can only come from cycles of <math_exp> two at a time. Now we show the reverse implication.  Given a cycle of <math_exp> of odd length <math_exp>, consider the corresponding cycle of <math_exp> of odd length.  Since <math_exp> when restricted to this cycle, make this a cycle of <math_exp>.  Given a pair of cycles of <math_exp> of the same even length, just weave them together to get a cycle of <math_exp>. I say \"answer\" instead of answer because it\\'s not obvious if you can always find the cycle decomposition of some complicated bijection on an infinite set.  In any case, if <math_exp> isn\\'t assumed to be a bijection this question becomes much harder; the analogue of cycle decomposition is much more difficult to work with.  I suggest you look at some examples where <math_exp> is finite if you really want to get a grip on this case; best of luck. '}\n",
      "1123 {'Id': '1123', 'Type': 'answer', 'ParentId': '855', 'urls': [], 'exp': ['n^2 = 1 \\\\pmod{24}', 'n=1,5,7,11', '(n+12)^2 = n^2 + 24n + 144 = n^2 \\\\pmod{24}', 'n^2 = 1 \\\\pmod{24}', 'n', '3', 'n^2-1', '24', 'n', 'p', '1', '5', '(n+6)^2 = n^2 + 12n + 36 = n^2 + 12(n+3)', 'n^2 \\\\pmod{24}', '(n+3)', 'n'], 'Body': 'Here\\'s a very simplistic proof: <math_exp> for <math_exp>, by checking each case individually. <math_exp>. Therefore, <math_exp> when <math_exp> is odd and not divisible by <math_exp>, and so <math_exp> is divisible by <math_exp> for these <math_exp>. You don\\'t need primality of <math_exp> here! A slight modification would be to use <math_exp> and <math_exp> as \"base cases\", and use the fact that <math_exp>, which is equal to <math_exp> when <math_exp> is even, i.e. <math_exp> is odd. '}\n",
      "1124 {'Id': '1124', 'Type': 'answer', 'ParentId': '1120', 'urls': [], 'exp': ['\\\\frac{1}{15^2}+\\\\frac{1}{20^2}=\\\\frac{1}{12^2}', '\\\\frac{1}{15^2}+\\\\frac{1}{150^2}+\\\\frac{1}{1500^2}+...+\\\\frac{1}{20^2}+\\\\frac{1}{200^2}+\\\\frac{1}{2000^2}+...=\\\\frac{1}{12^2}+\\\\frac{1}{120^2}+\\\\frac{1}{1200^2}...', 'a^2+b^2=c^2', '\\\\frac{1}{a^2 b^2}=\\\\frac{1}{a^2 c^2}+\\\\frac{1}{b^2 c^2}'], 'Body': 'Yes to both cases: 1) <math_exp> 2) <math_exp> for first case - if we have pythagorean triple (a,b,c), such that <math_exp>, then: <math_exp> '}\n",
      "1125 {'Id': '1125', 'Type': 'question', 'Title': 'Interpolating between volume preserving diffeomorphisms of sphere', 'Tags': ['geometry', 'abstract-algebra'], 'AcceptedAnswerId': '1161', 'urls': [], 'exp': ['sphere^2', 'S_2'], 'Body': \"I know volume preserving diffeomorphisms of a <math_exp> make a grou'p sdiff(<math_exp>). I would to know if it is a Lie group, which I assume if it is that makes interpolation easier (like with rotations). So that is one question, is it a Lie group? Also is the group path connected? If so, how can I interpolate between two elements in the group? These are not subjects I know very little about. I apologize if I'm phrasing it in some way that sounds ridiculous. \"}\n",
      "1126 {'Id': '1126', 'Type': 'answer', 'ParentId': '1113', 'urls': ['http://en.wikipedia.org/wiki/Carmichael_number'], 'exp': [], 'Body': 'as Qiaochu Yuan pointed out, take a Carmichael number q; by definition, 3q-1 and 2q-1 are both congruent to 1 mod q, so their difference is a multiple of q. Since Carmichael numbers are infinite, you are done. '}\n",
      "1127 {'Id': '1127', 'Type': 'question', 'Title': 'Watchdog Problem', 'Tags': ['discrete-mathematics', 'recreational-mathematics', 'puzzle', 'word-problem'], 'AcceptedAnswerId': '1128', 'urls': [], 'exp': [], 'Body': \"I just came up with this problem yesterday. Problem: Assume there is an important segment of straight line AB that needs to be watched at all time. A watchdog can see in one direction in front of itself and must walk at a constant non-zero speed at all time. (All watchdogs don't need to have the same speed.) When it reaches the end of the segment, it must turn back (at no time) and keep watching the line. How many watchdogs does it need to guarantee that the line segment is watched at all time? And how (initial positions and speeds of the dogs)? Note:  It's clear that two dogs are not enough. I conjecture that four will suffice and three will not. For example, the below configuration doesn't work from 7.5 second if AB's length is 10 meters. Or it can be illustrated as: Please provide your solutions, hints, or related problems especially in higher dimensions or looser conditions (watchdogs can walk with acceleration, etc.) \"}\n",
      "1128 {'Id': '1128', 'Type': 'answer', 'ParentId': '1127', 'urls': [], 'exp': [], 'Body': 'Three dogs is enought I think. Let the length of line segment be equal to 1 (with coordinates from 0 to 1).<br > First dog: start position = 0, speed = +1/3<br /> Second dog: start position = 2/3, speed = +1/3<br /> Third dog: start position = 2/3, speed = -1/3<br /> After 1 second the position becomes similar. '}\n",
      "1130 {'Id': '1130', 'Type': 'question', 'Title': 'Preserving the extrema of one function after applying another', 'Tags': ['calculus', 'signal-processing'], 'AcceptedAnswerId': '1137', 'urls': [], 'exp': ['f(x)', 'x_1, x_2, \\\\dots', 'g(x)', 'x_i', 'g(f(x))', 'x_i', 'g'], 'Body': \"Suppose we have some function <math_exp> with local extrema at <math_exp>, and a second function <math_exp> which is continuous, strictly increasing and non-zero everywhere over the range of the <math_exp>. Will <math_exp> have its local extrema at the same <math_exp> and no others? If so, are there any obvious loosenings of the constraints on <math_exp> for which this will remain true? (I'm really thinking of this in the context of signal processing, looking at transformations that preserve the visual structure of an image, but it seems like a general question that must have been trivially proved by someone 250 years ago...) \"}\n",
      "1131 {'Id': '1131', 'Type': 'answer', 'ParentId': '1130', 'urls': [], 'exp': ['g^{-1}', 'a&lt;b\\\\Leftrightarrow g(a)&lt;g(b)\\\\Leftrightarrow g^{-1}(a)&lt;g^{-1}(b)', 'x_i', 'a&lt;b\\\\Leftrightarrow g(a)&gt;g(b)', 'a&lt;b\\\\Leftrightarrow g^{-1}(a)&gt;g^{-1}(b)'], 'Body': 'Since g is continuous and strictly increasing, its inverse <math_exp> is a function and strictly increasing.  Since both are strictly increasing, <math_exp>.  From this, it follows that <math_exp> is a local max (min) of g(f(x)) iff it is a local max (min) of f(x). If g were continuous and strictly decreasing, it would exchange local maximums and minimums (because <math_exp> and <math_exp>). '}\n",
      "1132 {'Id': '1132', 'Type': 'question', 'Title': 'Taylor expansion to show that for Stratonovich stochastic calculus the chain rule takes the form of the classical one', 'Tags': ['calculus', 'intuition', 'stochastic-processes'], 'AcceptedAnswerId': '13930', 'urls': [], 'exp': [], 'Body': \"How can I show with a heuristic argument based on a Taylor expansion that for Stratonovich stochastic calculus the chain rule takes the form of the classical (Newtonian) one? Concerning Ito calculus the fact that dX^2 = dt results via a Taylor expansion in Ito's lemma - this fact should stay the same with Stratonovich but it should somehow cancel out in there - I just don't know how... \"}\n",
      "1133 {'Id': '1133', 'Type': 'question', 'Title': 'Natural derivation of the complex exponential function?', 'Tags': ['complex-analysis', 'real-analysis', 'general-topology'], 'AcceptedAnswerId': '1168', 'urls': [], 'exp': ['1', 'f_a(x)=a^x', '0', 'a\\\\neq 1', 'a=e', 'f_a', \"(f_a)'=g(a)f_a\", 'g', 'e', 'g(e)=1', 'g', '1/t', '[1,x)'], 'Body': \"Bourbaki shows in  a very natural way that every continuous group isomorphism of the additive reals to the positive multiplicative reals is determined by its value at <math_exp>, and in fact, that every such isomorphism is of the form <math_exp> for <math_exp> and <math_exp>.  We get the standard real exponential (where <math_exp>) when we notice that for any <math_exp>, <math_exp> where <math_exp> is a continuous group isomorphism from the positive multiplicative reals to the additive reals.  By the intermediate value theorem, there exists some positive real <math_exp> such that <math_exp> (by our earlier classification of continuous group homomorphisms, we notice that <math_exp> is in fact the natural log). Notice that every deduction above follows from a natural question.  We never need to guess anything to proceed. Is there any natural way like the above to derive the complex exponential?  The only way I've seen it derived is as follows: Derive the real exponential by some method (inverse function to the natural log, which is the integral of <math_exp> on the interval <math_exp>, Bourbaki's method, or some other derivation), then show that it is analytic with infinite radius of convergence (where it converges uniformly and absolutely), which means that it is equal to its Taylor series at 0, which means that we can, by a general result of complex analysis, extend it to an entire function on the complex plane. This derivation doesn't seem natural to me in the same sense as Bourbaki's derivation of the real exponential, since it requires that we notice some analytic properties of the function, instead of relying on its unique algebraic and topological properties. Does anyone know of a derivation similar to Bourbaki's for the complex exponential? \"}\n",
      "1134 {'Id': '1134', 'Type': 'answer', 'ParentId': '1125', 'urls': [], 'exp': ['S^2\\\\to S^2', '1'], 'Body': 'The term \"volume preserving\" sounds a bit ambiguous to me: do you mean that your map preserves the total volume or do you mean that its differential at every point preserves volume (i.e. has determinant 1)? The former is weaker than the latter, and gives you more room for interpolation. In any case, there is a famous invariant of continuous maps <math_exp> called the degree. Any two maps with the same degree are homotopic to each other. Being volume preserving (in the former sense) implies that the degree is <math_exp> (taking orientation into account!), so you can interpolate between any two volume preserving maps. However, the intermediate maps in this line of reasoning are only continuous, not necessarily diffeomorphisms. I\\'m confident that with a standard argument \"approximate continuous functions by differentiable ones\" you can get them to be differentiable, but I don\\'t about \"is diffeomorphism\" and \"is locally volume preserving\" parts. '}\n",
      "1135 {'Id': '1135', 'Type': 'question', 'Title': 'If and only if, which direction is which?', 'Tags': ['terminology', 'logic'], 'AcceptedAnswerId': '1136', 'urls': [], 'exp': ['A', 'B', '(A \\\\iff B)', 'A', 'B', '(A \\\\implies B)', 'A', 'B', '(A \\\\impliedby B)', 'A', 'B', 'A', 'B'], 'Body': 'I can never figure out (because the English language is imprecise) which part of \"if and only if\" means which implication. (<math_exp> if and only if <math_exp>) = <math_exp>, but is the following correct: (<math_exp> only if <math_exp>) = <math_exp> (<math_exp> if <math_exp>) = <math_exp> The trouble is, one never comes into contact with \"<math_exp> if <math_exp>\" or \"<math_exp> only if <math_exp>\" using those constructions in everyday common speech. '}\n",
      "1136 {'Id': '1136', 'Type': 'answer', 'ParentId': '1135', 'urls': ['http://www.imgftw.net/img/171340741.png'], 'exp': [], 'Body': \"I wouldn't say that I never come into contact with those phrasings--they are certainly rare in technical use, but perhaps more common in plain language.  Below is a table of equivalent phrasings of p=>q, from UCSMP Precalculus and Discrete Mathematics, 3rd ed., &copy; 2010 Wright Group/McGraw Hill (Lesson 1-5). table http://www.imgftw.net/img/171340741.png \"}\n",
      "1137 {'Id': '1137', 'Type': 'answer', 'ParentId': '1130', 'urls': [], 'exp': ['g', 'g', 'f', '\\\\mathbb{R}', 'g', 'y=f(z)', 'f', 'z', 'g\\\\circ f', 'z', 'g', 'f'], 'Body': 'You don\\'t need to assume that <math_exp> is non-zero, and it could be strictly decreasing as well. Furthermore, the conditions on <math_exp> only need to hold on the image of <math_exp> (which doesn\\'t need to the be whole of <math_exp>, for example). On the other hand, if <math_exp> has a local extremum at <math_exp> and <math_exp> is strictly increasing around <math_exp>, then you\\'re obviously in trouble, because <math_exp> will have a local extremum at <math_exp>. But having no local extrema is equivalent to being strictly monotonic. The only that might relax the conition on <math_exp> is that it has local extrema exactly where <math_exp> does and they \"cancel each other out\" or \"amplify each other\". '}\n",
      "1138 {'Id': '1138', 'Type': 'answer', 'ParentId': '1135', 'urls': [], 'exp': [], 'Body': 'This example may be more clear, because apples ⊂ fruits is more obvious: \"This is an apple if it is a fruit\" is false. \"This is an apple only if it is a fruit\" is true. \"This is a fruit if it is an apple\" is true. \"This is a fruit only if it is an apple\" is false. A is an apple => A is a fruit '}\n",
      "1139 {'Id': '1139', 'Type': 'answer', 'ParentId': '740', 'urls': [], 'exp': [], 'Body': 'The function f(x) = x over the rationals and 2x over the irrationals is locally increasing in 0 but it is neither increasing nor decreasing. '}\n",
      "1140 {'Id': '1140', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://en.wikipedia.org/wiki/Conway_base_13_function'], 'exp': ['[a, b]'], 'Body': 'Also: Conway base 13 function. This function has the following properties. 1. On every closed interval <math_exp>, it takes every real value. 2. It is continuous nowhere. '}\n",
      "1143 {'Id': '1143', 'Type': 'question', 'Title': 'A definition of Conway base-<span class=\"math-container\" id=\"10248\">13</span> function', 'Tags': ['calculus', 'number-theory', 'definition'], 'urls': ['http://www.google.it/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBgQFjAA&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FConway_base_13_function&amp;ei=empRTNK-OsqWON2_uPUE&amp;usg=AFQjCNFqIS-UhBV9Miw1QnAZJaxnswI3Yg&amp;sig2=AlFPiLqMkh3gd4VPEHJI-Q'], 'exp': ['13'], 'Body': \"Can you give a definition of the Conway base-<math_exp> function better than the one actually presented on wikipedia, which isn't clear? Maybe with some examples? \"}\n",
      "1144 {'Id': '1144', 'Type': 'answer', 'ParentId': '740', 'urls': ['https://mathoverflow.net/questions/22189/what-is-your-favorite-strange-function'], 'exp': [], 'Body': 'Have also a look here: https://mathoverflow.net/questions/22189/what-is-your-favorite-strange-function '}\n",
      "1145 {'Id': '1145', 'Type': 'answer', 'ParentId': '514', 'urls': ['https://mathoverflow.net/questions/15444/the-phenomena-of-eventual-counterexamples'], 'exp': [], 'Body': 'Further counterexamples can be found here: https://mathoverflow.net/questions/15444/the-phenomena-of-eventual-counterexamples '}\n",
      "1146 {'Id': '1146', 'Type': 'question', 'Title': 'Intuitive Way To Understand Principal Component Analysis', 'Tags': ['statistics', 'terminology', 'intuition', 'visualization', 'descriptive-statistics'], 'AcceptedAnswerId': '1188', 'urls': [], 'exp': [], 'Body': 'I know that this is meant to explain variance but the description on Wikipedia stinks and it is not clear how you can explain variance using this technique Can anyone explain it in a simple way? '}\n",
      "1147 {'Id': '1147', 'Type': 'question', 'Title': 'Tetrahedron volume', 'Tags': ['geometry', 'polyhedra'], 'AcceptedAnswerId': '1148', 'urls': [], 'exp': [], 'Body': \"How to calculate volume of tetrahedron given lengths of all it's edges? \"}\n",
      "1148 {'Id': '1148', 'Type': 'answer', 'ParentId': '1147', 'urls': ['http://mathworld.wolfram.com/Cayley-MengerDeterminant.html'], 'exp': [], 'Body': 'Cayley-Menger Determinant - A generalization of Herons Formula. '}\n",
      "1149 {'Id': '1149', 'Type': 'answer', 'ParentId': '1143', 'urls': [], 'exp': [], 'Body': 'You just need to switch back and forth from the lexicographic meaning of the base-13 expansion of the number (think of having ABC instead of .-+) and the loaded meaning you give to the well-formed string as a base-10 number. An example of a number for which Conway base-13 function is 0 is 0.12-34++1+2-34..11111111111... where the leftmost . is the threedecimal point (that is, it has a semantic meaning), the three rightmost . mean that the base-13 representation has an infinite number of 1 (that is, they have a metameaning), and the other two . are \"digits\" of the number (that is, they have a syntactic meaning). An example of a number for which Conway base-13 function is not 0 is 0.12-34++1+2-34.11111111111... At that number, the function has value -34.11111111111... (in base 10) '}\n",
      "1150 {'Id': '1150', 'Type': 'answer', 'ParentId': '1064', 'urls': [], 'exp': [], 'Body': 'Consider the set of reals x whose binary expansion, if you look only at the even digit places, is some fixed non-eventually-repeating pattern z. This is perfect, since we have branching at the odd digits, but they are all irrational, since z is not eventually repeating. You can draw a picture of this set, and it looks something like the Cantor middle third set, except that you divide into four pieces, and take either first+third or second+fourth, depending on the digits of z. Another solution: Begin with an interval having irrational endpoints, and perform the usual Cantor middle-third construction, except that at stage n, be sure to exclude the n-th rational number (with respect to some fixed enumeration), using a subinterval having irrational endpoints. By systematically excluding all rational numbers, you have the desired perfect set of irrationals. (Hi François!) '}\n",
      "1151 {'Id': '1151', 'Type': 'answer', 'ParentId': '1133', 'urls': [], 'exp': [], 'Body': \"So, what's unnatural about the complex differential equation... f : C -> C satisfying f'(z) = f(z) and f(0)=1 ? \"}\n",
      "1152 {'Id': '1152', 'Type': 'answer', 'ParentId': '381', 'urls': ['http://en.wikipedia.org/wiki/Euclidean_algorithm#Algorithmic%20efficiency'], 'exp': [], 'Body': \"Consecutive Fibonacci numbers are the worst-case (maximum number of steps) numbers for Euclid's gcd algorithm. \"}\n",
      "1153 {'Id': '1153', 'Type': 'answer', 'ParentId': '954', 'urls': ['http://en.wikipedia.org/wiki/Inverse_Laplace_transform'], 'exp': ['f(t)', 't&lt;0', '\\\\omega', 's=-i\\\\omega', '\\\\mathcal{L}^{-1} \\\\{F(s)\\\\} = f(t) = \\\\frac{1}{2\\\\pi i}\\\\lim_{T\\\\to\\\\infty}\\\\int\\\\limits_{\\\\gamma-iT}^{\\\\gamma+iT}e^{st}F(s)\\\\,ds, \\\\qquad s=\\\\Re(\\\\gamma)'], 'Body': 'The Laplace transform can be simply interpreted as a Wick rotated Fourier transform of a function <math_exp> which vanishes for <math_exp>. Wick rotation means (in this case) changing the frequency <math_exp> of the Fourier transform into an imaginary parameter <math_exp> of the Laplace transform. The reason for the inversion     <math_exp> to be a bit more complicated than the inverse Fourier transform (see wikipedia, to quote @Akhil) is this imaginary frequency which, if left purely imaginary, would lead to a non-convergent integral. The formula is still similar to the inverse Fourier transform. In fact I think (but have not verified) you could use it for that purpose to. '}\n",
      "1154 {'Id': '1154', 'Type': 'answer', 'ParentId': '1133', 'urls': [], 'exp': [], 'Body': 'Let Then So that Since f(0) = 1, C = 0, so (I will update this with LaTeX when that functionality becomes available) '}\n",
      "1155 {'Id': '1155', 'Type': 'answer', 'ParentId': '1143', 'urls': ['http://en.wikipedia.org/wiki/Intermediate_value_theorem'], 'exp': [], 'Body': \"The idea of the Conway base-13 number is to find a function that is not continuous, yet if f(a)&lt;x&lt;f(b), then there is some c between a and b with f(c)=x. This a counterexample to the converse of the intermediate value theorem. The function is defined by encoding base-10 values in the tail (the digits left after skipping a finite number). We use +, -, . and the digits to represent an encoded number in the tail and require the encoded number to start with a + or -. In base 10, every number ending in an infinite number of 9s can be rewritten to end in an infinite number of 0s instead (ie. 0.999...=1.0). Similarly, we decide we will rewrite each Conway number ending in an infinite number of +, to ensure that each real number has a unique decimal representation. Each number can have up to one base-10 encoded value, which is the result of applying Conway's Base 13 function if it exists. If no such encoding exists for x(ie. + occurring infinite times in the expansion), then we define f(x)=0. We then show that for each a and b that we can find a c in between with an arbitrary encoded value. We first ensure the number being constructed is between a and b by copying enough digits from a and incrementing a digit that won't matter. This is easier because we have disallowed ending in an infinite number of +. We then concatenate the digits of the signed base-10 representation of the value we wish the function to have to those digits  we have already fixed. \"}\n",
      "1156 {'Id': '1156', 'Type': 'answer', 'ParentId': '1143', 'urls': [], 'exp': [], 'Body': 'I understand why the Wikipedia article uses the notation it does, but I find it annoying. Here is a transliteration, with some elaboration. Expand x&nbsp;&isin;&nbsp;(0,1) in base 13, using digits {0, 1, ...&nbsp;, 9, d, m, p} --- using the convention d&nbsp;=&nbsp;10, m&nbsp;=&nbsp;11, p&nbsp;=&nbsp;12. N.B. for rational numbers whose most reduced expression a/b is such that b is a power of 13, there are two such expansions: a terminating expansion, and a non-terminating one ending in repeated p digits. In such a case, use the terminating expansion. Let S&nbsp;&sub;&nbsp;(0,1) be the set of reals whose expansion involves finitely many p, m, and d digits, such that the final d digit occurs after the final p digit and the final m digit. (We may require that there be at least one digit 0--9 between the final p or m digit and the final d digit, but this does not seem to be necessary.) Then, every x&nbsp;&isin;&nbsp;S has a base 13 expansion of the form 0.x1x2&nbsp;...&nbsp;xn&nbsp;[&nbsp;p or m&nbsp;]&nbsp;a1a2&nbsp;...&nbsp;ak&nbsp;[&nbsp;d&nbsp;]&nbsp;b1b2&nbsp;... for some digits xj&nbsp;&isin;&nbsp;{0, ...&nbsp;, p} and where the digits aj and bj are limited to {0, ...&nbsp;, 9} for all j. The square brackets above are only intended for emphasis; and in particular the n+1st base-13 digit of x is the final occurance of either p or m in the expansion of x. For x&nbsp;&isin;&nbsp;S, we define f(x) by transliterating the string format above. We ignore the digits x1 through xn&nbsp;, transliterate the p or m as a plus-sign or minus-sign, and the d as a decimal point. This yields a decimal expansion for a real number, either +a1a2&nbsp;...&nbsp;ak&nbsp;.&nbsp;b1b2&nbsp;... or &minus;a1a2&nbsp;...&nbsp;ak&nbsp;.&nbsp;b1b2&nbsp;...according to whether the n+1st base-13 digit of x is a p or an m respectively. For x&nbsp;&isin;&nbsp;S, we set f(x) to this number; for x&nbsp;&notin;&nbsp;S, we set f(x)&nbsp;=&nbsp;0. Note: this function is not computable, as there is no way that you can determine in advance whether the base-13 expansion of x&nbsp;&isin;&nbsp;(0,1) has only finitely many occurances of any of the digits p, m, or d; even if you are provided with a number which is promised to have only finitely many, in general you cannot know when you have found the last one. However, if you are provided with a number x&nbsp;&isin;&nbsp;(0,1) for which you know the location of the final p, m, and d digits, you can compute f(x) very straightforwardly. '}\n",
      "1157 {'Id': '1157', 'Type': 'answer', 'ParentId': '1057', 'urls': ['http://en.wikipedia.org/wiki/Burnside%27s_problem#Bounded_Burnside_problem'], 'exp': ['\\\\mathbb {Z}/3\\\\mathbb{Z}', 'B(m,3)', 'B(2,3)'], 'Body': \"You don't, as the group is not necessarily abelian! The group of upper triangular 3-by-3 matrices with ones along the diagonal and coefficients in the three-element field  <math_exp> has exponent three, so your equation holds, but it is not abelian. There are lots of examples: the most famous ones are the Burnside groups <math_exp>: the group I described above is <math_exp>. \"}\n",
      "1158 {'Id': '1158', 'Type': 'answer', 'ParentId': '1133', 'urls': [], 'exp': ['\\\\sum_{n=0}^{\\\\infty}c_n\\\\left(z-z_0\\\\right)^n', 'c_n,z,z_0\\\\in\\\\mathbb{C}', 'R\\\\in[0,\\\\infty]', 'z\\\\in\\\\mathbb{C}', '\\\\mid z-z_0\\\\mid &lt; R', 'D\\\\left(z_0,\\\\rho\\\\right)', '\\\\rho &lt; R', 'z', ' R', '\\\\sum_{n=1}^{\\\\infty}nc_n\\\\left(z-z_0\\\\right)^{n-1}', 'y(z)=\\\\sum c_nz^n', \"y'(z)=y(z)\", '\\\\mathbb{C}', 'nc_n=c_{n-1}', 'E\\\\left(z\\\\right):=\\\\sum_{n=0}^{\\\\infty}\\\\frac{1}{n!}z^n', \"E'=E\", 'E\\\\left(z_1+z_2\\\\right)=E\\\\left(z_1\\\\right)E\\\\left(z_2\\\\right)', 'E_{\\\\mid_{\\\\mathbb{R}}}', 'E(\\\\mathbb{R})=(0,\\\\infty)', 'x\\\\mapsto E(ix)', '\\\\mathbb{R}', '\\\\mathbb{T}', '0', 'E\\\\left(\\\\frac{\\\\pi}{2}i\\\\right)=i', 'z_1, z_2\\\\in\\\\mathbb{C}', 'E\\\\left(z_1\\\\right)=E\\\\left(z_2\\\\right)', '\\\\frac{z_1-z_2}{2\\\\pi i}\\\\in\\\\mathbb{Z}', 'E\\\\left(\\\\mathbb{C}\\\\right)=\\\\mathbb{C}\\\\setminus\\\\left\\\\lbrace 0\\\\right\\\\rbrace', 'E', '\\\\mathbb{C}', '\\\\mathbb{C}\\\\setminus\\\\left\\\\lbrace 0\\\\right\\\\rbrace', '\\\\gamma(t)=e^{it}', 't\\\\in[0,2\\\\pi]', '\\\\mathbb{T}', '\\\\pi', '\\\\frac{1}{x}'], 'Body': \"Some Assumptions I will assume that you are ok with power series being used, just not Taylor's theorem. I will also assume you will allow us to observe a solution to a DE since you used it in your derivation. Defn A series of the form <math_exp> for <math_exp> is called a power series. Thm There is some <math_exp> such that the power series above converges absolutely for all <math_exp> with <math_exp> and uniformly in <math_exp> for all <math_exp>. Further, the terms are unbounded for all <math_exp> with <math_exp>. pf Use the geometric series' convergence. Lemma Inside the disk of convergence <math_exp> is the derivative of the power series. Construction For power functions <math_exp> can we find a unique solution to <math_exp> in <math_exp>? We can observe that this implies <math_exp>. Hence Defn Let <math_exp>. Thm 1) <math_exp> 2) <math_exp> 3)<math_exp> is strictly increasing and <math_exp> 4) <math_exp> sends <math_exp> onto <math_exp>. 5) There is some real <math_exp> such that <math_exp> and for all <math_exp>, then <math_exp> iff <math_exp> 6) <math_exp>. Note that all of these can be shown purely at the level of power series using no complex analysis. Further the proofs are not hard, if you want more details here let me know. Corollary <math_exp> is a homomorphism of the additive group <math_exp> onto the multiplicative group with <math_exp>. Application If you care about loops(which I think you do!) lets observe that <math_exp> for <math_exp>, is <math_exp> (Note here that <math_exp> is simply the real number we found before, not some existential thing!). We could now push this to get winding numbers. Or you could use this definition of exponential, and the power series definition of inverse to get a (branchless) logarithm. In particular, you can show that the derivative of that fella is <math_exp>. So we don't have to define it that way. :) Comments I agree with you that the definitions of complex exponentials feel contrived and the logarithm is even worse. The branched logarithms is the only part of Palka that I dislike as a complex book. These definitions I can stomach, as they require no Dues ex Machina. \"}\n",
      "1159 {'Id': '1159', 'Type': 'answer', 'ParentId': '107', 'urls': ['http://en.wikipedia.org/wiki/Characteristic_function', 'http://en.wikipedia.org/wiki/Norm_(mathematics)#p-norm', 'http://en.wikipedia.org/wiki/Pointwise_convergence'], 'exp': ['n', '2^n', 'n', 'S(0) = \\\\{1\\\\}', 'S(1) = \\\\{2,3\\\\}', 'S(2) = \\\\{4,5,6,7\\\\}', 'S(3) = \\\\{7,8,9,10,11,12,13,14,15\\\\}', 'S(4) = \\\\{11,12,\\\\dots,31\\\\}', 'n', 'I(n)', 'S(t)', 't \\\\geq n', 'I(n) = S(n) \\\\cap S(n+1) \\\\cap S(n+2) \\\\cap \\\\dots ', 'I(0) = I(1) = I(2) = \\\\dots = \\\\varnothing', 'S(n)', 'f_n : \\\\mathbb{N} \\\\to \\\\{ 0, 1\\\\}', '1', 'S(n)', '0', 'S(n)', '1', 'f_n', 'f_n', '\\\\mathbf{0}', '1', 'f_n', 'f_n', 'x', 'f_n (x) = 0', 'f_n', '\\\\mathbf{0}', 'f_n', '\\\\mathbf{0}', '\\\\mathbf{0}', 'p'], 'Body': '[Edit.] I am editing my answer to try to give some more insight, prompted by comments on my original response. I hope to develop a deeper idea into what\\'s going on here. The short answer is that what is important is not the size of the set of balls at any particular time, but rather how the set of balls changes; and ultimately what the limit of those sets are. The key is to determine what that limit is, and then determine how many balls are in that limiting set. The answer is that the limit is the empty set, which has size 0. The rest of this answer is devoted to describing this in some detail. Part of what I have added to my answer is to point out that although there are multiple ways of measuring convergence &mdash; in terms of various norms on characteristic functions &mdash; only one of these actually defines the limit of the sequence, and in this case the limit is well-defined. In this problem, we have more than just a number of balls which changes with time. What\\'s different is that each of these balls has a unique identity. This might not seem like it should matter, but it means that the state of the \"system\" is not a quantity of balls but a set of balls. That set has a certain size, but the size is a derived feature of the system; it follows from which particular set of balls is present. So it is important to determine what the limit of the sequence of sets is. Let\\'s consider how the set of balls in the box change with time in the game you present. At step <math_exp>, you add <math_exp> balls, and remove the <math_exp> lowest-numbered balls. The \"state of the system\" is given by the following sets: <math_exp> <math_exp> <math_exp> <math_exp> <math_exp> etc. Note that after step 1, the first ball is removed, never to be added again; so it is not an element of the final set. Similarly, at step 2, the second and third balls are removed, never to be added again; so they aren\\'t elements of the final set. And so on. So... none of the balls are in the final set. So then it must be empty! It doesn\\'t matter that the number of balls in the sequence are increasing; what matters is that the number of balls which will never again be in the box is also increasing, and in the limit includes all of the balls. We can make this more striking by considering, for each step <math_exp>, the set of balls <math_exp> which is in the set <math_exp> for all <math_exp>: that is, <math_exp>. Because each ball is eventually removed, never to be added again; this means that <math_exp> So while the original description makes it look like, moment-to-moment, the final state of the box should be to hold infinitely many balls, a more \"forward looking\" approach shows that it\\'s clear that the final state of the box is to be empty. We can contrast the \"intuitive\" answer of infinitely many balls in the box, and the more precise answer that there ultimately are no balls in the box, using characteristic functions: that is, we replace each set <math_exp> by a function <math_exp>  which is <math_exp> for those integers belonging to <math_exp>, and <math_exp> otherwise. Consider the various -norms on these functions. The cardinality of each set <math_exp> is precisely equal to the <math_exp>-norm of the function <math_exp> , which grows without bound. The fact that the 1-norm grows without bound &mdash; and in particular, the distance between the function <math_exp> and the zero function <math_exp> in the <math_exp>-norm grows without bound &mdash; is essentially the source of most people\\'s intuition about this problem, and exactly the reason why they find it counterintuitive that the final set should be empty. But just as the size of a set is a derived quantity, the norm of a function &mdash; or of a sequence of functions &mdash; is also a derived quantity; and the norm of the limit of a sequence of functions is not necessarily the limit of the norms. In fact, the functions <math_exp> don\\'t converge to anything, in any of the p-norms; it simply diverges to nothing in particular. But there is at least one notion of convergence which applies to the functions <math_exp>, and that is point-wise convergence &mdash; the form of convergence which is the broadest, in the sense that it applies to the most cases (and with which all other notions of convergence must agree, if they show that a sequence of functions converges at all). We may simply show that for each <math_exp>, we have <math_exp> for sufficiently large n. It then follows that the sequence <math_exp> converges to <math_exp>. The fact that the sequence <math_exp> doesn\\'t converge to <math_exp> under any of the p-norms doesn\\'t matter; ultimately what matters is that the sequence converges point-wise, because what we\\'re interested in is the cardinality of the limit itself, which is defined in terms of point-wise convergence. At worst, from a certain aesthetic point of view, one might say that it doesn\\'t converge particularly gracefully (informally speaking) to <math_exp>; but it does indeed converge, and that is all that matters. So: using characteristic functions, which is ultimately equivalent to the sets described in the first place, one can show that the sequence of sets does converge, and what they converge to is the empty set. But take comfort that your intuition that they should not reflects a certain awareness of the concept of <math_exp>-norms. :-) '}\n",
      "1160 {'Id': '1160', 'Type': 'answer', 'ParentId': '740', 'urls': [], 'exp': [], 'Body': \"Here's an example of a strictly increasing function on &#8477; which is continuous exactly at the irrationals. Pick your favorite absolutely convergent series &sum;an in which all the terms are positive (mine is &sum;1/2n) and your favorite enumeration of the positive rationals: &#8474;={q1,q2,...}. For a real number x, define f(x) to be the sum of all the an for which qn &le; x. \"}\n",
      "1161 {'Id': '1161', 'Type': 'answer', 'ParentId': '1125', 'urls': [], 'exp': [], 'Body': 'First, as others have pointed out, the group of volume preserving diffeomorphisms will be infinite dimensional. For the second question, there is a beautiful technique known as Moser\\'s trick which answers it.  Moser\\'s trick, in fancy language, says that if (M,w) and (M,w\\') are two symplectic structures on the same manifold, and if [w] = [w\\'] in H^2(M;R) (de Rham cohomology), then there is a family of diffeomorphism f_t:M->M with f_0 = Id and such that f_1 pulls w\\' back to w. For a 2-dimensional compact, oriented manifold (like the sphere), we have H^2(M;R) = R (the real numbers), and a symplectic form is nothing but a nonzero element in R (which can be interpreted as the total volume).  Since in this setting, [w] = [w\\'] iff they both give the same signed volume, it follows from Moser\\'s trick that the group of (signed) volume preserving maps is connected. If we consider unsigned volume, there will be 2 components to the group diffeomorphisms preserving the unsigned volume.  This is because, as others have pointed out, one has the notion of \"degree\" which shows the map x-> -x is not homotopic to Id, even through just continuous maps (not neccesarily volume preserving).  This shows there are AT LEAST two componenets.  There are at most two components because every volume preserving diffeo can be connected to Id or (x -> -x) by Moser\\'s trick again. Edit:  I misspoke a little bit.  Moser\\'s trick says that if you have a family w_t of symplectic forms, then there is a family of diffeomorphisms as I described above.  It\\'s not clear to me that what I said (that it\\'s enough to have [w] = [w\\'] in H^2) is enough to guarantee that there is a family of symplectic forms connecting them.  Further, it seems that Moser\\'s trick only guarantees you have a path of diffeos which starts and ends at a volume preserving diffeo, but may not preserve volume for all time. However, in the case of S^2 (or any closed, orientable 2-manifold), I can patch things up. Given w and w\\', volume forms, with [w] = [w\\'] (i.e, they have the same volume), then the form w_t = tw + (1-t)w\\' is a path of symplectic forms which connects them.  For a fixed t, the form w_t is closed since it\\'s a sum of closed forms (or, even easier, because it has top degree), and is nondegenerate because it\\'s a volume form (integration shows the volume given is that of w).  The fact that the volume is constant for each w_t implies that the path of diffeos preserves volume for all time. (I wasn\\'t able to immediately convince myself that in general, the convex sum of symplectic forms was nondegenerate, hence my initial hesitation.  In fact, I think that it need not be nondegenerate.) '}\n",
      "1163 {'Id': '1163', 'Type': 'answer', 'ParentId': '128', 'urls': ['http://terrytao.wordpress.com/2010/03/28/254b-notes-1-equidistribution-of-polynomial-sequences-in-torii/'], 'exp': ['p(n)= \\\\chi n^d + a_{d-1} n^{d-1} + \\\\cdots + a_1 n + a_0', '\\\\chi', 'p(n) \\\\mod 1', 'd', 'd=1', 'e(x) = e^{2 \\\\pi i x}', '\\\\sum_{n=0}^{N-1} e(p(n)) = o(N).', 'h', '\\\\sum_{n=0}^{N-1} (1/h) \\\\left( e(p(n)) + e(p(n+1)) + \\\\cdots + e(p(n+h-1)) \\\\right).', '\\\\frac{\\\\sqrt{N}}{h} \\\\left[ \\\\sum_{n=0}^{N-1}  \\\\left( e(p(n))  + \\\\cdots + e(p(n+h-1)) \\\\right) \\\\overline{ \\\\left( e(p(n))  + \\\\cdots + e(p(n+h-1)) \\\\right)} \\\\right]^{1/2}.', 'h^2', 'e(p(n) - p(n+k))', 'h', 'k=0', 'N', 'h^2-h', '\\\\sum_{n=0}^{N-1} e(q(n))', 'q', '\\\\chi d n^{d-1}', 'o(N)', 'hN+o(N)', 'o', 'h', '\\\\chi', 'N/\\\\sqrt{h} + o(N).', 'h'], 'Body': \"There is a fairly good exposition in Terry Tao's post, see Corollaries 4-6. Here is a sketch: We prove the more general statement: Let <math_exp> be any polynomial, with <math_exp> irrational. Then <math_exp> is equidistributed. Our proof is by induction on <math_exp>; the base case <math_exp> is standard. Set <math_exp>. By the standard trickery with exponential polynomials, it is enough to show  <math_exp> Choose a positive integer <math_exp>. With a small error, we can replace the sum by <math_exp> By Cauchy-Schwarz, this is bounded by <math_exp> Expanding the inner sum, we get <math_exp> terms of the form <math_exp>. There are <math_exp> terms where <math_exp>; these each sum up to <math_exp>. For the other <math_exp> terms, the sum is of the form <math_exp>, where <math_exp> has leading term <math_exp>. By induction, each of these sums is <math_exp>. So the quantity in the square root is <math_exp> where the constant in the <math_exp> depends on <math_exp> and <math_exp>. Putting it all together, we get a bound of <math_exp> Since <math_exp> was arbitrary, this proves the result. \"}\n",
      "1165 {'Id': '1165', 'Type': 'answer', 'ParentId': '1057', 'urls': ['http://groupprops.subwiki.org/wiki/Cube_map_is_endomorphism_iff_abelian_%28if_order_is_not_a_multiple_of_3%29'], 'exp': [], 'Body': 'On the other hand, if the order of your group is not a multiple of 3 then it must be abelian! You can read a proof here '}\n",
      "1166 {'Id': '1166', 'Type': 'answer', 'ParentId': '1133', 'urls': [], 'exp': [], 'Body': \"Also, if you are OK with power series, the Prologue to Walter Rudin's Real and Complex Analysis seems like exactly what you want.  It's a beautiful development of exp, as well as sin, cos, e, and even &pi;, all quite organically. \"}\n",
      "1168 {'Id': '1168', 'Type': 'answer', 'ParentId': '1133', 'urls': [], 'exp': ['\\\\mathbb{C}', '\\\\mathbb{C}^*'], 'Body': 'I think essentially the same characterization holds. The complex exponential is the unique Lie group homomorphism from <math_exp> to <math_exp> such that the (real) derivative at the identity is the identity matrix. '}\n",
      "1169 {'Id': '1169', 'Type': 'answer', 'ParentId': '740', 'urls': ['http://en.wikipedia.org/wiki/Cauchy%27s_functional_equation'], 'exp': [], 'Body': 'The Cauchy functionals, which satisfy the very simple equation f(a+b) = f(a)+f(b) for all real a,b.  These are either a line through the origin (the \"nice\" ones) or really \"ugly\" functions that are discontinuous and unbounded in every interval.  The latter are possible because the Axiom of Choice implies (actually is equivalent to) that infinite dimensional vector spaces have bases; i.e. the reals over the rationals have a Hamel basis.  A great explanation of all this (including the nice/ugly terminology) is in Horst Herrlich\\'s monograph The Axiom of Choice. '}\n",
      "1170 {'Id': '1170', 'Type': 'answer', 'ParentId': '1146', 'urls': ['http://blog.ephorie.de/intuition-for-principal-component-analysis-pca', 'https://i.stack.imgur.com/cdIRZ.png', 'https://i.stack.imgur.com/NbrD7.png', 'http://blog.ephorie.de/intuition-for-principal-component-analysis-pca'], 'exp': [], 'Body': 'PCA basically is a projection of a higher-dimensional space into a lower dimensional space while preserving as much information as possible. I wrote a blog post where I explain PCA via the projection of a 3D-teapot...  ...onto a 2D-plane while preserving as much information as possible:  Details and full R-code can be found in the post: http://blog.ephorie.de/intuition-for-principal-component-analysis-pca '}\n",
      "1174 {'Id': '1174', 'Type': 'answer', 'ParentId': '1079', 'urls': [], 'exp': [], 'Body': 'If each function has a smallest period, and otherwise fits the conditions, then a proof may be forthcoming by attempting to compute the smallest period of the sum and failing.  However, things become unclear if there is no smallest period, as in the case of the characteristic function of the rationals.  Progress might be made in this case by decomposing such a function as an infinite sum of periodic functions, or at least give more counterexamples to study.  (e.g. Write the characteristic function of the rationals as an infinite sum of functions of smallest period 1. ) '}\n",
      "1175 {'Id': '1175', 'Type': 'answer', 'ParentId': '1079', 'urls': [], 'exp': ['f+g', 'f', 'g', 'T', 'f(x)', 'x', 'f(x+T) = f(x)', 'S', 'g(x)', 'x', 'g(x+S) = g(x)', 'f+g', 'Q', '\\\\frac{Q}{T} = \\\\frac{m}{n}', 'x', 'f(x+nQ)+g(x+nQ) = f(x)+g(x)', 'f(x+nQ)=f(x+mT)=f(x)', 'x', 'g(x+nQ)=g(x)', 'nQ', 'g', '\\\\frac{T}{S}'], 'Body': 'As a first start, if <math_exp> is periodic the period cannot be rational w.r.t to the period of <math_exp>  and <math_exp>. Let us suppose that <math_exp> is the smallest period of <math_exp>, i.e. for all <math_exp>, <math_exp>.Similarly <math_exp> is the smallest period of <math_exp>, i.e. forall <math_exp>, <math_exp>. If <math_exp> had a period <math_exp>, and <math_exp>, we have that forall <math_exp>, <math_exp>. But <math_exp>, Thus, forall <math_exp>, <math_exp> and therefore <math_exp> is a period of <math_exp>, which is impossible since it would mean that <math_exp> is rational. '}\n",
      "1176 {'Id': '1176', 'Type': 'answer', 'ParentId': '998', 'urls': ['http://www.mathematik.uni-kl.de/~gathmann/alggeom.php'], 'exp': [], 'Body': \"I'm really enjoying Andreas Gathmann's lecture notes.  They are pretty elementary and surprisingly complete (for lecture notes). Reid also has a really nice text on algebraic geometry («Undergraduate algebraic geometry»). \"}\n",
      "1177 {'Id': '1177', 'Type': 'question', 'Title': 'Comparing/Contrasting Cosine and Fourier Transforms', 'Tags': ['fourier-analysis'], 'AcceptedAnswerId': '1205', 'urls': [], 'exp': [], 'Body': 'What are the differences between a (discrete) cosine transform and a (discrete) Fourier transform? I know the former is used in JPEG encoding, while the latter plays a big part in signal and image processing. How related are they? '}\n",
      "1178 {'Id': '1178', 'Type': 'answer', 'ParentId': '1127', 'urls': [], 'exp': [], 'Body': \"I'll make the trivial answer: 1 dog at point A, facing point B, walking with a velocity of 0.  Presumably, you should really highlight that the dogs' velocities must be non-zero...this is the kind of side case that math people love to exploit. \"}\n",
      "1179 {'Id': '1179', 'Type': 'answer', 'ParentId': '123', 'urls': [], 'exp': ['v_1', 'v_2', 'v', 'v_1 + v_2', 'v/c', '\\\\tanh^{-1}(v/c)=\\\\tanh^{-1}(v_1/c) + \\\\tanh^{-1}(v_2/c)'], 'Body': \"Velocity addition in (special) relativity is not linear, but becomes linear when expressed in terms of hyperbolic tangent functions. More precisely, if you add two motions in the same direction, such as a man walking at velocity <math_exp> on a train that moves at <math_exp> relative to the ground, the velocity <math_exp> of the man relative to ground is not <math_exp>; velocities don't add (otherwise by adding enough of them you could exceed the speed of light).  What does add is the inverse hyperbolic tangent of the velocities (in speed-of-light units, i.e., <math_exp>). <math_exp> This is one way of deriving special relativity: assume that a velocity addition formula holds, respecting a maximum speed of light and some other assumptions, and show that it has to be the above. \"}\n",
      "1180 {'Id': '1180', 'Type': 'answer', 'ParentId': '501', 'urls': [], 'exp': [], 'Body': 'Disclaimer: I am not a finitist --- but as a theoretical computer scientist, I have a certain sympathy for finitism. The following is the result of me openly speculating what an \"official\" finitist response would be, based on grounds of computability. The short version is this: (a)&nbsp;It depends on what you mean by a \\'number\\', but there\\'s a reasonable approach which makes it reasonable to talk about finitistic approaches to real numbers; (b)&nbsp;What you can do finitisitically with numbers, real, rational, or otherwise, depends on how you represent those numbers. What is a number? Is &minus;1 a number? Is sqrt(2) a number? Is i&nbsp;=&nbsp;sqrt(&minus;1) a number? What about quaternions? --- I\\'m going to completely ignore this question and suggest a pragmatic, formalist approach: a \"number\" is an element of a \"number system\"; and a \"number system\" is a collection of expressions which you can transform or describe properties of in some given ways (i.e. certain given arithmetic operations) and test certain properties (e.g. tests for equality, ordering, etc.) These expressions don\\'t have to have a meaningful interpretation in terms of quantities or magnitudes as far as I\\'m concerned; you get to choose which operations/tests you care about. A finitist would demand that any operation or property be described by an algorithm which provably terminates. That is, it isn\\'t sufficient to prove existence or universality a la classical logic; existence proofs must be finite constructions --- of a \"number\", that is a representation in some \"number system\" --- and univserality must be shown by a computable test. Representation of numbers: How we represent the numbers matters. A finitist should have no qualms about rational numbers: ratios which ultimately boil down to ordered pairs. Despite this, the decimal expansions of these numbers may be infinitely long: 1/3 = 0.33333... what\\'s going on here? Well, the issue is that we have two representations for the same number, one of which is finite in length (and allows us to perform computations) and another which is not finite in length. However, the decimal expansion can be easily expressed as a function: for all k, the kth decimal place after the point is \\'3\\'; so you can still characterize it precisely in terms of a finite rule. What\\'s important is that there exists some finite way to express the number. But the way in which we choose to define the number (as a part of system or numbers, using some way of expressing numbers) will affect what we can do with it... there is now a question about what operations we can perform.--- For rationals-as-ratios, we can add/subtract, multiply/divide, and test order/equality. So this representation is a very good one for rationals. --- For rationals-as-decimal-expansions, we can still add/subtract and multiply/divide, by defining a new digit-function which describes how to compute the result from the decimal expansions; these will be messier than the representations as ratios. Order comparisons are still possible for distinct rationals; but you cannot test equality for arbitrary decimal-expansion representations, because you cannot necessarily verify that all decimal places of the difference |a&minus;b| are 0. The best you can do in general is testing \"equality up to precision &epsilon;\", wherein you show that |a&minus;b|&nbsp;&lt;&nbsp;&epsilon;, for some desired precision &epsilon;. This is a number system which informally we may say has certain amount of \"vagueness\"; but it is in principle completely specified --- there\\'s nothing wrong with this in principle. It\\'s just a matter of how you wish to define your system of arithmetic. What representation of reals? Obviously, because there are uncountably many real numbers, you cannot represent all real numbers even if you aren\\'t a finitist. But we can still express some of them. The same is true if you\\'re a finitist: you just don\\'t have access to as many, and/or you\\'re restricted in what you can do with them, according to what your representation can handle. --- Algebraic irrational numbers such as sqrt(2) can be expressed simply like that: \"sqrt(2)\". There\\'s nothing wrong with the expressions \"sqrt(2)&nbsp;&minus;&nbsp;1\" or \"[1&nbsp;+&nbsp;sqrt(5)]/2\" --- they express quantities perfectly well. You can perform arithmetic operations on them perfectly well; and you can also perform ordering/equality tests by transforming them into a normal form of the type \"[sum of integers and roots of integers]/[positive integer]\"; if the difference of two quantities is zero, the normal form of the difference will just end up being \\'0\\'. For order comparisons, we can compute enough decimal places of each term in the sum to determine whether the result is positive or negative, a process which is guaranteed to terminate. --- Numbers such as &pi; and e can be represented by decimal expansions, and computed with in this form, as with the rational numbers. The decimal expansions can be gotten from classical equalities (e.g. \"infinite\" series, except computing only partial sums; a number such as e may be expressed by some finite representation of such an \\'exact\\' formula, together with a computable function which describes how many terms of the series are required to get a correct evaluation of the first k decimal places.) Of course, what you can do finitistically with these representations is limited in the same way as described above with the rationals; specifically, you cannot always test equality. '}\n",
      "1181 {'Id': '1181', 'Type': 'question', 'Title': 'Intuitive explanation of the Burnside Lemma', 'Tags': ['group-theory', 'intuition'], 'AcceptedAnswerId': '1182', 'urls': ['http://en.wikipedia.org/wiki/Burnside%27s_lemma'], 'exp': [], 'Body': 'The Burnside Lemma looks like it should have an intuitive explanation. Does anyone have one? '}\n",
      "1182 {'Id': '1182', 'Type': 'answer', 'ParentId': '1181', 'urls': [], 'exp': ['n', 'r', 'O', 'c', 'O', 'p', 'q', 'd', 'c', 'p^{-1} q', 'd', 'r', 'd', 'p^{-1} pr', 'pr', 'c', 'd', 'pr', 'r', 'p^{-1} q', 'q', 'q', 'r', 'O', 'x', 'O', 'x', 'O', 'r', 'O'], 'Body': 'As an example, we consider the number of ways of colouring a cube with <math_exp> colours with uniqueness up to rotation. We call each unique colouring where rotations are not allowed a static colouring and each unique one where they are allowed a dynamic colouring. We define the the set of orbits to be the (disjoint) static colourings that correspond to each dynamic colouring. We will use rotations to mean a rotation that makes the cube occupy the same space, and as being unique if it is a unique function from the cube to the cube. This includes the identity rotation. Intuitively, the lemma says: Proposition 1. #Orbits * #Rotations = Sum for each rotation <math_exp> of #static colourings unchanged by this rotation We will now consider each orbit <math_exp> separately. Pick a static colouring <math_exp> inside <math_exp>. Suppose two (possibly equal) rotations <math_exp>, <math_exp> give the same static colouring, <math_exp>, when applied on <math_exp>. Then <math_exp> fixes <math_exp>. Additionally, suppose <math_exp> (possibly the identity) fixes <math_exp>. <math_exp> will also fix d. So <math_exp> will take <math_exp> to <math_exp>. Since <math_exp> is different for each <math_exp>, and <math_exp> is different for each <math_exp>, the mapping functions are injective in both directions and there is a bijection between the <math_exp> and <math_exp> values. So, for each <math_exp>, the number of rotations is the sum over each static colorings <math_exp> in <math_exp> times the number of rotations producing <math_exp>. This can be rewritten as the sum over each rotation r of the number of static colourings in <math_exp> fixed by <math_exp> (due to the bijection in the previous paragraph). We get proposition 1 by adding over all <math_exp>. The general proof is quite similar to this, except that it uses group theory. '}\n",
      "1183 {'Id': '1183', 'Type': 'question', 'Title': 'Division by imaginary number', 'Tags': ['algebra-precalculus'], 'AcceptedAnswerId': '1184', 'urls': ['http://en.wikipedia.org/wiki/Complex_numbers#Operations'], 'exp': ['2 \\\\over i', '{2 \\\\over i} = {2i \\\\over i^2} = {2i \\\\over -1} = -2i', '{2 \\\\over i} = {2 \\\\over \\\\sqrt{-1}} = {\\\\sqrt{4} \\\\over \\\\sqrt{-1}} = \\\\sqrt{4 \\\\over -1} = \\\\sqrt{-4} = 2i'], 'Body': 'I ran into a problem dividing by imaginary numbers recently. I was trying to simplify: <math_exp> I came up with two methods, which produced different results: Method 1: <math_exp> Method 2: <math_exp> I know from using the formula from this Wikipedia article that method 1 produces the correct result. My question is: why does method 2 give the incorrect result? What is the invalid step? '}\n",
      "1184 {'Id': '1184', 'Type': 'answer', 'ParentId': '1183', 'urls': [], 'exp': ['\\\\sqrt{4}/\\\\sqrt{-1} = \\\\sqrt{4/-1}', '\\\\sqrt{a}/\\\\sqrt{b} = \\\\sqrt{a/b}', 'a', 'b'], 'Body': 'The incorrect step is saying: <math_exp> The identity: <math_exp> is only justified when <math_exp> and <math_exp> are positive. '}\n",
      "1185 {'Id': '1185', 'Type': 'answer', 'ParentId': '1183', 'urls': ['https://math.stackexchange.com/questions/438/1-is-not-1-so-where-is-the-mistake/482#482'], 'exp': ['\\\\sqrt{a}*\\\\sqrt{b}=-\\\\sqrt{ab}', '\\\\frac{\\\\sqrt{a}}{\\\\sqrt{b}}=\\\\sqrt{\\\\frac{a}{b}}', 'a', 'b', '\\\\sqrt{ab}=\\\\sqrt{a}\\\\sqrt{b}', '\\\\sqrt{\\\\frac{a}{b}}=-\\\\frac{\\\\sqrt{a}}{\\\\sqrt{b}}', '\\\\sqrt{\\\\frac{b}{a}}=\\\\frac{\\\\sqrt{b}}{\\\\sqrt{a}}'], 'Body': \"This is exactly the same issue as in this question. Each non-zero complex number has two numbers that square to give it, with the same magnitude, but with opposite sign. When we define the square root function, we have to decide which of the roots we want. For positive numbers, it is obvious to choose the positive root. For negative number, we choose to have the positive imaginary values, although because of symmetry the choice doesn't mean much anyway. So, to see if the standard multiplication and division laws apply, then we have to consider domain the numbers are in. We already know they apply for non-negative real numbers. It is easy enough to verify that for negative numbers <math_exp> and <math_exp> We also see that, if <math_exp> is positive and <math_exp> negative, then <math_exp> <math_exp> and <math_exp> \"}\n",
      "1186 {'Id': '1186', 'Type': 'question', 'Title': 'What calculation shortcuts exist to help or speed-up mental (or paper) calculations?', 'Tags': ['big-list', 'arithmetic'], 'urls': [], 'exp': ['19', '38 \\\\cdot 19 = 38 \\\\cdot 20 - 38'], 'Body': 'Anything to speed up or simplify calculations. A simple example might be to get a multiple of <math_exp>, for instance, <math_exp>. (This is hard to tag with so few tags in play!) mental-calculations tips tricks shortcut cheats time-saver '}\n",
      "1187 {'Id': '1187', 'Type': 'question', 'Title': \"What's the most effective ways of teaching kids - times tables?\", 'Tags': ['soft-question', 'education'], 'urls': [], 'exp': ['6', '2', '5', '10'], 'Body': \"I'd like to help a <math_exp> year old who already has a pretty good grasp of <math_exp>, <math_exp>, and <math_exp> times tables. \"}\n",
      "1188 {'Id': '1188', 'Type': 'answer', 'ParentId': '1146', 'urls': [], 'exp': [], 'Body': 'Principal component analysis is a useful technique when dealing with large datasets. In some fields, (bioinformatics, internet marketing, etc) we end up collecting data which has many thousands or tens of thousands of dimensions. Manipulating the data in this form is not desirable, because of practical considerations like memory and CPU time. However, we can\\'t just arbitrarily ignore dimensions either. We might lose some of the information we are trying to capture! Principal component analysis is a common method used to manage this tradeoff. The idea is that we can somehow select the \\'most important\\' directions, and keep those, while throwing away the ones that contribute mostly noise. For example, this picture shows a 2D dataset being mapped to one dimension:   <img src=\"https://i.imgur.com/s4Q1Aru.gif\" alt=\"alt text\"> Note that the dimension chosen was not one of the original two: in general, it won\\'t be, because that would mean your variables were uncorrelated to begin with. We can also see that the direction of the principal component is the one that maximizes the variance of the projected data. This is what we mean by \\'keeping as much information as possible.\\' '}\n",
      "1189 {'Id': '1189', 'Type': 'answer', 'ParentId': '1186', 'urls': ['http://www.youtube.com/watch?v=M4vqr3_ROIk', 'http://rads.stackoverflow.com/amzn/click/0307338401'], 'exp': [], 'Body': 'Art Benjamin is your man! He has many tricks to speed up mental calculation and other fun mathemagical tricks. He also wrote two books on the subject! Here is a video of him in action: http://www.youtube.com/watch?v=M4vqr3_ROIk Here is his new book: http://www.amazon.com/Secrets-Mental-Math-Mathemagicians-Calculation/dp/0307338401 '}\n",
      "1190 {'Id': '1190', 'Type': 'answer', 'ParentId': '1187', 'urls': ['http://numberwarrior.wordpress.com/2010/04/15/are-learning-styles-dead/', 'http://www.abbeytape.com/ttrap/', 'http://overthecrescentmoon.blogspot.com/2009/09/skip-counting.html', 'http://numberwarrior.wordpress.com/2010/03/11/multiplication-table-game/', 'http://bit.ly/9SBY5U'], 'exp': [], 'Body': 'I\\'m guessing you are looking for a hard data research answer, but unfortunately, the studies I know of are dubious, partly because they rely heavily on learning style myths, partly because some are funded by commercial parties pushing particular systems. However, I can give some general advice -- Give the system for the 9s that the digits add to 9, let your pupil fiddle with the patterns, and they tend to get this one fairly quickly. Asian countries do fairly well with a \"musical\" or \"chant\" method. I\\'ve heard of a couple commercial products, including Times Table Rap. Play with the visual representation using marbles or tokens, especially so the pupil can understand why 3 x 7 is the same as 7 x 3. The \"circle\" method of visualizing skip counting (samples of what I mean here) can be brought in to help with understanding the \"rotating\" nature of counting on the times tables. There are a fair number of games that help multiplication table prowess, either intentionally or unintentionally. Yahtzee in particular naturally covers the 1-6 portion of the table. I\\'ve written a game myself, although I don\\'t know how well it would work with 6 year olds. While I wouldn\\'t necessarily recommend the DS game itself, check out how Professor Kageyama\\'s Maths Training: The Hundred Cell Calculation Method mixes up the columns and rows of the times table to force students to think about the arrangement. '}\n",
      "1192 {'Id': '1192', 'Type': 'answer', 'ParentId': '1186', 'urls': [], 'exp': [], 'Body': \"To square a number ending in 5: Remove the ending 5. Let the resulting number be n, and compute n(n+1). Append 25 to the end of n(n+1) and that's your answer. Example: 852. Here, we drop the last digit to get 8, compute 8*9 = 72, so 852 = 7225. Similarly, we can compute  1152. Here, we drop the last digit to get 11, compute 11*12 = 132, so 1152 = 13225. How does this work?: Note that (10n + 5)2 = 100n2 + 100n + 25 = 100 * n(n+1) + 25. \"}\n",
      "1193 {'Id': '1193', 'Type': 'answer', 'ParentId': '566', 'urls': [], 'exp': [], 'Body': \"Here are three of my favorite variations on the hats and prisoners puzzle that I've collected over time: Fifteen prisoners sit in a line, and hats are placed on their heads. Each hat can be one of two colors: white or black. They can see the colors of the people in front of them but not behind them, and they can’t see their own hat colors. Starting from the back of the line (with the person who can see every hat except his own), each prisoner must try to guess the color of his own hat. If he guesses correctly, he escapes. Otherwise, he is fed to cannibals (because that’s the canonical punishment for failing at hat problems). Each prisoner can hear the guess of each person behind him. By listening for painful screaming and the cheering of cannibals, he can also deduce if each of those guesses was accurate. Of course, this takes place in some magical mathematical universe where people don’t cheat. Assuming that they do not want to be eaten, find the optimal guessing strategy for the prisoners. (The cannibals should eat no more than one prisoner.) In the year 3141, Earth’s population has exploded. A countably infinite number of prisoners sit in a line (there exists a back of the line, but the other end extends forever). As in the previous problem, white and black hats are placed on their heads. Due to modern technology, each person can see the hat colors of all infinitely many people in front of them. However, they cannot hear what the people behind them say, and they do not know if those people survive. Assuming that they do not want to be eaten, find the optimal guessing strategy for the prisoners. Assume that there are enough cannibals to eat everyone who fails. (The cannibals should eat no more than finitely many prisoners. Assume the Axiom of Choice.) There are seven prisoners, and colored hats will be placed on their heads. The hats have seven possible colors (red, orange, yellow, green, blue, indigo, violet), and may be placed in any arrangement: all the same color, all different colors, or some other arrangement. Each person can see everyone else’s hat color but cannot see his own hat color. They may not communicate after getting their hats, and as in the previous problems, they remain in a magical universe where no one cheats. They must guess their hat colors all at the same time. If at least one person guesses correctly, they are all released. If no one guesses correctly, however, the entire group is fed to cannibals. Assuming that they don’t want to be eaten, find the optimal guessing strategy for the prisoners. (By this point, the cannibals have probably eaten far too much. It would be cruel to force them to eat any more, so spare the cannibals and find a way to guarantee that the seven prisoners survive.) \"}\n",
      "1194 {'Id': '1194', 'Type': 'answer', 'ParentId': '187', 'urls': ['http://en.wikipedia.org/wiki/Honeycomb_(geometry)#Space-filling_polyhedra.5B2.5D'], 'exp': [], 'Body': \"The Wikipedia article on honeycombs has several examples of 3d tilings by a single polyhedron which aren't vertex-transitive.  I am not positive if they are vertex-uniform, though, since (again) I don't really understand your definition and can't find a good one online. \"}\n",
      "1195 {'Id': '1195', 'Type': 'answer', 'ParentId': '1187', 'urls': [], 'exp': [], 'Body': 'My personal experience with my 7/8 year old is that Math War works very well for practice. You can play with a normal deck of cards---each player plays two cards, and announces the product, with the higher number winning all the cards. In case of tie, then there is \"War\", which means you put two (or more) cards face down and then have aother battle on top, with the winner taking the whole pile. You can also play the game with any set of ordinary flash cards. My son is interested to go for a long time with this... I would also add, however, that although I have heard many people say that one must master the basic math facts before learning more advanced math concepts, I think that this is  nonsense. Go ahead and talk about any kind of mathematics (at the right level) with your child. '}\n",
      "1196 {'Id': '1196', 'Type': 'question', 'Title': 'Proof that <span class=\"math-container\" id=\"10324\">n^3+2n</span> is divisible by <span class=\"math-container\" id=\"10325\">3</span>', 'Tags': ['elementary-number-theory', 'induction'], 'AcceptedAnswerId': '1199', 'urls': [], 'exp': ['n , n^3 + 2n', '3.', 'n = 0,', 'n^3 + 2n = 0^3 +', '2 \\\\times 0 = 0.', '3.', 'n', 'n^3+ 2n', '3.', 'n+1,', '( n + 1 )^3 + 2( n + 1 )', 'n^3 + 2n', '( n + 1 )^3+ 2( n + 1 ) = (  n^3 + 3n^2+ 3n + 1 ) + ( 2n + 2 ) \\\\{\\\\text{Just some simplifying}\\\\}', ' = ( n^3 + 2n ) + ( 3n^2+ 3n + 3 ) \\\\{\\\\text{simplifying  and regrouping}\\\\}', ' = ( n^3 + 2n ) + 3( n^2 + n + 1 ) \\\\{\\\\text{factored out the 3}\\\\}', '3', '(n^3 + 2n )', '3', '(n^3+ 2n ) + 3( n^2 + n + 1 )', '3.'], 'Body': \"I'm trying to freshen up for school in another month, and I'm struggling with the simplest of proofs! For any natural number <math_exp> is divisible by <math_exp>   This makes sense Basis Step: If <math_exp> then <math_exp>   <math_exp> So it is divisible by <math_exp> Induction: Assume that for an arbitrary natural number <math_exp>,   <math_exp> is divisible by <math_exp> Induction Hypothesis: To prove this for <math_exp> first try to express <math_exp> in terms of <math_exp> and use   the induction hypothesis. Got it <math_exp> <math_exp>   <math_exp> which is divisible by <math_exp>, because <math_exp> is divisible by <math_exp>   by the induction hypothesis. What? Can someone explain that last part? I don't see how you can claim <math_exp> is divisible by <math_exp> \"}\n",
      "1197 {'Id': '1197', 'Type': 'answer', 'ParentId': '1196', 'urls': [], 'exp': ['n.', 'k.', 'k+1.', '(n=1)', 'n=2', 'n=3,', 'n.', '( n^3+ 2n ) + 3( n^2+ n + 1 )', '3', '[(n+1)^3 + 2(n+1)] + 3[ (n+1)^2 + (n+1) + 1 ].'], 'Body': 'In a proof by induction, we try to prove that a statement is true for all integers <math_exp> To do this, we first check the base case, which is the \"Basic Step\" above. Then, we have the induction hypothesis, where we assume that the statement is true for an integer <math_exp> Using this fact, we prove that the statement is also true for the next integer <math_exp> This produces a bootstrapping ladder where we use the base case <math_exp> to show that the statement is true for <math_exp> via the inductive hypothesis, and then for <math_exp> etc, off to infinity; this shows that the statement is true for all integers <math_exp> Here, we claimed that <math_exp> is divisible by <math_exp> because this was the inductive hypothesis; we were using this to show that <math_exp> '}\n",
      "1198 {'Id': '1198', 'Type': 'answer', 'ParentId': '1196', 'urls': [], 'exp': [], 'Body': \"Why don't you just test the validity of this using modular arithmetic? A useful idea when thinking of induction is to think of dominos. If you know something is true for one fixed tile and if you know that it being true for one tile means that it's true for the neighbour on the right, then it's like knocking one over knocks them all over. \"}\n",
      "1199 {'Id': '1199', 'Type': 'answer', 'ParentId': '1196', 'urls': [], 'exp': ['n^3 + 2n', 'n', 'n+1', '(n^3 + 2n) + 3(n^2 + n + 1)', 'n^3 + 2n', '3(n^2 + n + 1)'], 'Body': \"In the inductive hypothesis, you assumed that <math_exp> was divisible by 3 for some <math_exp>, and now you're proving the same for <math_exp>. It's like knocking down dominoes: if you can prove that the first domino falls over (base case) and each domino knocks over the next (inductive step), then that means that all of the dominoes get knocked down eventually. You know that <math_exp> is divisible by 3 because <math_exp> is (because of the inductive hypothesis) and <math_exp> is (because it's 3 times an integer). So, their sum is as well. \"}\n",
      "1200 {'Id': '1200', 'Type': 'answer', 'ParentId': '1196', 'urls': [], 'exp': ['n^3+2n = n^3 - n + 3n = (n-1)(n)(n+1) + 3n'], 'Body': \"Presumably you're only looking for a way to understand the induction problem, but you can note that <math_exp>. Since any three consecutive integers has a multiple of three, we're adding two multiples of three and so get another multiple of 3. \"}\n",
      "1201 {'Id': '1201', 'Type': 'answer', 'ParentId': '1186', 'urls': [], 'exp': [], 'Body': \"One simple one that you're probably familiar with already: When you multiply a one-digit number, n, by 9, the result has n-1 in the 10s place, and then the ones digit is such that the sum of the digits is 9. Example: 9*6 = 54 because 5 is 6-1, and then 5+4 is 9. What's really cool is that you can use this trick, plus your fingers, to get the answer instantly. Hold out your hands with all 10 fingers up, then put down the n'th finger (which might be on either hand. Then the number is just (how many fingers there are to the left of the finger you put down)*10 + (how many fingers there are to the right of the finger you put down) \"}\n",
      "1202 {'Id': '1202', 'Type': 'question', 'Title': '<span class=\"math-container\" id=\"10486\">f(a(x))=f(x)</span> - functional equation', 'Tags': ['functional-equations'], 'AcceptedAnswerId': '1203', 'urls': [], 'exp': ['a(x)', 'f(a(x))=f(x)', 'f(x) = T\\\\,[x,a(x)]', 'T', 'u', 'v', '(1)', '(2)'], 'Body': 'I was reading \"Functional Equations and How to Solve Them\" by Small and the following comment pops up without much justification on p. 13: If <math_exp> is an involution, then <math_exp> has as solutions <math_exp>, where <math_exp> is an arbitrary symmetric function of <math_exp> and <math_exp>. I was wondering why this was true (it works for examples I\\'ve tried, but I am not sure <math_exp> how to prove this and <math_exp> if there\\'s anything obvious staring at me in the face here). '}\n",
      "1203 {'Id': '1203', 'Type': 'answer', 'ParentId': '1202', 'urls': [], 'exp': [], 'Body': 'Any function f(x) that is a solution to your functional equation f(a(x)) = f(x) must satisfy the property that it is unchanged when you plug in a(x) instead of x. In addition, f(x) clearly must be a function f(x) = T[x, a(x)] depending on the x and a(x); the question is to see why T must be symmetric. Now, T[x, a(x)] = f(x) = f(a(x)) = T[a(x), a(a(x))] = T[a(x), x] since a(x) is an involution. In particular, this means that T must be symmetric in its two variables. '}\n",
      "1204 {'Id': '1204', 'Type': 'question', 'Title': 'Why are superalgebras so important?', 'Tags': ['abstract-algebra'], 'AcceptedAnswerId': '1208', 'urls': [], 'exp': ['\\\\mathbb Z/2\\\\mathbb Z'], 'Body': \"I know that a superalgebra is a <math_exp>-graded algebra and that it behaves nicely. I know very little physics though, so even though I know that the super- prefix is related to supersymmetry, I don't know what that means; is there a compelling mathematical reason to consider superalgebras? \"}\n",
      "1205 {'Id': '1205', 'Type': 'answer', 'ParentId': '1177', 'urls': ['http://books.google.com/books?id=ozcvQUqr-GIC', 'http://books.google.com/books?id=coq49_LRURUC'], 'exp': [', but note that in usual Fourier work, the indexing is taken to be from ', ' to ', ' instead of ', ' to ', ') is easily seen. DCT-II requires only half of the given sequence to do its job: (We see in this example that the exploitation of symmetry in this case led to a slightly more accurate result.) The other two types of discrete cosine transforms, as well as the four types of discrete sine transforms, are intended to be redundancy-free methods for computing discrete Fourier transforms. For DCT-I, one can deal with a sequence of length ', ' instead of a sequence of length ', ', while for DCT-II, only a length '], 'Body': 'Cosine transforms are nothing more than shortcuts for computing the Fourier transform of a sequence with special symmetry (e.g. if the sequence represents samples from an even function). To give a concrete example in Mathematica (<math_exp>0<math_exp>n-1<math_exp>1<math_exp>n<math_exp>\\\\frac{N}{2}+1<math_exp>N<math_exp>\\\\frac{N}{2}$ sequence is required. This represents a savings in computational time and effort. (I assume the case of even length here; a similar symmetry property can be established for the case of odd length.) In any event, I wish to point out two good references on how FFT and the DCTs/DSTs are related:  and . '}\n",
      "1206 {'Id': '1206', 'Type': 'answer', 'ParentId': '524', 'urls': [], 'exp': [], 'Body': 'It\\'s all dependent on the 2-norm condition number of your matrix (the ratio of the largest singular value to the smallest); as a nice rule of thumb, if the base-10 logarithm of the reciprocal of the condition number is much less than the number of digits your computer uses to store numbers, QR (and maybe even the normal equations) might be sufficient. Otherwise, SVD is a \"safer bet\": it always works, but is much slower than the other methods for solving least squares problems. Good references would be the classic \"Solving Least Squares Problems\" by Lawson and Hanson, and the newer \"Numerical Methods for Least Squares Problems\" by Björck. To add to the answer I gave previously, one way you can proceed for a matrix A whose conditioning you don\\'t know would be as follows: The advantage of proceeding in this manner is if your matrix is badly conditioned enough to necessitate the use of SVD, the algorithm for computing the SVD has to handle only a triangular matrix instead of the original matrix (which may have more rows than columns as is usual in least-squares applications). '}\n",
      "1207 {'Id': '1207', 'Type': 'answer', 'ParentId': '1091', 'urls': ['http://www.mathworks.com/moler/eigs.pdf'], 'exp': [], 'Body': 'Instead of giving an answer, let me point out to you this chapter in Cleve Moler\\'s book \"Numerical Computing with MATLAB\", there is a nice geometric demonstration in MATLAB on how eigenvalues/eigenvectors (as well as singular values/vectors) of an order-2 square matrix are involved in how a circle is transformed into an ellipse after a linear transformation represented by the matrix. '}\n",
      "1208 {'Id': '1208', 'Type': 'answer', 'ParentId': '1204', 'urls': [], 'exp': ['\\\\mathbb{C}', 'ab = (-1)^{\\\\deg b \\\\deg a} ba', '\\\\mathbb{Z}', '\\\\mathbb{Z}/2'], 'Body': 'Sure, supermodules over a superalgebra form a nice and nontrivial example of a tensor category. In fact, it is a theorem that Deligne that all tensor categories over <math_exp> with suitable growth conditions can be obtained as categories of super-representations. Another reason is given by Qiaochu, that supercommutativity arises naturally in a lot of places. For instance, the exterior algebra is supercommutative; thus the wedge product on differential forms acts the same way.  Interestingly, the exterior algebra of a direct sum corresponds to taking the super tensor product. Another example: With singular cohomology on a topological space, there is a way to define a cup product, which satisfy the supercommutative law <math_exp>. (Note that given a <math_exp>-graded super-commutative algebra, you can get a <math_exp>-graded superalgebra as you indicate in your question by taking the sum of the odd parts and the sum of the even parts.) '}\n",
      "1209 {'Id': '1209', 'Type': 'answer', 'ParentId': '1204', 'urls': [], 'exp': ['V', 'n', '\\\\left( {n \\\\choose k} \\\\right) = {n+k-1 \\\\choose k}', 'k', 'n', 'V', '\\\\displaystyle \\\\left( {n \\\\choose 1} \\\\right), \\\\left( {n \\\\choose 2} \\\\right), ... ', 'V', '\\\\displaystyle {n \\\\choose 1}, {n \\\\choose 2}, ....', '{n \\\\choose k} = (-1)^k \\\\left( {-n \\\\choose k} \\\\right)', 'k^{th}', 'n', 'k^{th}', '-n', 'V', 'V_0 \\\\oplus V_1', '\\\\dim V_0 + \\\\dim V_1', '\\\\mathbb{Z}/2\\\\mathbb{Z}', '\\\\mathbb{Z}/2\\\\mathbb{Z}', '\\\\dim V_0 - \\\\dim V_1', 'k^{th}', 'n', 'n', '\\\\left( {n \\\\choose k} \\\\right)'], 'Body': \"I can summarize one really basic reason, which is actually the reason I originally got interested in the definition.  Take a finite-dimensional vector space <math_exp> of dimension <math_exp>, and let <math_exp> denote the number of multisets of size <math_exp> on a set of size <math_exp>.  (Multisets are like subsets except that more than one copy of a given element is possible.)  Then the symmetric powers of <math_exp> have dimensions <math_exp> whereas the exterior powers of <math_exp> have dimensions <math_exp> Now here is a funny identity: it is not hard to see that <math_exp>.  One way we might interpret this identity is that the <math_exp> exterior power of a vector space of dimension <math_exp> is like the <math_exp> symmetric power of a vector space of dimension <math_exp>, whatever that means.  So what could that possibly mean? The answer (and I'll let you work this out for yourself, because it's fun) is to work in the category of supervector spaces!  A supervector space <math_exp> is a direct sum <math_exp>, and while one notion of dimension is to take <math_exp>, another (which can be motivated by thinking of <math_exp>-graded vector spaces as the category of representations of <math_exp>) is to take <math_exp>.  So while a purely even vector space has positive dimension, a purely odd vector space has negative dimension. Then: graded-commutativity implies that the symmetric power of a purely odd vector space is the exterior power of the corresponding purely even vector space.  More generally, the <math_exp> symmetric power of a vector space of dimension <math_exp> (for all integers <math_exp>) has dimension <math_exp>. (And, of course, the symmetric algebra of a supervector space is naturally a graded-commutative superalgebra.) (The physics connection is that symmetric powers = bosons, exterior powers = fermions, and there is a duality between the two.) \"}\n",
      "1210 {'Id': '1210', 'Type': 'answer', 'ParentId': '631', 'urls': [], 'exp': [], 'Body': 'It\\'s worth considering a few ways of showing that the problem is neither in P, nor NPC. I\\'ve marked this answer \"community wiki\", so please feel free to add suggestions and flesh out ideas here. Based on my experience playing the 24 game, it seems that most combinations of numbers are solvable. If we could formalize this, we could show that the 24 game is not NPC. Formally, consider the 2^n inputs of length n. If all but polynomially-many of them solvable, then the language is sparse and cannot be NPC (unless P=NP). '}\n",
      "1211 {'Id': '1211', 'Type': 'question', 'Title': 'Non-integer powers of negative numbers', 'Tags': ['complex-numbers', 'exponentiation'], 'AcceptedAnswerId': '1269', 'urls': ['https://math.stackexchange.com/questions/1183/division-by-imaginary-number/1185#1185'], 'exp': [], 'Body': 'Roots behave strangely over complex numbers. Given this, how do non-integer powers behave over negative numbers? More specifically: '}\n",
      "1212 {'Id': '1212', 'Type': 'answer', 'ParentId': '1211', 'urls': [], 'exp': ['z^w := e^{(l(z)*w)}', 'z,w \\\\in \\\\mathbb{C}', 'l', 'l:U\\\\to\\\\mathbb{C}', 'U', '\\\\mathbb{C}\\\\setminus\\\\{0\\\\}'], 'Body': 'We can define complex powers as <math_exp> for <math_exp> and a complex logarithm function <math_exp> of course you need to make sure there actually exists such a logarithm <math_exp> function (<math_exp> needs to be a simply-connected subset of <math_exp> '}\n",
      "1213 {'Id': '1213', 'Type': 'answer', 'ParentId': '1211', 'urls': ['http://en.wikipedia.org/wiki/Exponentiation#Powers_of_complex_numbers', 'http://en.wikipedia.org/wiki/Complex_logarithm#Problems_with_inverting_the_complex_exponential_function', 'https://math.stackexchange.com/questions/1133/natural-derivation-of-the-complex-exponential-function/1158#1158'], 'exp': ['z^a=e^{a log(t)}', 'Log(z)=ln\\\\mid z\\\\mid +iArg(z)'], 'Body': 'Wikipedia says \"Complex powers of positive reals are defined via e^x as in section Complex powers of positive real numbers above. These are continuous functions. Trying to extend these functions to the general case of noninteger powers of complex numbers that are not positive reals leads to difficulties. Either we define discontinuous functions or multivalued functions.\" So let me say a few more words. Essentially what we are looking for is <math_exp>. But as you may or may not know, complex logarithms are problematic depending on your viewpoint. In an answer to Harry\\'s question, I describe a more careful way to define the complex exponential function, and suggest that one should just inverse this to get your logarithm. This way is safe and will yield your logarithm. If you are less fickle, and are willing to take on the branched logarithms, may I suggest <math_exp> where Arg is the principle argument. This will lead you to branched exponents. '}\n",
      "1214 {'Id': '1214', 'Type': 'answer', 'ParentId': '768', 'urls': [], 'exp': ['N(\\\\lambda P)\\\\approx\\\\lambda^2(i+\\\\frac{b}{2}-1)S(P)'], 'Body': \"(For an integer &lambda; and a lattice polygon P) denote by &lambda;P polygon P stretched by &lambda; times. Then number N(&lambda;P) of points inside the polygon &lambda; is a quadratic polynomial in &lambda; with leading coefficient S(P). This is the form of Pick's theorem that holds for any lattice (and obvious analogue works in any dimension&nbsp;&mdash; unlike usual Pick's formula that has no analogue in 3d even for the cubic lattice). For the square lattice it yields ordinary Pick's theorem since for a parallelogram P on the square lattice <math_exp> as &lambda;&rarr;&infin; and general theorem follows from the theorem for parallelograms by additivity. \"}\n",
      "1215 {'Id': '1215', 'Type': 'question', 'Title': 'Solution to <span class=\"math-container\" id=\"10567\">1-f(x) = f(-x)</span>', 'Tags': ['algebra-precalculus', 'functional-equations'], 'urls': [], 'exp': ['f(x)', '1-f(x) = f(-x)', 'x', 'f(-x) + f(x) = 1', 'f(x) = |x|', 'x'], 'Body': 'Can we find <math_exp> given that <math_exp> for all real <math_exp>? I start by rearranging to: <math_exp>. I can find an example such as <math_exp> that works for some values of <math_exp>, but not all. Is there a method here? Is this possible? '}\n",
      "1216 {'Id': '1216', 'Type': 'answer', 'ParentId': '1215', 'urls': [], 'exp': ['f(x)=\\\\frac{1}{2}+\\\\text{(any odd function)}.', 'f(x)=\\\\frac{1}{2}+x', 'f(x)=\\\\frac{1}{2}+99x^3+7x^5'], 'Body': '<math_exp> For example, <math_exp> or, say, <math_exp>. '}\n",
      "1217 {'Id': '1217', 'Type': 'answer', 'ParentId': '1215', 'urls': [], 'exp': ['f(x) = \\\\begin{cases} 1 \\\\quad x&gt;0, \\\\\\\\ 1/2 \\\\quad x=0, \\\\\\\\ 0 \\\\quad x&lt;0\\\\end{cases}.', 'x &gt; 0', '-x &lt; 0', 'f(x)+f(-x)=1+0=1', 'x&lt;0', 'x=0', 'f(x)+f(-x)=(1/2)+(1/2)=1'], 'Body': 'Let <math_exp> If <math_exp>, then <math_exp> so that <math_exp> Likewise with <math_exp>. If <math_exp>, then <math_exp>. '}\n",
      "1218 {'Id': '1218', 'Type': 'answer', 'ParentId': '1215', 'urls': ['https://math.stackexchange.com/questions/1215/solution-to-1-fx-f-x/1216#1216'], 'exp': [], 'Body': 'Clearly, we only have relations between f(x) and f(-x). The relation means that 0 has to have value 1/2. We can divide all non-zero real numbers into disjoint pairs of x and -x and define the function f on each pair separately. For each pair, f(x) can be given any value and then f(-x) has a single valid value. As mentioned by Grigory, the valid functions can be characterised as any odd function plus 1/2. '}\n",
      "1219 {'Id': '1219', 'Type': 'answer', 'ParentId': '1215', 'urls': [], 'exp': ['f(x) = ax + b', 'f(x) + f(-x) = 1 \\\\implies ax + b + a \\\\cdot (-x) + b = 1 \\\\implies 2b = 1 \\\\implies b = 1/2', 'f(x) = ax + 1/2'], 'Body': 'Usually simple problems like this ask you to find a function that respects the condition, not all of them. And (again) usually you start by checking if a simple polynomial function of the first degree could be a solution. So, if <math_exp> Then <math_exp> So the condition is satisfied by any function of the type: <math_exp> '}\n",
      "1220 {'Id': '1220', 'Type': 'question', 'Title': 'Uniqueness of Characterstic Functions in Probability', 'Tags': ['probability', 'probability-theory'], 'AcceptedAnswerId': '1232', 'urls': ['http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29'], 'exp': [' \\\\text{Char of }X (t)=E[e^itX]', 'e^{iz}', '2 \\\\pi', 'z'], 'Body': 'According to Wikipedia, a characteristic function completely determines the properties of a probability distribution. This means it must be unique. However, the definition given is: <math_exp> Now <math_exp> repeats for every <math_exp> increase in <math_exp>. So how can it be unique? '}\n",
      "1221 {'Id': '1221', 'Type': 'answer', 'ParentId': '1183', 'urls': [], 'exp': ['\\\\frac{a+bi}{c+di}', 'x+yi', '(c+di)(x+yi) = a+bi'], 'Body': 'The only soundproof way to be sure to find the right result while dividing two complex numbers <math_exp> is reducing it to a multiplication. The answer is of the form <math_exp>; therefore <math_exp> and you will end up with two linear equations, one for the real coefficient and another for the imaginary one. As Simon and Casebash already wrote, taking a square root leads to problems, since you cannot be sure which value must be chosen. '}\n",
      "1222 {'Id': '1222', 'Type': 'question', 'Title': 'Books/Notes recommendation request: Multivalued functions/Riemann surfaces', 'Tags': ['reference-request', 'learning', 'multivalued-functions'], 'AcceptedAnswerId': '1243', 'urls': [], 'exp': [], 'Body': 'I\\'m trying to read a document that applies Riemann-Roch left, right and center. I don\\'t know this theorem or the theory it comes from so I need to build up a bit more background before I can tackle this. Can you please recommend good books or (online) lecture notes which cover \"multi-valued functions\", Riemann Surfaces and similar up to (at least) Riemann-Roch? (I also want to pick up a bit about modular forms and the relation between lattices and elliptic curves). (I\\'ve done a bit of complex calculus but it was all with analogy to real analysis so I am not sure that it will really give me any head start here. Also apologies for being so vauge with this but I don\\'t know enough about this subject to be any more precise) '}\n",
      "1223 {'Id': '1223', 'Type': 'answer', 'ParentId': '1215', 'urls': [], 'exp': ['g', '-g(x)=g(-x)', 'x', '(x,g(x))', 'g', '(-x,-g(x))', 'g(-x)=-g(x)', 'g', '180', '(0,0)', '1-f(x)=f(-x)', '(0,1/2)', '(x,f(x))', '(-x,1-f(x))', 'f', '1-f(x)=f(-x)', 'x', '(0,1/2)', '180'], 'Body': \"Consider instead the functions <math_exp> that satisfy the identity <math_exp> for all <math_exp>.  If <math_exp> is a point of the function <math_exp>, then <math_exp> is also a point (since <math_exp>).  Therefore, every function <math_exp> is symmetric when rotated by <math_exp> degrees about the point <math_exp>. How do things change for the identity <math_exp>?  We merely shift the point of symmetry to <math_exp>.  Here the point <math_exp> implies the point <math_exp>. The function <math_exp> satisfies the identity <math_exp> for all real numbers <math_exp> if and only if it is symmetric when rotated about the point <math_exp> by <math_exp> degrees. There's going to be many of these functions; some of which will be polynomials, some of which will not. \"}\n",
      "1224 {'Id': '1224', 'Type': 'answer', 'ParentId': '97', 'urls': ['http://people.math.sfu.ca/~cbm/aands/page_297.htm', 'http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.90.6481', 'http://homepages.physik.uni-muenchen.de/~Winitzki/erf-approx.pdf', 'https://stats.stackexchange.com/questions/7200/10300#10300'], 'exp': ['\\\\exp(-x^2)'], 'Body': 'I am assuming that you need the error function only for real values. For complex arguments there are other approaches, more complicated than what I will be suggesting. If you\\'re going the Taylor series route, the best series to use is formula 7.1.6 in Abramowitz and Stegun. It is not as prone to subtractive cancellation as the series derived from integrating the power series for <math_exp>. This is good only for \"small\" arguments. For large arguments, you can use either the asymptotic series or the continued fraction representations. Otherwise, may I direct you to these papers by S. Winitzki that give nice approximations to the error function. (added on 5/4/2011) I wrote about the computation of the (complementary) error function (couched in different notation) in this answer to a CV question. '}\n",
      "1225 {'Id': '1225', 'Type': 'answer', 'ParentId': '697', 'urls': ['http://dx.doi.org/10.1007/BF02162565'], 'exp': [], 'Body': 'The usual cases treated apart from least squares are the one-norm and infinity-norm (Chebyshev) cases; they crop up in function approximation for instance. Usually both of these are solved via linear programming techniques. '}\n",
      "1227 {'Id': '1227', 'Type': 'answer', 'ParentId': '220', 'urls': [], 'exp': ['(x_n)_{n\\\\in\\\\mathbb{N}}', 'f_n', 'f_0 = 0', 'f_{n+1} = (1-c)f_n + c\\\\cdot x_{n+1}', 'x_n=\\\\sin(\\\\omega n)', 'f_n', '\\\\omega', 'f_n = Ae^{i\\\\omega n}', 'x_n = e^{i\\\\omega n}', 'f_{n+1} = e^{i\\\\omega (n+1)} = e^{i\\\\omega} e^{i\\\\omega n} = e^{i\\\\omega} f_n', 'e^{i\\\\omega} A e^{i\\\\omega n} = (1-c)Ae^{i\\\\omega n} + c e^{i\\\\omega} e^{i\\\\omega n}', 'A(\\\\omega) = \\\\frac{c}{1 - (1-c)e^{-i\\\\omega}}', 'x_n = \\\\sin(\\\\omega n) = \\\\frac1{2i}(e^{i\\\\omega n}-e^{-i\\\\omega n})', 'f_n = \\\\frac1{2i}(A(\\\\omega)e^{i\\\\omega n}-\\\\bar A(\\\\omega)e^{-i\\\\omega n}) = |A(\\\\omega)| \\\\sin(\\\\omega n + \\\\phi)', 'A(\\\\omega) =: |A(\\\\omega)| e^{i\\\\phi}', 'A(\\\\omega)', '|A(\\\\omega)|'], 'Body': \"As I understand it, you are given a sequence <math_exp> of input values from which you calculate a sequence <math_exp> that is given by the following recurrence relations: <math_exp> <math_exp> Your question is: given a sine wave <math_exp>, you assert that <math_exp> is also a sine wave and you want to know its amplitude in dependence on the frequency <math_exp>. Answer: It's easier to calculate the frequency response with exponential functions instead of sine waves. <math_exp> <math_exp> Since <math_exp>, the recurrence relation gives <math_exp> which implies <math_exp> To calculate the response for sine waves, you can represent the sine function as a linear combination of two exponential functions <math_exp> and obtain <math_exp> where <math_exp>. In other words, the complex frequency response <math_exp> encodes both the change in amplitude and the phase shift. I'll leave the exact calculation of <math_exp> to you, so that you can get familiar with this method of calculation. Without exponential functions, you'd have to use the addition and subtraction theorems for sine and cosine. Put differently, exponential functions provide a very convenient formulation of the trigonometric identities. \"}\n",
      "1228 {'Id': '1228', 'Type': 'answer', 'ParentId': '83', 'urls': [], 'exp': [], 'Body': 'Show them large buildings, tallest buildings, pyramids, visualize them about different structure. Explain importance of maths Brain remember things easily when comparison is done. '}\n",
      "1229 {'Id': '1229', 'Type': 'answer', 'ParentId': '1220', 'urls': [], 'exp': [], 'Body': \"It's just a Fourier transform. E(x) is an integral over the probability distribution. The function is unique; if you focus on the value inside the expectation integral, that's not, but so what? \"}\n",
      "1230 {'Id': '1230', 'Type': 'answer', 'ParentId': '1186', 'urls': ['http://en.wikipedia.org/wiki/FOIL_method'], 'exp': [], 'Body': 'When squaring a number, break the calculation into three smaller calculations and add them using the FOIL method. Example: 302^2 = 300^2 + 2*(300*2) + 2^2. '}\n",
      "1231 {'Id': '1231', 'Type': 'answer', 'ParentId': '220', 'urls': ['http://www.wolframalpha.com/input/?i=plot+|.5/%281-.5+exp%28-ix%29%29|'], 'exp': ['A(\\\\omega)', 'c/(1-(1-c)e^{-i\\\\omega})', 'A(\\\\omega)', '\\\\sin \\\\omega n', 'e^{i\\\\omega n}', '\\\\sin\\\\omega n', 'A(\\\\omega)e^{i\\\\omega n}', '|A(\\\\omega)|', 'c=1/2'], 'Body': 'I don\\'t have enough mojo to comment on Greg\\'s answer. Greg made a silly calculational mistake: The transfer function <math_exp> should be <math_exp>. What you want is the modulus of <math_exp>. Note that <math_exp> is precisely the imaginary part of <math_exp>. Because the relation between input and output is linear, the response to <math_exp> will be the imaginary part of <math_exp>. That\\'s going to be a sinusoid with some shifting and the amplitude <math_exp>. Here is a plot for <math_exp>. To read more about this sort of things, google \"IIR filter\" or \"infinite impulse response\". '}\n",
      "1232 {'Id': '1232', 'Type': 'answer', 'ParentId': '1220', 'urls': [], 'exp': ['X', 'X+2k\\\\pi/t'], 'Body': 'I thought at first you were asking how the characteristic function can be unique. There\\'s no issue here, because e^ix is a well-defined function: for a given value of x, there is a unique value of e^ix. And so the expectation (an integral) has a unique value as well. On second thought, it appears you\\'re asking how it\\'s possible, considering that e^ix is not injective (i.e., multiple x can have the same value of e^ix), that the original distribution can be completely recovered from the characteristic function. The answer to that is that you\\'re probably missing the \"t\" in the expression: the characteristic function is a function of t, and although for a given value of t (e.g. t=1), the random variables <math_exp> and <math_exp> would have the same value of the characteristic function at that point, they would have different values at other t. So the characteristic functions are different, and yes, the distribution can be completely recovered from the characteristic function. '}\n",
      "1233 {'Id': '1233', 'Type': 'answer', 'ParentId': '967', 'urls': ['http://www.cs.kent.ac.uk/people/staff/sjt/TTFP/'], 'exp': [], 'Body': 'I like Type Theory and Functional Programming by Simon Thompson. '}\n",
      "1234 {'Id': '1234', 'Type': 'question', 'Title': 'Is this version of the Hanoi towers problem NP-complete?', 'Tags': ['algorithms', 'recreational-mathematics', 'np-complete'], 'urls': ['http://rgrig.blogspot.com/2010/07/solitaire.html'], 'exp': ['x_1', 'x_2', 'x_n', '|x_k-x_{k+1}|', 'x_k', 'O(n2^n)', 'x_1', 'x_2', 'x_n', '2^n', 'n'], 'Body': \"This was really inspired by Solitaire, but a few people reacted with ``oh, it's like the towers of Hanoi, isn't it?'' so I'll try to pose the problem in terms of discs here. Let's start. There are n disks on the real line, one of size 1 at position <math_exp>, one of size 2 at position <math_exp>, ..., and one of size n at position <math_exp>. Your goal is to make a tower with all n discs, consuming as little energy as possible in the process. You are allowed to move a tower whose base is a disk of size k only on top of the disk with size k+1 (which may be the top of another mini-tower). The energy you consume to perform such a move is the distance traveled by the moved mini-tower. For example, the energy consumed by the first move is <math_exp>. Now, you'd like to write a program that tells you whether the energy you have is enough to perform the task. It just needs to say Y or N. (If the answer is Y, then clearly the list of moves is proof enough that the answer is correct, so the problem is NP. If the answer is N, there's no point in even attempting the task---you are too tired.) What's the fastest such program you can find? Is the problem NP-complete? If it helps, consider a simplifications that restrict <math_exp>'s to be rational, integers, integers in a certain range, etc. Here's an upper bound: <math_exp>. Represent the initial state by the list <math_exp>, <math_exp> ,... ,<math_exp>. A move affects the state by deleting a number in this list and the energy consumed by the move is the absolute difference between the deleted number and the number coming after. Clearly there are <math_exp> states with at most <math_exp> moves each. (See my blog post for an example run of this algorithm if it's not clear. The description there is in terms of cards.) Note: Some of the comments below refer to older versions of the questions. See the history of edits if they seem confusing. \"}\n",
      "1235 {'Id': '1235', 'Type': 'answer', 'ParentId': '83', 'urls': ['http://rads.stackoverflow.com/amzn/click/0821804308'], 'exp': [], 'Body': 'Check out the book «Mathematical circles».  It\\'s written by some Russian mathematicians who used to run \"math circles\" for middle and high school students and has a lot of interesting material. '}\n",
      "1236 {'Id': '1236', 'Type': 'question', 'Title': 'How do you estimate the flow rate of one fluid into another like the Deep Horizon Oil leak?', 'Tags': ['applications'], 'urls': [], 'exp': [], 'Body': \"How have experts estimated the amount of oil that was shooting out of that pipe in the Gulf? I bet there's some neat math or physics involved here, and some interesting assumptions considering how little concrete data are available. \"}\n",
      "1237 {'Id': '1237', 'Type': 'question', 'Title': 'What is your favorite estimation exercise?', 'Tags': ['soft-question'], 'urls': [], 'exp': [], 'Body': \"A fun question I ask students or interviewees (in engineering) is: This is not my question, this is an example:   Using only what you know now, how many   cans of soda would you estimate are   produced per day (on average) in the   United States? For this question, the result doesn't matter so much as the process you use. In this theme of estimation, what's your favorite question? \"}\n",
      "1238 {'Id': '1238', 'Type': 'answer', 'ParentId': '519', 'urls': [], 'exp': [], 'Body': 'Another approach, which can\\'t work in general (see Noah\\'s answer), but will surely work in this case, is to find a normal form for each element of the group, and then see whether there are finitely or infinitely many. In practice, that means imagining a word in x and y, and then applying the relations as much as possible to simplify it, and then trying to figure out (and prove!) what the possible different \"irreducible words\" (i.e. words that can no longer be simplified) are. In your case, the first thing one would note is that x can only appear to the 1st power (since any higher power can be simplified using x^2 = 1), while y can only appear to the powers +1 or -1 (for the same reason).  Also, we can\\'t have too many expressions of the form  xy  or  yx  in a row, because of the third relation. One can keep going like this.  I didn\\'t, but what I imagine is that one can have  expressions of the form  x y x y^{-1} x y x y^{-1}   ...   that are arbitrarily long, and inequivalent, explaining the infinite order of the group. It shouldn\\'t be so hard to settle the question from this point of view by sitting down with pencil and paper and just playing around with different words to get a feel for what kinds of reductions can take place.  (In geometric arguments like Grigory\\'s, one uses a geometric context, such as an action on a hyperbolic tiling, as a more conceptual way of understanding the relations and distinguishing inequivalent words.  But in this case I\\'m sure it won\\'t be hard to see everything directly from the presentation.) Added after rereading the question: what I am suggesting is precisely that even when the answer might be infinite, you can still hope to find a coset enumeration, at least in a case like this with relatively simple relations. '}\n",
      "1240 {'Id': '1240', 'Type': 'answer', 'ParentId': '1237', 'urls': [], 'exp': [], 'Body': '\"How many estimation questions are asked in interviews across the world during a typical 24h period?\" '}\n",
      "1241 {'Id': '1241', 'Type': 'question', 'Title': 'Which average to use? (RMS vs. AM vs. GM vs. HM)', 'Tags': ['average'], 'AcceptedAnswerId': '1248', 'urls': [], 'exp': ['p', 'n', 'x_1, x_2, \\\\ldots, x_n', ' \\\\bar x = \\\\left(\\\\frac{1}{n} \\\\sum x_i^p\\\\right)^{1/p}. ', 'p = -1', 'p = 1', 'p = 2', 'p = 0'], 'Body': \"The generalized mean (power mean) with exponent <math_exp> of <math_exp> numbers <math_exp> is defined as <math_exp> This is equivalent to the harmonic mean, arithmetic mean, and root mean square for <math_exp>, <math_exp>, and <math_exp>, respectively. Also its limit at <math_exp> is equal to the geometric mean. When should the different means be used? I know harmonic mean is useful when averaging speeds and the plain arithmetic mean is certainly used most often, but I've never seen any uses explained for the geometric mean or root mean square. (Although standard deviation is the root mean square of the deviations from the arithmetic mean for a list of numbers.) \"}\n",
      "1242 {'Id': '1242', 'Type': 'answer', 'ParentId': '1215', 'urls': ['http://www.wolframalpha.com/input/?i=1-f(x)+%3D+f(-x'], 'exp': [], 'Body': 'WolframAlpha provides a solution to this (and many other) recurrence equations: http://www.wolframalpha.com/input/?i=1-f(x)+%3D+f(-x) '}\n",
      "1243 {'Id': '1243', 'Type': 'answer', 'ParentId': '1222', 'urls': [], 'exp': [], 'Body': \"The book by Otto Forster on Riemann Surfaces is pretty good. I never finished reading it myself, but it covers things like Riemann-Roch and Abel's theorem from a sheafish viewpoint. In particular, the proof of Riemann-Roch is analogous to the one in Hartshorne; it follows from the Serre duality theorem and an inductive argument. Learning about sheaves is definitely a plus. Also, there's a book by Springer, though the level is a bit more elementary. \"}\n",
      "1244 {'Id': '1244', 'Type': 'answer', 'ParentId': '1241', 'urls': ['http://en.wikipedia.org/wiki/Lp_norm#lp_spaces', 'http://en.wikipedia.org/wiki/Holder%27s_inequality', 'http://en.wikipedia.org/wiki/Singular_integral', 'http://en.wikipedia.org/wiki/Root_mean_square_speed'], 'exp': ['L^p', 'L^q', 'l^p', 'p,q', 'L^p', '1 \\\\leq p \\\\leq 2', 'p=2', 'L^1', '1-2', '2'], 'Body': 'I admit I don\\'t really know what type of answer your looking for. So, I\\'ll say something that might very well be entirely irrelevant for your purposes but which I enjoy. At least, it\\'ll provide some context for the power means you asked about. These generalized power means are basically the discrete (finitary) analogs of the L^p norms. So, for instance, it\\'s with these norms that you prove (using, say, elementary calculus) the finitary version of Holder\\'s inequality, which is really important in analysis, because it leads (via a limiting argument) to the more important fact that <math_exp> and <math_exp> spaces (which are continuous analogs of these finitary <math_exp> spaces) are dual for <math_exp> conjugate exponents. This duality is really important: one example is that if you are trying to prove something about the <math_exp> spaces that is preserved under duality, you just have to restrict yourself to the case <math_exp>. The theory of singular integral operators provides examples of this: basically, it\\'s easy to prove they are bounded (i.e., reasonably well-behaved) for <math_exp> by Fourier analysis; you prove that they\\'re \"weak-bounded\" on <math_exp> (in some sense which I won\\'t make precise); then you apply to general results on interpolation to get boundedness in the range <math_exp>; finally, this duality operation gives it for <math_exp> as well. Also, root-mean-square speed is used to define temperature in physics. '}\n",
      "1245 {'Id': '1245', 'Type': 'answer', 'ParentId': '632', 'urls': ['http://en.wikipedia.org/wiki/Barycentric_coordinates_%28mathematics%29', 'http://en.wikipedia.org/wiki/File:LagrangeMultipliers2D.svg'], 'exp': ['f_1', 'f_2', 'f_1 + f_2 = 1', 'x_1', 'x_2', 'f_1x_1 + f_2x_2 =: \\\\bar x', 'G_1', 'G_2', 'f_1', 'f_2', '\\\\bar x', 'x_1', 'x_2', '\\\\tilde G', '(x_1,G_1(x_1))', '(x_2,G_2(x_2))', '\\\\bar x', 'G_1', 'G_2', '\\\\bar x', '(x_1,G_1(x_1))', '(x_2,G_2(x_2))', '(x_3,G_3(x_3))', '\\\\bar x', '\\\\tilde G', 'x', 'f', 'g', '\\\\nabla f = \\\\lambda \\\\nabla g', 'f', 'g', 'g'], 'Body': 'Concerning the physical meaning, I take it that <math_exp> and <math_exp> represent the fractions of the two phases in the alloy (this implies <math_exp>). I imagine <math_exp> and <math_exp> to correspond to an intensional variable like pressure, whose average <math_exp> is held constant in the experiment. Now, the alloy minimizes the free energy under these constraints. To answer your first question: there is a geometric reason why the solution is the common tangent to <math_exp> and <math_exp> in the case of two dimensions. Namely, the fractions <math_exp> and <math_exp> are exactly the Barycentric coordinates of the average <math_exp> sitting between <math_exp> and <math_exp>. In particular, the value of the total energy <math_exp> is the height of the line drawn between <math_exp> and <math_exp> evaluated at <math_exp>. Here\\'s a sketch: <img src=\"https://i.stack.imgur.com/VUoYp.png\" alt=\"Free energy of an alloy of two compounds\"> From this picture, it is clear that if this line is not tangent to both <math_exp> and <math_exp>, then you can move it a bit so that the value at <math_exp> will decrease. To answer your second question, the geometry readily extends to higher dimensions. For instance, for an alloy of three compounds, one has to consider the triangle enclosed by the three points <math_exp>, <math_exp> and <math_exp>. The situation is a bit degenerate here, any point inside this triangle whose first coordinate is <math_exp> represents a valid value of <math_exp>. Of these, nature will choose the smallest one. Consequently, the lower side of the triangle has to be tangent to two of the individual free energies. <img src=\"https://i.stack.imgur.com/Bo4bz.png\" alt=\"alloy of three compounds\"> Similar reasoning applies when the variable <math_exp> is not just a number, but, say, a pair of numbers, then we\\'re dealing with a plane tangent to three individual Gibbs functions. While not terribly useful in this case, there is also a very general geometric interpretation of the method of Lagrange multipliers. Namely, consider a goal function <math_exp> and a holonomic constraint <math_exp>. Then, the Euler-Lagrange-equations give <math_exp> which means that <math_exp> changes only in directions orthogonal to the surface <math_exp>. But since we\\'re confined to the surface <math_exp>, this must be an extremum. Wikipedia has a picture. '}\n",
      "1246 {'Id': '1246', 'Type': 'answer', 'ParentId': '595', 'urls': [], 'exp': [], 'Body': 'Computing the determinant and Gaussian elimination are both fine if you are using exact computations, for instance if the entries of your matrix are rational numbers and you are using only rational numbers during the computations. The disadvantage is that the numerator and denominator can get very large indeed. So the number of operations may indeed be O(n2.376) or O(n3), but the cost of every addition and multiplication gets bigger as n grows because the numbers get bigger. This is not an issue if you are using floating point numbers, but then you have the problem that floating point computations are not exact. Some methods are more sensitive to this than others. In particular, checking invertibility by computing the determinant is a bad idea in this setting. Gaussian elimination is better. Even better is to use the singular value decomposition, which will be treated towards the end of the MIT course. '}\n",
      "1247 {'Id': '1247', 'Type': 'answer', 'ParentId': '1237', 'urls': [], 'exp': [], 'Body': 'I like \"how many licks does it take to get to the center of a tootsie roll pop\". '}\n",
      "1248 {'Id': '1248', 'Type': 'answer', 'ParentId': '1241', 'urls': [], 'exp': [], 'Body': 'One possible answer is for defining unbiased estimators of probability distributions. Often times you want some transformation of the data that gets you closer to, or exactly to, a normal distribution. For example, products of lognormal variables are again lognormal, so the geometric mean is appropriate here (or equivalently, the additive mean on the natural log of the data). Similarly, there are cases where the data are naturally reciprocals or ratios of random variables, and then the harmonic mean can be used to get unbiased estimators. These show up in actuarial applications, for example. '}\n",
      "1249 {'Id': '1249', 'Type': 'question', 'Title': 'Motivating Example for Algebraic Geometry/Scheme Theory', 'Tags': ['algebraic-geometry'], 'urls': [], 'exp': ['k^n', 'R:=k[x_1,\\\\ldots,x_n]', 'R', '\\\\operatorname{Spec} R', '\\\\operatorname{MaxSpec} R', 'k^n'], 'Body': \"I am in the process of trying to learn algebraic geometry via schemes and am wondering if there are simple motivating examples of why you would want to consider these structures. I think my biggest issue is the following: I understand (and really like) the idea of passing from a space to functions on a space.  In passing from <math_exp> to <math_exp>, we may recover the points by looking at the maximal ideas of <math_exp>.  But why consider <math_exp> instead of <math_exp>?  Why is it helpful to have non-closed points that don't have an analog to points in <math_exp>?  On a wikipedia article, it mentioned that the Italian school used a (vague) notion of a generic point to prove things.  Is there a (relatively) simple example where we can see the utility of non-closed points? \"}\n",
      "1250 {'Id': '1250', 'Type': 'answer', 'ParentId': '1249', 'urls': ['http://sbseminar.wordpress.com/2009/08/06/algebraic-geometry-without-prime-ideals/'], 'exp': ['Spec', 'MaxSpec', 'Spec \\\\mathbb{Z}', 'X', 'X', 'Spec \\\\mathbb{Z}', 'Spec \\\\mathbb{Z}', '\\\\mathbb{Q}', 'X \\\\to \\\\mathbb{Z}'], 'Body': 'To start off,  the discussion at Sbseminar has comments from lots of people who actually know algebraic geometry, and if anything I say contradicts something they say, please trust them and not me. One reason is that you lose the functoriality of <math_exp> if you stick to <math_exp>: the inverse image of a maximal ideal is not necessarily maximal. Nevertheless, if you stick to schemes of finite type over a field, this is true (it\\'s basically a version of the Nullstellensatz).  In particular, in Serre\\'s FAC paper he defines a \"variety\" by gluing together regular affine algebraic sets in the sense of classical algebraic geometry. But this is less general.  One natural example of a scheme which is not of finite type over a field is simply <math_exp>. Then given a scheme <math_exp> over this (well, admittedly every scheme <math_exp> is a scheme over <math_exp> in a canonical way), the fibers at the non-closed point of <math_exp> is still interesting and basically amounts to studying polynomial equations over <math_exp> (when <math_exp> is of finite type). As a (simple) example of how generic points can be used, one can prove that a coherent sheaf on a noetherian integral scheme is free on a dense open subset. Why? Because it must be free at the generic point (since the local ring there is a field), and it is a general fact that two coherent sheaves whose stalks are isomorphic are isomorphic in a neighborhood. (This is true actually for sheaves of finite presentation over a ringed space.) '}\n",
      "1252 {'Id': '1252', 'Type': 'answer', 'ParentId': '1249', 'urls': [], 'exp': ['\\\\text{Hom}(-, \\\\mathbb{C})', '\\\\mathbb{C}', '\\\\mathbb{C}', '\\\\text{Hom}(-, \\\\mathbb{C}[x]/x^2)', '\\\\mathbb{C}', '\\\\mathbb{C}', '\\\\text{Spec } \\\\mathbb{C}[x]/x^2', '\\\\mathbb{C}[x]/x^2', '\\\\mathbb{C}', '\\\\mathbb{C}[x]/x^2'], 'Body': \"I think I can at least convince you that it's a good idea to work with non-reduced rings, which aren't really captured by their maximal ideals.  The idea is that you get more functors.  Just as <math_exp> is the functor which sends a finitely-generated domain over <math_exp> to its set of <math_exp>-points, it turns out that <math_exp> sends a finitely-generated domain over <math_exp> to its set of <math_exp>-points together with a choice of tangent vector.   In fact this is one way to define the Zariski tangent space.  So, on the scheme side, it's a good idea to study morphisms from <math_exp> to your scheme, and you can't do this if you identify <math_exp> with its set of <math_exp>-points.  (You can't even do this if you identify <math_exp> with its prime ideals; you really need the entire structure sheaf.) (A really nice application: an algebraic definition of the Lie algebra of an algebraic group.) \"}\n",
      "1253 {'Id': '1253', 'Type': 'question', 'Title': 'Are isosceles always and only similar to other isosceles?', 'Tags': ['geometry', 'triangles'], 'AcceptedAnswerId': '1255', 'urls': [], 'exp': [], 'Body': 'In my geometry class last year I remember putting down the statement in a column proof \"That all isosceles are always and only similar to other isosceles\". I do not remember what I was trying to prove. But, I do remember that I was stressed and that was the only thing I could think of and made a  guess thinking I would probably get the proof wrong on my test. Funny enough though, I didn\\'t get the proof wrong and I was wondering if anyone could show a proof as to why this would be true. I mean I makes sense but, I do not see any way to prove it. Could you please explain how this is true? '}\n",
      "1254 {'Id': '1254', 'Type': 'answer', 'ParentId': '1249', 'urls': ['https://math.stackexchange.com/questions/942/meaning-of-closed-points-of-a-scheme/984#984'], 'exp': ['\\\\mathbb Q', '\\\\mathbb Z', '\\\\mathbb Z[x_1,...,x_n] \\\\to \\\\mathbb Q[x_1,...,x_n]', '\\\\mathbb C[t]', '\\\\mathbb Z', '\\\\mathbb Z', '\\\\mathbb Q', '\\\\mathbb C[t]', '\\\\mathbb C(t)', 'y^2 = x^3 + t', 't', '\\\\mathbb C(t)', 't_0', 't', '\\\\mathbb C(t)', '\\\\mathbb C[t]', '\\\\mathbb C[t] \\\\to \\\\mathbb C', 't \\\\mapsto t_0', 't_0', 'p', '\\\\mathbb Z'], 'Body': 'See my answer here for a brief discussion of how points that are closed in one optic (rational solutions to a Diophantine equation, which are closed points on the variety over <math_exp> attached to the Diophantine equation) become non-closed in another optic (when we clear denominators and think of the Diophantine equation as defining a scheme over <math_exp>). In terms of rings (and connecting to Qiaochu\\'s answer), under the natural map <math_exp>, the preimage of maximal ideals are prime, but not maximal. These examples may give impression that non-closed points are most important in arithmetic situations, but actually that is not the case.  The ring <math_exp> behaves much like <math_exp>, and so one can have the same discussion with <math_exp> and <math_exp> replaced by <math_exp> and <math_exp>.  Why would one do this? Well, suppose you have an equation (like <math_exp>) which you want to study, where you think of <math_exp> as a parameter.  To study the generic behaviour of this equation, you can think of it as a variety over <math_exp>.  But suppose you want to study the geometry for one particular value of <math_exp> of <math_exp>.  Then you need to pass from <math_exp> to <math_exp>, so that you can apply the  homomorphism <math_exp> given by <math_exp>  (specialization at <math_exp>).  This is completely analogous to the situation considered in my linked answer, of taking integral solutions to a a Diophantine equation and then reducing them mod <math_exp>. What is the upshot?  Basically, any serious study of varieties in families (whether arithmetic families, i.e. schemes over <math_exp>, or geometric families, i.e. parameterized families of varieties) requires scheme-theoretic techniques and the consideration of non-closed points. (Of course, serious such studies were made by the Italian geometers, by Lefschetz, by Igusa, by Shimura, and by many others before Grothendieck\\'s invention of schemes, but the whole point of schemes is to clarify what came before and to give a precise and workable theory that encompasses all of the contexts considered in the \"old days\", and is also more systematic and more powerful than the older techniques.) '}\n",
      "1255 {'Id': '1255', 'Type': 'answer', 'ParentId': '1253', 'urls': [], 'exp': [], 'Body': 'Re-reading your question, I see two possible interpretations of your statement. First (and my original answer), \"If &#x25B3;ABC is isosceles and &#x25B3;ABC~&#x25B3;DEF, then &#x25B3;DEF is isosceles.\"  Two triangles are similar if and only if the three angles of one are congruent to the three angles of the other.  Since a triangle is isosceles if and only if two of its angles are congruent, if a triangle is similar to an isosceles triangle, then it will also have two congruent angles and must be isosceles. Second, \"If &#x25B3;ABC and &#x25B3;DEF are isosceles, then they are similar.\"  This is not true.  Suppose one triangle has angles with measures 20&deg;, 20&deg;, and 140&deg; and another other triangle has angles with measures 85&deg;, 85&deg;, and 10&deg;.  Both triangles are isosceles (since within each triangle, there is a pair of congruent angles), but the triangles are not similar (because the angles of one are not congruent to the angles of the other). '}\n",
      "1256 {'Id': '1256', 'Type': 'answer', 'ParentId': '20', 'urls': ['http://www.marriedtothesea.com/031107/zero.gif'], 'exp': [], 'Body': 'There are many types of numbers, though the natural numbers, the integers, the rational, the decimal, the real, and the complex form a nice self-complete expository whole. Hopefully, the following block(s) of text aren\\'t too poorly formatted. 1) The set of natural numbers consists of numbers with which we count {0,1,2,3,...}. As noted in some of the other answers, some people think that 0 is not a natural number (see one of my desktop backgrounds). Whether or not it is, it\\'s a matter of taste. What is not a matter of taste are the defining properties of the natural numbers. In particular, there is a (binary) operation called +, which takes two numbers a and b and spits out a third number a+b, which is the sum of a and b. It satisfies the usual properties that you would expect from counting: it is commutative (the order of the summands doesn\\'t matter, i.e. a+b=b+a) and associative (the order in which you add summands to each other doesn\\'t matter, i.e. (a+b)+c=a+(b+c), so you can always write a+b+c for a specific number). There is also an order relation &lt; such that for any two different numbers a and b either a  The order plays nice with addition in that a &lt; b implies a+c &lt; b+c. We also have a cancellation property that if a &lt; c, then there exists a number b such that a+b=c. The order also satisfies one unobvious property called the well-ordering principle and states that if you take any collection of natural numbers, there is a smallest one, i.e. a number that is smaller than all the other numbers in the collection. The well-ordering principle, together with cancellation, implies that there is a smallest number, and that any two consecutive numbers differ by the same number. From this it follows that either the smallest number is 0 (in the sense that 0 is the number such that 0+a=a+0=a) or that it is 1, where 1 is the common difference between any two consecutive numbers. Choosing one or the other of the two possibilities defines the natural numbers uniquely as either {0,1,2,...} or {1,2,...}. It is a fun exercise to show that the above axioms are equivalent to the axioms of Peano (note that the axioms of Peano state nothing about addition, so in fact BOTH {0,1,2,...} and {1,2,...} satisfy the axioms; how we define addition with respect to the smallest element is what specifies one of the two sets) 2) The set of integers, also known as whole numbers, is {..., -3, -2, -1, 0, 1, 2, 3, ...}. It arises as we try to \"complete\" the arithmetic of the integers in the following sense. For any two numbers a &lt; b we have a number b-a such that (b-a)+a=b, thus we have subtraction of a smaller number from a bigger number. We do not have such numbers b-a if a>b (i.e. there is no natural number which when you add a to you get b if a>b), but from experience we see that numbers such as ... -3, -2, -1  seem to be sensical to add and subtract as we try to solve equations and whatnot. But while numbers such as b-a when a &lt; b are defined, the same number could be represented in different ways. For example, 2 is both 5-3 and 8-6. So we need to answer the question of when b-a=d-c for a &lt; b, c &lt; d, and by algebra the answer is that they\\'re equal whenever b+c=d+a. Hence, we can define all the integers, positive and negative, by taking them to be pairs of natural numbers (a,b) with the caveat that (a,b)=(c,d) whenever a+c=b+c. For example, the integer -2 consists of (among others) the pairs (5,3), (6,8), (0,2). This is all seems like much formal ado about intuitive nothings, but that is only because we can represent any integer as either the pair (a,0), which we write simply as a, which we write as -a, and think of as (a,0) is the integer which when you add a to you get 0, or as (0,a), which we think of as the integer to which which you add 0 you get the natural number a. Hence what we actually do arithmetic with are the pairs {... (3,0), (2,0), (1,0), (0,0), (0,1), (0,2), (0,3),...}, but the fact that we only need to throw in a negative sign to make the arithmetic meaningful is precisely because there exists these special representative pairs. Exercise: define the addition and order on the pairs of natural numbers for the above to work (we lose of course the property that any set of integers has a smallest integers [e.g. the set of negative integers], but the principle still holds for sets of positive integers). 3) The rational numbers (or fractions) are obtained by the exact same process of completion as above, except with respect to multiplication. the fraction a/b, where a and b are integers corresponds to the number which when multiplied by b gives you a. Note that a/b=c/d if and only if ad=bc, which mirrors the fact that a-b=c-d if and only if a+b=b+c. 4) The real numbers are best understood as coming from decimals (or decimal expansions), so what are decimals and where do they come from? Rational numbers as fractions are hard to compare to one another. Is 7/5 bigger or smaller than 4/3? The answer is yes, becayse 7*3>3*5. In general if you have a/b and c/d, we have a/b &lt; c/d, a/b=c=d or a/b>c/d if correspondingly adbc. But that\\'s a tedious process, is there some way of writing them down so that it is clearer who\\'s bigger than who? Better, can the process of writing them down be easier than the process of checking who\\'s bigger than who by cross-multiplication? The answer is yes, and we only need consider the usual way in which we write natural numbers.  We can express the number 1729 compactly based on the distributive properties of addition and exponentiation: 1729=1000+700+20+9=10^3+7*10^2+2*10+9, where the digits 1,2,3,4,5,6,7,8,9,10 are the first ten successors of zero, and zero is denoted by 0. So we can write rational numbers with denominator 10^n as a.b=a+b/10^n with a and b are natural numbers and b is smaller than 10^n. For example, 1729/100 can be written as 17.29 where 17 and 29 are natural numbers with 29&lt;100. Now, for any rational number c/d there exists a rational number with denominator closest 10^n that\\'s closest to and smaller than c/d (this is intuitively obvious and not that hard to formalize). Then we can say that c/d ~ a.b where a+b/10^n is that rational number. For example, the closest number with denominator 10 to 1/3 is 3/10=.3, with denominator 100 is 33/100=.33, and so on. Then that the infinite string 0.3333... equals 1/3 means that the closest rational number less than or equal to 1/3 with denominator 10^n is 333...3/10^n where you have n 3\\'s. From this, one can prove that rational numbers correspond have periodic expansions, i.e. that the string a.b has a repeating sequence of digits from some point onward. (even better, we can compute decimal expansions by the usual process of long division by just adding a decimal point, and we can efficiently compare numbers by looking at the expansion until the first point of difference). 5) Real numbers come from taking arbitrary infinite sequences of digits as decimals. From above, we know that decimals actually encode sequences of rational numbers with denominators 1,10, 10^2, 10^3, ... When should two such sequences be equivalent? Consider the standard question of whether .999...=1. The decimal expansion of 1 is just 1.000... because the sequence of rational numbers with denominators 10, 10^2, 10^3, ... has to be a sequence of rational numbers less than or equal to 1. If we drop the requirement that the numbers be less than or equal to 1, keep the requirement that the sequence of rational numbers (approximations really) is non-decreasing, and say allow the rational number to be either the closest number less than 1 or 1 itself, then we obtain the other expansion 1=.999... If you had a measuring instrument that was accurate to n decimal places, then upon measuring .999... and 1.000... you would always get .999...9 with n-2 9s. So it turns out that with finite precision measuring, you can\\'t tell the difference between certain decimal expansions, so we might as way say those two decimal expansions are equal. This is the process of completion by Cauchy sequences put into words. And those expansions then are the real numbers, and they have addition, multiplication, subtraction and division and ordering just like the rational numbers, except that they\\'re complete in that every sequence of real numbers converges. 6) Complex numbers It turns out that every polynomial with real coefficients of degree >2 has a square root. -1 does not have a a real square root, so we wish to throw a root of -1 in to algebraically complete the real numbers, and thus get the complex numbers. The algebraic construction is best done by explaining concepts such as rings and ideals and quotients and homomorphisms, which is too much machinery. Instead, consider Euclidean geometry. It is a(n advanced) fact that you can coordinize it with real numbers, i.e. you can represent points by pairs of real numbers (a,b), lines by the solutions of the two variable equation ax+by=c, lengths given by square root of ((a-c)^2+(b-d)^2), etc. Adding pairs (a,b) and (c,d) gives you (a,b)+(c,d) so addition of pairs corresponds to translations. Another way of interpreting a pair of (a,b) is by interpreting it in polar form (r, theta) where r is the distance of (a,b) form the origin and theta is the angle from the x-axis to (a,b). Then we can think of (a,b)~(r,theta) as a dilation and rotation with center (0,0) and sending (1,0) to (a,b), i.e. a dilation by a factor of r and  a rotation by an angle of theta. But this then defines a multiplication of vectors, which turns out to distribute over addition. Clearly we have inverse rotations/dilations and inverse translations, giving us subtraction and division. Finally, we can identify the real numbers with pairs (r,0) which are only a dilation without any rotation. These pairs are then our complex numbers and you can deduce all their properties from the above description. Note that the vector (0,1) multiplied by (0,1) gives (-1,0) since it corresponds to a rotation of 90 followed by a rotation of 90, which is a rotation of 180 of (1,0) which equals (-1,0). See Qiaouchu\\'s answer for more details. '}\n",
      "1257 {'Id': '1257', 'Type': 'question', 'Title': 'Is there a known mathematical equation to find the nth prime?', 'Tags': ['number-theory', 'prime-numbers'], 'AcceptedAnswerId': '1259', 'urls': [], 'exp': [], 'Body': \"I've solved for it making a computer program, but was wondering there was a mathematical equation that you could use to solve for the nth prime? \"}\n",
      "1258 {'Id': '1258', 'Type': 'answer', 'ParentId': '1257', 'urls': ['http://en.wikipedia.org/wiki/Formula_for_primes'], 'exp': ['P(n) = n^2 − n + 41', '41', 'n'], 'Body': \"No such formula is known, but there are a few that give impressive results. A famous one is Euler's: <math_exp> Which yields a prime for every natural number lower than <math_exp>, though not necessarily the <math_exp>th prime. See more here. \"}\n",
      "1259 {'Id': '1259', 'Type': 'answer', 'ParentId': '1257', 'urls': ['http://en.wikipedia.org/wiki/Prime_number_theorem', 'http://en.wikipedia.org/wiki/Generating_primes#Prime_sieves', 'http://en.wikipedia.org/wiki/Formula_for_primes', 'http://mathworld.wolfram.com/PrimeFormulas.html'], 'exp': ['n', 'n', 'n \\\\ln n', 'm', 'm/\\\\ln m = n', 'n', 'p_n', 'n \\\\ln n + n(\\\\ln\\\\ln n - 1) &lt; p_n &lt;  n \\\\ln n + n \\\\ln \\\\ln n', 'n\\\\ge{}6', 'n', 'n', 'f(n)=n^2-n+41'], 'Body': 'No, there is no known formula that gives the nth prime, except artificial ones you can write that are basically equivalent to \"the <math_exp>th prime\". But if you only want an approximation, the <math_exp>th prime is roughly around <math_exp> (or more precisely, near the number <math_exp> such that <math_exp>) by the prime number theorem. In fact, we have the following asymptotic bound on the <math_exp>th prime <math_exp>: <math_exp> for <math_exp> You can sieve within this range if you want the <math_exp>th prime. [Edit: There are better ideas than a sieve, see the answer by Charles.] Entirely unrelated: if you want to see formulae that generate a lot of primes (not the <math_exp>th prime) up to some extent, like the famous <math_exp>, look at the Wikipedia article formula for primes, or Mathworld for Prime Formulas. '}\n",
      "1261 {'Id': '1261', 'Type': 'answer', 'ParentId': '548', 'urls': ['http://www2.math.su.se/~jesper/research/wheels/wheels.pdf'], 'exp': [], 'Body': \"EDIT:I botched my original answer, here's what I actually meant: Fractions are equivalence classes of pairs of integers subject to (a,b)=(c,d) iff ad=bc, with addition and multiplication extended from {(a,1)} as a copy of the integers by multiplication defined component-wise in general. If we allow pairs of the form (a,0) for some integer a, we have with the multiplication axioms (a,0)=(b,0) for any a and b, and in particular all of them coincide with (0,1)(a,0)=(0,0) as multiplication is component-wise. But (0,0)=(a,b) for every pair, so our construction gives us the trivial ring. Hence, to get a non-trivial structure, we must disallow pairs of the form (a,0), including (0,0) and that gives us the rationals. Anyway, the above is the standard reason why 0/0 is undefined. You can actually define it, though to do that you must change some of the fundamental arithmetic properties. For a fun read, check out the following paper on the topic: http://www2.math.su.se/~jesper/research/wheels/wheels.pdf Original answer: By definition (or by construction) a/b, where a and b are natural numbers, is defined to be the rational number which when multiplied by b gives you a. From this you have that 0/0=0. Different fractions a/b and c/d are defined to be equal if ad=bc. This allows us to extend the arithmetic of the integers to an arithmetic of fractions. From this you have that 0/0=c/d for every rational number c/d since x0=0 for all x in any system where multiplication distributes over addition (proof: x0=x(0-0)=x0-x0=0). Thus, if you allow 0/0 among your fractions, and you attempt to extend the arithmetic of the integers, all your fractions must be 0. (note: the reason you cannot allow a/0 among your fractions is when a is not equal to 0 is because if you extend the arithmetic to where you have multiplication distributing over addition, you would have 0x=0 for all x and so the existence of a/0 is only consistent if a=0, which is the case above). \"}\n",
      "1262 {'Id': '1262', 'Type': 'answer', 'ParentId': '1017', 'urls': [], 'exp': ['x_1y_2 - x_2y_1', 'O', 'P_k', 'P_{k+1}', 'x', '\\\\frac12 \\\\oint_{polygon} x\\\\,dy - y\\\\,dx'], 'Body': 'One way is to note that <math_exp> is a signed area, i.e. it may positive or negative. Adding up all the signed areas of the triangles formed by the points <math_exp>, <math_exp> and <math_exp> will cancel all the superfluous parts, as can be seen from the sketch: <img src=\"https://i.stack.imgur.com/KuezB.png\" alt=\"Adding signed areas for the sides of a polygon\"> The same argument also works for trapezoids with the <math_exp>-axis instead of triangles with the origin. (I think this is the most illuminating argument, because the key trick is to give the area a sign depending on orientation.) One could argue that this is not very rigorous, however. A more rigorous proof is to divide the polygon into two smaller polygons (it\\'s not trivial to show that this is possible) and argue that adding the shoelace sums of the two parts gives the shoelace sum of the whole. That\\'s because the two additional terms for the extra side cancel each other. (This cancellation is well-known from line integrals, we are in essence calculating <math_exp> here.) By induction, you then only have to verify the formula for a triangle. <img src=\"https://i.stack.imgur.com/hIKol.png\" alt=\"Shoelace sums of two polygons\"> '}\n",
      "1263 {'Id': '1263', 'Type': 'answer', 'ParentId': '548', 'urls': ['http://www.bbc.co.uk/berkshire/content/articles/2006/12/06/divide_zero_feature.shtml'], 'exp': [], 'Body': 'Everyone knows 0/0 is nullity. (yes, this is a joke post, but that was a serious article) '}\n",
      "1265 {'Id': '1265', 'Type': 'answer', 'ParentId': '1211', 'urls': [], 'exp': [], 'Body': \"Below is my geometric understanding of why irrational powers of negatives are difficult to define. As such it is probably not rigorous and may be wrong. The way irrational powers of the real numbers are usually defined is by limits of fractional powers. For complex numbers, the same is true, except the limiting process is more complicated. We can of course coast as usual on our real numbers result and see that we only need to define irrational powers on the unit circle, since every other points is some positive real multiple of a point on the unit circle. Now in general z -> z^n wraps the circle around itself n times. What does z->z^(1/n) do? Well, it's not clear since each point has n possible points it could have come from, in particular if you partition the circle into n arcs of length 2pi/n, each of those gets mapped to the full circle. Once you choose a starting arc though, z -> z^m maps the starting arc to other arcs in the following way. Partition your starting arc of length 2pi/n into arcs of length 2pi/(nm) and then each of the little arcs gets mapped to a big arc that is 2pim/n away from the previous big arc. The reason you choose arcs as opposed to weirdly distributed discrete sets, is because you want exponentiation to be continuous and hence you want the inverse image sets to be as the whole circle is connected. There is no problem with fractional powers of -1, you have n choices for starting arc. But if you want your exponentiation to have some semblance of continuity with respect to the exponent, then you have to be choosing branches (arcs) for z->z^(1/n) that cohere, i.e. such that for very large n the roots get closer and closer together. This is done by requiring that the nth root of 1 is always 1, which makes all of the arcs into neighborhoods of 1. But this means that all the points with argument between 0 and pi get mapped in the half-arc above 1, and all the points with argument between pi and 2pi get mapped in the half-arc below 1. Hence if you approach -1 from above and from below, the two limits of the nth root will be different, and hence you cannot have continuous exponentiation at -1. As a result (you can do some more visualization if you wish), the irrational powers of -1 cannot even be defined as limits of rational powers. \"}\n",
      "1266 {'Id': '1266', 'Type': 'answer', 'ParentId': '1257', 'urls': ['http://en.wikipedia.org/wiki/Formula_for_primes', 'http://primes.utm.edu/glossary/xpage/MatijasevicPoly.html', 'http://en.wikipedia.org/wiki/Diophantine_set'], 'exp': ['p(x)', 'n', 'n'], 'Body': 'There are formulas on Wikipedia, though they are messy. No polynomial <math_exp> can output the <math_exp>th prime for all <math_exp>, as is explained in the first section of the article. There is, however, a polynomial in 26 variables whose nonnegative values are precisely the primes. (This is fairly useless as far as computation is concerned.) This comes from the fact that the property of being a prime is decidable, and the theorem of Matiyasevich. '}\n",
      "1268 {'Id': '1268', 'Type': 'answer', 'ParentId': '756', 'urls': [], 'exp': [], 'Body': 'There is a fairly simple strategy which requires (1&nbsp;+&nbsp;&epsilon;)log(n)&nbsp;+&nbsp;1/&epsilon;&nbsp;+&nbsp;O(1) queries, for any constant &epsilon;&nbsp;&gt;&nbsp;0. I illustrate this strategy below. First, I ask you whether or not X, the secret number, is n/2. Without loss of generality, suppose you answer \"less\". I learn nothing at first, because you may be lying; but for the moment I give you the benefit of the doubt. I next ask you whether X is n/4. If you say \"less\", I don\\'t know whether or not 0&nbsp;&lt;&nbsp;X&nbsp;&lt;&nbsp;n/4, but I do know that 0&nbsp;&lt;&nbsp;X&nbsp;&lt;&nbsp;n/2, because you can\\'t lie twice. Similarly, if I go about a normal binary search, so long as you continue to answer \"less\", I know that your answer to the preceding query was honest. So we may reduce to the case where you say \"more\" to my query of whether X is n/4. If I continue to take you at your word, persue a binary search, and enquire about whether X is 3n/8, you may say either \"more\" or \"less\". If you say \"more\", then I don\\'t know whether X&nbsp;&gt;&nbsp;n/2, but I do know that X&nbsp;&gt;&nbsp;n/4, again because you can\\'t lie twice. So again so long as you continue to answer \"more\" in my normal binary search, I know that your answer to the preceding query was honest. More generally: if I consistently guess under the hypothesis that you are being honest, in any \"monotonic\" sequence of responses, I know that all but (possibly) the last of them are honest. So it might seem as though the worst case scenario is where your responses alternate a lot, as would occur for instance if you had chosen something like X&nbsp;=&nbsp;&lfloor;&thinsp;n/3&thinsp;&rfloor;. But in the alternating case too, I can be confident of the honesty of some of your answers: More generally: not only do I know that monotonic subsequences of answers are mostly honest, I know that any time that you answer \"more\" or \"less\", your previous answer of the same kind was also honest. So in fact I should be most suspicious, when I encounter long monotonic subsequences in your answers, that the answer previous to that monotonic subsequence was a lie. What I need is a strategy that will tell me when to revisit an old question, depending on how large n is. Ideally, revisiting old questions would require very little overhead. If I encounter a monotonic sequence of f(n) responses, I revisit the last question before that monotonic sequence started. If I do a double-check like this every time I encounter a monotonic sequence of length r, I will in the worst case query you about (r+1)log(n)/r = (1&nbsp;+&nbsp;1/r)log(n) times. If I catch you in a lie, I will have only \"wasted\" r queries, and my strategy afterwards can be just a simple binary search without double-checks; so your optimal strategy for maximizing the number of queries I make is actually not to lie at all, or to save your lie for nearly the end of the game to cost me about r additional queries. Here r can be an arbitrarily large integer; thus for any &epsilon;, I can achieve a query rate of (1&nbsp;+&nbsp;&epsilon;)log(n)&nbsp;+&nbsp;1/&epsilon;&nbsp;+&nbsp;O(1) by setting r&nbsp;&gt;&nbsp;1/&epsilon;. Bonus problem #1. Without too much extra work, I think you can improve this to a strategy requiring only log(n)&nbsp;+&nbsp;O(log&nbsp;log(n)) queries, but I\\'m too lazy to work out the details right now. Bonus problem #2. Generalize this strategy to the regime where you are allowed to lie to me some fixed number of times L&nbsp;>&nbsp;0. '}\n",
      "1269 {'Id': '1269', 'Type': 'answer', 'ParentId': '1211', 'urls': ['https://math.stackexchange.com/questions/1183/division-by-imaginary-number', 'http://en.wikipedia.org/wiki/Riemann_surface'], 'exp': ['\\\\mathbb{C}', '\\\\sqrt{z} = e^{ \\\\frac{\\\\log z}{2} }', 'e^z : \\\\mathbb{C} \\\\to \\\\mathbb{C}', '\\\\mathbb{C}', '\\\\mathbb{C}/2\\\\pi i \\\\mathbb{Z}', '\\\\mathbb{C}', '\\\\mathbb{C}/2\\\\pi i \\\\mathbb{Z}', '\\\\log 1 = 0, 2 \\\\pi i, - 2 \\\\pi i, ...', '\\\\log 1 = 0', '\\\\log (1 + z)', 'z', '|z| &lt; 1', '1', '\\\\log z', '1', 'z', 'z = e^{2 \\\\pi i t}, 0 \\\\le t \\\\le 1', '1', '1', '\\\\log 1 = 2 \\\\pi i', '\\\\frac{1}{z}', '(z, e^z) \\\\subset \\\\mathbb{C}^2', '\\\\mathbb{C}', 'z = e^{2\\\\pi i t}'], 'Body': \"As other posters have indicated, the problem is that the complex logarithm isn't well-defined on <math_exp>.  This is related to my comments in a recent question about the square root not being well-defined (since of course <math_exp>). One point of view is that the complex exponential <math_exp> does not really have domain <math_exp>.  Due to periodicity it really has domain <math_exp>.  So one way to define the complex logarithm is not as a function with range <math_exp>, but as a function with range <math_exp>.  Thus for example <math_exp> and so forth. So what are we doing when we don't do this?  Well, let us suppose that for the time being we have decided that <math_exp>.  This is how we get other values of the logarithm: using power series, we can define <math_exp> for any <math_exp> with <math_exp>.  We can now pick any number in this circle and take a power series expansion about that number to get a different power series whose circle of convergence is somewhere else.  And by repeatedly changing the center of our power series, we can compute different values of the logarithm.  This is called analytic continuation, and typically it proceeds by choosing a (say, smooth) path from <math_exp> to some other complex number and taking power series around different points in that path. The problem you quickly run into is that the value of <math_exp> depends on the choice of path from <math_exp> to <math_exp>.  For example, the path <math_exp> is a path from <math_exp> to <math_exp>, and if you analytically continue the logarithm on it you will get <math_exp>.  And that is not what you wanted.  (This is essentially the same as the contour integral of <math_exp> along this contour.) One way around this problem is to arbitrarily choose a ray from the origin and declare that you are not allowed to analytically continue the logarithm through this ray.  This is called choosing a branch cut, and it is not canonical, so I don't like it. There is another way to resolve this situation, which is to consider the Riemann surface <math_exp> and to think of the logarithm as the projection to the first coordinate from this surface to <math_exp>.  So all the difficulties we have encountered above have been due to the fact that we have been trying to pretend that this projection has certain properties that it doesn't have.  A closed path like <math_exp> in which the logarithm starts and ends with different values corresponds to a path on this surface which starts and ends at different points, so there is no contradiction.  This was Riemann's original motivation for defining Riemann surfaces, and it is this particular Riemann surface that powers things like the residue theorem. \"}\n",
      "1271 {'Id': '1271', 'Type': 'question', 'Title': 'Solving systems of equations in roots of unity', 'Tags': ['linear-algebra', 'number-theory'], 'AcceptedAnswerId': '1272', 'urls': [], 'exp': ['z^b w^c = 1,\\\\quad z^d w^e = 1.', 'z', 'w', 'a^{\\\\textrm{th}}', 'a', '(z, w) = (1, 1)', 'a, b, c, d,', 'e', '(z, w) = (1, 1)', '\\\\textrm{gcd}(a, b, d) = \\\\textrm{gcd}(c, e) = 1', 'z', '\\\\textrm{gcd}(a, b, d)^{\\\\textrm{th}}', '1', 'a^{\\\\textrm{th}}', '1', '(z, 1)', 'z', 'w', '\\\\textrm{gcd}(a, be - cd)^{\\\\textrm{th}}', '1', '\\\\textrm{gcd}(a, be - cd) = \\\\textrm{gcd}(a, b, d) = \\\\textrm{gcd}(c, e) = 1', '3', 'z^a w^b = 1,\\\\quad z^c w^d = 1,\\\\quad z^e w^f = 1,', 'z', 'w', '1', 'a, b, c, d, e', 'f', '(z, w) = (1, 1)', 'b = 0', 'z', 'b', '0', '\\\\textrm{gcd}(d, f) = 1', 'w', 'a^{\\\\textrm{th}}'], 'Body': 'In part of my research, the following problem has come up. Consider the system of equations (in complex numbers) <math_exp> I am interested in the solution set when we restrict both <math_exp> and <math_exp> to be <math_exp> roots of unity, for some positive integer <math_exp>. Of course, one immediately sees that <math_exp> is a solution. What are some nice necessary and sufficient conditions on <math_exp> and <math_exp> which guarantee that <math_exp> is the ONLY solution? To give an idea of the flavor of answer I\\'d be most happy with, one must have <math_exp>, because if <math_exp> is any <math_exp> root of <math_exp> (which is neccesarily an <math_exp> root of <math_exp>), then <math_exp> is a solution to both equations. It also turns out that <math_exp> and <math_exp> must both be <math_exp> roots of <math_exp>. I\\'d love to have an answer like \"<math_exp> is necessary and sufficient\", with, perhaps, a few more estimates on gcd terms. This problem can also been generalized (and I am interested in that case as well).  Suppose you are given <math_exp> equations <math_exp> with <math_exp> and <math_exp> complex numbers of modulus <math_exp>. What are necessary and sufficient conditions on <math_exp> and <math_exp> which guarantee that the only simultaneous solution is <math_exp>? The previous problem is a special case of this (which comes from setting <math_exp>.  Clearly then, <math_exp> must be an ath root of unity.  It turns that if <math_exp> is <math_exp>, using the fact that <math_exp> one can show <math_exp> must also be an <math_exp> root of unity). And please feel free to retag as appropriate! Thank you in advance. '}\n",
      "1272 {'Id': '1272', 'Type': 'answer', 'ParentId': '1271', 'urls': ['http://en.wikipedia.org/wiki/Smith_normal_form'], 'exp': ['2 \\\\times 2', '\\\\mathbb{Z}/a\\\\mathbb{Z}', '\\\\gcd(a, be-dc) = 1', '\\\\mathbb{Z}'], 'Body': \"You are computing the nullspace of a <math_exp> matrix over <math_exp>, so a necessary and sufficient condition is that the matrix in question is invertible, hence <math_exp> should be necessary and sufficient.  In the second case you should compute the Smith normal form of your matrix over <math_exp>.  I believe a necessary and sufficient condition is that the Smith normal form is [[1 0][0 1][0 0]] (looks like matrices aren't working yet). \"}\n",
      "1273 {'Id': '1273', 'Type': 'answer', 'ParentId': '74', 'urls': ['http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29', 'http://books.google.com/books?id=B7Ch-c2G21MC&amp;pg=PA205&amp;dq=cramer+theorem'], 'exp': [' \\\\varphi_Y(t) = E[e^{itY}] = E[e^{it(X+Z)}] = E[e^{itX}]E[e^{itZ}]', ' \\\\varphi_Z(t) = E[e^{itZ}] = E[e^{itY}]/E[e^{itX}] '], 'Body': 'Your question: given that X and Z are independent, X is Gaussian (I\\'ll use \"normal\"), and Y = X+Z, prove that Y is normal iff Z is normal. Right? As you observed, one direction is easy: if Z is normal, then so is Y=X+Z. So for the other direction, assume that Y is normal. We need to prove that is Z normal too. Perhaps there\\'s an even easier way, but it\\'s straightforward to use characteristic functions, which completely characterise distributions. Because X and Z are independent, <math_exp>, and so, <math_exp> This means that Z has exactly the right characteristic function for a normal variable, and hence it\\'s normal. More interestingly and much more generally, there is a theorem of Cramer (e.g. see here) which says that if X and Z are independent and X+Z is normally distributed, then both X and Z are! '}\n",
      "1275 {'Id': '1275', 'Type': 'answer', 'ParentId': '1249', 'urls': [], 'exp': ['y=0', 'y=x^2', '\\\\operatorname{Spec} k[x,y]/(y,y-x^2) \\\\cong \\\\operatorname{Spec} k[x]/(x^2)', '\\\\operatorname{Spec} k', 'y=x^3', 'y=x^2'], 'Body': \"Here's a simple intersection theoretic example. Take the intersection of the line <math_exp> and the parabola <math_exp>. Classically, the intersection is a point. But note that there is more to the intersection than just the point; there is the fact that the two curves are tangent at that point. Scheme-theoretically, the intersection is <math_exp>. This reflects the tangency. If the intersection were transverse, then the scheme-theoretic intersection would have been just <math_exp>. Higher order tangencies can be seen in the scheme-theoretic intersection as well; for example, repeat this exercise with <math_exp> in place of <math_exp>. \"}\n",
      "1276 {'Id': '1276', 'Type': 'answer', 'ParentId': '1241', 'urls': [], 'exp': [], 'Body': 'An important special case of the AM-GM inequality is that the product of two (positive) numbers with a constant sum is at a maximum when they are equal. This comes up a lot. '}\n",
      "1278 {'Id': '1278', 'Type': 'answer', 'ParentId': '574', 'urls': ['https://mathoverflow.net/questions/25513/zariski-closed-sets-in-cn-are-of-measure-0', 'https://mathoverflow.net/questions/19688/what-does-generic-mean-in-algebraic-geometry', 'https://mathoverflow.net/questions/2162/what-are-the-most-important-instances-of-the-yoga-of-generic-points'], 'exp': ['P(x)', 'x', 'x', 'P(x)', '\\\\mathbb{C}^n'], 'Body': 'In general*, if something is \"generic\", it means it happens or is true \"almost all of the time\" or \"almost everywhere\". In measure theory, for example, when you say \"<math_exp> is true for almost all <math_exp>\", this has the precise meaning that the set of <math_exp>\\'s for which <math_exp> does not hold has measure zero. One can relate this to the Zariski topology via the fact that Zariski closed subsets of <math_exp> have Lebesgue measure zero: https://mathoverflow.net/questions/25513/zariski-closed-sets-in-cn-are-of-measure-0 See also these MO posts: https://mathoverflow.net/questions/19688/what-does-generic-mean-in-algebraic-geometry https://mathoverflow.net/questions/2162/what-are-the-most-important-instances-of-the-yoga-of-generic-points *or perhaps I should say... generically ;-) '}\n",
      "1279 {'Id': '1279', 'Type': 'question', 'Title': 'What is the name for a shape that is like a capsule, but with two different radii?', 'Tags': ['geometry', 'terminology', 'circles'], 'AcceptedAnswerId': '1282', 'urls': [], 'exp': [], 'Body': 'I\\'m looking for the name of a shape that is like a capsule, but where each circle can have different radii. The shape could be described using two circles (two centers and two radii). Something like this (this is a 2D shape, not a lame attempt at 3D): <img src=\"https://i.stack.imgur.com/GoBEt.png\" alt=\"figure\"> '}\n",
      "1282 {'Id': '1282', 'Type': 'answer', 'ParentId': '1279', 'urls': [], 'exp': [], 'Body': 'You can just say it is the convex hull of two circles. '}\n",
      "1283 {'Id': '1283', 'Type': 'question', 'Title': 'Closed form of a partial sum of the power series of <span class=\"math-container\" id=\"10793\">\\\\exp(x)</span>', 'Tags': ['combinatorics', 'sequences-and-series'], 'urls': ['http://mathworld.wolfram.com/IncompleteGammaFunction.html'], 'exp': ['\\\\exp_n(x) = \\\\sum_{k=0}^n x^k / k!', '1 + x + \\\\cdots + x^n', '\\\\frac{1-x^{n+1}}{1-x}', 'f(x) = (1+x+x^2+x^3)(1+x+x^2)(1+x+x^2+\\\\cdots)', 'f(x) = \\\\left(\\\\frac{1-x^4}{1-x}\\\\right)\\\\left(\\\\frac{1-x^3}{1-x}\\\\right)\\\\left(\\\\frac{1}{1-x}\\\\right)', 'x^5', 'g(x) = \\\\exp_{n_1}(x)^{p_1} \\\\exp_{n_2}(x)^{p_2} \\\\cdots \\\\exp_{n_j}(x)^{p_j}', 'x^p / p!', '\\\\exp_n(x)', 'f', '1+x+x^2+\\\\cdots+x^n = \\\\frac{1-x^{n+1}}{1-x}', 'S = 1 + x + x^2 + \\\\cdots + x^n', 'S - Sx = 1 - x^{n+1}', 'y(x) = \\\\exp_n(x)', \"y - y' = x^n/n!\", 'y(x) = \\\\frac{c+\\\\Gamma(n+1,x)}{n!}e^x', 'y(0) = 0', 'c=0', '\\\\Gamma(n+1,x) = n! e^{-x} \\\\exp_n(x)', 'y(x) = \\\\exp_n(x)', '\\\\exp_n(x) = \\\\exp_{n-1}(x) + x^n/n!', '\\\\mathcal{Z}[\\\\exp_n(x)] = (z e^{x/z})/(z-1)', '\\\\exp_n(x) = \\\\mathcal{Z}^{-1}\\\\left[(z e^{x/z})/(z-1)\\\\right] = \\\\frac{1}{2 \\\\pi i} \\\\oint_C z^n e^{x/z}/(z-1)\\\\;dz', '(z^n e^{x/z})/(z-1)', 'z = 1', 'z = 0', 'z = 1', 'e^x', 'z = 0', 'z^n e^{x/z} (-1/(1-z)) = -z^n \\\\left( \\\\sum_{m=0}^\\\\infty x^m z^{-m} / m! \\\\right) \\\\left( \\\\sum_{j=0}^\\\\infty x^j \\\\right)', 'z^{-1}', 'n - m + j = -1', 'z=0', '-\\\\sum_{m,j} x^m / m! = -\\\\sum_{m=n+1}^\\\\infty x^m / m!', 'C', '\\\\frac{1}{2 \\\\pi i} \\\\oint_C z^n e^{x/z}/(z-1)\\\\;dz = \\\\frac{1}{2 \\\\pi i} 2 \\\\pi i \\\\left(e^x - \\\\sum_{m=n+1}^\\\\infty x^m / m!\\\\right) = \\\\exp_n(x)'], 'Body': \"I am looking for a closed form (ideally expressed as elementary functions) of the function <math_exp>.  I am already aware of expressing it in terms of the gamma function. When counting combinations of objects with generating functions, it is useful to be able to express the partial sum <math_exp> as <math_exp>. For example, to count the number of ways to pick 5 marbles from a bag of blue, red, and green marbles where we pick at most 3 blue marbles and at most 2 red marbles, we can consider the generating function <math_exp>. By using the partial sum identity, we can express it as <math_exp>.  Simplify, express as simpler product of series, and find the coefficient of the <math_exp> term. I want to be able to do the same for a generating function in the form <math_exp> The easiest way to extract the coefficient of a given term <math_exp> would be to use a similar closed form expression for <math_exp> and a similar technique to <math_exp>. Recall that the way to prove the identity <math_exp> is to define <math_exp> and notice that: <math_exp>. Likewise, notice that <math_exp> satisfies <math_exp>.  Via SAGE, the solution is <math_exp>.  Our initial condition <math_exp> so <math_exp>.  By (2), <math_exp> so we have <math_exp>. Notice that <math_exp>. Using the unilateral Z-Transform and related properties, we find that <math_exp>. Therefore, <math_exp>. <math_exp> has two singularities: <math_exp> and <math_exp>.  The point <math_exp> is a pole of order one with residue <math_exp>.  To find the residue at <math_exp> consider the product <math_exp>.  The coefficient of the <math_exp> term is given when <math_exp>. The residue of the point <math_exp> is then <math_exp>. Let <math_exp> by the positively oriented unit circle centered at the origin.  By Cauchy's Residue Theorem, <math_exp>. I've tried to evaluate the sum using finite calculus, but can't seem to make much progress. \"}\n",
      "1285 {'Id': '1285', 'Type': 'question', 'Title': 'Area of a triangle from some of its parts', 'Tags': ['geometry', 'contest-math'], 'AcceptedAnswerId': '1288', 'urls': [], 'exp': ['ABC', 'P', 'AP', 'BP', 'CP', 'D', 'E', 'F', 'ABC', 'x', 'y', 'z'], 'Body': 'I found this question a while ago on a SAT practice exam or something, can\\'t quite remember. So given an acute triangle <math_exp> with <math_exp> a point inside it and <math_exp>, <math_exp>, and <math_exp> meeting the opposite sides at <math_exp>, <math_exp>, and <math_exp> respectively: <img src=\"https://imgur.com/tnZew.png\" alt=\"alt text\"> How can you find the area of triangle <math_exp> given the areas of triangles <math_exp>, <math_exp>, and <math_exp>? '}\n",
      "1286 {'Id': '1286', 'Type': 'answer', 'ParentId': '544', 'urls': ['http://scf.usc.edu/~csci102b/curr/assgts/PA3/pa3.html'], 'exp': [], 'Body': 'I had a programming project in college where we generated large amounts of psuedo-English text using Markov chains. The assignment is here, although I don\\'t know if that link will be good forever. From that page: For example, suppose that [our Markov chains are of length] 2 and the sample file   contains Here is how the first three words   might be chosen: A two-word sequence is chosen at random to become the initial prefix.   Let\\'s suppose that \"the big\" is   chosen. The first word must be chosen based on the probability that it   follows the prefix (currently \"the   big\") in the source. The source   contains three occurrences of \"the   big\". Two times it is followed by   \"blue\", and once it is followed by   \"elephant\". Thus, the next word must   be chosen so that there is a 2/3   chance that \"blue\" will be chosen, and   a 1/3 chance that \"elephant\" will be   chosen. Let\\'s suppose that we choose   \"blue\" this time. The next word must be chosen based on the probability that it   follows the prefix (currently \"big   blue\") in the source. The source   contains two occurrences of \"big   blue\". Once it is followed by \"dog\",   and the other time it is followed by   \"hat\". Thus, the next word must be   chosen so that there is a 50-50   probability of choosing \"dog\" vs.   \"hat\". Let\\'s suppose that we choose   \"hat\" this time. The next word must be chosen based on the probability that it   follows the prefix (currently \"blue   hat\") in the source. The source   contains only one occurrence of \"blue   hat\", and it is followed by \"on\".   Thus, the next word must be \"on\" (100%   probability). Thus, the first three words in the output text would be \"blue hat   on\". You keep going like that, generating text that is completely nonsensical, but ends up having sort of the same \"tone\" as the original text. For example, if your sample file is the complete text of Alice In Wonderland (one of the texts we tried it on) then your nonsense comes out kind of whimsical and Carrollian (if that\\'s a word). If your sample file is The Telltale Heart, you get somewhat dark, morbid nonsense. Anyway, while not a rigorous, formal, definition, I hope this helps give you a sense of what a Markov chain is. '}\n",
      "1287 {'Id': '1287', 'Type': 'answer', 'ParentId': '1285', 'urls': ['http://en.wikipedia.org/wiki/Affine_transformation'], 'exp': [], 'Body': 'Method 1: We can perform an affine transformation to make ADC any triangle we wish, while preserving colinearality and the ratios of areas. In particular, this allows us to assign exact coordinates to A, D and C. We can then use the ratio of x and y to find E and the ratio of z and x+y+z to find P. We can then use these coordinates to find B. Once we know B, we know the ratio of BC to DC and hence the ratio of the area of the whole triangle to the known areas. Method 2: Let Q be the area of the whole triangle '}\n",
      "1288 {'Id': '1288', 'Type': 'answer', 'ParentId': '1285', 'urls': ['http://en.wikipedia.org/wiki/Mass_points'], 'exp': ['AE:EC = x:y', 'AC', 'AP:PD = (x+y):z', 'AE:EC', 'zy', 'A, zx', 'C', 'x:y', 'AE:EC', 'y(x+y)', 'D', '(x+y):z', 'AP:PD', 'y(x+y)-zx', 'B', 'BD:DC = zx:(y(x+y)-zx).', '(x+y+z) = zx:(y(x+y)-zx)'], 'Body': 'This was not likely to have been an SAT practice problem, though it is a typical contest problem. <math_exp> (since those two triangles have the same altitude to <math_exp>, the ratios of their areas is the ratios of their bases with respect to that altitude) and <math_exp> (same idea as <math_exp>).  Knowing these two ratios, apply the technique of mass points, putting masses <math_exp> at <math_exp> at <math_exp> (gives the ratio <math_exp> for <math_exp>), and <math_exp> at <math_exp> (gives <math_exp> for <math_exp>).  This results in a mass of <math_exp> at <math_exp>, so the ratio <math_exp>  This must also be the ratio of the areas of &#x25B3;ABD to &#x25B3;ADC (common altitude again), so (area of &#x25B3;ABD):<math_exp>.  Solving from there is a matter of bashing out the algebra. '}\n",
      "1289 {'Id': '1289', 'Type': 'answer', 'ParentId': '1279', 'urls': ['http://mathworld.wolfram.com/Circle-CircleTangents.html', 'https://i.stack.imgur.com/oGVWZ.gif', 'http://mathworld.wolfram.com/images/eps-gif/CircleCircleTangentGeneral_1000.gif'], 'exp': [], 'Body': 'Here is an article at Mathworld on Circle-Circle Tangents. Perhaps \"CirclesWithTangents\"?  (source: wolfram.com) '}\n",
      "1290 {'Id': '1290', 'Type': 'answer', 'ParentId': '164', 'urls': [], 'exp': ['r_1', 'r_2', '\\\\int_{r_1}^{r_2}'], 'Body': \"To expand on what anosov_diffeomorphism said: Suppose we agree that the surface area of a sphere is 4 &pi; r2 (If we do not agree on this, that would be a great question to ask on this site, and then someone can edit this answer to link to it!). Now if I have a sphere of radius r, and I increase the radius by a tiny amount, dr, then the new, expanded sphere has a volume that is bigger, by the volume of the thin spherical shell that was just added. If dr is small enough, then it doesn't much matter whether we look at the inner or outer radius of that shell, and we can just say that the shell's volume is approximately 4 &pi; r2 dr. So the difference in volume between two spheres of radius <math_exp> and <math_exp> is <math_exp> 4 &pi; r2 dr, which is 4/3 &pi; r^3 \"}\n",
      "1291 {'Id': '1291', 'Type': 'answer', 'ParentId': '1237', 'urls': [], 'exp': [], 'Body': 'One that I remember from some book (I think it was Innumeracy by John Allen Paulos) was \"How fast does your hair grow, in miles per hour?\" '}\n",
      "1292 {'Id': '1292', 'Type': 'question', 'Title': 'How can I understand and prove the \"sum and difference formulas\" in trigonometry?', 'Tags': ['intuition', 'trigonometry'], 'AcceptedAnswerId': '1293', 'urls': ['http://en.wikipedia.org/wiki/Angle_addition_formula#Angle_sum_and_difference_identities', 'http://en.wikipedia.org/wiki/Euler%27s_formula'], 'exp': [], 'Body': 'The \"sum and difference\" formulas often come in handy, but it\\'s not immediately obvious that they would be true. \\\\begin{align} \\\\sin(\\\\alpha \\\\pm \\\\beta) &amp;= \\\\sin \\\\alpha \\\\cos \\\\beta \\\\pm \\\\cos \\\\alpha \\\\sin \\\\beta \\\\\\\\ \\\\cos(\\\\alpha \\\\pm \\\\beta) &amp;= \\\\cos \\\\alpha \\\\cos \\\\beta \\\\mp \\\\sin \\\\alpha \\\\sin \\\\beta \\\\end{align} So what I want to know is, Ideally, I\\'m looking for answers that make no reference to Calculus, or to Euler\\'s formula, although such answers are still encouraged, for completeness. '}\n",
      "1293 {'Id': '1293', 'Type': 'answer', 'ParentId': '1292', 'urls': [], 'exp': ['u + v', 'u', 'v', '(1, 0)', '(0, 1)', '(1, 0)', '\\\\theta', '(\\\\cos \\\\theta, \\\\sin \\\\theta)', '(0, 1)', '\\\\theta', '(-\\\\sin \\\\theta, \\\\cos \\\\theta)', '\\\\theta', '2 \\\\times 2', '\\\\theta', \"\\\\theta'\", '2 \\\\times 2', 'a \\\\cdot b = |a| |b| \\\\cos \\\\theta', 'a', 'b'], 'Body': \"The key fact here is that rotation is a linear transformation, e.g. the rotation of <math_exp> is the rotation of <math_exp> plus the rotation of <math_exp>.  You should draw a diagram that shows this carefully if you don't believe it.  That means a rotation is determined by what it does to <math_exp> and to <math_exp>. But <math_exp> rotated by <math_exp> degrees counterclockwise is just <math_exp>, whereas <math_exp> rotated by <math_exp> degrees counterclockwise is just <math_exp>.  (Again, draw a diagram.)  That means a rotation by <math_exp> is given by a <math_exp> matrix with those entries.  (Matrices don't work here yet.) So take a rotation by <math_exp> and another one by <math_exp>, and multiply the corresponding matrices.  What you get is the sine and cosine angle addition formulas.  (The connection to complex numbers is that one can represent complex numbers as <math_exp> real matrices.) Also, if you believe that <math_exp>, this implies the cosine angle difference formula when <math_exp> and <math_exp> are unit vectors.  Ditto for the cross product and the sine angle difference formula. \"}\n",
      "1294 {'Id': '1294', 'Type': 'answer', 'ParentId': '1292', 'urls': ['http://www.imgftw.net/img/400545892.png'], 'exp': ['c^2=1^2+1^2-2\\\\cdot1\\\\cdot1\\\\cdot\\\\cos(\\\\alpha-\\\\beta)', 'c=\\\\sqrt{(\\\\cos\\\\alpha-\\\\cos\\\\beta)^2+(\\\\sin\\\\alpha-\\\\sin\\\\beta)^2}', '1^2+1^2-2\\\\cdot1\\\\cdot1\\\\cdot\\\\cos(\\\\alpha-\\\\beta)=(\\\\cos\\\\alpha-\\\\cos\\\\beta)^2+(\\\\sin\\\\alpha-\\\\sin\\\\beta)^2', '2-2\\\\cos(\\\\alpha-\\\\beta)=\\\\cos^2\\\\alpha-2\\\\cos\\\\alpha\\\\cos\\\\beta+\\\\cos^2\\\\beta+\\\\sin^2\\\\alpha-2\\\\sin\\\\alpha\\\\sin\\\\beta+\\\\sin^2\\\\beta', '=2-2\\\\cos\\\\alpha\\\\cos\\\\beta-2\\\\sin\\\\alpha\\\\sin\\\\beta', '\\\\cos(\\\\alpha-\\\\beta)', '\\\\cos(\\\\alpha-\\\\beta)=\\\\cos\\\\alpha\\\\cos\\\\beta+\\\\sin\\\\alpha\\\\sin\\\\beta', '\\\\frac{\\\\pi}{2}-\\\\alpha', 'z_\\\\theta', 'z_\\\\alpha=\\\\cos\\\\alpha+i\\\\sin\\\\alpha', 'z_\\\\beta=\\\\cos\\\\beta+i\\\\sin\\\\beta', '\\\\cos(\\\\alpha+\\\\beta)+i\\\\sin(\\\\alpha+\\\\beta)'], 'Body': \"There are several typical derivations used in high school texts.  Here's one: diagram http://www.imgftw.net/img/400545892.png Take two points on the unit circle, one a rotation of (1,0) by &alpha;, the other a rotation of (1,0) by &beta;.  Their coordinates are as shown in the diagram.  Let c be the length of the segment joining those two points.  By the Law of Cosines (on the blue triangle), <math_exp>.  Using the distance formula, <math_exp>.  Squaring the latter and setting the two equal, <math_exp>.  Simplifying both sides, <math_exp> <math_exp> (using the Pythagorean identity).  Solving for <math_exp>, <math_exp>. From this identity, the other three can be derived by substituting <math_exp> for &alpha; (gives sin(&alpha;+&beta;)), then -&beta; for &beta; (gives the remaining two). As to understanding the formulas intuitively if you accept that multiplying by a complex number <math_exp> for which |z|=1 rotates by &theta;, then you can think about what happens when you multiply <math_exp> and <math_exp> (by expanding the binomial product), which should result in <math_exp>. \"}\n",
      "1295 {'Id': '1295', 'Type': 'answer', 'ParentId': '1237', 'urls': [], 'exp': [], 'Body': 'What is the mass of the atmosphere? '}\n",
      "1296 {'Id': '1296', 'Type': 'answer', 'ParentId': '405', 'urls': ['http://en.wikibooks.org/wiki/Haskell/Category_theory#Monads'], 'exp': ['T: C \\\\to C', '\\\\eta_X : X \\\\to T(X)', '\\\\mu_X : T(T(X)) \\\\to T(X)', '\\\\mu_X \\\\circ T(\\\\eta_X) = \\\\mu_X \\\\circ \\\\eta_{T(X)} = 1_{T(X)}', '\\\\mu_X \\\\circ \\\\mu_{T(X)} = \\\\mu_X \\\\circ T(\\\\mu_X)', 'S'], 'Body': 'Monads in Haskell and monads in category theory are very much the same: A monad consists of a functor <math_exp> and two natural transformations <math_exp> (return in Haskell) and <math_exp> (join in Haskell) subject to the following laws <math_exp> (left and right unit laws) <math_exp> (associativity) So, compared to Haskell, the monad is defined in terms of return, join and fmap instead of return and (&gt;&gt;=). For more details on this, see also the Haskell wikibook. Two examples may illuminate this definition. The powerset functor The closure operation on the subsets of a topological space <math_exp> is a monad, too. '}\n",
      "1297 {'Id': '1297', 'Type': 'answer', 'ParentId': '1292', 'urls': ['http://en.wikipedia.org/wiki/Cosine#Relationship_to_exponential_function_and_complex_numbers'], 'exp': ['\\\\cos(x) = \\\\frac{1}{2}(e^{ix} + e^{-ix})', '\\\\sin(x) = \\\\frac{1}{2i}(e^{ix} - e^{-ix})', 'a^{x+y}=a^x a^y'], 'Body': 'You can use the complex representation, <math_exp> <math_exp> and the rules for powers (<math_exp>) '}\n",
      "1298 {'Id': '1298', 'Type': 'answer', 'ParentId': '1292', 'urls': [], 'exp': ['\\\\sin(\\\\alpha + \\\\beta) = \\\\mbox{PR} = \\\\mbox{PQ} + \\\\mbox{QR} = \\\\sin(\\\\beta)\\\\cos(\\\\alpha) + \\\\cos(\\\\beta)\\\\sin(\\\\alpha)', '\\\\cos(\\\\alpha + \\\\beta) = \\\\mbox{OR} = \\\\mbox{OM} - \\\\mbox{RM} = \\\\cos(\\\\beta)\\\\cos(\\\\alpha) - \\\\sin(\\\\beta)\\\\sin(\\\\alpha)'], 'Body': 'Though the standard high-school derivations are not the most useful way to remember it in the long run, here\\'s another one which I like because you can \"see\" it directly without much algebra. <img src=\"https://imgur.com/QnIhq.png\" alt=\"Angle sum formulae\"> Let P be the point on the unit circle got by rotating (1,0) by angle α+β. Drop a perpendicular N to the α-rotated line, and R to the x-axis. So from the right triangle ONP, you see ON = cos &beta;. You can see that the angle RPN is &alpha; too: it\\'s the complement of ∠PNQ, and so is ∠QNO = &alpha;. Now, <math_exp>, and <math_exp>. '}\n",
      "1299 {'Id': '1299', 'Type': 'answer', 'ParentId': '1292', 'urls': [], 'exp': ['e^{i\\\\alpha}=\\\\cos\\\\alpha+i\\\\sin\\\\alpha', 'i^2=-1', 'e^{i(\\\\alpha+\\\\beta)}=e^{i\\\\alpha}e^{i\\\\beta}', 'e^{i\\\\alpha}=\\\\cos\\\\alpha+i\\\\sin\\\\alpha'], 'Body': 'I remember that <math_exp> and that <math_exp>. Both these relations are useful in many other situations and pretty fundamental to understanding complex numbers. Then your equalities are the real and, respectively, the imaginary part of <math_exp>. This is not very different from the other answers, but I actually prefer the algebra perspective. The only place where I think geometrically is in interpreting <math_exp> by thinking of the unit circle in the complex plane. '}\n",
      "1300 {'Id': '1300', 'Type': 'answer', 'ParentId': '1234', 'urls': [], 'exp': ['i\\\\le j', 'x_j', 'E(i,j)=\\\\min_k E(i,k)+E(k,j)+|x_k-x_j|'], 'Body': \"Here's a polynomial time solution given to me by Javi Gomez. Let (i,j) with <math_exp> represent the situation where disks i, i+1, ..., j are on top of each other in position <math_exp>, and let E(i,j) represent the energy needed to obtain that configuration. Clearly E(i,i) is zero for all i. Also, <math_exp>. What we want is E(1,n). (I made this a community wiki since the answer wasn't really found by me. Feel free to edit.) \"}\n",
      "1302 {'Id': '1302', 'Type': 'question', 'Title': 'How to test if a point is inside the convex hull of two circles?', 'Tags': ['geometry'], 'AcceptedAnswerId': '1305', 'urls': ['https://math.stackexchange.com/questions/1279/what-is-the-name-for-a-shape-that-is-like-a-capsule-but-with-two-different-radii', 'http://mathworld.wolfram.com/Circle-CircleTangents.html'], 'exp': [], 'Body': 'Following my previous question, I\\'m wondering how I can determine if a point is within the convex hull of two circles (a boolean value). There\\'s no problem testing if the point is in either of the two circles, but it can also be \"between\" them and I don\\'t know how to test for that. Seeing Wolfram MathWorld\\'s article on Circle-Circle Tangeants, it seems that an inequation that tests if the point is on the internal side the two external circle tangeants would do the trick, but I\\'m afraid my solving skills are too far away to be able to turn the tangeant equations into a fitting inequality. I\\'m defining the convex hull of two circles using both centers and radii. '}\n",
      "1303 {'Id': '1303', 'Type': 'answer', 'ParentId': '1302', 'urls': ['http://en.wikipedia.org/wiki/Isosceles_trapezoid'], 'exp': [\"x=((y'-y'_1)\\\\sin\\\\alpha+(x'-x'_1)\\\\cos\\\\alpha)/D\", \"y=((y'-y'_1)\\\\sin\\\\alpha-(x'-x'_1)\\\\cos\\\\alpha)/D\", \"D^2=(x'_1-x'_2)^2+(y'_1-y'_2)^2\", '\\\\alpha', '(a,b)^T', '(a,b)^T'], 'Body': \"The solution to this problem is indeed to check whether the point is in one of the circles or in the isosceles trapezoid determined by the points were circles touch the tangent lines. However, without some care the equations get messy. Let's start with a circle at (x1', y1') of radius r1', a circle at (x2', y2') with radius r2', and a point (x', y') to test. We may assume that r1' is at least r2'. By shifting followed by rotation followed by scaling this can be transformed into a simpler instance: a circle at (0,0) of radius r1, a circle at (1,0) of radius r2, and a point (x,y) to test. We have: r1=r1'/D, r2=r2'/D, <math_exp> <math_exp> where <math_exp> and <math_exp> is the angle where (x2'-x1',y2'-y1') lies. (In C, there's a function atan2 that takes the two coordinates of a point and gives the angle. In math, atan doesn't really distinguish between points symmetric wrt (0,0).) There's only one thing I used, really, namely that certain triangles are similar. Draw the tangent that touches the two circles 'above'. Let's say that it intersects the big circle at A=(x1,y1), the small circle at B=(x2,y2), and the horizontal axis at C=(d,0). Let's also write D=(0,0) and E=(x1,0) and F=(1,0) and G=(x2,0). The similar triangles are ADC, AED, BFC, and BGD. From ADC similar to BFC you find d = r1/Dr. From ADC similar to AED you find x1 = r1 Dr and from BFD similar to BGD you find x2 - 1 = r2 Dr. To check that a point is on the right side of a line write its equation as ax+by+c=0 and replace equality by inequality. (It's good to remember that the vector <math_exp> is perpendicular to the line and that the equation can be interpreted as defining all vectors/points whose scalar/dot product with a fixed vector <math_exp> gives the constant c.) Let me know if I messed up again :P This is the old answer, which is wrong as pointed by the first comment below. It starts with the same transformations (without giving details) but then it uses some line that is clearly not tangent to the circles. Say the first circle has the center at (0,0) (if not, shift the figure) with radius r_1, and the second at (1,0) (if not, rotate the figure and scale) with radius r_2. If your point is (x,y) and is not in one of the two circles, then x must be between 0 and 1 and |y| must be at most r_1+(r_2-r_1)x. \"}\n",
      "1305 {'Id': '1305', 'Type': 'answer', 'ParentId': '1302', 'urls': ['http://en.wikipedia.org/wiki/Tangent_lines_to_circles', 'http://en.wikipedia.org/wiki/Point_in_polygon'], 'exp': [], 'Body': 'You can first calculate the four points of tangency, then use the Point in Polygon algorithm to determine if your point is inside the quadrilateral (this is assuming you have a programming related problem) Otherwise, once you have found the four tangent points, you can form four lines and get four simultaneous inequalities. '}\n",
      "1306 {'Id': '1306', 'Type': 'answer', 'ParentId': '77', 'urls': [], 'exp': [], 'Body': \"Two more uses that I haven't seen mentioned yet:  if you want to find the area of the parallelogram formed by two vectors (each vector gives a pair of parallel sides), then you would use the magnitude of the cross product of the two vectors. One use of this is to aid in defining a surface integral.  Let x(u,v) be a parameterization of a surface.  Then at each point, we can find tangent vectors Tu =  &part;x/&part;u and Tv = &part;x/&part;v.  From the idea of a linear approximation, Tu and Tv will define a tangent plane at that particular point.  Consider the parallelogram formed with Tu and Tv as sides.  Informally, we can see that each area element will be |Tu x Tv| du dv.  Then a function f(u,v) integrated over this surface is &int;&int; f(u,v) |Tu x Tv| du dv. As for the second use, if you wanted to find the volume of the parallelepiped having the three vectors a, b, c as sides, then you would use the magnitude of the scalar triple product |a &sdot; (b x c)|. \"}\n",
      "1307 {'Id': '1307', 'Type': 'question', 'Title': 'Concrete examples of valuation rings of rank two.', 'Tags': ['abstract-algebra', 'commutative-algebra', 'valuation-theory'], 'AcceptedAnswerId': '1309', 'urls': [], 'exp': ['A', 'A', '\\\\mathrm{Spec}(A)', 'A', 'A', 'A'], 'Body': 'Let <math_exp> be a valuation ring of rank two. Then <math_exp> gives an example of a commutative ring such that <math_exp> is a noetherian topological space, but <math_exp> is non-noetherian. (Indeed, otherwise <math_exp> would be a discrete valuation ring.) Is there a concrete example of such a ring <math_exp>? '}\n",
      "1308 {'Id': '1308', 'Type': 'answer', 'ParentId': '1307', 'urls': ['http://en.wikipedia.org/wiki/Valuation_ring#Construction'], 'exp': ['\\\\mathbb{C}', '\\\\mathbb{Z}^2'], 'Body': 'Take the ring of formal power series (over <math_exp>, say) with exponents in <math_exp> under lex order. Edit:  As Robin Chapman mentions, one must be careful about what this means.  The precise construction for any totally ordered abelian group is described in the Wikipedia article. '}\n",
      "1309 {'Id': '1309', 'Type': 'answer', 'ParentId': '1307', 'urls': [], 'exp': ['K', 'A', 'f=\\\\sum_{r=-r_0}^\\\\infty x^r\\\\sum_{s=-s_0(r)}^\\\\infty a_{r,s}y^s.', 'r_0', 'r', 's_0(r)', 'r', 'x', 'r', 'x^r y^s', 's', 'r', 'K', 'K', 'A', 'r_0=0', 's_0(0)=0', 'f', '(r,s)', 'a_{r,s}\\\\ne0', \"(r,s)&lt;(r',s')\", \"r &lt; r'\", \"r=r'\", \"s &lt; s'\", 'K', 'f', '(r,s)', 'a_{r,s}\\\\ne0', 'n', 'n'], 'Body': \"Qiaochu's answer is sound in principle, but in practice one needs to be more careful with the definition of the ring. The quotient field <math_exp> of <math_exp> consists of formal Laurent series of the form <math_exp> Here <math_exp> is an integer and for each integer <math_exp>, <math_exp> is an integer (depending on <math_exp>). So these are the power series where the powers of <math_exp> are bounded below and for each integer <math_exp> the coefficient of <math_exp> is zero for all <math_exp> below a bound depending on <math_exp>. This complicated-looking condition ensures that the product of two elements of <math_exp> is also an element of <math_exp> (note that one cannot multiply two general Laurent series). Then <math_exp> will consist of all such series with the additional conditions that <math_exp> and <math_exp>. The valuation of an element <math_exp> is the least <math_exp> under lexicographic ordering with <math_exp>. Here the ordering is <math_exp> if <math_exp> or <math_exp> and <math_exp>. A more high-brow interpretation of the condition for memebership of <math_exp> is that the support of <math_exp>, the setof <math_exp> for which <math_exp>, should be well-ordered, that is each subset of the support has a least element. (With repsct to this lexicographic ordering of course.) By considering a version of this construction in <math_exp> variables one can construct explicitly a ring with a valuation of rank <math_exp>. \"}\n",
      "1310 {'Id': '1310', 'Type': 'answer', 'ParentId': '803', 'urls': ['http://terrytao.wordpress.com/2007/09/14/pythagoras-theorem/', 'http://math.berkeley.edu/~giventh/papers/eu.pdf'], 'exp': [], 'Body': 'More than dissection proofs, I find the proof using similarity most enlightening and intuitive. See post by Terence Tao: drop a perpendicular from C to the hypotenuse AB. <img src=\"https://www.math.ucla.edu/~tao/pythagoras.jpeg\" alt=\"Image from Tao\"> In the figure, the triangles whose are areas are marked x and y are similar to the original triangle (which has area x+y). So accepting that areas of similar right-angled triangles are proportional to the squares of the hypotenuse, x:y:x+y are in ratio a2:b2:c2, which is Pythagoras\\'s theorem. See also the linked discussion by Alexander Giventhal where he remarks that this proof is more general than tiling or dissection proofs, and is even proved by Euclid. If you think of a2+b2=c2 as the geometrical result that the sum of areas of squares constructed with sides a and b is the area of a square placed on c, then the Pythagorean theorem is true not just for constructing squares on the sides, but any similar figures. For instance, as Euclid himself proves, something like the following is true (though it\\'s still just saying a2+b2=c2): <img src=\"https://imgur.com/OpDUA.png\" alt=\"From Giventhal\"> '}\n",
      "1311 {'Id': '1311', 'Type': 'question', 'Title': 'Are there more rational numbers than integers?', 'Tags': ['elementary-set-theory', 'infinity'], 'AcceptedAnswerId': '1312', 'urls': [], 'exp': [], 'Body': 'I\\'ve been told that there are precisely the same number of rationals as there are of integers. The set of rationals is countably infinite, therefore every rational can be associated with a positive integer, therefore there are the same number of rationals as integers. I\\'ve ignored sign-related issues, but these are easily handled. To count the rationals, consider sets of rationals where the denominator and numerator are positive and sum to some constant. If the constant is 2 there\\'s 1/1. If the constant is 3, there\\'s 1/2 and 2/1. If the constant is 4 there\\'s 1/3, 2/2 and 3/1. So far we have counted out 6 rationals, and if we continue long enough, we will eventually count to any specific rational you care to mention. The trouble is, I find this very hard to accept. I have two reasons. First, this logic seems to assume that infinity is a finite number. You can count to and number any rational, but you cannot number all rationals. You can\\'t even count all positive integers. Infinity is code for \"no matter how far you count, you have never counted enough\". If it were possible to count to infinity, it would be possible to count one step less and stop at count infinity-1 which must be different to infinity. The second reason is that it\\'s very easy to construct alternative mappings. Between zero and one there are infinitely many rational numbers, between one and two there are infinitely many rational numbers, and so on. To me, this seems a much more reasonable approach, implying that there are infinite rational numbers for every integer. But even then, this is just one of many alternative ways to map between ranges of rationals and ranges of integers. Since you can count the rationals, you can equally count stepping by any amount for each rational. You can use 1..10 for the first rational and 11..20 for the second etc. Or 1..100 and 101..200 etc, or 1..1000 and 1001..2000 etc. You can map finite range of integers of any size to each rational this way and, since there is no finite upper bound to the stepping amount, you could argue there are potentially infinite integers for every single rational. So... can anyone convince me that there is a single unambiguous correct answer to this question? Are there more rational numbers than integers, or not? EDIT Although I\\'ve already accepted an answer, I\\'ll just add some extra context. My reason for questioning this relates to the Hilbert space-filling curve. I find this interesting because of applications to multi-dimensional indexing data structures in software. However, I found Hilberts claim that the Hilbert curve literally filled a multi-dimensional space hard to accept. As mentioned in a comment below, a one meter line segment and a two meter line segment can both be seen as sets of points and, but (by the logic in answers below), those two sets are both the same size (cardinality). Yet we would not claim the two line segments are both the same size. The lengths are finite and different. Going beyond this, we most certainly wouldn\\'t claim that the size of any finite straight line segment is equal to the size of a one-meter-by-one-meter square. The Hilbert curve reasoning makes sense now - the set of points in the curve is equal to the set of points in the space it fills. Previously, I was thinking too much about basic geometry, and couldn\\'t accept the size of a curve as being equal to the size of a space. However, this isn\\'t based on a fallacious counting-to-infinity argument - it\\'s a necessary consequence of an alternative line of reasoning. The two constructs are equal because they both represent the same set of points. The area/volume/etc of the curve follows from that. '}\n",
      "1312 {'Id': '1312', 'Type': 'answer', 'ParentId': '1311', 'urls': ['http://www.math.upenn.edu/~wilf/website/recounting.pdf'], 'exp': [], 'Body': 'Mathematicians have very precise definitions for terms like \"infinite\" and \"same size\".  The single unambiguous correct answer to this question is that using the standard mathematical definitions, the rationals have the \"same size\" as the integers. First, here are the definitions: Define \"0\" = emptyset, \"1\" = {0}, \"2\" = {0,1}, \"3\" = {0,1,2}, etc.  So, the number \"n\" is really a set with \"n\" elements in it. A set A is called \"finite\" iff there is some n and a function f:A->n which is bijective. A set A is called \"infinite\" iff it is not finite.  (Note that this notion says nothing about \"counting never stops\" or anything like that.) Two sets A and B are said to have the \"same size\" if there is a some function f:A-> B which is a bijection.  Note that we do NOT require that ALL functions be bijections, just that there is SOME bijection. Once one accepts these definitions, one can prove that the rationals and integers have the same size.  One just needs to find a particular bijection between the two sets.  If you don\\'t like the one you mentioned in your post, may I suggest that Calkin-Wilf enumeration of the rationals?  (Simply google search Calkin Wilf counting rationals.  The first .pdf has what I\\'m talking about). Of course, these give bijections between the naturals (with out 0) and the rationals, but once you have a bijection like this, it\\'s easy to construct a bijection from the integers to the rationals by composing with a bijection from the naturals to the integers. '}\n",
      "1313 {'Id': '1313', 'Type': 'answer', 'ParentId': '1311', 'urls': [], 'exp': ['f(x) = 2x'], 'Body': 'You can think about it a different way. Consider the set of real numbers between 0 and 1, and then the set of real numbers between 0 and 2. By intuition, it seems that the set of real numbers between 0 and 2 has double the size of the set between 0 and 1. However, this is not the case, because the two sets have the same cardinality. Consider the function <math_exp>. Every real between 0 and 1 is bijected to a real between 0 and 2. Therefore the sets are of the same size. '}\n",
      "1314 {'Id': '1314', 'Type': 'answer', 'ParentId': '803', 'urls': ['http://www.amazon.com/exec/obidos/ISBN=0873530365/ctksoftwareincA/'], 'exp': [], 'Body': 'Not a proof in itself, but the book The Pythagorean Proposition by Loomis has probably the most comprehensive collection of proofs of the Pythagorean theorem. '}\n",
      "1315 {'Id': '1315', 'Type': 'question', 'Title': 'How do I calculate expected value of partial normal distribution?', 'Tags': ['calculus', 'statistics', 'probability-theory'], 'AcceptedAnswerId': '1320', 'urls': [], 'exp': [], 'Body': 'Suppose you have a normal distribution with mean=0, and stdev=1.  So the expected value is 0. Now suppose you limit the outcomes, such that no values can be below 0.  So 50% of values now equal 0, and rest of distribution is still normal.  Running 1000000 trials, I come out with an expected value of .4 My question is how can I get this expected value through calculation? Thanks '}\n",
      "1316 {'Id': '1316', 'Type': 'answer', 'ParentId': '1311', 'urls': [], 'exp': [], 'Body': 'The cardinality of the set of rationals is the same as the cardinality of the integers is the same as the cardinality of the natural numbers. When we count a finite set of elements, we are constructing a one-one map from the set onto a finite initial segment of the natural numbers. If we want to know if two finite sets have the same cardinality (are equi-cardinal) we can either: 1) count both sets and see if we get the same number, or 2) attempt to construct a one-one map from one set onto the other. If we can construct the map aimed at in (2), then the sets are equi-cardinal. Generalizing that procedure from the finite sets to arbitrary sets, we get that for any two sets, the sets have the same cardinality (are equi-cardinal) if there exists a bijection (a one-one map between the sets that is onto the target rather than merely into). For the finite case, if there is a one-one map that is a bijection, all one-one maps are bijective. That is not the case for infinite sets, which is the root of your second concern. To address that second concern, consider the map from the negative integers to the positive integers which maps each negative integer to its absolute value. The existence of that map shows that the two sets are equi-cardinal. We can, of course, construct one-one maps from the negative integers to the positive integers that are into rather than on to. (Consider the map that takes each negative integer to its product with -2.) But, the existence of these alternative maps doesn\\'t affect the fact that there is at least one bijection between the sets, and that is all it takes for those sets to be equi-cardinal. As for your first concern, I don\\'t see why you think the procedure assumes that \"infinity is a finite number\". What it involves is specifying a mapping function from one set to the other that is one-one and onto. That attempt can certainly fail, as Cantor\\'s Diagonalization Argument that the cardinality of a set is always strictly less than the cardinality of its power set shows. (A relevant application of that technique is the well-known proof that the cardinality of the reals is greater than the cardinality of the natural numbers.) '}\n",
      "1317 {'Id': '1317', 'Type': 'answer', 'ParentId': '1311', 'urls': [], 'exp': [], 'Body': \"In mathematics a set is called infinite if it can be put into a 1-1 correspondence with a proper subset of it, and finite it is not infinite. (I know it seems crazy to have the concept of infinite as primitive and finite as a derivate, but it's simpler to do this, since otherwise you must assume that the integers exist before saying that a set is finite) As for your remarks:  - with your method (if you don't forget to throw out fractions like 4/6 which is equal to 2/3) you actually counted the rationals, since for each number you have a function which associates it to a natural number. It's true that you cannot count ALL rationals, or all integers; but you cannot either draw a whole straight line, can you? - with infinite sets you may build infinite mappings, but you just need a single 1-1 mapping to show that two sets are equal. \"}\n",
      "1318 {'Id': '1318', 'Type': 'answer', 'ParentId': '329', 'urls': ['http://rads.stackoverflow.com/amzn/click/0321500318'], 'exp': [], 'Body': 'My favorite is Elementary Number Theory by Rosen, which combines computer programming with number theory, and is accessible at a high school level. '}\n",
      "1319 {'Id': '1319', 'Type': 'answer', 'ParentId': '1311', 'urls': [], 'exp': [], 'Body': 'You may not be very satisfied with this answer, but I\\'ll try to explain anyway. Countability. We\\'re not really talking about whether you can \"count all of the rationals\", using some finite process. Obviously, if there is an infinite number of elements, you cannot count them in a finite amount of time using any reasonable process. The question is whether there is the same number of rationals as there are positive integers; this is what it means for a set to be \"countable\" --- for there to exist a one-to-one mapping from the positive integers to the set in question. You have described such a mapping, and therefore the rationals are \"countable\". (You may disagree with the terminology, but this does not affect whether the concept that it labels is coherent.) Alternative mappings. You seem to be dissatisfied with the fact that, unlike the case of a finite set, you can define an injection from the natural numbers to the rationals which is not surjective --- that you can in fact define a more general relation in which each integer is related to infinitely many rationals, but no two integers are related to the same rational numbers. Well, two can play at that game: you can define a relation in which every rational number is related to infinitely many integers, and no two rationals are related to the same integers! Just define the relation that each positive rational a/b is related to all numbers which are divisible by 2a but not 2a+1, and by 3b but not 3b+1; or more generally respectively 2ka and 3kb for any positive integer k. (There are, as you say, sign issues, but these can be smoothed away.) You might complain that the relation I\\'ve defined isn\\'t \"natural\". Perhaps you have in mind the fact that the integers are a subset of the rationals --- a subgroup, in fact, taking both of them as additive groups --- and that the factor group &#8474;/&#8484; is infinite. Well, this is definitely interesting, and it\\'s a natural sort of structure to be interested in. But it\\'s more than what the issue of \"mere cardinality\" is trying to get at: set theory is interested in size regardless of structure, and so we don\\'t restrict to maps which have one or another kind of \"naturalness\" about them. Of course, if you are interested in mappings which respect some sort of structure, you can build theories of size based on that: this is what is done in measure theory (with measure), linear algebra (with dimension), and indeed group theory (with index). So if you don\\'t like cardinality as set theorists conceive it, you can look at more structured measures of size that you find more interesting! Immediate predecessors. A somewhat unrelated (but still important) complaint that you make is this: \"If it were possible to count to infinity, it would be possible to count one step less and stop at count infinity-1 which must be different to infinity.\" The question is: why would you necessarily be able to stop at \\'infinity minus one\\'? This is true for finite collections, but it does not necessarily hold that anything which is true of finite collections is true also for infinite ones. (In fact, obviously, some things necessarily will fail.) --- This is important if you study ordinals, which mirrors the process of counting itself in some ways (labelling things as being \"first\", \"second\", \"third\", and so forth), because of the concept of a limit ordinal: the first \"infinitieth\" element of a well-ordering doesn\\'t have any immediate predecessors! Again, you are free to say that these are concepts that you are not interested in exploring personally, but this does not mean that they are necessarily incoherent. To summarize: the set theorists measure \"the size of a set\" using a simple definition which doesn\\'t care about structure, and which may violate your intuitions if you like to take the structure of the integers (and the rational numbers) very seriously, and also want to preserve your intuitions about finite sets. There are two solutions to this: try to stretch your intuition to accomodate the ideas of the set theorists, or study a different branch of math which you find more interesting! '}\n",
      "1320 {'Id': '1320', 'Type': 'answer', 'ParentId': '1315', 'urls': [], 'exp': ['f(x)=\\\\frac{e^{-\\\\frac{x^2}{2}}}{\\\\sqrt{2\\\\pi}}', 'P(0)=\\\\frac{1}{2}', 'P(x)=0', '0\\\\cdot\\\\frac{1}{2}+\\\\int_{0}^{\\\\infty}x\\\\cdot f(x)dx=\\\\frac{1}{\\\\sqrt{2\\\\pi}}\\\\approx0.398942', 'x=c', 'x=0', 'f(x)=\\\\frac{e^{-\\\\frac{x^2}{2}}}{\\\\sqrt{2\\\\pi}}', 'x&gt;c', 'P(c)=\\\\int_{-\\\\infty}^{c}\\\\frac{e^{-\\\\frac{x^2}{2}}}{\\\\sqrt{2\\\\pi}}dx', 'P(x)=0', 'x&lt;c', 'c\\\\cdot P(c) + \\\\int_{c}^{\\\\infty}x\\\\cdot \\\\frac{e^{-\\\\frac{x^2}{2}}}{\\\\sqrt{2\\\\pi}}dx', '-\\\\frac{x^2}{2}'], 'Body': 'The normal distribution has density function <math_exp>; your new distribution has that density function on the positive reals, <math_exp>, and <math_exp> for the negative reals.  The expected value is <math_exp>. edit: If you were to cut off at <math_exp> (assigning all the probability from below c to c itself) instead of <math_exp>, your density function would be <math_exp> for <math_exp>, <math_exp>, and <math_exp> for <math_exp>, so the expected value is <math_exp>. edit 2: note that the exponent on e in all of the above is <math_exp> (the exponent 2 on the x is, in the current TeX rendering, positioned and sized such as to be somewhat ambiguous) edit 3: my explanation incorrectly mixed probability density functions and literal probabilities--this was solely an issue of terminology and the analytic results still stand, but I have attempted to clarify the language above. '}\n",
      "1321 {'Id': '1321', 'Type': 'answer', 'ParentId': '460', 'urls': [], 'exp': [], 'Body': 'One source of monoids is given by taking rings with identity, and forgetting about addition. So similarly, one source of semigroups that are not monoids is taking rings without identity, and forgetting about addition.  With this in mind, let me explain one basic source of rings without identity. A basic source of rings is given by taking functions satisfying some reasonable condition on a space, e.g. continuous real or complex valued functions on a space, with pointwise addition and multiplication.  Of course, the constant function 1 is continuous, and so this gives a ring with identity. But suppose now that we impose some condition, such as \"all functions that are continuous, and which furthermore vanish at some specified point\".  This throws out the constant function 1, and so gives a ring without identity.  Now you could naturally object that this is artificial (as per the requirement in the question that there not be an obvious extension to a monoid), so let me add more explanation as to why it need not be. One example of a point to consider is \"the point at infinity\", i.e. we could look at all functions which vanish at infinity, i.e. which on the complement of larger and larger compact sets, grow smaller and smaller.  This is a natural condition to impose in many analytic contexts, and so gives a natural example.   (The reason that this kind of growth condition is natural in analysis is that, on a non-compact space, e.g. the real line, a random continuous function may not be integrable (just as an example), and imposing some decay at infinity (perhaps of the kind I specified, or perhaps something more quantitive) becomes a way to rescue the situation.)  (Note also that the example that Tomer Vromen gives is exactly of this form.) Finally, note that if your semigroup doesn\\'t have an identity, then you can always formally adjoin one, just by throwing in an extra element e and declaring that ex = x for all x. One can do a similar thing for rings without identity.  If A is a C-algebra (say) without an identity, then one can form A + C e  (the direct sum), and declare that e acts as a multiplicative identity.  This is a frequently-used technique in the theory of rings-without-identity. P.S.  I don\\'t know much literature about semigroups without identity, but for rings without identity, the best literature I know of is in functional analysis books; e.g. Naimark\\'s classic Normed Rings often treats the case of Banach algebras (and the like) without identity in addition to the case when they do have identity, exactly so as to be able to handle examples such as the ring of continuous functions on a locally compact space that vanish at infinity. '}\n",
      "1323 {'Id': '1323', 'Type': 'question', 'Title': 'Is there an atlas of Algebraic Groups and corresponding Coordinate rings?', 'Tags': ['reference-request', 'algebraic-geometry', 'algebraic-groups'], 'urls': [], 'exp': ['G', 'B', 'G/B', 'R', 'Proj(R)'], 'Body': 'I was wondering if there was a resource that listed known algebraic groups and their corresponding coordinate rings. Edit: The previous wording was terrible. Given an algebraic group <math_exp>, with Borel subgroup <math_exp> we can form the Flag Variety <math_exp> which is projective. I am hoping for a list of the graded ring <math_exp> such that <math_exp> corresponds to this Flag Variety. '}\n",
      "1324 {'Id': '1324', 'Type': 'question', 'Title': 'What is a primitive polynomial?', 'Tags': ['polynomials', 'field-theory'], 'AcceptedAnswerId': '1325', 'urls': [], 'exp': [], 'Body': \"What is a primitive polynomial? I was looking into some random number generation algorithms and 'primitive polynomial' came up a sufficient number of times that I decided to look into it in more detail. I'm unsure of what a primitive polynomial is, and why it is useful for these random number generators. I'd find it particularly helpful if an example of a primitive polynomial could be provided. \"}\n",
      "1325 {'Id': '1325', 'Type': 'answer', 'ParentId': '1324', 'urls': ['http://en.wikipedia.org/wiki/Primitive_polynomial'], 'exp': ['F_p', 'm', 'm'], 'Body': \"Consider a finite field <math_exp>, then we know that it is cyclic. We call an element primitive if it generates this field. Further, given a field and some polynomial over that field(all the coefficients are in the field), we can form a field extension by any of its roots. This is adjoining on that root and making a field of it. It is a simple result of Galois Theory that if we take a field and extend by some root of some polynomial and get a finite extension(one who's degree as a vector space over the original field is finite), that we can find a polynomial <math_exp> over our ground field such that <math_exp> vanishes at this root and is minimal(smallest degree, i.e. it divides all other polys which vanish at this root). If we consider a primitive element and its minimal polynomial, that poly is call primitive. more details on wiki \"}\n",
      "1326 {'Id': '1326', 'Type': 'question', 'Title': \"Is a curve's curvature invariant under rotation and uniform scaling?\", 'Tags': ['geometry', 'differential-geometry'], 'AcceptedAnswerId': '1328', 'urls': [], 'exp': [], 'Body': \"The title really say's it all, but once again is a curve's curvature invariant under rotation and uniform scaling? \"}\n",
      "1327 {'Id': '1327', 'Type': 'answer', 'ParentId': '1324', 'urls': [], 'exp': [], 'Body': \"BBischof's answer is correct, but unfortunately there's another, quite different possible meaning of the same term: that is a polynomial whose coefficients have no common prime factor (this makes sense over the integers, or other UFDs as well). \"}\n",
      "1328 {'Id': '1328', 'Type': 'answer', 'ParentId': '1326', 'urls': [], 'exp': ['\\\\gamma(s)', \"\\\\kappa(s) = ||\\\\gamma''(s)||\", \"\\\\gamma''(s)\"], 'Body': \"A curve's curvature is invariant under rotation. Intuitively, a curve turns just as much no matter how it is oriented. More formally, for a curve <math_exp> that is parametrized by arc length, the curvature is <math_exp>. Rotation does not change the length of the <math_exp> vector, only the direction; therefore, rotation does not affect curvature. A curve's curvature is not invariant under uniform scaling, however. Consider the example of a circle. All circles are the same up to scaling, but they don't all have the same curvature; in general, a circle of radius r has curvature 1/r. \"}\n",
      "1329 {'Id': '1329', 'Type': 'question', 'Title': 'What is the form of curvature that is invariant under rotations and uniform scaling', 'Tags': ['geometry', 'differential-geometry'], 'AcceptedAnswerId': '1332', 'urls': ['https://math.stackexchange.com/questions/1326/is-a-curves-curvature-invariant-under-rotation-and-uniform-scaling', 'http://en.wikipedia.org/wiki/Affine_curvature#Affine_curvature'], 'exp': [], 'Body': 'This is a followup to this question, where I learned that curvature is invariant to rotations. I have learned of a version of curvature that is invariant under affine transformations. I am wondering if there a is a form of curvature between the two. Invariant under uniform scaling and rotation but not all affine transformations? '}\n",
      "1332 {'Id': '1332', 'Type': 'answer', 'ParentId': '1329', 'urls': [], 'exp': [], 'Body': \"I don't know if this would suit you, but one thing you can consider (much more naive than the notion of affine curvature) is to fix a point P_0 on your curve, and then consider the function on the curve given by sending a point P to the quantity curvature(P)/curvature(P_0)  . This is a kind of relative curvature, where you measure how much everything is curving in comparison to the curvature at P_0, and is invariant under scaling and rotation. \"}\n",
      "1333 {'Id': '1333', 'Type': 'question', 'Title': 'Is there a possibility to choose fairly from three items when every choice can only have 2 options', 'Tags': ['probability-theory', 'algorithms'], 'AcceptedAnswerId': '1335', 'urls': [], 'exp': [], 'Body': \"Me and my wife are often not knowing which DVD to watch. If we have two options we have a simple solution, I put one DVD in one hand behind my back and the other DVD in the other hand. She will randomly choose a hand and the DVD I have in that hand will be the one to watch. This procedure is easy to expand to any power of 2. If we have 4 DVD's I hold 2 in one hand, 2 in the other. When a pair of DVD's is chosen, I split them out to two hands and she choses again. The question is, what can we do when we have  3 DVD's. The assumptions we make are: As requirement we have that it must be a procedure with a predetermined number of steps. Not more, not less. If this is not possible, a solution that guarantees to finish with an upperbound, this is a good second choice. Off course the DVD we choose must be really random! \"}\n",
      "1334 {'Id': '1334', 'Type': 'answer', 'ParentId': '1333', 'urls': [], 'exp': ['N', '1', '2', 'N', '\\\\frac{x}{2^N}', '1', '\\\\frac{x_1}{2_N}', '\\\\frac{1}{3}', '\\\\frac{x_1}{2_N}', '\\\\frac{1}{3}'], 'Body': 'There is no such procedure which has an upperbound on number of steps. Here is proof. Let there is such procedure with no more then <math_exp> steps. On each step you basicly generate random integer between <math_exp> and <math_exp>. Consider all possible sequences of no more than <math_exp> generated numbers. Every such sequence has probability of form <math_exp>. Consider sequences for which the procedure says \"1\" (the result is equal to <math_exp>). The sum of theirs probabilities is <math_exp>. It also must be equal to <math_exp>. But <math_exp> can\\'t be equal to <math_exp>. '}\n",
      "1335 {'Id': '1335', 'Type': 'answer', 'ParentId': '1333', 'urls': ['http://en.wikipedia.org/wiki/Geometric_distribution', 'https://stackoverflow.com/questions/137783/given-a-function-which-produces-a-random-integer-in-the-range-1-to-5-write-a-fun'], 'exp': [], 'Body': 'Nice question. Firstly, there\\'s no solution which will always take a fixed number of steps each time: after n independent random trials (choosing a hand), there are 2n possible choices, and 2n is never divisible by 3. (Equivalently: 1/3 does not have a terminating representation in base 2.) But you can come up with solutions that terminate within a fixed number of steps with high probability. Note that you need at least 2 trials, since log2(3) > 1. Here\\'s one simple method: Hold 2 DVDs in one hand and 1 DVD in other (your wife doesn\\'t know which is which). If the hand with 2 DVDs is chosen: play them against each other. Else: If the 1 DVD is chosen: play it against an empty hand. Each of the 3 DVDs has equal probability 1/4 of being chosen, and with probability 1/4, you have to repeat from scratch. This solution takes 2 trials with probabilty 3/4, 4 trials with probability (1/4)(3/4), 6 trials with probability (1/4)2(3/4) and so on, so takes an expected number of 8/3 (≈ 2.66) trials. (See geometric distribution.) I believe this is optimal (famous last words?) if you just have to choose out of 3 DVDs once. Probably, if you\\'re going to choose from 3 DVDs many times, you can do better in the long run (amortised) and achieve the lower bound of log2 3 ≈ 1.58 trials on average, by \"caching\" some random choices from last time. (But will you remember them? :-)) At least you can do something similar when generating random numbers (see  also this mildly related Stack Overflow thread), but in this case with game-theoretic fairness complications I\\'m not so sure. '}\n",
      "1336 {'Id': '1336', 'Type': 'answer', 'ParentId': '803', 'urls': ['http://www.cut-the-knot.org/pythagoras/', 'http://www.cut-the-knot.org/pythagoras/'], 'exp': [], 'Body': 'My favorite proof of the Pythagorean Theorem is a special case of this picture-proof of the Law of Cosines: <img src=\"https://i.imgur.com/w7kGHKK.jpg\" alt=\"Proof Without Words: The Law of Cosines\"> Drop three perpendiculars and let the definition of cosine give the lengths of the sub-divided segments. Then, observe that like-colored rectangles have the same area (computed in slightly different ways) and the result follows immediately. When C is a right angle, the blue rectangles vanish and we have the Pythagorean Theorem via what amounts to Proof #5 on Cut-the-Knot\\'s Pythagorean Theorem page. (Note that, as mentioned on CtK, the use of cosine here doesn\\'t amount to an invalid \"trigonometric proof\".) The picture works for obtuse C as well. (Proof left as an exercise for the reader.) A final note ... Because the same-colored rectangles have the same area, they\\'re \"equidecomposable\" (aka \"scissors congruent\"): it\\'s possible to cut one into a finite number of polygonal pieces that reassemble to make the other. While there\\'s at least one standard procedure for determining how to make the cuts, the resulting pieces aren\\'t necessarily pretty. Some popular dissection proofs of the Pythagorean Theorem --such as Proof #36 on Cut-the-Knot-- demonstrate a specific, clear pattern for cutting up the figure\\'s three squares, a pattern that applies to all right triangles. I have yet to find a similarly straightforward cutting pattern that would apply to all triangles and show that my same-colored rectangles \"obviously\" have the same area. (Another exercise for the reader, perhaps? :) '}\n",
      "1337 {'Id': '1337', 'Type': 'question', 'Title': 'Dot product in coordinates', 'Tags': ['geometry', 'linear-algebra'], 'AcceptedAnswerId': '1339', 'urls': [], 'exp': ['(x_1, y_1)', '(x_2, y_2)', 'x_1x_2 + y_1y_2'], 'Body': 'Dot product of two vectors on plane could be defined as product of lengths of those vectors and cosine of angle between them. In cartesian coordinates dot product of vectors with coordinates <math_exp> and <math_exp> is equal to <math_exp>. How to prove it? '}\n",
      "1338 {'Id': '1338', 'Type': 'question', 'Title': 'Compute the Centroid of a <span class=\"math-container\" id=\"11117\">3D</span> Planar Polygon Without Projecting It To Specific Planes', 'Tags': ['geometry'], 'AcceptedAnswerId': '1345', 'urls': ['http://en.wikipedia.org/wiki/Centroid#Centroid_of_polygon'], 'exp': ['\\\\left(pt_1, pt_2, pt_3, \\\\cdots \\\\right)', 'XY', 'YZ'], 'Body': \"Given a list of coordinates of a coplanar plane <math_exp>, how to compute the centroid of the coplanar plane? One way to do it is to project the plane onto <math_exp> and <math_exp> plane, but I don't really favor this approach as you have to check the orientation of the coplanar plane first before doing the projection and computing the centroid. More specifically, I'm looking for a natural extension of the 2D centroid plane algorithm in 3D: \\\\begin{align} C_x&amp;=\\\\frac1{6A}\\\\sum_{i=0}^{n-1}(x_i+x_{i+1})(x_iy_{i+1}-x_{i+1}y_i)\\\\\\\\ C_y&amp;=\\\\frac1{6A}\\\\sum_{i=0}^{n-1}(y_i+y_{i+1})(x_iy_{i+1}-x_{i+1}y_i)\\\\\\\\ A&amp;=\\\\frac12\\\\sum_{i=0}^{n-1}(x_iy_{i+1}-x_{i+1}y_i) \\\\end{align} Any idea? \"}\n",
      "1339 {'Id': '1339', 'Type': 'answer', 'ParentId': '1337', 'urls': [], 'exp': ['(\\\\vec{u}, \\\\vec{v}) = |\\\\vec{u}| |\\\\vec{v}| \\\\cos \\\\theta', '(\\\\vec{u}, \\\\vec{v}) = |\\\\vec{u}| |\\\\vec{v}| \\\\cos \\\\theta', '(\\\\vec{u}, \\\\vec{v}) = x_1 x_2 + y_1 y_2', '(\\\\vec{v_1}, \\\\vec{v_2} + \\\\alpha \\\\vec{v_3}) = (\\\\vec{v_1}, \\\\vec{v_2}) + \\\\alpha (\\\\vec{v_1}, \\\\vec{v_3})', '(\\\\vec{v_1}, \\\\vec{v_2})', '|\\\\vec{v_1}|', '\\\\vec{v_2}', '\\\\vec{v_1}', '\\\\vec{e_1}', '\\\\vec{e_2}', '(1, 0)', '(0, 1)', '\\\\vec{v_1} = x_1 \\\\vec{e_1} + y_1 \\\\vec{e_2}', '\\\\vec{v_2} = x_2 \\\\vec{e_2} + y_2 \\\\vec{e_2}', '(\\\\vec{v_1}, \\\\vec{v_2}) = x_1 x_2 (\\\\vec{e_1}, \\\\vec{e_1}) + x_1 y_2 (\\\\vec{e_1}, \\\\vec{e_2}) + x_2 y_1 (\\\\vec{e_2}, \\\\vec{e_1}) + x_2 y_2 (\\\\vec{e_2}, \\\\vec{e_2})', '(\\\\vec{e_1}, \\\\vec{e_1}) = (\\\\vec{e_2}, \\\\vec{e_2}) = 1', '(\\\\vec{e_1}, \\\\vec{e_2}) = (\\\\vec{e_2}, \\\\vec{e_1}) = 0', '(\\\\vec{v_1}, \\\\vec{v_2}) = x_1 x_2 + y_1 y_2'], 'Body': 'I suppose you want to prove that two your definitions of dot product are the same. We start with definition of dot product as <math_exp>. We start with definition of dot product as <math_exp> and prove that it also satisfies <math_exp>. At first you can prove that dot product is linear: <math_exp>. This is true because <math_exp> is equal to the product of <math_exp> and projection of <math_exp> on <math_exp>. Projection of sum of vectors is equal to sum of projections. Hence dot product is linear. Let <math_exp> and <math_exp> be vectors with coordinates <math_exp> and <math_exp>. After that if <math_exp> and <math_exp> then by linearity of dot product we have <math_exp>. Since <math_exp> and <math_exp> we have <math_exp>. '}\n",
      "1340 {'Id': '1340', 'Type': 'question', 'Title': 'In a graph, is it always possible to construct a set of cycle basis, with each and every edge Is shared by at most 2 cycle bases?', 'Tags': ['graph-theory'], 'AcceptedAnswerId': '1343', 'urls': ['http://www.mathreference.com/gph,basis.html', 'https://math.stackexchange.com/questions/1340/in-a-graph-is-it-always-possible-to-construct-a-set-of-cycle-basis-with-each-an/1343#1343'], 'exp': ['(E,V)', '2'], 'Body': \"Let's say we have a graph, with a list of edges and vertexes <math_exp>, all the vertexes are connected to at least one edge at one end. There are many ways a complete set of cycle basis can be found out from it. Now the issue is, is it always possible to find a complete set of cycle basis that each edge is shared by at most <math_exp> cycles? Edit: There is a mathematical argument proving why it is not possible. But admittedly such a highly abstract reasoning is a bit hard for me to grasp. I would appreciate if someone can provide a graphical example of such a graph. \"}\n",
      "1341 {'Id': '1341', 'Type': 'answer', 'ParentId': '1337', 'urls': [], 'exp': ['v = (|v|, 0)', 'w = (x,y)', 'v \\\\cdot w = |v| x', 'x', '\\\\theta', '\\\\cos \\\\theta = x/|w||', 'x = |w| \\\\cos \\\\theta', 'v\\\\cdot w = |v||w| \\\\cos \\\\theta'], 'Body': 'The dot product is invariant under rotations, we may therefore rotate our coordinate system so that v is along the x-axis.  In this case, <math_exp>.  Letting <math_exp> we have (using the definition of dot product in Cartesian coordinates) <math_exp>.  But what is <math_exp>?  Well, if you draw the picture and let <math_exp> be the angle between v and w, then we see that <math_exp> so that <math_exp>.  Thus <math_exp>. '}\n",
      "1342 {'Id': '1342', 'Type': 'answer', 'ParentId': '1292', 'urls': ['https://math.stackexchange.com/a/1382809/409'], 'exp': ['\\\\alpha', '\\\\beta', '\\\\alpha', '\\\\beta'], 'Body': 'Here are my favorite diagrams: <img src=\"https://daylateanddollarshort.com/misc/angaddsub.jpg\" alt=\"Proof Without Words: Angle Sum and Difference for Sine and Cosine\"> As given, the diagrams put certain restrictions on the angles involved: neither angle, nor their sum, can be larger than 90 degrees; and neither angle, nor their difference, can be negative. The diagrams can be adjusted, however, to push beyond these limits. (See, for instance, this answer.) Here\\'s a bonus mnemonic cheer (which probably isn\\'t as exciting to read as to hear): Sine, Cosine, Sign, Cosine, Sine!<br />   Cosine, Cosine, Co-Sign, Sine, Sine! The first line encapsulates the sine formulas; the second, cosine. Just drop the angles in (in order <math_exp>, <math_exp>, <math_exp>, <math_exp> in each line), and know that \"Sign\" means to use the same sign as in the compound argument (\"+\" for angle sum, \"-\" for angle difference), while \"Co-Sign\" means to use the opposite sign. '}\n",
      "1343 {'Id': '1343', 'Type': 'answer', 'ParentId': '1340', 'urls': ['https://mathoverflow.net/questions/30759/in-a-graph-is-it-always-possible-to-construct-a-set-of-cycle-bases-with-each-an', 'http://www.ams.org/journals/proc/1973-037-02/S0002-9939-1973-0313098-X/S0002-9939-1973-0313098-X.pdf'], 'exp': ['K_{3,3}', 'K_{5}', 'K_{3,3}', '\\\\cdot', 'K_{5}', '\\\\cdot'], 'Body': \"The question was answered in the negative on Math Overflow.  See https://mathoverflow.net/questions/30759/in-a-graph-is-it-always-possible-to-construct-a-set-of-cycle-bases-with-each-an Edit: We get a counterexample from any non-planar graph where every edge is part of at least one cycle. Here's a reference: P. V. O'Neil, Proc. AMS, 37 (2), Feb. 1973, 617-8 I'll repeat the argument here.  Take a nonplanar graph with every edge in at least one cycle. If that had a cycle basis like the one you want, then we could generate one for <math_exp> or <math_exp>. Suppose we had a basis for <math_exp>. There are 4 cycles in that basis. (A cycle basis has m-n+1 elements, where m is the number of vertices and n is the number of edges.) Also, take the binary sum of the four cycles. The five cycles include every edge exactly twice, so there are a total of 2 <math_exp> (number of edges) = 18 edges in those five cycles. But each cycle has at least 4 edges, so the five cycles must have at least 20 edges. Contradiction. Now let's look at <math_exp>. A cycle basis has 6 cycles. Also take the binary sum. The 7 cycles include each edge exactly twice, so there are 2 <math_exp> (number of edges) = 20 edges in the collection. But each cycle has at least three edges, so the 7 cycles have at least 7*3=21 edges. Again, a contradiction. \"}\n",
      "1344 {'Id': '1344', 'Type': 'question', 'Title': 'Examples/other references for EGA 0.4.5.4', 'Tags': ['reference-request', 'algebraic-geometry'], 'AcceptedAnswerId': '1373', 'urls': [], 'exp': ['F', 'S', 'F_i', 'F', 'F_i \\\\to F', 'Hom(-, X) \\\\to F', 'F_i \\\\times_F Hom(-,X)', 'Hom(-,X_i)', 'X_i', 'X', 'X_i \\\\to X', 'U', 'X', 'U \\\\to F(U)', 'F'], 'Body': \"Proposition 0.4.5.4 in EGA appears to be a general representability theorem. It reads: Suppose <math_exp> is a contravariant functor from the category of locally ringed spaces over <math_exp> to the category of sets. Suppose given representable sub-functors <math_exp> of <math_exp>, such that the morphisms <math_exp> are representable by open immersions. Suppose furthermore that if <math_exp> is a morphism and the functors <math_exp> are representable by <math_exp>, the family <math_exp> forms an open covering of <math_exp>. (That <math_exp> is an open immersion follows from the definitions.)  Finally, suppose that if <math_exp> ranges over the open subsets of a locally ringed space <math_exp>, the functor <math_exp> is a sheaf.   Then, <math_exp> is representable. I haven't yet been able to grok the proof, but it appears to be some sort of extended gluing construction. This result appears to be used in proving that fibered products exist in the category of schemes. However, it's fairly easy to directly construct fibered products by gluing open affines. Are there examples where this result actually makes the life of algebraic geometers easier?  Also, I'd appreciate any links to examples (outside of EGA) where this result is used. \"}\n",
      "1345 {'Id': '1345', 'Type': 'answer', 'ParentId': '1338', 'urls': [], 'exp': ['\\\\vec{e_1}', '\\\\vec{e_2}', '(x_0, y_0, z_0)', '(x_1, y_1, z_1)', 'x = (x_1 - x_0) e_{1x} + (y_1 - y_0) e_{1y} + (z_1 - z_0) e_{1z}', 'y = (x_1 - x_0) e_{2x} + (y_1 - y_0) e_{2y} + (z_1 - z_0) e_{2z}', 'C_x', 'C_y', 'x = x_0 + e_{1x} C_x + e_{2x} C_y', 'y = y_0 + e_{1y} C_x + e_{2y} C_y', 'z = z_0 + e_{1z} C_x + e_{2z} C_y'], 'Body': \"You can take any two orthogonal vectors <math_exp> and <math_exp> on the plane and use them as a basis. You also need some point <math_exp> on the plane as origin. Given point with coordinates <math_exp> on your plane you calculate it's coordinates with respect to new basis: <math_exp><br /> <math_exp><br /> And after that you can apply your formulae to get <math_exp> and <math_exp>. Those coordinates are easyly transformed back into original 3d coordinates:<br /> <math_exp><br /> <math_exp><br /> <math_exp><br /> \"}\n",
      "1346 {'Id': '1346', 'Type': 'answer', 'ParentId': '164', 'urls': [], 'exp': ['a', '\\\\displaystyle V = \\\\iiint_{\\\\ S}\\\\mathrm dx\\\\,\\\\mathrm dy\\\\,\\\\mathrm dz,', 'S', 'x^2+y^2+z^2=a^2', '\\\\displaystyle V = \\\\int_{0}^{2\\\\pi}\\\\mathrm d\\\\phi\\\\int_{0}^{\\\\ \\\\pi}\\\\mathrm d\\\\theta\\\\int_{0}^{\\\\ a}r^2\\\\sin\\\\theta \\\\mathrm dr = \\\\int_{0}^{2\\\\pi}\\\\mathrm d\\\\phi\\\\int_{0}^{\\\\pi}\\\\sin\\\\theta \\\\mathrm d\\\\theta\\\\int_{0}^{\\\\ a}r^2\\\\mathrm dr = \\\\frac{4\\\\pi a^3}{3},'], 'Body': 'The volume of a sphere with radius <math_exp> may be found by evaluating the triple integral <math_exp> where <math_exp> is the volume enclosed by the sphere <math_exp>. Changing variables to spherical polar coordinates, we obtain <math_exp> as expected. '}\n",
      "1347 {'Id': '1347', 'Type': 'question', 'Title': 'Hyperbolic critters studying Euclidean geometry', 'Tags': ['geometry', 'euclidean-geometry', 'hyperbolic-geometry'], 'AcceptedAnswerId': '1383', 'urls': [], 'exp': ['L', 'p', 'L', 'p', 'L', 'L', 'p', 'L', 'p', 'L'], 'Body': \"You've spent your whole life in the hyperbolic plane. It's second nature to you that the area of a triangle depends only on its angles, and it seems absurd to suggest that it could ever be otherwise. But recently a good friend named Euclid has raised doubts about the fifth postulate of Poincaré's Elements. This postulate is the obvious statement that given a line <math_exp> and a point <math_exp> not on <math_exp> there are at least two lines through <math_exp> that do not meet <math_exp>. Your friend wonders what it would be like if this assertion were replaced with the following: given a line <math_exp> and a point <math_exp> not on <math_exp>, there is exactly one line through <math_exp> that does not meet <math_exp>. You begin investigating this Euclidean geometry, but you find it utterly impossible to visualize intrinsically. You decide your only hope is to find a model of this geometry within your familiar hyperbolic plane. What model do you build? I do not know if there's a satisfying answer to this question, but maybe it's entertaining to try to imagine. For clarity, we Euclidean creatures have built models like the upper-half plane model or the unit-disc model to visualize hyperbolic geometry within a Euclidean domain. I'm wondering what the reverse would be. \"}\n",
      "1348 {'Id': '1348', 'Type': 'question', 'Title': 'How to explain fractals to a layperson and to someone with more math training?', 'Tags': ['fractals'], 'AcceptedAnswerId': '1351', 'urls': ['http://forthescience.org/blog/2010/07/12/the-mandelbrot-set-in-python/'], 'exp': [], 'Body': \"I have a Ph.D. in computational and theoretical chemistry with advanced but field-oriented knowledge of mathematics. I am fascinated by fractals, but I am unable to understand them from the formal point of view. To my level of understanding, they look like a graphical rendering of an ill-conditioned iterative problem, where small variations of the initial condition lead to huge changes in the final result, but that's just what I got out of it with my current knowledge. How would you explain fractals (such as the Mandelbrot set) to a layperson with basic mathematics knowledge from high school, and how would you instead explain it to someone which has more math training, but not formal. This question is collateral to a post on the Mandelbrot set I did on my blog some time ago. If you have any comments on what I was doing with my tinkering of the parameters (to get some keywords for further exploration), it's greatly appreciated. I would like to explain it better to my readers, but I am unable to do it. Thanks \"}\n",
      "1349 {'Id': '1349', 'Type': 'answer', 'ParentId': '1348', 'urls': ['http://en.wikipedia.org/wiki/Smale_horseshoe'], 'exp': [], 'Body': 'There are a lot of examples that are not hard to define rigorously, e.g. the Cantor set. This can be defined rigorously simply by taking a suitable intersection of nested intervals when the middle thirds are successively removed. It is easy to make all sorts of variants, constructing \"Cantor-like\" sets when you replace \"middle third\" with \"middle fourth\" or even with a construction where the ratio of the interval removed changes with the iteration. (This is important because you can then get a set of positive measure.) Leaving aside the remark about measure, all this uses nothing more than elementary mathematics. One way to think of fractals is as of subsets of the phase space of certain dynamical systems. For instance, consider the Smale horseshoe. This is a two-dimensional map that can be visualized geometrically; this is explained in the Wikipedia article. Then the collection of points that stay in the square when you iterate this procedure both forwards and backwards is a fractal set, basically a two-dimensional Cantor set, which is very interesting from the perspective of dynamical systems (it\\'s chaotic---one consequence of this is that knowing a point to arbitrary precision will not tell you what its orbit looks like later on with much precision).  So much of the interesting behavior of dynamical systems is on fractals. You can define a map of the interval into itself such that the points in the interval that stay in the interval no matter how many times you iterate the map is just the ordinary Cantor set, as well. '}\n",
      "1351 {'Id': '1351', 'Type': 'answer', 'ParentId': '1348', 'urls': ['http://en.wikipedia.org/wiki/Koch_snowflake', 'http://en.wikipedia.org/wiki/Barnsley%27s_fern', 'http://en.wikipedia.org/wiki/Ricker_model', 'http://www.phaser.com/modules/ecology/ricker/index.html', 'https://i.stack.imgur.com/vlvEt.jpg', 'http://www.phaser.com/modules/ecology/ricker/bif.jpg'], 'exp': [], 'Body': 'For a \"high-level\" explanation, I would say this: Fractals are surprisingly complex patterns that result from the repeated application of relatively simple operations/rules. One of the easiest to visualize examples is the Koch snowflake, constructed by adding smaller triangles to each face of the figure at each iteration: <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/KochFlake.svg/280px-KochFlake.svg.png\" alt=\"Koch snowflake\"> A more real-world example is the fern leaf. The DNA in a single plant cell encodes enough information to describe the structure of an entire leaf (and the entire plant, for that matter) without explicitly describing the location of each cell. Instead, the cells grow according to a set of simple rules that result in the self-similar appearance of the fern, even at smaller and smaller levels: <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/76/Barnsley_fern_plotted_with_VisSim.PNG\" alt=\"Barnsley fern\"> For a more complex mathematical explanation that still remains tied to the real world, have a look at the basic Ricker model of population growth and the resulting bifurcation diagrams:  (source: phaser.com) The x-axis on this graph is population growth rate and the y-axis is population density. Although it looks complex, All it takes is a handful of iterations of the basic formula on a hand calculator to see how the results can oscillate between seemingly random population levels. '}\n",
      "1352 {'Id': '1352', 'Type': 'question', 'Title': 'Surprising Generalizations', 'Tags': ['soft-question', 'big-list', 'intuition'], 'urls': ['https://mathoverflow.net/questions/10014/applications-of-the-chinese-remainder-theorem/10017#10017', 'http://www.artofproblemsolving.com/Forum/blog.php?b=10595'], 'exp': [], 'Body': \"I just learned (thanks to Harry Gindi's answer on MO and to Qiaochu Yuan's blog post on AoPS) that the chinese remainder theorem and Lagrange interpolation are really just two instances of the same thing.  Similarly the method of partial fractions can be applied to rationals rather than polynomials. I find that seeing a method applied in different contexts, or just learning a connection that wasn't apparent helps me appreciate a deeper understanding of both. So I ask, can you help me find more examples of this? Especially ones which you personally found inspiring. \"}\n",
      "1353 {'Id': '1353', 'Type': 'answer', 'ParentId': '1352', 'urls': ['http://ncatlab.org/nlab/show/localization'], 'exp': [], 'Body': 'Localization When I learned that you could localize categories(and not just abelian!) I was floored. The general idea that we take a class of morphisms in a category and send them functorially to another category where they are isos is awesome. It is also very important in my work, which is generalizing some ideas of Algebraic Geometry to a more categorical setting. Here is a link! '}\n",
      "1354 {'Id': '1354', 'Type': 'answer', 'ParentId': '1352', 'urls': ['http://en.wikipedia.org/wiki/Galois_connection#Algebraic_topology%3a_covering_spaces'], 'exp': [], 'Body': 'Galois Connections Let\\'s be honest, the correspondence between Galois groups and field extension is pretty hott. The first time I saw this I was duly impressed. However, about two years ago, I learned about universal covering spaces. Wow! I swear my understanding of covering spaces doubled when the prof told me that this was a \"Galois correspondence for fundamental groups and covering spaces\". Again here is a link! '}\n",
      "1355 {'Id': '1355', 'Type': 'answer', 'ParentId': '1352', 'urls': ['http://en.wikipedia.org/wiki/Structure_theorem_for_finitely_generated_modules_over_a_principal_ideal_domain'], 'exp': [], 'Body': 'Classification of finitely-generated abelian groups and Jordan normal form are two instances of the structure theorem for finitely generated modules over a principal ideal domain. '}\n",
      "1356 {'Id': '1356', 'Type': 'answer', 'ParentId': '1352', 'urls': [], 'exp': [], 'Body': 'Model categories as a framework for both complexes of R-modules and topological spaces (making precise, for example, analogy between taking projective resolution and replacing a space with weakly homotopy equivalent CW-complex). '}\n",
      "1357 {'Id': '1357', 'Type': 'answer', 'ParentId': '1333', 'urls': ['http://www.prismmodelchecker.org/bibitem.php?key=KY76', 'http://www.youtube.com/watch?v=3JBE5ykC1No#t=1m'], 'exp': [], 'Body': 'In 1976, Knuth and Yao proved a result that may help you choose a DVD out of 5, 6, etc.  (I couldn\\'t find a good reference online.) Consider the following problem: You have a fair coin and you must write an algorithm that outputs 1 with probability p1, outputs 2 with probability p2, ..., and outputs n with probability pn. (p1+p2+...+pn=1) Note that any possible algorithm can be described in terms of certain (possibly infinite) binary trees: A node that has two children means \"throw a coin and pick one of the children according to the result\"; a leaf is tagged with one of the numbers 1, 2, ..., n and means \"output that number\". An optimal tree that solves the problem has one leaf k on level m if and only if the m-th bit of pk is 1. Otherwise there are zero such leafs. \"Optimal\" here means not only that the average runtime is as good as possible, but the  stronger guarantee that the probability of running more than m steps is not worse than that of any other algorithm that solves the problem correctly. In your case, p1=p2=p3=0.01010101... The digit just before dot corresponds to the root (level 0) and and the subsequent digits to subsequent levels. So a tree that has no leaf on levels 0 and 1, has leafs (1, 2, and 3) on level 2, has no leaf on level 3, ... is optimal. This is exactly the solution given by ShreevatsaR. Such a tree is fairly easy to find once you know what leafs you need to put on each level. As another example, if p1=...=p6=1/6=0.00101010101..., then you should have leafs (one of each 1, 2, 3, 4, 5, 6) on levels 3, 5, 7, and so on. If you draw a symmetric tree that obeys this you\\'ll find how to choose one DVD out of 6: First grab three with each hand, then do what ShreevatsaR said for three.  You are also guaranteed that it\\'s optimal. Now you can mock me and my friends for using a suboptimal algorithm for allocating hotel rooms. '}\n",
      "1358 {'Id': '1358', 'Type': 'answer', 'ParentId': '457', 'urls': ['http://mathworld.wolfram.com/PeaucellierInversor.html'], 'exp': [], 'Body': 'Some principles of inversive geometry can be demonstrated using Peaucellier\\'s Inversor, a linkage device designed to transform circular motion into motion in a straight line and vice versa. There are various online demonstrations however, you may wish to build a \"real\" one for yourself. The book \"Mathematical Models\" by Cundy and Rollett has an entire chapter devoted to making mechanical models, including the above and related likages. '}\n",
      "1359 {'Id': '1359', 'Type': 'answer', 'ParentId': '457', 'urls': [], 'exp': ['P'], 'Body': \"I'm not sure how good of an example this is since the original problem already has a natural physical interpretation. Nonetheless this is an example of something that can be proved using Euclidean geometry or mechanics. Starting on one of the edges of a plane polygon <math_exp> one can set into motion a point-mass which moves along a straight trajectory except on collision with another edge, when it is subjected to an elastic collision response (i.e., its velocity vector is reflected in the edge it has collided with). The trajectory traced out by such a point-mass is its orbit, and an orbit is periodic if the point-mass eventually returns to its starting spot with its starting velocity. One of the most basic questions one can ask is whether periodic orbits exist in acute triangles. The answer is yes, and a particularly nice periodic orbit in an acute triangle can be constructed by drawing the three altitudes and connecting their bases. This can be proved using geometry, but it can also be seen in the following physical manner. Assume the triangle's edges are thin wires. Place around each wire a small ring which is free to move along its edge. Now thread a rubber band through the three rings. Tighten this rubber band. This system reaches equilibrium when the rings occupy the positions of the bases of the altitudes. In particular, in the equilibrium state the triangle whose vertices are the rings will be the inscribed triangle of minimal perimeter. There's no such minimum in an obtuse triangle, so the construction doesn't work in that case. As of today it is unknown whether every obtuse triangle admits a periodic orbit, although much partial progress has been achieved. \"}\n",
      "1361 {'Id': '1361', 'Type': 'answer', 'ParentId': '1347', 'urls': ['http://en.wikipedia.org/wiki/horoball'], 'exp': [], 'Body': 'Look up \"horosphere\" (for example, in page 90 of the Princeton Companion to Mathematics).  Wikipedia describes it on its Horoball page. '}\n",
      "1362 {'Id': '1362', 'Type': 'answer', 'ParentId': '1348', 'urls': [], 'exp': [], 'Body': 'That\\'s easy.  I would buy them some of this stuff: <img src=\"https://farm2.static.flickr.com/1340/614003477_2d7fd27ca5.jpg\"> (Then I would tell them to stare at it for awhile.)  It\\'s called Romanesco broccoli, and it grows like this for the same reasons, as explained in e.James\\' answer, that ferns do. Alternately, I would tell them to check out Indra\\'s Pearls by Mumford, Series, and Wright. '}\n",
      "1363 {'Id': '1363', 'Type': 'answer', 'ParentId': '1352', 'urls': ['http://en.wikipedia.org/wiki/Adjacency_matrix', 'http://en.wikipedia.org/wiki/Spectral_graph_theory', 'http://en.wikipedia.org/wiki/Matrix_mechanics'], 'exp': [], 'Body': \"I agree!  I spend much of my mathematical free time exploring such connections. Here is a basic one that I constantly ponder.  The rules of matrix multiplication encode two things: This means that one can study walks on graphs by studying how a matrix called the adjacency matrix behaves.  This leads into all sorts of beautiful mathematics; for example, this is the basic tool behind Google's PageRank algorithm, and it also in some sense motivated Heisenberg's matrix mechanics formulation of quantum mechanics.   I often try to recast results in linear algebra in terms of some combinatorial statement about walks on graphs. \"}\n",
      "1364 {'Id': '1364', 'Type': 'question', 'Title': 'The zero component of tensor products of graded mods', 'Tags': ['reference-request', 'graded-modules'], 'AcceptedAnswerId': '1365', 'urls': [], 'exp': ['R_{\\\\bullet}', 'R_{\\\\bullet}^{*}', 'R_{\\\\bullet}', 'M_{\\\\bullet}', '\\\\left(R_{\\\\bullet}^{*}\\\\otimes M_{\\\\bullet}\\\\right)_0=\\\\left(R_{\\\\bullet}^{*}\\\\right)_0\\\\otimes \\\\left(M_{\\\\bullet}\\\\right)_0'], 'Body': 'I guarantee there is an easy reference on this, but for some reason I cannot find it. If you can point me to a reference or just write a short proof for me, I would be very appreciative. Given a graded ring <math_exp> and a localization <math_exp>. We also have a graded <math_exp>-mod, <math_exp>. So what I want to know; is <math_exp>? '}\n",
      "1365 {'Id': '1365', 'Type': 'answer', 'ParentId': '1364', 'urls': [], 'exp': ['R=k[t]', 'M=R(1)', '1'], 'Body': 'For a counterexample, take <math_exp> with its usual grading and <math_exp>, the free module of rank one generated in degree <math_exp>. '}\n",
      "1366 {'Id': '1366', 'Type': 'answer', 'ParentId': '1352', 'urls': ['http://www.johndcook.com/blog/2010/06/17/covariance-and-law-of-cosines/'], 'exp': [], 'Body': 'The law of cosines and the equation for the variance of a sum of (possibly correlated) random variables are both consequences of basic inner product space properties. Details here. '}\n",
      "1367 {'Id': '1367', 'Type': 'answer', 'ParentId': '1352', 'urls': [], 'exp': ['\\\\int_{\\\\partial R} \\\\phi = \\\\int_R d\\\\phi'], 'Body': \"I loved learning about how differential forms and the exterior derivative generalize 3-d vector calculus (div, grad, curl).  Differential forms are so elegant in comparison, work in arbitrary dimensions, and give rise to beautiful mathematics (e.g. de Rham cohomology, Hodge theory).  And of course, the generalized Stoke's theorem is one of the prettiest equations: <math_exp>. \"}\n",
      "1368 {'Id': '1368', 'Type': 'question', 'Title': 'Prove: <span class=\"math-container\" id=\"11281\">(a + b)^{n} \\\\geq a^{n} + b^{n}</span>', 'Tags': ['algebra-precalculus'], 'AcceptedAnswerId': '1370', 'urls': [], 'exp': ['n: (a + b)^n \\\\geq a^n + b^n', 'a, b &gt; 0:', '3', '1.', '2.'], 'Body': \"Struggling with yet another proof: Prove that, for any positive integer <math_exp> for all <math_exp> I wasted <math_exp> pages of notebook paper on this problem, and I'm getting nowhere slowly. So I need some hints. <math_exp> What technique would you use to prove this (e.g. induction, direct, counter example) <math_exp> Are there any tricks to the proof? I've seen some crazy stuff pulled out of nowhere when it comes to proofs... \"}\n",
      "1369 {'Id': '1369', 'Type': 'answer', 'ParentId': '1368', 'urls': ['http://en.wikipedia.org/wiki/Binomial_theorem'], 'exp': ['n-1', '(a+b)^n = (a+b)(a+b)^{n-1} \\\\geq (a+b)(a^{n-1} + b^{n-1})', '(a+b)^n \\\\geq  a(a^{n-1}+ b^{n-1}) + b(b^{n-1} + a^{n-1})', 'a^n + b^n'], 'Body': 'This follows directly from the binomial theorem. Alternatively, you can prove it inductively (which is probably more fun): suppose the inequality true for <math_exp>. Then <math_exp> by the inductive hypothesis. So <math_exp>, and this is at least <math_exp>. '}\n",
      "1370 {'Id': '1370', 'Type': 'answer', 'ParentId': '1368', 'urls': [], 'exp': ['(a + b)^n = \\\\sum \\\\limits_{k = 0}^n {n \\\\choose k} a^{n-k} b^k = a^n + b^n + \\\\sum \\\\limits_{k=1}^{n-1} {n \\\\choose k} a^{n-k} b^k'], 'Body': 'Hint: Use the binomial theorem. This states that  <math_exp>. Now, note that every term in the second sum is positive; this is because a, b, and the binomial coefficients are all positive. Therefore, (a+b)n = an + bn + (sum of positive terms) >= an + bn. '}\n",
      "1372 {'Id': '1372', 'Type': 'answer', 'ParentId': '1368', 'urls': [], 'exp': ['n=2', 'a \\\\times a', 'b \\\\times b', '(a+b) \\\\times (a+b)', 'n=3', 'a \\\\times a \\\\times a', 'b \\\\times b \\\\times b', '(a+b) \\\\times (a+b) \\\\times (a+b)', 'n', 'a+b', 'a', 'b'], 'Body': 'It might also be helpful for you to think a little about the geometry of the inequality. For <math_exp>, find a way to put an <math_exp> square and a <math_exp> square into a <math_exp> square without any  overlaps. For <math_exp>, see if you can fit an <math_exp> cube and a <math_exp> cube within an <math_exp> cube without overlaps. Next, the notion of having more than three dimensions might seem a little weird, but  think of the box in <math_exp> dimensions whose sides have length <math_exp>. Can you fit two boxes within it, one with side length <math_exp> and one with side length <math_exp>? '}\n",
      "1373 {'Id': '1373', 'Type': 'answer', 'ParentId': '1344', 'urls': [], 'exp': [], 'Body': 'I think one typical situation when this would be used (which you probably already know) is when you want to reduce the construction of some object which is supposed to lie over a base to the case when the base is affine. Namely, imagine you want to build a scheme T over a base S, and you cover S by open affines S_i; then if you construct the schemes T_i (pull-back of T over S_i) with appropriate gluing data, you will be done. So, in practice, you won\\'t have T but will instead have the functor it represents on S-schemes, T_i will be the fibre product of that functor and S_i (i.e. just restrict the functor to S_i-schemes), and now if T is a sheaf, and the T_i are representable, you are done. You could imagine applying this to build projective spaces or Grassmanians over T. E.g. suppose you had a locally free sheaf from which you wanted to form the associated projective space bundle.  One could do this directly, with some Proj construction, but one could instead figure out the universal property, and then replace S by an open cover S_i over which the locally free sheaf is actually free, in which case the usual projective space of the appropriate dimension (taken over S_i) will obviously represent your functor (provided you figured the functor out correctly!). I should say that, outside of the context of EGA itself, I don\\'t imagine that people cite this result (or analogous ones) very often.  They are more likely just to write something like \"since F is a sheaf, we can reduce our construction to the case when S is affine\".  It is just one of the standard techniques that float around for trying to represent moduli problems. '}\n",
      "1374 {'Id': '1374', 'Type': 'answer', 'ParentId': '1368', 'urls': [], 'exp': ['a', 'b', 'f_b(a)=(a+b)^n-a^n-b^n', 'a', \"f'_b(a)=n((a+b)^{n-1}-a^{n-1})\", '0', \"f'_b(a,b)\", 'f_b', 'a'], 'Body': \"Let's have a precalculus answer: Consider the function of <math_exp> depending on the parameter <math_exp> that is <math_exp>. Its derivative relative to <math_exp> is <math_exp> because <math_exp> <math_exp> is nonnegative and <math_exp> is an increasing function of <math_exp> and you can conclude. \"}\n",
      "1375 {'Id': '1375', 'Type': 'answer', 'ParentId': '1368', 'urls': [], 'exp': ['a^n', 'b^n', '(1+r)^n \\\\ge 1 + r^n', 'r \\\\le 1', 'r &lt; 1', 'r^n &lt; r', '1 + r^n &lt; 1 + r &lt; (1+r)^n', 'r = 1', 'n', '\\\\ge 1'], 'Body': 'Here is another way to look at the inequality.  Pick the larger of <math_exp> and <math_exp> and divide through by that quantity.  This reduces the problem to showing that <math_exp> for some positive real number <math_exp>.  If <math_exp>, we have <math_exp>, so <math_exp>. I leave the case <math_exp> (and <math_exp> rational and <math_exp>)  to others. '}\n",
      "1376 {'Id': '1376', 'Type': 'answer', 'ParentId': '760', 'urls': [], 'exp': [], 'Body': 'If you have the Euclidean distance d between the two points, and set D = the diameter of the sphere (so D = 2R), then the great circle distance is D*InvSin(d/D), where InvSin is the inverse sine function.  If you know that d/D is usually very small, then you can  approximate the great circle distance using a few terms of the Taylor series for inverse sine. '}\n",
      "1377 {'Id': '1377', 'Type': 'answer', 'ParentId': '1352', 'urls': [], 'exp': [], 'Body': 'Galois connections are further reaching than one first realizes.  One small application is in model theory, where the relation R between sentences of a theory and models given by t R M if sentence t is true in model M, leads to deductively closed theories versus classes of models closed under certain operations, many of which are algebraic in nature. They also arise in algebraic geometry and computer science, among other fields.  There is a book on Galois connections edited by Marcel Erne; for the strongly inquisitive I recommend checking it out. '}\n",
      "1378 {'Id': '1378', 'Type': 'question', 'Title': 'Mixing solutions of specified strength', 'Tags': ['algebra-precalculus'], 'urls': [], 'exp': ['35\\\\%', '60\\\\%', '50\\\\%'], 'Body': \"While reading one aptitude book, I came across this question: <math_exp> alcohol mixed with <math_exp> alcohol to get a <math_exp> alcohol. In what ratio were they mixed? I spent 20 minutes on this question, but I couldn't even figure out what it means! Could anyone explain this to me? \"}\n",
      "1379 {'Id': '1379', 'Type': 'answer', 'ParentId': '1378', 'urls': [], 'exp': [], 'Body': 'Call the ratio you want to find r=(volume of 35% alcohol):(volume of 60% alcohol).  Suppose that there are r liters of the 35% alcohol (so there are 0.35 L of alcohol and 0.65 L of water), 1 liter of the 60% alcohol (so the ratio is r), and 1+r liters of the 50% alcohol.  From there, you can construct an equation about the alcohol (or about the water) and solve for r. '}\n",
      "1380 {'Id': '1380', 'Type': 'answer', 'ParentId': '1378', 'urls': [], 'exp': [], 'Body': \"Given a liters of 35% alcohol solution, and b liters of 60% alcohol solution, what would be the alcohol ratio after we'll mix this two solutions? The a liters are actually a*35/100 alcohol, and a*(1-(35/100)) water. Likewise the b liters contains b*60/100 alcohol and b*(1-(60/100)) water. Mixing them together would give us a*35/100+b*60/100 alcohol and a*(1-(35/100))+b*(1-(60/100)) water. The question is for which a,b we have the ratio or I believe you can take it from here... \"}\n",
      "1381 {'Id': '1381', 'Type': 'answer', 'ParentId': '1368', 'urls': [], 'exp': [], 'Body': 'Induction. For n=1 it is trivially true Assume true for n=k i.e. (a + b)^k >= a^k + b^k Consider case n=k+1 (a+b)^(k+1)=(a+b)(a+b)^k>=(a+b)(a^k+b^k)=a^(k+1)+b^(k+1)+ab^k+ba^k>=a^(k+1)+b^(k+1) as required '}\n",
      "1382 {'Id': '1382', 'Type': 'question', 'Title': 'Counting subsets with r mod 5 elements', 'Tags': ['combinatorics', 'binomial-coefficients'], 'AcceptedAnswerId': '1746', 'urls': ['https://math.stackexchange.com/users/232/qiaochu-yuan', 'https://math.stackexchange.com/questions/918/how-do-i-count-the-subsets-of-a-set-whose-number-of-elements-is-divisible-by-3-4/'], 'exp': ['r\\\\bmod 5', 'P_n (r \\\\bmod 5)', 'n', \"P_n(r \\\\bmod 5)-P_n(r' \\\\bmod 5)\", 'r \\\\bmod 3', 'r \\\\bmod 2', 'r \\\\bmod 3', 'P_n(r \\\\bmod l)', 'l', 'l'], 'Body': 'Some time ago Qiaochu Yuan asked about counting subsets of a set whose number of elements is divisible by 3 (or 4). The story becomes even more interesting if one asks about number of subsets of n-element set with <math_exp> elements. Denote this number by, say, <math_exp>. An experiment shows that for small <math_exp>, <math_exp> is always a Fibonacci number (recall that for \"<math_exp>\" corresponding difference is always 0 or 1 and for \"<math_exp>\" they are all 0). It\\'s not hard to prove this statement by induction but as always inductive proof explains nothing. Does anybody have a combinatorial proof? (Or maybe some homological proof — I\\'ve heard one for \"<math_exp>\"-case.) And is there some theorem about <math_exp> for arbitrary <math_exp> (besides that it satisfies some recurrence relation of degree growing with <math_exp>)? '}\n",
      "1383 {'Id': '1383', 'Type': 'answer', 'ParentId': '1347', 'urls': [], 'exp': [], 'Body': \"Here's another version of Doug Chatham's answer, but with details. If you lived in Hyperbolic space, then Euclidean geometry would be natural to you as well.  The reason is that you can take what is called a horosphere (in the half-space model for us, this is just a hyperplane which is parallel to our limiting hyperplane) and this surface actually has a Euclidean geometry on it! So unlike for us, where the hyperbolic plane cannot be embedded into Euclidean 3-space, the opposite is true: the Euclidean plane can be embedded into hyperbolic 3-space! So this is analogous to our understanding of spherical geometry.  It's no surprise the spherical geometry is slightly different, however, it fits nicely into our Euclidean view of things, because spherical geometry is somewhat contained in three-dimensional geometry because of the embedding. \"}\n",
      "1385 {'Id': '1385', 'Type': 'answer', 'ParentId': '548', 'urls': ['http://en.wikipedia.org/wiki/Consistency', 'http://en.wikipedia.org/wiki/Field_%28mathematics%29#Definition_and_illustration'], 'exp': ['0+0=0', '(0+0)\\\\cdot x=0\\\\cdot x', '0\\\\cdot x + 0\\\\cdot x = 0\\\\cdot x', '0\\\\cdot x', '(0\\\\cdot x + 0\\\\cdot x) + (-(0\\\\cdot x)) = 0\\\\cdot x + (-(0\\\\cdot x))', '0\\\\cdot x + (0\\\\cdot x + (-(0\\\\cdot x))) = 0\\\\cdot x + (-(0\\\\cdot x))', '0\\\\cdot x + 0 = 0', '0\\\\cdot x = 0', '0\\\\times Z=1', '1\\\\ne0'], 'Body': 'The existence of a (multiplicative) inverse for the (additive) zero is inconsistent with the other field axioms. As you noticed, the crux is that anything multiplied by zero gives zero. Let us then establish this fact first. 0 is identity for addition: <math_exp> <br/> multiply by some x: <math_exp><br/> distributivity: <math_exp><br/> add the additive inverse of <math_exp> to both sides: <math_exp><br/> associativity of addition: <math_exp><br/> definition of \"additive inverse\": <math_exp><br/> zero is additive identity: <math_exp> Let\\'s assume now that there is a multiplicative inverse of 0, denoted by Z. (*) <math_exp> From the last two relations, 1=0, which contradicts another field axiom (often forgotten), which is: <math_exp> Therefore, you either accept that 0 has no inverse or you change at least one of the field axioms---you can\\'t have both at the same time. In a sense, it is a matter of convention which axioms you choose. In practice, some sets of axioms lead to more useful consequences. For example, if you want 0 to have an inverse and drop the axiom saying that 1 does not equal 0, then the \\'arithmetic\\' you end up doing won\\'t be very interesting. In short, you are right. '}\n",
      "1386 {'Id': '1386', 'Type': 'answer', 'ParentId': '536', 'urls': ['http://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm'], 'exp': [], 'Body': \"If you'd need only the 1d case, you can get this statement from the Law of the iterated logarithm. This does not give much combinatorial insight, and well... requires said law, which is a little more work to proof. \"}\n",
      "1387 {'Id': '1387', 'Type': 'answer', 'ParentId': '1113', 'urls': [], 'exp': ['n = 3^{2^k} - 2^{2^k}', 'k', '2^k | n - 1', '3^{2^k} - 2^{2^k} | 3^{n-1} - 2^{n-1}', 'a - b | a^r - b^r', 'a, b', 'k'], 'Body': \"I think I know how to prove it without using of Carmichael numbers. Take <math_exp>. Then it's not hard to prove by induction on <math_exp> that <math_exp>. Hence <math_exp> (that's true because <math_exp> for any natural <math_exp> and <math_exp>). \"}\n",
      "1388 {'Id': '1388', 'Type': 'question', 'Title': 'How to solve a cyclic quintic in radicals?', 'Tags': ['abstract-algebra', 'polynomials'], 'AcceptedAnswerId': '1399', 'urls': [], 'exp': ['\\\\frac{z^{11}-1}{z-1} = z^{10} + z^9 + z^8 + z^7 + z^6 + z^5 + z^4 + z^3 + z^2 + z + 1', '\\\\zeta^1,\\\\zeta^2,\\\\ldots,\\\\zeta^{10}', '[2,4,8,5,10,9,7,3,6,1]', '[2,9]', '[4,7]', '[8,3]', '[5,6]', '[10,1]', 'q_1 = \\\\zeta^2+\\\\zeta^9', 'q_2 = \\\\zeta^4+\\\\zeta^7', 'q_3 = \\\\zeta^8+\\\\zeta^3', 'q_4 = \\\\zeta^5+\\\\zeta^6', 'q_5 = \\\\zeta^{10}+\\\\zeta^1', '(q - q_1)(q - q_2)(q - q_3)(q - q_4)(q - q_5) = q^5 + q^4 - 4q^3 - 3q^2 + 3q + 1 = 0', '\\\\omega', '(q_1 + \\\\omega q_2  + \\\\omega^2 q_3  + \\\\omega^3 q_4  + \\\\omega^4 q_5)^5', '\\\\omega'], 'Body': \"Galois theory tells us that <math_exp> can be solved in radicals because its group is solvable. Actually performing the calculation is beyond me, though - here what I have got so far: Let the roots be <math_exp>, following Gauss we can split the problem into solving quintics and quadratics by looking at subgroups of the roots. Since 2 is a generator of the group <math_exp> we can partition into the five subgroups of conjugate pairs <math_exp>,<math_exp>,<math_exp>,<math_exp>,<math_exp>. Now put <math_exp>,<math_exp>,<math_exp>,<math_exp>,<math_exp>. So if we can solve the quintic <math_exp> we would just be left to solve a few quadratic equations. Now pari/gp tells me this quintic has the cyclic group C(5): I've worked through examples of solving quadratic and cubic equations based on the galois group but when it comes to this quintic I'm completely stumped so any advice would be tremendously helpful! Thanks. Edit: Thanks to Robin Chapman the problem is reduced significantly. Let <math_exp> be a primitive 5th root of unity (which is easy to express in radicals), it only remains to express <math_exp> in terms of rationals and powers of <math_exp> (and then everything can be substituted back and solved easily). We know this is possible because the term is fixed by the quintics galois group, how to actually perform this evades me but I will try to find a way. \"}\n",
      "1389 {'Id': '1389', 'Type': 'question', 'Title': \"Not quite Fermat's Last Theorem\", 'Tags': ['number-theory', 'algebra-precalculus'], 'AcceptedAnswerId': '1391', 'urls': [], 'exp': ['n^a + n^b = n^c', 'a,b,c,n', 'n=2', 'n\\\\ge3'], 'Body': 'Prove that the equation <math_exp>, with <math_exp>  positive integers, has infinite solutions if <math_exp>, and no solution if <math_exp>. '}\n",
      "1390 {'Id': '1390', 'Type': 'answer', 'ParentId': '1389', 'urls': [], 'exp': ['b&gt;a', 'n^b&lt;n^a+n^b&lt;n^{b+1}'], 'Body': 'Assuming <math_exp>: <math_exp> '}\n",
      "1391 {'Id': '1391', 'Type': 'answer', 'ParentId': '1389', 'urls': [], 'exp': [], 'Body': 'So this is fermats last theorem upside down? It occurs to me if we have two binary numbers we may add them to get another power of two, but if we had two numbers in base 3, say we would not have so much luck. '}\n",
      "1392 {'Id': '1392', 'Type': 'answer', 'ParentId': '1389', 'urls': [], 'exp': ['n=2', 'a=k, b=k, c=k+1', 'k \\\\in \\\\mathbb{N}', 'n \\\\ge 3', 'a, b, c \\\\ge 0', 'n^k', 'c \\\\ge a', 'c \\\\ge b', 'n^a | n^c', 'n^a | n^a + n^b', 'a \\\\le b', 'b \\\\le a', 'a = b', '2n^a = n^c', 'n=2'], 'Body': \"If <math_exp> we can take <math_exp> for any <math_exp>. Let <math_exp>. We can assume that <math_exp> because if not we could multiply left and right side by <math_exp> to make them positive. Now it's clear that <math_exp> and <math_exp>. Then we have <math_exp>, hence <math_exp> and <math_exp>. In the same way <math_exp>. So <math_exp>. Hence <math_exp> and <math_exp>. \"}\n",
      "1393 {'Id': '1393', 'Type': 'question', 'Title': 'Relative sizes of sets of integers and rationals revisited - how do I make sense of this?', 'Tags': ['elementary-set-theory'], 'AcceptedAnswerId': '1404', 'urls': ['https://math.stackexchange.com/questions/1311/are-there-more-rational-numbers-than-integers'], 'exp': ['f x = x+2', 'x', 'x+2'], 'Body': 'I already asked if there are more rationals than integers here... Are there more rational numbers than integers? However, there is one particular argument that I didn\\'t give before which I still find compelling... Every integer is also a rational. There exist (many) rationals that are not integers. Therefore there are more rationals than integers. Obviously, in a sense, I am simply choosing one particular bijection, so by the definition of set cardinality this argument is irrelevant. But it\\'s still a compelling argument for \"size\" because it\\'s based on a trivial/identity bijection. EDIT please note that the above paragraph indicates that I know about set cardinality and how it is defined, and accept it as a valid \"size\" definition, but am asking here about something else. To put it another way, the set of integers is a proper subset of the set of rationals. It seems strange to claim that the two sets are equal in size when one is a proper subset of the other. Is there, for example, some alternative named \"size\" definition consistent with the partial ordering given by the is-a-proper-subset-of operator? EDIT clearly it is reasonable to define such a partial order and evaluate it. And while I\\'ve use geometric analogies, clearly this is pure set theory - it depends only on the relevant sets sharing members, not on what the sets represent. Helpful answers might include a name (if one exists), perhaps for some abstraction that is consistent with this partial order but defined in cases where the partial order is not. Even an answer like \"yes, that\\'s valid, but it isn\\'t named and doesn\\'t lead to any interesting results\" may well be correct - but it doesn\\'t make the idea unreasonable. Sorry if some of my comments aren\\'t appropriate, but this is pretty frustrating. As I said, it feels like I\\'m violating some kind of taboo.  EDIT - I was browsing through random stuff when I was reminded this was here, and that I actually ran into an example where \"size\" clearly can\\'t mean \"cardinality\" fairly recently (actually a very long time ago and many times since, but I didn\\'t notice the connection until recently). The example relates to closures of sets. Please forgive any wrong terminology, but if I have a seed set of {0} and an operation <math_exp>, the closure of that set WRT that operation is the \"smallest\" set that is closed WRT that operation, meaning that for any member <math_exp> of the set, <math_exp> must also be a member. So obviously the closure is {0, 2, 4, 6, 8, ...} - the even non-negative integers. However, the cardinality of the set of even non-negative integers is equal to the cardinality of the set of all integers, or even all rationals. So if \"smallest\" means \"least cardinality\", the closure isn\\'t well-defined - the set {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...} is no larger than the set {0, 2, 4, 6, 8, ...}. Therefore, the meaning of \"smallest\" WRT set closures refers to some measure of size other than cardinality. I\\'m not adding this as a late answer because it\\'s already covered by the answers below - it\\'s just a particular example that makes sense to me. '}\n",
      "1394 {'Id': '1394', 'Type': 'answer', 'ParentId': '1393', 'urls': [], 'exp': ['13 = 1101_2', 'N(q) = 1+q', 'D(q) = 1/(1+1/q)', '1101_{\\\\mathbb{Q}} = (N \\\\circ N \\\\circ D \\\\circ N) 1 = 8/3'], 'Body': 'Actually to elaborate a bit more on: It seems strange to claim that the two sets are equal in size Let us consider binary representations, every natural number can be written in binary. For example <math_exp> but we can define two functions <math_exp>, <math_exp> and interpret a binary sequence as a composition of these functions applied to 1, for example <math_exp> and by Euclids algorithm this defines every (positive) rational number exactly once. If all they are, are different interpretations of binary sequences, it would be weird not to claim have equal size! '}\n",
      "1395 {'Id': '1395', 'Type': 'answer', 'ParentId': '1393', 'urls': [], 'exp': [], 'Body': 'It may seem strange that the set of integers is a proper subset of the set of rationals, but this is exactly the definition of an infinite set. Maybe it could be easier for you to see that there cannot be a size-based definition when talking about infinite set if you consider this: with two finite sets, in whichever way you choose elements to be removed one at a time from both sets at one, you\\'ll end up with the larger set having still some element while the smaller remained empty. This does not happen with infinite sets; even if you find a way to remove all elements (for example, remove he first at T=0, the second a T=1/2, the third at T=3/4, and so on) you may always choose an ordering in which the \"larger\" set becomes empty, and the \"smaller\" has still some element. '}\n",
      "1396 {'Id': '1396', 'Type': 'answer', 'ParentId': '1393', 'urls': ['http://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel'], 'exp': [], 'Body': 'I\\'ve found Hilbert\\'s Hotel a useful example to understand (or fail to understand, but on a higher level), how much \"infinity\" really is, and how much the naive view on things fails when confronted with infinity. It deals with the easier case, comparing the integers with even integers, but maybe it will help. =) Edit: The wikipedia article linked is not great, but google will surely turn out more useful. '}\n",
      "1397 {'Id': '1397', 'Type': 'answer', 'ParentId': '1393', 'urls': ['http://en.wikipedia.org/wiki/Order_relation'], 'exp': [], 'Body': 'You could consider the order relation generated by the operation of inclusion between sets. It is a partial order, and it is, in some way, related to the \"size\" of the sets. '}\n",
      "1398 {'Id': '1398', 'Type': 'answer', 'ParentId': '1393', 'urls': [], 'exp': ['A \\\\subset B', 'A', 'B'], 'Body': 'Is there, for example, some alternative named \"size\" definition consistent with the partial ordering given by the is-a-proper-subset-of operator? I think it is important to keep in mind the context you are looking at something when you want to talk about sizes.  In a topological or geometric context, if <math_exp> then we may want to think of <math_exp> as smaller than <math_exp>.  However, when talking about cardinality of a set, we need to think of the structure of the set as a set (and not part of another set like the real numbers or in its geometric/topological context).  In this way, the only reasonable definition for sets to be the same (isomorphic) is if there is a bijection between them.  Therefore if we want to assign some cardinality to something as a set, it should be the same for all sets that are in bijection with it. '}\n",
      "1399 {'Id': '1399', 'Type': 'answer', 'ParentId': '1388', 'urls': ['http://dx.doi.org/10.1090/S0025-5718-1991-1079014-X'], 'exp': ['x_1,\\\\ldots,x_5', '\\\\tau', 'x_i\\\\mapsto x_{i+1}', '\\\\zeta=\\\\exp(2\\\\pi i/5)', '\\\\begin{align*}  A_0&amp;=x_1+x_2+x_3+x_4+x_5\\\\\\\\  A_1&amp;=x_1+\\\\zeta x_2+\\\\zeta^2 x_3+\\\\zeta^3 x_4+\\\\zeta^4 x_5\\\\\\\\  A_2&amp;=x_1+\\\\zeta^2 x_2+\\\\zeta^4 x_3+\\\\zeta x_4+\\\\zeta^3 x_5\\\\\\\\  A_3&amp;=x_1+\\\\zeta^3 x_2+\\\\zeta x_3+\\\\zeta^4 x_4+\\\\zeta^2 x_5\\\\\\\\  A_4&amp;=x_1+\\\\zeta^4 x_2+\\\\zeta^3 x_3+\\\\zeta^2 x_4+\\\\zeta x_5  \\\\end{align*}', 'A_0,\\\\ldots,A_4', 'x_1,\\\\ldots,x_5', 'A_0', '\\\\tau', 'A_j', '\\\\zeta^{-j}A_j', 'A_j^5', 'A_j^5', 'A_j^5', '\\\\zeta', '\\\\zeta', 'A_1^5', 'A_1', 'A_j', 'A_1'], 'Body': 'For full details of this and more, the best place to look is the following paper: D. S. Dummit, Solving solvable quintics. Math. Comp. 57 (1991), 387-401. The main idea (which extends to any equation with a cyclic Galois group) is to consider Lagrange resolvents. Let the equation have roots <math_exp> with an element <math_exp> of the Galois group permuting them as <math_exp>. Let <math_exp> be the standard fifth root of unity. Then the Lagrange resolvents are <math_exp> Once one has <math_exp> one easily gets <math_exp>. It\\'s easy to find <math_exp> :-) The point is that <math_exp> takes <math_exp> to <math_exp> and so takes <math_exp> to <math_exp>. Thus <math_exp> can be written down in terms of rationals (if that\\'s your starting field) and powers of <math_exp>. Alas, here is where the algebra becomes difficult. The coefficients of powers of <math_exp> in <math_exp> are complicated. They can be expressed in terms of a root of a \"resolvent polynomial\" which will have a rational root as the equation is cyclic. Once one has done this, you have <math_exp> as a fifth root of a certain explicit complex number. Then one can express the other <math_exp> in terms of <math_exp>. The details are not very pleasant, but Dummit skilfully navigates through the complexities, and produces formulas which are not as complicated as they might be. Alas, I don\\'t have the time nor the energy to provide more details. '}\n",
      "1400 {'Id': '1400', 'Type': 'question', 'Title': \"Probability that the convex hull of random points contains sphere's center\", 'Tags': ['geometry', 'geometric-probability'], 'AcceptedAnswerId': '1407', 'urls': [], 'exp': ['n+2', 'n'], 'Body': \"What is the probability that the convex hull of <math_exp> random points on <math_exp>-dimensional sphere contains sphere's center? \"}\n",
      "1401 {'Id': '1401', 'Type': 'question', 'Title': 'Why <span class=\"math-container\" id=\"11552\">PSL_3(\\\\mathbb F_2)\\\\cong PSL_2(\\\\mathbb F_7)</span>?', 'Tags': ['group-theory', 'simple-groups', 'exceptional-isomorphisms'], 'AcceptedAnswerId': '1498', 'urls': [], 'exp': ['PSL_3(\\\\mathbb{F}_2)', 'PSL_2(\\\\mathbb{F}_7)'], 'Body': 'Why are groups <math_exp> and <math_exp> isomorphic? Update. There is a group-theoretic proof (see answer). But is there any geometric proof? Or some proof using octonions, maybe? '}\n",
      "1402 {'Id': '1402', 'Type': 'answer', 'ParentId': '1401', 'urls': [], 'exp': ['168', 'PSL_2(7)', 'G', '168', '8', '7', 'A_8', '\\\\infty,0,1,\\\\ldots,6', '7', 'g=(0\\\\ 1\\\\ 2\\\\ 3\\\\ 4\\\\ 5\\\\ 6)', 'G', '2', '\\\\langle g\\\\rangle', 'g', 'h=(1\\\\ 2\\\\ 4)(3\\\\ 6\\\\ 5)', 'H', '\\\\{\\\\infty,0\\\\}', 'h', 'k', '(\\\\infty\\\\ 0)', 'H', '2', 'G', 'H', 'k=(\\\\infty\\\\ 0)(1\\\\ 6)(2\\\\ 3)(4\\\\ 5)', 'G', 'PSL_2(7)'], 'Body': 'Both are simple groups of order 168, and each simple group of order <math_exp> is isomorphic to <math_exp>. An extended exercise, with hints. Prove the following: Let <math_exp> be a simple group of order <math_exp>. It has <math_exp> Sylow <math_exp>-subgroups. It can be identified with a subgroup of <math_exp>. Labelling the objects it acts on as <math_exp> one Sylow <math_exp>-subgroup is generated by <math_exp>. The group <math_exp> is <math_exp>-transitive. The normalizer of <math_exp> is generated by <math_exp> and <math_exp>. The setwise stabilizer <math_exp> of <math_exp> is generated by <math_exp> and another element <math_exp> which is the product of <math_exp> and three other disjoint transpositions. If <math_exp> is cyclic, then the Sylow <math_exp>-subgroup of <math_exp> would be unique, leading to a contradiction. So <math_exp> is nonabelian and we can take <math_exp>. Finally <math_exp> is <math_exp>. '}\n",
      "1403 {'Id': '1403', 'Type': 'answer', 'ParentId': '1393', 'urls': ['http://en.wikipedia.org/wiki/Natural_density', 'http://en.wikipedia.org/wiki/Measure_%28mathematics%29'], 'exp': ['f:A\\\\to B', 'x', 'A', 'f(x)', 'A', '\\\\mathbb N', 'A', '\\\\mathbb N', '1/2', 'A', 'B', 'd(A)', 'd(B)', 'A\\\\subseteq B\\\\subseteq \\\\mathbb N', 'd(A)\\\\leq d(B)', '\\\\mathbb N', 'X', 'm', 'A', 'B', 'X', 'A\\\\subseteq B', 'm(A)\\\\leq m(B)', 'm', 'X=\\\\mathbb R', '[0,1]', '1/2', '[0,1/2]', 'X', '\\\\mathbb N', 'X', '\\\\mathbb R'], 'Body': 'Purely set theoretically, cardinality is the right way to think of the \"size\" of a set. A bijection <math_exp> simply renames each element <math_exp> in <math_exp> to <math_exp>, and one reasonably wants the size of a set not to depend on the names given to its elements. There are other notions of size if you let your sets have more structure. The natural density (if it exists) of a subset <math_exp> of the natural numbers <math_exp> can be thought as the relative size of <math_exp> to <math_exp>. The natural density of the even numbers is <math_exp>, for example, so one might say there are half as many even natural numbers as there are natural numbers altogether. If <math_exp> and <math_exp> have natural densities <math_exp> and <math_exp>, and <math_exp>, then <math_exp>. Not all subsets of <math_exp> have a natural density though, so in particular we can\\'t compare the \"sizes\" of all sets of naturals. Another possibility is considering the (measurable) subsets of a set <math_exp> equipped with a measure <math_exp>. If <math_exp> and <math_exp> are measurable subsets of <math_exp>, and <math_exp>, then <math_exp>. For example, we can use the Lebesgue measure <math_exp> on <math_exp>, which gives measure 1 to the interval <math_exp> and measure <math_exp> to the interval <math_exp>. But again, not all subsets of <math_exp> are measurable, so not all sets can be compared size-wise this way. Note that in both the approaches above, we can only compare the size of a set relative to some other fixed set (<math_exp> or <math_exp>). Any finite set and the set of rational numbers both have measure 0 with respect to the Lebesgue measure on <math_exp>, for example, so we would be forced to admit them to have the same size in this setting. '}\n",
      "1404 {'Id': '1404', 'Type': 'answer', 'ParentId': '1393', 'urls': [], 'exp': [], 'Body': 'Of course, there are other notions of size.  In particular, your notion of \"a partial order based on inclusion of sets\" is a very fruitful concept which has been used frequently.  As a quick example, there is a technique in mathematical logic/set theory called \"forcing\" which is used to show that certain mathematical statements are unprovable.  Forcing often starts with a partial ordered set where the order is given by inclusion of subsets. In terms of the everyday world interpretation of the word \"size\", there are (at least) two problems with the using the partial order given by inclusion of subsets.  The first is, as you said, a partial order: there are two sets which cannot be compared, i.e., there are 2 sets where you cannot say one is bigger than the other.  The second is that two things will have the same size precisely when the two things are absolutely the same.  There is no notion of different things which happen to be the same size - that can\\'t happen in this partial order. For example, lets say we\\'re looking at subsets of the integers.  You pull out your favorite subset: all the odd integers and I pull out mine: all the even integers.  Using the partial order definition of size, these two sets are incomparable.  Mine is neither bigger than, smaller than, or the same size as yours.  To contrast that, using the cardinality notion of size, they have the same size.  This is evidenced by simply taking everything in your set and adding 1 to it to get everything in my set.  For an even more absurd example, consider the set {0} and the set {1}.  One would expect these two sets to have the same notion of \"size\" (for any notion of \"size\"!), but using the partial order notion, one cannot compare these two sets. By contrast, cardinality (or, the way I used \"size\" in the previous link) is defined on ALL sets (assuming the axiom of choice), even those which a priori have no subset relation.  And there are many examples of sets which have the same cardinality, but which are not equal.  (For example, the set of evens and odds, or the sets {0} and {1}). '}\n",
      "1405 {'Id': '1405', 'Type': 'answer', 'ParentId': '398', 'urls': [], 'exp': [], 'Body': 'Almost all of this misses the idea that you can\\'t do traditional arithmetic with infinity that way that most of us think of it.  10/2 = 5, five is half of 10, no problem.  If Judy has five apples and Joe has 10 apples, then it is proper to say \"Judy has half as many apples as Joe.\"  But to say \"there are half as many even positive integers as there are positive intergers\" is to infer something about infinity/2, or half of infinity, which is not entirely kosher. '}\n",
      "1406 {'Id': '1406', 'Type': 'answer', 'ParentId': '1400', 'urls': ['http://www.mscand.dk/article/view/10655/8676', 'http://www.math.uni-bielefeld.de/~sillke/PUZZLES/ranpoint.txt'], 'exp': ['N', 'n', '2^{-N+1}\\\\sum_{k=0}^{n-1} {{N-1}\\\\choose k}'], 'Body': \"This problem is discussed in J. G. Wendel; A Problem in Geometric Probability, Mathematica Scandinavica 11 (1962) 109-111. Wendel showed that the probability of <math_exp> random points lying on the surface of the unit sphere in dimension <math_exp> all lie on one hemisphere is <math_exp> I've found this here. \"}\n",
      "1407 {'Id': '1407', 'Type': 'answer', 'ParentId': '1400', 'urls': ['http://mathworld.wolfram.com/SphereTetrahedronPicking.html'], 'exp': ['n+2', 'S^n', '\\\\mathbb{R}^{n+1}', '2^{-n-1}', 'P_1,\\\\ldots,P_{n+2}', '\\\\pm P_1,\\\\pm P_2,\\\\ldots,\\\\pm P_{n+1}', 'P_{n+2}', 'm+1', '\\\\mathbb{R}^m', '2^{-m}', '\\\\mathbb{R}^m', 'm', 'v_1,\\\\ldots,v_m', 'v_0,v_1,\\\\ldots,v_m', 'a_i', '\\\\sum_i a_i v_i=0', '(a_0,\\\\ldots,a_m)', 'a_i', 'a_0=1', 'a_1,\\\\ldots,a_m', 'v_i', 'a_i', 'v', 'A', '-v', 'A', '2^m', 'a_1,\\\\ldots,a_m', 'v_i', 'a_i', '\\\\mathbb{R}^m', 'm+1', '2^{-m}'], 'Body': \"This is one of those old chestnuts that come up again and again. To be precise, the probability that the convex hull of <math_exp> points in <math_exp> (the unit sphere in <math_exp>) contains the origin is <math_exp>. There's a brief argument at Wolfram's mathworld which I don't find entirely convincing but which certainly can be patched to form a convincing argument. In brief, show that for random points <math_exp> on the sphere, then with probability one, exactly one choice of signs will put the centre in the convex hull of <math_exp> and <math_exp>. Added (3/8/2010) Thanks to Grigory for his comment. Changing the notation slightly, one can show that under some fairy weak hypotheses, if we choose <math_exp> points randomly and indepedently in <math_exp> the probability their convex hull contains the origin is <math_exp>. Take a probability distribution on <math_exp> and choose a sequence of points (which we identify with vectors) independently from that distribution. Our first condition on this distribution is that <math_exp> vectors <math_exp> chosen independently from it are linearly independent with probability one. This can fail if say some point occurs with nonzero probability or the distribution lies in a hyperplane through the origin. Assume this condition. Now a sequence <math_exp> of random points chosen according to our distribution are linearly independent: there are reals <math_exp> not all zero with <math_exp>. By our condition, with probability one, the sequence <math_exp> is unique up to constant multiple, and moreover all the <math_exp> are nonzero. So we may assume <math_exp> and <math_exp> are nonzero and uniquely determined. Then the convex hull of the <math_exp> contains the origin if an only if all the <math_exp> are positive. Now we introduce another condition: that the distribution is centrally symmetric; in detail the probability that a random vector <math_exp> lies in a set <math_exp> equals the probability that <math_exp> lies in <math_exp>. A condition like this is clearly necessary; it stops the distribution being supported on a small region far from the origin. This condition shows that all the <math_exp> possibilities of signs for <math_exp> are equiprobable; since changing the sign of some <math_exp> changes the sign of <math_exp>. To conclude, if our probability distribution on <math_exp> satisfies these two condition, the probability that the convex hull of <math_exp> indepdendently chosen points contains the origin is <math_exp>. These conditions are satisfied by the uniform distribution on a sphere with centre at the origin, but also by many others. \"}\n",
      "1408 {'Id': '1408', 'Type': 'question', 'Title': 'Computing stalks: do direct limits behave like limits?', 'Tags': ['commutative-algebra', 'sheaf-theory'], 'AcceptedAnswerId': '1411', 'urls': [], 'exp': ['X', '\\\\mathcal{O}_X', 'p \\\\in X', '\\\\mathcal{O}_X(U)', 'U', 'p', '\\\\mathcal{O}_X', 'p \\\\in X', 'X', 'p', '\\\\mathcal{O}_X', 'p \\\\in X', 'p', 'p'], 'Body': 'Suppose that <math_exp> is a topological space with a sheaf of rings <math_exp>. In general, the stalk at a point <math_exp> is the direct limit of the rings <math_exp> for all open sets <math_exp> containing <math_exp>. Here are two questions on computing stalks - I think both should be true, since a direct limit should be some sort of \"limiting process\", but that\\'s far from convincing for me. Can I compute the stalk of <math_exp> at a point <math_exp> by only limiting over basic open sets of <math_exp> containing <math_exp>? Can I compute the stalk of <math_exp> at a point <math_exp> by excluding some finite number of \"large\" open sets around <math_exp>, and then limiting over the remaining open sets around <math_exp>? '}\n",
      "1409 {'Id': '1409', 'Type': 'answer', 'ParentId': '1388', 'urls': [], 'exp': [], 'Body': 'I recommend you go and look at this reference. www.passhema.org/proceedings/2007/2007DavisGupta.pdf '}\n",
      "1410 {'Id': '1410', 'Type': 'answer', 'ParentId': '398', 'urls': [], 'exp': [], 'Body': 'You can also get it the other way around -- N is (kind of) a subset of E. Map each integer n into 4n. Then there\\'s a bijection (1-1, onto function) between N and (let\\'s call it) N4, the set of all integers divisible by 4, so N and N4 are, in your intuition, of the same \"size\".  But N4 is a proper subset of E !  Of course, you could just as well have mapped N directly to E with bijection f(n) = 2n. Anyway, that shows you how slippery it is to apply concepts of finite size to infinite sets, and why bijection is the way to go. '}\n",
      "1411 {'Id': '1411', 'Type': 'answer', 'ParentId': '1408', 'urls': ['http://en.wikipedia.org/wiki/Cofinal'], 'exp': [], 'Body': 'Yes. The general statement is the following: limit over a poset is equal to limit over its any coinitial subset. Formal proof is easy (hint: construct maps in both directions) and informally it\\'s an analogue of \"subsequence has the same limit as a sequence\" theorem. '}\n",
      "1412 {'Id': '1412', 'Type': 'answer', 'ParentId': '398', 'urls': [], 'exp': [], 'Body': \"The density argument above is not quite to the point, since it involves not only a set but an ordering. In fact, if you play with orderings, you can get even stranger results, such as the integers and rationals can both be ordered as a continuum (according to the ordering, between any two members in the set there is another member) and a non-continuum (there are two members with no other member in between). Or worse, between any two members is at most a finite set of other members (non-continuum) or an infinite set of other members (continuum).  That's kind of an extreme form of density. \"}\n",
      "1413 {'Id': '1413', 'Type': 'question', 'Title': 'Why should I care about fields of positive characteristic?', 'Tags': ['abstract-algebra', 'field-theory'], 'AcceptedAnswerId': '1414', 'urls': [], 'exp': [], 'Body': 'This is what I know about why someone might care about fields of positive characteristic: Some people might read this and think, \"What more could you need?\" But I\\'ve never been able to make myself care about number theory, so (1) doesn\\'t help me. (2) is nice for what it is, but I\\'m hoping there\\'s something more. My understanding of (2) is that this is only geometry in a rather abstract sense and, for instance, there\\'s no generally useful way to directly visually represent these fields or varieties over them the way we can over the reals or complex numbers. (Drawing a curve in R^2 and saying it\\'s the curve over some other field may be helpful for some purposes, but it\\'s not what I\\'m after here.) Is there anything else? The ideal (surely impossible) answer for me would be \"Yes, such fields are very good models for these common and easy to understand physical systems: A, B, C. Also, we can visualize them and varieties over them quite easily by method D. Finally, here\\'s a bunch of surprising and helpful applications to 500 other areas of mathematics.\" UPDATE: to answer Qiaochu\\'s comment about what I do care about. Let\\'s say I care about: algebraic &amp; geometric topology differential geometry &amp; topology applications to physics and I certainly care about algebraic geometry over C (this is to say I understand the motivations behind these subjects and the general idea, not necessarily that I know them in depth) '}\n",
      "1414 {'Id': '1414', 'Type': 'answer', 'ParentId': '1413', 'urls': ['http://en.wikipedia.org/wiki/Weil_conjectures#Statement_of_the_Weil_conjectures', 'http://arxiv.org/abs/0903.0517', 'http://books.google.de/books?id=ffV07pGT6soC&amp;pg=PA518&amp;lpg=PA518&amp;dq=Manin+%22arithmetical+physics%22&amp;source=bl&amp;ots=Bl0wfHR7iG&amp;sig=kC9sfstSnBdY4eXtZZvWmHBAQgY&amp;hl=de&amp;ei=bA9PS4vOA4PqnAOu38GiCg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CAcQ6AEwAA#v=onepage&amp;q&amp;f=false', 'https://mathoverflow.net/questions/11716/mirror-symmetry-mod-p-physics-mod-p/11739#11739'], 'exp': ['\\\\mathbb{C}', '\\\\mathbb{Z}', '\\\\mathbb{F}_p', 'p'], 'Body': \"Let's suppose you care about finite groups, one way or another.  Then you probably care about the classification of finite simple groups.  The bulk of this classification is the groups of Lie type, which were discovered by finding analogues of Lie groups over finite fields. Finite fields are also (as one might guess) very important in computer science.  I'm certainly no expert, but here are some applications I know of: Finally, even if you are only interested in varieties over <math_exp> (say), if your variety happens to also be nice and defined over <math_exp> then it can be nice and defined over <math_exp> for all but finitely many <math_exp> and you can use the Weil conjectures to compute its Betti numbers by counting.  This is particularly easy to do for varieties with nice moduli interpretations like flag varieties. Edit:  You might also be interested in reading Serre's expository article How to use finite fields for problems concerning infinite fields, as well as Manin's Reflections on arithmetical physics.  I got the latter link from an excellent answer to an MO question on mirror symmetry over finite fields. \"}\n",
      "1415 {'Id': '1415', 'Type': 'answer', 'ParentId': '1408', 'urls': [], 'exp': ['O_x', '(f, U)', 'f', 'U', 'U', 'x', '(f,U), (g,V)', 'f=g', 'W', 'x', 'U \\\\cap V', 'x', 'F', 'F', 'x'], 'Body': 'Basically, here\\'s a way to think of the stalk that is more \"down-to-earth\" than direct limits. An element of the stalk <math_exp> is given by a pair <math_exp> where <math_exp> is a section over the open set <math_exp> and <math_exp> contains <math_exp>. Two pairs <math_exp> are considered equivalent if <math_exp> on a neighborhood <math_exp> of <math_exp> (contained in <math_exp>). With this definition, it\\'s easy to see that what happens at <math_exp> doesn\\'t depend on what happens on <math_exp>, where <math_exp> is any closed set disjoint from <math_exp>. The stalk is a purely local construction. As for why this is equivalent to the direct limit: that\\'s a direct corollary of how the construction works in most familiar categories with which one might define a sheaf (sets, groups, rings, etc.) '}\n",
      "1416 {'Id': '1416', 'Type': 'question', 'Title': 'Algebra of Random Variables?', 'Tags': ['probability-theory', 'probability'], 'AcceptedAnswerId': '1424', 'urls': [], 'exp': ['h(z)', 'x', 'y', 'z=xy', 'z=y/x'], 'Body': \"I've been looking online (and in teaching journals) for a good introduction to Algebras of Random Variables (on an undergraduate level) and their usage, and have come up short. I know I can find the probability distribution of <math_exp> where: \\\\begin{equation*} z = x + y. \\\\end{equation*} If <math_exp> and <math_exp> are from known independent probability distributions (the solution is simply a convolution). Two other operations <math_exp> and <math_exp> can be solved for quite easily as well. Does anyone know of any other, more complicated, uses for treating random variables as objects to be manipulated? \"}\n",
      "1417 {'Id': '1417', 'Type': 'answer', 'ParentId': '1401', 'urls': ['http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.163.1487&amp;rep=rep1&amp;type=pdf'], 'exp': ['G', '168', '3', 'V', 'k', '2', '3', 'G', '\\\\mathbb{F}_2', 'G', '\\\\text{GL}_3(\\\\mathbb{F}_2)', '7', '3', 'G', '\\\\mathbb{F}_7', '\\\\text{PSL}_2(\\\\mathbb{F}_7)'], 'Body': \"There is a reasonably geometric proof in Section 1.4 of Noam Elkies' The Klein Quartic in number theory.  A very rough summary is as follows.  The unique simple group <math_exp> of order <math_exp> has a <math_exp>-dimensional irreducible representation <math_exp> which is defined over a number field <math_exp>.  By reducing this representation modulo a prime over <math_exp> we obtain a <math_exp>-dimensional representation of <math_exp> over <math_exp> which identifies <math_exp> with <math_exp> by simplicity (and a counting argument).  By reducing this representation modulo a prime over <math_exp> we obtain a <math_exp>-dimensional representation of <math_exp> over <math_exp> which is, as it turns out, the symmetric square of the defining representation of <math_exp>. \"}\n",
      "1418 {'Id': '1418', 'Type': 'answer', 'ParentId': '1416', 'urls': ['http://www.johndcook.com/distribution_chart.html'], 'exp': [], 'Body': 'There are nice algebraic relationships for special families.  The sum of normal (Cauchy, Levy) random variables is normal (Cauchy, Levy). The product of log-normal random variables is log-normal.  The sum of gamma random variables is gamma if the distributions have a common scale parameter, etc. See more here. '}\n",
      "1419 {'Id': '1419', 'Type': 'answer', 'ParentId': '1413', 'urls': [], 'exp': [], 'Body': 'If you are interested in algebraic geometry over C, here is another reason. A basic technique in birational geometry is the bend and break, which roughly amounts to taking any curve on a projective variety and deforming it until it becomes a union of curves of lower genus. Eventually this technique can be used to produce rational curves (under appropriate hypothesis). This is a major tool, and has been used for instance to study the geometry of Fano varieties, in particular to prove that they are uniruled, or to prove Hartshorne conjecture on the characterization of projective space by the positivity of its tangent bundle. The fact is, to make the trick work you have to have a curve whose space of embedded deformations is big enough. This is easy to achieve in positive characteristic: the curve itself may not deform, but if you compose the inclusion with a sufficiently high power of the Frobenius morphism, the map that you obtain will have enough deformations. To prove the result in characteristic 0, a technique of reduction to characteristic p is used. '}\n",
      "1420 {'Id': '1420', 'Type': 'answer', 'ParentId': '1389', 'urls': [], 'exp': ['\\\\,a \\\\le b', 'n^a', '\\\\,1 + n^{b-a} = n^{c-a}', '\\\\Rightarrow', 'b=a\\\\ ', '\\\\,n\\\\mid1)\\\\,', '\\\\Rightarrow', '\\\\, n = 2,\\\\, c = a\\\\!+\\\\!1'], 'Body': 'Wlog <math_exp>. Dividing by <math_exp> yields <math_exp> <math_exp> <math_exp> (else <math_exp> <math_exp> <math_exp>. '}\n",
      "1421 {'Id': '1421', 'Type': 'answer', 'ParentId': '1413', 'urls': ['http://arxiv.org/abs/0903.0517', 'http://en.wikipedia.org/wiki/Ax%E2%80%93Grothendieck_theorem', 'http://en.wikipedia.org/wiki/Compactness_theorem', 'http://terrytao.wordpress.com/2009/03/07/infinite-fields-finite-fields-and-the-ax-grothendieck-theorem/'], 'exp': ['P: \\\\mathbb{C}^n \\\\to \\\\mathbb{C}^n', '\\\\mathbb{C}', 'p', 'p', 'p', 'p', '\\\\mathbb{C}'], 'Body': \"There is a whole paper of Serre on this, which I haven't read; I will explain briefly one of my favorite applications. One of the really cool applications of this is to the Ax-Grothendieck theorem.  Which is a 100% analytic statement: if <math_exp> is an injective polynomial map, then it's surjective. And yet the proof uses finite fields. How? Well, first note that the theorem is trivial when <math_exp> is replaced by a finite field: an injective map on finite sets is surjective. Using a little algebra (and a simple direct limit argument), it follows that the theorem is true for the algebraically closed field of characteristic <math_exp> which is the algebraic closure of the integers mod <math_exp>. Now apply to a bit of model theory; the theorem of Robinson says that any statement of first-order logic that is true in an alg. closed field of char. <math_exp> for each <math_exp> is true in any algebraically closed field of char 0, in particular the complex numbers. The trick is that the Ax-Grothendieck theorem can be phrased as a collection of (infinitely many) statements in first-order logic. And each is true in the algebraic closure of the finite fields. Whence, by Robinson's theorem, it's true in <math_exp>! This would be cheating if Robinson's theorem were something obscure. In fact, however, it is a direct application of the compactness theorem for first-order logic. For another proof via the Nullstellensatz (but based on finite fields, still), see Terry Tao's post. \"}\n",
      "1422 {'Id': '1422', 'Type': 'question', 'Title': 'Which continuous functions are polynomials?', 'Tags': ['algebraic-geometry', 'general-topology', 'differential-topology'], 'urls': [], 'exp': ['f \\\\in C(\\\\mathbb{R}^n)', '\\\\mathbb{R}', '\\\\mathbb{R}^n', 'f', '\\\\phi:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}^n', 'f \\\\circ \\\\phi \\\\in \\\\mathbb{R}[x_1,\\\\ldots, x_n]'], 'Body': \"Suppose <math_exp>, the space of continuous <math_exp>-valued functions on <math_exp>.  Are there conditions on <math_exp> that guarantee it is the pullback of a polynomial under some homeomorphism?  That is, when can I find <math_exp> such that <math_exp>?  I have tried playing around with the implicit function theorem but haven't gotten far.  It feels like I may be missing something very obvious. Some related questions: \"}\n",
      "1423 {'Id': '1423', 'Type': 'question', 'Title': 'Number of finite simple groups of given order is at most <span class=\"math-container\" id=\"11839\">2</span> - is a classification-free proof possible?', 'Tags': ['group-theory', 'simple-groups'], 'urls': ['http://en.wikipedia.org/wiki/List_of_finite_simple_groups', 'http://en.wikipedia.org/wiki/Classification_of_finite_simple_groups'], 'exp': ['g', '0', '1', '2', 'g'], 'Body': 'This Wikipedia article states that the isomorphism type of a finite simple group is determined by its order, except that: I think this means that for each integer <math_exp>, there are <math_exp>, <math_exp> or <math_exp> simple groups of order <math_exp>. Do we need the full strength of the Classification of Finite Simple Groups to prove this, or is there a simpler way of proving it? '}\n",
      "1424 {'Id': '1424', 'Type': 'answer', 'ParentId': '1416', 'urls': ['http://www.math.upenn.edu/~wilf/DownldGF.html', 'http://algo.inria.fr/flajolet/Publications/books.html', 'http://qchu.wordpress.com/2009/06/24/gila-vi-the-cycle-index-polynomials-of-the-symmetric-groups/'], 'exp': ['f(x) = \\\\sum_{n \\\\ge 0} a_n x^n', 'a_n \\\\ge 0', 'f(1) = 1', \"f'(1)\", \"f''(1) + f'(1) - f'(1)^2\", 'n', '1', '\\\\displaystyle P_n(x) = \\\\frac{1}{n!} \\\\sum_{\\\\pi \\\\in S_n} x^{c_1(\\\\pi)}', 'c_1(\\\\pi)', \"P_n'(1)\", '\\\\displaystyle P(x, y) = \\\\sum_{n \\\\ge 0} P_n(x) y^n = \\\\frac{1}{1 - y} \\\\exp \\\\left( xy - y \\\\right).', '\\\\frac{\\\\partial}{\\\\partial x} P(x, y)', 'x = 1', '\\\\frac{1}{1 - y}', '1', '\\\\displaystyle Q_n(x) = \\\\frac{1}{n!} \\\\sum_{\\\\pi \\\\in S_n} x^{c(\\\\pi)}', 'c(\\\\pi)', \"Q_n'(1)\", '\\\\displaystyle Q(x, y) = \\\\sum_{n \\\\ge 0} Q_n(x) y^n = \\\\frac{1}{(1 - y)^x}', '\\\\exp \\\\left( x \\\\log \\\\frac{1}{1 - y} \\\\right)', '\\\\frac{\\\\partial}{\\\\partial x} Q(x, y)', 'x = 1', '\\\\displaystyle \\\\frac{1}{1 - y} \\\\log \\\\frac{1}{1 - y} = \\\\sum_{n \\\\ge 1} H_n y^n', 'H_n', 'n^{th}', 'n', '\\\\log n', 'r', '\\\\frac{1}{r}', 'x = 1', '\\\\displaystyle \\\\frac{1}{1 - y} \\\\log^2 \\\\frac{1}{1 - y} = \\\\sum_{n \\\\ge 1} G_n y^n', '\\\\displaystyle G_n = \\\\sum_{k=1}^{n-1} \\\\frac{1}{k} H_{n-k}', 'G_n + H_n - H_n^2', 'G_n \\\\le H_n^2', 'H_n'], 'Body': \"In the special case that the sample space is the non-negative integers (or a subset thereof), one can think of a probability distribution as a generating function <math_exp> where <math_exp> and <math_exp>.  Then the sum of random variables corresponds to the product of generating functions, so one can bring generating function techniques (see, for example, Wilf) to bear on such random variables.  For example, it is particularly easy to compute expected values this way: the expected value is <math_exp>, and the product rule expresses the fact that expected value is additive.  Similarly, the variance is <math_exp>. I don't really know a place where these issues are discussed in detail, but one spectacular family of examples is the computation of the expected values and variances of certain statistics on permutations.  For example, suppose we want to compute the expected number of fixed points that a permutation of <math_exp> elements has.  By Burnside's lemma, the answer is <math_exp>.  But another way to do this computation is to construct the family of polynomials <math_exp> where <math_exp> is the number of fixed points.  Then the number we want is <math_exp>.  It turns out we can compute all these numbers at the same time because the bivariate generating function is <math_exp> Then the family of numbers we want is <math_exp> evaluated at <math_exp>, which (as it is not hard to verify) is <math_exp>.  In fact this is true for the second derivative and all higher derivatives as well; in particular, the variance of the number of fixed points is also <math_exp>. What if we want to know the expected number and variance of, say, the total number of cycles?  Now we want to look at the family of polynomials <math_exp> where <math_exp> is the total number of cycles.  Then the number we want is <math_exp>.  Now it turns out that the bivariate generating function is <math_exp> (which should be interpreted as <math_exp>).  The partial derivative <math_exp> evaluated at <math_exp> is now <math_exp> where <math_exp> is the <math_exp> harmonic number.  Thus the expected number of cycles of a permutation of <math_exp> elements is about <math_exp>.  (It actually turns out that the expected number of cycles of length <math_exp> is <math_exp>, from which this result immediately follows.)  The second partial derivative evaluated at <math_exp> is <math_exp> where <math_exp>; I'm not sure of the asymptotic growth of this sequence, though, but whatever it is, the variance of the total number of cycles is <math_exp>.  (In any case <math_exp>, so the variance is less than or equal to <math_exp>, and this is probably about right asymptotically.)  One can deduce asymptotics for these kind of sequences using methods such as those in Flajolet and Sedgewick's Analytic Combinatorics, which is my best guess for more examples of using generating functions in this way.  There are probably examples there related to statistics of trees. All the generating function identities I used above are a consequence of the exponential formula, one version of which is proven and discussed in this blog post. \"}\n",
      "1425 {'Id': '1425', 'Type': 'answer', 'ParentId': '1413', 'urls': [], 'exp': ['p'], 'Body': 'You can\\'t avoid finite fields if you do complex algebraic geometry, just as you can\\'t avoid (and it is an advantage to utilize) variables that square to zero.  Some major techniques work by reduction mod <math_exp> and there aren\\'t always substitutes that accomplish the same directly in characteristic 0. A lot of algorithms work by reducing modulo many primes and then patching together the results (Chinese remainder theorem).  For the analysis of what at happens the primes you need to understand finite fields, and in some cases, p-adic fields. Error-correcting codes have a fundamental significance as constructions of \"uniformly distributed\" or \"well spaced\" objects in many dimensions.  This is true irrespective of their additional commercial, cryptographic or computational significance.  And it happens that the comprehensible part of error-correcting code theory is the part related to linear algebra, number theory, and algebraic geometry over finite fields.  To the extent that error-correcting codes are seen to be a model for phenomena in nature or its mathematical idealizations (crystals, packings, coverings, spin glasses, combinatorial phase transitions, etc) then routine use of finite fields is unavoidable, even if you don\\'t care about number theory per se. '}\n",
      "1426 {'Id': '1426', 'Type': 'answer', 'ParentId': '1204', 'urls': [], 'exp': ['R^{(0 | 1)}', 'Z/2'], 'Body': 'If you are willing to build a formalism of supergeometry then some constructions are easier to state in this language, notably differential forms and the De Rham complex.  The first one is just functions from the odd line (<math_exp>) to your manifold, and the De Rham differential comes from super-diffeomorphisms acting on that line. Also, there is a symplectic/orthogonal duality in representation theory that some people (most famously Kontsevich) have advocated as best understood in terms of Lie superalgebras. Constructions using a formally negative-dimensional object (or calculations that look like they might come from such an object) sometimes can be interpreted in terms of bona fide <math_exp> graded objects whose superdimension (even dimension minus odd dimension) is the negative dimension in question. edit: you will get more knowledgeable answers if you post to Mathoverflow, where some of the contributors have written papers on super- or noncommutative geometry. '}\n",
      "1427 {'Id': '1427', 'Type': 'answer', 'ParentId': '530', 'urls': [], 'exp': ['R_0', 'K', 'x_0', '(x_0)', 'x_0', 'R_0', '(x_0)', '(x_0^2)', '(x_0^3)', 'R_i', 'R_{i+1}', 'x_{i+1}', 'R_i[x_{i+1}]', '(x_{i+1}^2 - x_i)', '(x_{i+1})', 'R_i : (x_{i+1})', '(x_{i+1}^2)=(x_i)', '(x_i^2)', 'R', 'R_i', 'x_i'], 'Body': \"How about this construction: Define a domain <math_exp> as follows. Take a field <math_exp>, adjoin an indeterminate <math_exp>, and localize at <math_exp> (that is, adjoin inverses to everything not a multiple of <math_exp>). <math_exp> has all its ideals principal and linearly ordered: <math_exp> contains <math_exp> contains <math_exp>... Now given <math_exp>, define <math_exp> inductively: Adjoin an indeterminate <math_exp>, so we have <math_exp>. Quotient by <math_exp>. Finally, localize at the prime ideal <math_exp>. This effectively just gives us one more principal ideal containing all the principal ideals from <math_exp> contains <math_exp> contains <math_exp>... Now let <math_exp> be the union of all the <math_exp>, and it's obvious that any finitely generated ideal is principal, but there's a non-fg one generated by all the <math_exp>. \"}\n",
      "1429 {'Id': '1429', 'Type': 'answer', 'ParentId': '1422', 'urls': ['http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.pja/1200672011&amp;page=record'], 'exp': ['\\\\phi', 'f', '\\\\sqrt{x}', 'x^2', 'f', 'x', 'f', '\\\\phi', '\\\\mathbb{R} \\\\to \\\\mathbb{R}', 'f', '\\\\mathbb{R}', '\\\\mathbb{C}', 'f', '\\\\phi', 'f^{(n)}', 'n', 'f'], 'Body': \"Since I can't leave comments I'm writing this here.  I think this question is made difficult by the condition that <math_exp> is just required homeomorphism versus say a diffeomorphism. In the case n = 1 you can certainly come up with continuous functions that are not differentiable on a discrete set but can be pulled back to yield a polynomial.  As a baby example consider the function <math_exp> that is <math_exp> on the positive reals and x on the negative reals.  Consider the homeomorphism that is <math_exp> on the positive reals and x on the negative reals, then <math_exp> pulls back to the polynomial <math_exp>. I don't think its sufficient that <math_exp> doesn't attain the same value infinitely many times.  I don't have a counterexample but I think a candidate might be contained in this article. The gist is that there are functions everywhere continuous and strictly monotonic but with derivative 0 almost everywhere. I think you'd have more luck using the implicit function theorem if you required <math_exp> to be a diffeomorphism.  Also I believe its true that 'most' continuous function from <math_exp> are not very nice (nonwhere differential) so a more tractable question might be the same question but requiring <math_exp> to be smooth. If you replace <math_exp> with <math_exp> and impose <math_exp> and <math_exp> both be holomorphic then I think it suffices that <math_exp> vanish for all sufficiently large <math_exp> because you can recover <math_exp> from its taylor series. \"}\n",
      "1431 {'Id': '1431', 'Type': 'question', 'Title': 'Prove that this function is bounded', 'Tags': ['real-analysis', 'functional-equations', 'asymptotics'], 'AcceptedAnswerId': '1470', 'urls': ['http://www.artofproblemsolving.com/Forum/viewtopic.php?t=290872'], 'exp': ['f : [0, 1) \\\\to \\\\mathbb{R}', '\\\\displaystyle f(x) = \\\\log_2 (1 - x) + x + x^2 + x^4 + x^8 + ...', 'f', 'f(x^2) = f(x) + \\\\log_2 (1 + x) - x'], 'Body': \"This is an exercise from Problems from the Book by Andreescu and Dospinescu.  When it was posted on AoPS a year ago I spent several hours trying to solve it, but to no avail, so I am hoping someone here can enlighten me. Problem:  Prove that the function <math_exp> defined by <math_exp> is bounded. A preliminary observation is that <math_exp> satisfies <math_exp>.  I played around with using this functional equation for awhile, but couldn't quite make it work. \"}\n",
      "1432 {'Id': '1432', 'Type': 'answer', 'ParentId': '1401', 'urls': ['http://www.ams.org/amsmtgs/2124_abstracts/1056-z1-379.pdf'], 'exp': [], 'Body': \"At the last joint meetings in SF there was a talk about exactly this. http://www.ams.org/amsmtgs/2124_abstracts/1056-z1-379.pdf I went to the talk but I don't remember the explicit isomorphism; but at the very least a very concrete answer exists. \"}\n",
      "1433 {'Id': '1433', 'Type': 'answer', 'ParentId': '530', 'urls': ['http://en.wikipedia.org/wiki/Bezout_domain'], 'exp': [], 'Body': 'See http://en.wikipedia.org/wiki/Bezout_domain '}\n",
      "1434 {'Id': '1434', 'Type': 'answer', 'ParentId': '1423', 'urls': ['http://www.jmilne.org/math/apocrypha.html'], 'exp': [], 'Body': 'Although I am not (by any stretch) an expert on finite simple groups, let me flesh out my above comment. Consider the following QCFSG (i.e., \"qualitative\" CFSG): with only finitely many exceptions, every finite simple group has prime order, is alternating, or is one of the finitely many known infinite families of Lie type.  QCFSG must have been conjectured rather early on, whereas the exact statement of CFSG was much harder to come by, as much of the early work on the classification problem resulted in discovery of new sporadic groups. I guess that early on someone must have looked at the nonsporadic finite simple groups and noticed that, except for the two exceptions listed above, they have distinct orders. [Assuming this is actually true, that is.  I have no reason to doubt it, but I haven\\'t checked it myself.]  Once you notice that, if you believe QCFSG, then you certainly think that the order of a simple group determines the group up to finitely many exceptions.  It is very hard for me to imagine how you could prove that the number of exceptions is precisely two without knowing the full CFSG. I cannot resist conveying a story of Jim Milne, whose moral is that you shouldn\\'t feel too bad when you say something absolutely stupid in public: better mathematicians than you or I have said stupider things. Finally, a story to keep in mind the next time you ask a totally stupid question at a major lecture. During a Bourbaki seminar on the status of the classification problem for simple finite groups, the speaker mentioned that it was not known whether a simple group (the monster) existed of a certain order. \"Could there be more than one simple group of that order?\" asked Weil from the audience. \"Yes, there could\" replied the speaker. \"Well, could there be infinitely many?\" asked Weil. For the source, and for some further fun stories, see http://www.jmilne.org/math/apocrypha.html '}\n",
      "1435 {'Id': '1435', 'Type': 'answer', 'ParentId': '804', 'urls': [], 'exp': [], 'Body': \"Sorry, I don't have enough reputation to comment on Andrea Ferretti's answer, but that proof of Wedderburn's theorem is also given in detail in Herstein's Topics in Algebra. \"}\n",
      "1436 {'Id': '1436', 'Type': 'answer', 'ParentId': '1388', 'urls': [], 'exp': ['\\\\zeta_n', 'n', '\\\\omega_1 = \\\\sqrt[5]{\\\\left(\\\\frac{66}{3125}\\\\zeta_{5} + \\\\frac{451}{3125}\\\\zeta_{5}^{2} + \\\\frac{176}{3125}\\\\zeta_{5}^{3} + \\\\frac{286}{3125}\\\\zeta_{5}^{4}\\\\right)}', '\\\\omega_2 = \\\\sqrt[2]{ - \\\\frac{11}{20} + \\\\left(\\\\frac{1}{4}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1 + \\\\left( - \\\\frac{5}{44}\\\\zeta_{5} + \\\\frac{15}{44}\\\\zeta_{5}^{2} + \\\\frac{5}{44}\\\\zeta_{5}^{3} + \\\\frac{5}{44}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1^2 + \\\\left(\\\\frac{25}{121}\\\\zeta_{5} - \\\\frac{75}{242}\\\\zeta_{5}^{2} - \\\\frac{75}{484}\\\\zeta_{5}^{3} + \\\\frac{75}{242}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1^3 + \\\\left( - \\\\frac{375}{2662}\\\\zeta_{5} + \\\\frac{625}{1331}\\\\zeta_{5}^{2} + \\\\frac{625}{2662}\\\\zeta_{5}^{3} + \\\\frac{4375}{5324}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1^4}', ' - \\\\frac{1}{10} + \\\\frac{1}{2}\\\\omega_1 + \\\\left( - \\\\frac{5}{22}\\\\zeta_{5}^{2} - \\\\frac{5}{11}\\\\zeta_{5}^{3} + \\\\frac{5}{11}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1^2 + \\\\left(\\\\frac{75}{242}\\\\zeta_{5} + \\\\frac{150}{121}\\\\zeta_{5}^{2} + \\\\frac{75}{121}\\\\zeta_{5}^{3} + \\\\frac{125}{121}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1^3 + \\\\left(\\\\frac{1625}{1331}\\\\zeta_{5} + \\\\frac{1000}{1331}\\\\zeta_{5}^{2} + \\\\frac{5125}{2662}\\\\zeta_{5}^{3} + \\\\frac{375}{1331}\\\\zeta_{5}^{4}\\\\right)\\\\omega_1^4-\\\\omega_2', '\\\\LaTeX'], 'Body': 'Just for fun: Gap+RadiRoot tells me that if <math_exp> is a primitive <math_exp>th root of unity, and we set <math_exp>, <math_exp>, the roots of the polynomial are given by <math_exp> NB: I generated this (in a gap4 session, with RadiRoot properly installed) as follows: That created a <math_exp> file, /tmp/tmp.sfoZ6C/Nst.tex, containing the formulas '}\n",
      "1437 {'Id': '1437', 'Type': 'question', 'Title': 'Solve an equation with linear and exponential functions, <span class=\"math-container\" id=\"11896\">x=10^{x/10}</span>', 'Tags': ['algebra-precalculus', 'exponential-function'], 'AcceptedAnswerId': '1440', 'urls': [], 'exp': [' x = 10^{x/10} '], 'Body': 'How to solve this equation? <math_exp> '}\n",
      "1438 {'Id': '1438', 'Type': 'answer', 'ParentId': '1431', 'urls': [], 'exp': ['(1-x)^{-1} = (1+x)(1+x^2)(1+x^4) \\\\dots', '\\\\log(2)', '\\\\Sigma (x^{2^k} - C\\\\log(1 + x^{2^k}))', 'n', 'n - Cn\\\\log(2)', 'x \\\\to 1-', 'C = 1/\\\\log(2)'], 'Body': 'Starting from (the natural logarithm of) <math_exp>, it becomes clearer where the <math_exp> factor comes from. One has to show that <math_exp> is bounded sum of positive terms.  The sum of the first <math_exp> terms approaches <math_exp> as <math_exp>, so we need <math_exp> if there is to be boundedness. '}\n",
      "1440 {'Id': '1440', 'Type': 'answer', 'ParentId': '1437', 'urls': [], 'exp': ['x = 10', ' 10', ' 1', 'x \\\\le 0', '(0, 10)', '(1, 2)', '1.37'], 'Body': 'There is an obvious solution <math_exp>.  For <math_exp> the derivative of the RHS is at least <math_exp> so there are no solutions.  For <math_exp> there are obviously no solutions.  By the IVT there is a solution in <math_exp>, and by convexity this solution is unique.  In fact this solution is in <math_exp>.  It can be expressed using the Lambert W-function, but it is really not worth writing down explicitly.  Numerically it is about <math_exp>. '}\n",
      "1441 {'Id': '1441', 'Type': 'answer', 'ParentId': '944', 'urls': [], 'exp': ['n', 'n', '(2n)^{100}', 'P \\\\neq NP'], 'Body': 'The point is that SHORTPROOF is an NP-complete problem: given a sentence of length <math_exp> in the language of some formal proof system (ZFC, Peano arithmetic, etc), does it have a proof of length at most some fixed polynomial in <math_exp>, such as <math_exp>?  It\\'s in NP because for a reasonable formal system you can check a given proof fairly quickly.  This problem was considered in Goedel\\'s letter to von Neumann that implicitly stated what we now call the <math_exp> question (the heart of the problem, the universality of concrete NP-complete problems, wasn\\'t known until much later). Any NP-complete problem has a zero-knowledge proof protocol for demonstrating solutions of instances, e.g. that we have a SHORTPROOF of the Riemann hypothesis.  These are \"proofs that reveal nothing other than their own validity\". The role of the PCP theorem is to show that the proof protocols (interactive challenge/response games) can be very efficient for any stipulated level of confidence.  The probability that the prover really does have a SHORTPROOF of Riemann, given that we follow the protocol and the prover wins, is at least 99 percent, or whatever specified degree of certainty. '}\n",
      "1442 {'Id': '1442', 'Type': 'question', 'Title': 'Is there a direct proof of this lcm identity?', 'Tags': ['number-theory', 'binomial-coefficients'], 'AcceptedAnswerId': '1655', 'urls': ['http://planetmath.org/kummerstheorem'], 'exp': ['\\\\displaystyle (n+1) \\\\text{lcm} \\\\left( {n \\\\choose 0}, {n \\\\choose 1}, ... {n \\\\choose n} \\\\right) = \\\\text{lcm}(1, 2, ... n+1)', 'p', '{a+b \\\\choose a}', 'a', 'b', 'p'], 'Body': \"The identity <math_exp> is probably not well-known.  The only way I know how to prove it is by using Kummer's theorem that the power of <math_exp> dividing <math_exp> is the number of carries needed to add <math_exp> and <math_exp> in base <math_exp>.  Is there a more direct proof, e.g., by showing that each side divides the other? \"}\n",
      "1445 {'Id': '1445', 'Type': 'answer', 'ParentId': '81', 'urls': ['http://gowers.wordpress.com/'], 'exp': [], 'Body': \"Timothy Gowers' blog is excellent.  Like Terence Tao, he is both a Fields medalist and an excellent writer.  Together their blogs were my first real introduction into how professional mathematicians think, and their writing has taught me a lot, both about mathematics and about mathematical writing.  If you are a serious student of mathematics you will find all the blogs you need by scrolling through their blogrolls. \"}\n",
      "1446 {'Id': '1446', 'Type': 'question', 'Title': 'Interesting properties of ternary relations?', 'Tags': ['logic', 'relations'], 'urls': [], 'exp': [], 'Body': 'Many people are familiar with some properties of binary relations, such as reflexivity, symmetry and transitivity. What are the commonly studied properties of ternary (3-ary) relations? If you could provide a motivating example of why the property is interesting that would also be helpful. '}\n",
      "1447 {'Id': '1447', 'Type': 'answer', 'ParentId': '1446', 'urls': [], 'exp': [], 'Body': 'One interesting kind of ternary relation is the \"betweenness\" relation characterised by the Axioms of Order in Hilbert\\'s Foundations of Geometry. I expect ternary relations are typically studied less because identifying an interesting one requires much more involved definitions than is normally the case for binary relations... '}\n",
      "1448 {'Id': '1448', 'Type': 'answer', 'ParentId': '1446', 'urls': ['http://en.wikipedia.org/wiki/Steiner_system'], 'exp': [], 'Body': 'One interesting example is \"being Steiner triple system\" (and this is has a connection with Qiaochu Yuan\\'s comment: any Steiner triple system defines commutative quasigroup). '}\n",
      "1449 {'Id': '1449', 'Type': 'answer', 'ParentId': '1442', 'urls': [], 'exp': ['n + 1 = 6', '6', '1, 5', '10', 'n + 1', 'n + 1'], 'Body': 'It will need something clever.  For <math_exp>, you need <math_exp> times the lcm of <math_exp> and <math_exp> in order to get enough powers of 2 ( and 3 ). Is there a characterization of those <math_exp> where the entire factor of <math_exp> is needed, and not the lcm, for the left hand side? '}\n",
      "1450 {'Id': '1450', 'Type': 'answer', 'ParentId': '1401', 'urls': ['http://www.mccme.ru/free-books/matprosd.html'], 'exp': ['\\\\mathbb{P}^1(\\\\mathbb{F}_7)', '\\\\mathbb{P}^1(\\\\mathbb{F}_7)', 'X', '\\\\mathbb{P}^2(\\\\mathbb{F}_2)', 'X', 'PSL_2(\\\\mathbb F_7)', 'P', 'L', 'P', 'L', '\\\\mathbb{P}^2(\\\\mathbb{F}_2)', 'PSL_2(\\\\mathbb{F}_7)\\\\to PSL_3(\\\\mathbb F_2)', 'X', '2^X', '\\\\mathbb F_2', 'V', '\\\\langle X\\\\rangle', 'Q', 'Gr(2,4)', 'PGr(1,3)', '\\\\mathbb P(V)', 'X', 'Q', '\\\\mathbb P^3', 'A_8\\\\to PSL_4(\\\\mathbb F_2)', 'X=\\\\mathbb P^1(\\\\mathbb F_7)', 'PSL_2(\\\\mathbb F_7)\\\\subset A_8', 'PSL_2(\\\\mathbb F_7)\\\\to PSL_3(\\\\mathbb F_2)\\\\subset PSL_4(\\\\mathbb F_2)'], 'Body': '(Idea of a proof from V.Dotsenko\\'s paper in http://www.mccme.ru/free-books/matprosd.html (in Russian)) Consider 28-element set of 4-tuples of points on <math_exp> with cross-ratio equal to 3. Identifying 4-tuple with its complement (recall that there are exactly 8 points on <math_exp>) one gets 14-element set <math_exp> — exactly the number of points + the number of lines on <math_exp>. The set <math_exp> consists of 2 orbits of <math_exp>, <math_exp> and <math_exp>. Define a tuple from <math_exp> and a tuple from <math_exp> to be incident if they intersect by 2 elements. Claim: 1) result is indeed <math_exp>; 2) induced homomorphism <math_exp> is an isomorphism. <img src=\"https://i.stack.imgur.com/VkZ9B.png\" alt=\"Fano plane\"> Update (2 years later). This is just Klein correspondence! For any 8-element set <math_exp> its powerset <math_exp> is a vector space over <math_exp>. Let <math_exp> be the quotient of the subspace generated by subset with even number of elements by <math_exp>. Now the set <math_exp> of 4-tuples of points of X up to complement can be viewed as a Klein quadric (aka <math_exp>, aka <math_exp>) inside <math_exp>. Permutations of <math_exp> act on planes on <math_exp>, and these planes correspond to points and planes of <math_exp>. Now even permutations map points to points and planes to plains and this gives isomorphism <math_exp>. Now for <math_exp> its restriction on <math_exp> gives isomorphism <math_exp>. '}\n",
      "1452 {'Id': '1452', 'Type': 'answer', 'ParentId': '1437', 'urls': [], 'exp': ['y = x', 'y = 10^{x/10}'], 'Body': 'You can study and graph the two functions <math_exp> and <math_exp>. <img src=\"https://i.stack.imgur.com/HNeUw.png\" alt=\"Graph of y=x and y=10^(x/10)\"> From which you can see that there are only two solutions. '}\n",
      "1453 {'Id': '1453', 'Type': 'question', 'Title': 'Intutive explanation of the PCP Theorem', 'Tags': ['proof-theory'], 'AcceptedAnswerId': '1606', 'urls': ['http://en.wikipedia.org/wiki/PCP_theorem', 'http://en.wikipedia.org/wiki/NP_(complexity)', 'http://en.wikipedia.org/wiki/Probabilistically_checkable_proof', 'http://en.wikipedia.org/wiki/Query_complexity'], 'exp': [], 'Body': 'The PCP theorem states that: Every decision problem in NP has   probabilistically checkable proofs of   constant query complexity and   logarithmic randomness complexity. Can anyone give an intuitive explanation of how this can be done? Links '}\n",
      "1454 {'Id': '1454', 'Type': 'question', 'Title': 'Symmetric nash equilibrium', 'Tags': ['game-theory', 'nash-equilibrium'], 'AcceptedAnswerId': '1544', 'urls': ['http://people.ischool.berkeley.edu/~hal/Papers/2006/position.pdf', 'http://en.wikipedia.org/wiki/Symmetric_equilibrium'], 'exp': ['x', 'B_i', 'i^\\\\text{th}', 'i^\\\\text{th}', 'P_i', 'B_{i+1}', 'i^\\\\text{th}', 'i^\\\\text{th}', 'i^\\\\text{th}', '(v_s-p_s)x_s\\\\ge(v_s-p_t)x_t', 's', '(v_s-p_s)x_s\\\\ge(v_s-P_{t-1})x_t', 't &lt; s', '(v_s-p_s)x_s\\\\ge(v_s-p_t)x_t', 't', 's'], 'Body': \"I was reading this paper on position auctions for web ads. Basically, there are N slots each with an expected number of clicks (in a particular time period) <math_exp>. Each agent makes a bid <math_exp> of how much they are willing to pay per click. The bids are put in decreasing order and agent who makes the <math_exp> highest bid receives receives the slot with the <math_exp> highest click through rate for a price <math_exp> equal to <math_exp> (except for the last agent who pays nothing). To obtain the least price to win the <math_exp> slot, we note that we have to beat the <math_exp> agent's bid if we are moving up, but that we only have to beat the <math_exp> agents price if we are moving down. We easily obtain the following equations for Nash Equilibria: <math_exp> for <math_exp> <math_exp> for <math_exp> The paper then defines the symmetric Nash equilibrium to be a set of prices with: <math_exp> for all <math_exp> and <math_exp> Basically, instead of having the second part of the previous conditions, the first part of the previous equations is valid anywhere. Is the symmetric Nash equilibrium defined more generally? In particular, is it the same as the symmetric equilibrium in this Wikipedia article \"}\n",
      "1456 {'Id': '1456', 'Type': 'answer', 'ParentId': '1446', 'urls': ['http://en.wikipedia.org/wiki/Pythagorean_triple'], 'exp': [], 'Body': 'Pythagorean triples induce a ternary relation that has many interesting properties. '}\n",
      "1457 {'Id': '1457', 'Type': 'answer', 'ParentId': '457', 'urls': [], 'exp': [], 'Body': 'How about probability &amp; statistics?  Not exactly physics, but lots of applications which can be demonstrated with empirical data.  Any example where \"taking an average\" seems reasonable is amenable to finding a distribution.  Many examples: frequencies of arrival (traffic, say) as Poisson or negative binomial; arrival times as geometric; insurance claims as lognormal or gamma (or other more complex skewed distributions, but no need to get that complicated); percentiles as beta; human physical characteristics as normal.  Depending upon your course, you could even take empirical data and try fitting distributions using various techniques, which employ calculus, numerical methods, power series (e.g. moments), etc. '}\n",
      "1458 {'Id': '1458', 'Type': 'answer', 'ParentId': '457', 'urls': [], 'exp': ['SO(3)'], 'Body': \"I cannot resist mentioning the waiter's trick as a physical demonstration of the fact that <math_exp> is not simply connected. For those who don't know it, it is the following: you can hold a dish on your hand and perform two turns (one over the elbow, one below) in the same direction and come back in the original position. I guess one can find it on youtube if it is not clear. To see why the two things are related, I borrow the following explanation by Harald Hanche-Olsen on MathOverflow: Draw a curve through your body from a stationary point, like your foot, up the leg and torso and out the arm, ending at the dish. Each point along the curve traces out a curve in SO(3), thus defining a homotopy. After you have completed the trick and ended back in the original position, you now have a homotopy from the double rotation of the dish with a constant curve at the identity of SO(3). You can't stop at the halfway point, lock the dish and hand in place, now at the original position, and untwist your arm: This reflects the fact that the single loop in SO(3) is not null homotopic. \"}\n",
      "1459 {'Id': '1459', 'Type': 'question', 'Title': 'Constructing a counterexample in category theory', 'Tags': ['category-theory', 'universal-algebra'], 'AcceptedAnswerId': '1491', 'urls': [], 'exp': ['10', 'A \\\\times B', 'A+', 'A \\\\times (B+C)', '(A\\\\times B)+(A \\\\times C)', '(A \\\\times B) \\\\times C', 'A \\\\times (B \\\\times C)', 'A \\\\times (B \\\\times C)', '(A \\\\times B) \\\\times C', 'A,B,C,A \\\\times B,A \\\\times,C,B+C,A\\\\times (B+C),(A\\\\times B)+(A\\\\times C)', 'A \\\\times B', 'A \\\\times B', 'A', 'B', 'A\\\\times (B+C)', '(A\\\\times B)+(A \\\\times C)', 'A\\\\times (B+C)', '(A\\\\times B)+(A \\\\times C)'], 'Body': 'Exercise <math_exp> in Geroch\\'s Mathematical Physics asks whether direct products distribute over direct sums in arbitrary categories. (They do in the category of sets, which is what motivates the question). That is, (using <math_exp> to mean the direct product and <math_exp>B to be the direct sum), whether there is an isomorphism between <math_exp> and <math_exp>. An earlier question asked you to show that associativity of products holds. <math_exp> is isomorphic to <math_exp>. This is true, even when not all objects have products (i.e. it is true that if  <math_exp> and <math_exp> exist, then they are isomorphic...). So I take this question to not require that all objects have products... I don\\'t believe there is the relevant isomorphism, but I\\'m not sure what I\\'ve done proves it. So my question is: Is the following argument a good way of approaching category theory questions? So I wrote out a <math_exp> and drew in all the arrows I know exist from the definitions of products and sums (e.g. <math_exp> guarantees that there is an arrow from <math_exp> to <math_exp> and one to <math_exp>...). Then I observed that this diagram has no arrows that go from <math_exp> to <math_exp> or the other way. That is, even allowing for composition of arrows, there need not be a morphism between the two above mentioned objects. So, I said, this means that this diagram is a diagram of a category where <math_exp> is not isomorphic to <math_exp>, since if there are no morphisms between them, there can\\'t be any isomorphisms. Is this a good way of constructing counterexamples in category theory? Is there any rigorous discussion of \"diagrams\" used this way? (I\\'ve heard it mentioned, but I don\\'t know where to look). This is a question about whether the strategy I am using is a good one, not really about the actual truth value of the statement in question. '}\n",
      "1460 {'Id': '1460', 'Type': 'answer', 'ParentId': '1459', 'urls': [], 'exp': [], 'Body': 'I am not an expert on this topic, so forgive me if this is obvious.  I see what you have done, but isn\\'t this a rather \"vacuous\" category?  Products and coproducts satisfy universal properties in that we can factor through them with respect to other morphisms.  For example, if there are arrows from B and C each to A, then suddenly you have a new arrow from B+C to A which is not in your diagram Now here\\'s where I lose the thread: if you assume products and coproducts exist for all objects A, B, and C, must not some other arrows exist between A, B, and C?  If not, then you may be OK, but I just don\\'t know if you can assume that.  The definitions of (co)products reference such other arrows, so if there are none, then aren\\'t all the (co)products vacuous? '}\n",
      "1461 {'Id': '1461', 'Type': 'question', 'Title': 'Time complexity to find the extremal value of a function', 'Tags': ['computer-science', 'algorithms'], 'AcceptedAnswerId': '1469', 'urls': [], 'exp': ['O(f(n))', '\\\\{a_i\\\\}', 'n', 'a_1, a_2, \\\\ldots, a_n', 'f(n)\\\\geq n', 'O(f(n))', 'O(n)', 'O(1)'], 'Body': 'Let <math_exp> be the minimum time complexity for output a real sequence <math_exp>. The input of the algorithm is a integer <math_exp>. It output the finite sequence <math_exp>. (Clearly <math_exp>.) What can we say about the time complexity of \\\\begin{equation*} \\\\max(a_1, a_2, \\\\ldots, a_n)?  \\\\end{equation*} Is there always an algorithm to solve this problem in time complexity less than <math_exp>? For example, the algorithm that output the sequence of natural numbers is a <math_exp> algorithm. Finding the maximum value between 1 and n is <math_exp>. If we have some more information, like Kolmogorov complexity of the sequence, how will it change the time bounds? It seems that if a sequence can be described easily, we can find a way to find the maximum value easier than naively compute the sequence and compare. '}\n",
      "1463 {'Id': '1463', 'Type': 'answer', 'ParentId': '1459', 'urls': [], 'exp': ['A\\\\times(B+C)', '(A\\\\times B)+(A\\\\times C)', 'A+(B\\\\times C)', '(A+B)\\\\times(A+C)', '\\\\mathcal{C}', '\\\\mathcal{C}^{\\\\mathrm{op}}'], 'Body': 'The dual to the statement that <math_exp> is isomorphic to <math_exp> is that <math_exp> is isomorphic to <math_exp>. Now if this statement fails in some category <math_exp> then the original fails in the opposite category <math_exp>. '}\n",
      "1464 {'Id': '1464', 'Type': 'answer', 'ParentId': '1347', 'urls': ['http://www.jstor.org/pss/2323257'], 'exp': [], 'Body': 'An alternative to the horosphere model ... In \"A Euclidean Model for Euclidean Geometry\", Adolf Madur discusses a Disk model of the Euclidean plane. (Madur says that David Gans has priority for discussing this model, so I\\'ll call it the \"Gans Disk\".) The \"lines\" consist of diameters of the Disk, and half-ellipses that have a diameter as a major axis; the measure of the angle between two \"lines\" is defined as the traditional measure of the angle between their respective major axes. With an appropriate metric (which I have forgotten, and which is just missing in the document preview linked), we get all of the Euclidean plane crammed into the Disk. Overlaying the Gans Disk on the Poincaré Disk (or a sub-disk thereof) provides another way for Hyperbolians to study Euclidean geometry. They just have to agree to treat these half-ellipse paths (which I don\\'t think are ellipses to them) as \"lines\", and to alter their concept of angle measure and length accordingly. This model might be considerably harder for Hyperbolians to wrap their minds around than the horosphere model, though. Edit. Since ellipses are projections of tilted circles, we can \"lift\" the Gans Disk to a \"Gans Hemisphere\". (This is actually a middle phase in the derivation of the Gans Disk model.) There, the \"lines\" are great semi-circles, with angles measured via their diameters in the equatorial plane. Not a major refinement of the Gans Disk, but at least the \"lines\" are naturally-occurring geometric objects, instead of the contrived ellipse-paths. Of course, the metric would need adjustment; off the top of my head, I don\\'t know how much more (or less?) complicated that metric would be. '}\n",
      "1465 {'Id': '1465', 'Type': 'question', 'Title': 'Recreating an Integer Sequence After Convolution', 'Tags': ['number-theory', 'combinatorics'], 'AcceptedAnswerId': '1468', 'urls': [], 'exp': ['N', 'A_{k}', 'B', 'L &gt; 0', '1 \\\\leq n \\\\leq N', 'S_{L}(n)', 'P(n)', 'e_{j}(k) = 1', 'j = k', '0', 'P(n)', 'A_{k}', 'A_{k}'], 'Body': \"...and encoding it as a probability distribution. Suppose we have a sequence of non-negative integers that is periodic with period <math_exp>: \\\\begin{equation*} A_{1},A_{2},...,A_{N},A_{1}... \\\\end{equation*} Each <math_exp> takes on a value no greater than some constant <math_exp>: \\\\begin{equation*} 0 \\\\leq A_{k} \\\\leq B \\\\end{equation*} We then take this sequence and do a simple convolution, for some constant <math_exp> and <math_exp>: \\\\begin{equation*} S_{L}(n) = A_{n} + A_{n+1} +...+ A_{n+L-1}. \\\\end{equation*} From <math_exp> we then form a probability distribution <math_exp> which gives the frequency of each of its values. Let <math_exp> if <math_exp> and <math_exp> otherwise. Then: \\\\begin{equation*} P(n) = (e_{n}(S_{L}(1)) + e_{n}(S_{L}(2)) +...+ e_{n}(S_{L}(N))) / N. \\\\end{equation*} What I would like to find out is the extent to which this process can be reversed. I have two data points: 1) I know (pretty much) everything about the probability distribution <math_exp>: the distribution itself, its mean, range, variance, skewness, kurtosis, etc. 2) I can tell you the frequency of values of <math_exp> in one period, so that if the sequence is 1,0,2,3,1,0, I can tell you there are two 0's, two 1's, one 2, and one 3. To what extent am I able to reconstruct the sequence <math_exp> from these two data points? \"}\n",
      "1466 {'Id': '1466', 'Type': 'answer', 'ParentId': '1461', 'urls': [], 'exp': [], 'Body': \"Not exactly what you're looking for, but probably related: global optimization of one-variable real functions is NP-complete. \"}\n",
      "1467 {'Id': '1467', 'Type': 'question', 'Title': 'Cardinality of all cardinalities', 'Tags': ['set-theory', 'cardinals'], 'AcceptedAnswerId': '1471', 'urls': [], 'exp': ['C = \\\\{0, 1, 2, \\\\ldots, \\\\aleph_0, \\\\aleph_1, \\\\aleph_2, \\\\ldots\\\\}', '\\\\left|C\\\\right|'], 'Body': 'Let <math_exp>. What is <math_exp>? Or is it even well-defined? '}\n",
      "1468 {'Id': '1468', 'Type': 'answer', 'ParentId': '1465', 'urls': [], 'exp': ['A_k', '(A_1, A_2, \\\\dots, A_N)', 'A_k', 'L=2', 'N=10', '(1, 1, 0, 0, 0, 1, 1, 0, 0, 0)', '(1, 1, 0, 0, 0, 0, 1, 1, 0, 0)', 'S_L', 'L'], 'Body': \"No, you cannot recover the sequence <math_exp>. As a trivial example, note that any cyclic permutation of <math_exp> would result in the same distribution (and frequencies). But since this is a periodic sequence, you probably don't care about distinguishing between cyclic permutations of the same sequence, so here's another example. Consider L=1. Then your probability distribution is just equivalent to the frequencies, so any permutation of the <math_exp>s would give the same distribution. If L=1 is too degenerate, here's another example with L=2. Say <math_exp>, and <math_exp>. Then, both sequences <math_exp> and <math_exp> would have the same distribution of sums <math_exp>: 2 twice, 1 four times, and 0 four times. You can easily extend this example to any <math_exp>. \"}\n",
      "1469 {'Id': '1469', 'Type': 'answer', 'ParentId': '1461', 'urls': [], 'exp': ['a_n = ', 'n', 'b_n', 'a_n', '2^{2^k} + 1', 'k \\\\leq n', 'n', 'a_n', 'a_n', 'n', 'a_i'], 'Body': \"Let <math_exp> number of 1's in the first <math_exp> terms of another given 0-1 sequence, <math_exp>. For example, <math_exp> could equal the number of odd perfect numbers between 1 and (2n-1), or the number of Fermat primes <math_exp> with <math_exp>, or the number of the first <math_exp> LISP programs that terminate within 4000*(program-length)^4 steps. <math_exp> is non-decreasing, but to find <math_exp> (which is the maximum of the first <math_exp> terms) is as difficult as calculating all the previous <math_exp>. \"}\n",
      "1470 {'Id': '1470', 'Type': 'answer', 'ParentId': '1431', 'urls': [], 'exp': ['f(x) = x \\\\log(2) - \\\\log(1+x)', 'S(x) = f(x) + f(x^2) + f(x^4) + \\\\dots', 'f(0)=f(1)=0', 'f', 'A', '|f(x)| \\\\leq Ax(1-x) = Ax - Ax^2', 'x^{2^k}', '\\\\log(2)', 'f(1)=0'], 'Body': 'OK, a second trick is needed (but it actually finishes the problem).  It is nice and simple enough that it\\'s probably what the authors intended by a \"Book\" solution. Let <math_exp>.  We want to show that <math_exp> is bounded.  Because <math_exp> and <math_exp> is differentiable, we can find a constant <math_exp> such that <math_exp>.  The sum of this bound over the powers <math_exp> is telescopic. Notice that the role of <math_exp> was to ensure that <math_exp>. '}\n",
      "1471 {'Id': '1471', 'Type': 'answer', 'ParentId': '1467', 'urls': [], 'exp': [' \\\\kappa', '\\\\kappa', ' |C|'], 'Body': 'C is not a set - it is infact a proper class. If C were a set, then |C| would be defined. It then follows that |C| would be the largest cardinality, since there is a total order between all the cardinalities, and <math_exp> for every cardinality <math_exp> (every cardinality is equivalent to the set of all smaller cardinalities). But <math_exp> and so there cannot be a largest cardinal. '}\n",
      "1472 {'Id': '1472', 'Type': 'answer', 'ParentId': '698', 'urls': ['https://mathoverflow.net/users/6579/t', 'https://mathoverflow.net/questions/33335/looking-for-a-probability-distribution/33342#33342', 'http://en.wikipedia.org/wiki/Coupon_collector%27s_problem', 'http://algo.inria.fr/flajolet/Publications/book.pdf'], 'exp': ['1−\\\\exp(−t/n)', 't=O(n^{1/2})'], 'Body': 'As answered by T. on MathOverflow. This is equivalent to (among other   names) the Coupon Collector problem.   Your are asking about the distribution   of the number of coupons collected   after t steps, when the total number   of possible coupons is n. http://en.wikipedia.org/wiki/Coupon_collector%27s_problem ADDED: this and related distributions   are also studied under other names   such as Birthday Problem, random   mappings, and random hashing.   Kolchin-Sevastyanov-Chistyakov Random   Allocations, Knuth The Art Of Computer   Programming, vol. 2, and Flajolet &amp;   Sedgwick  all   discuss these problems and may contain   the precise asymptotics of the   distribution you are looking for. III.10 in Flajolet and Sedgewick gives   the Poisson answer <math_exp> when   the ratio is held constant, but other   asymptotic regimes are also of   interest especially in hashing   problems. Birthday problem is when   <math_exp> and one gets statistics   of the number of collisions. For t=n^k   with 1/2  '}\n",
      "1474 {'Id': '1474', 'Type': 'answer', 'ParentId': '1467', 'urls': [], 'exp': ['|X|', 'X', 'X', 'C', 'C'], 'Body': 'The most common way to define the cardinal number <math_exp> of a set <math_exp> is as the least ordinal which is in bijection with <math_exp>. Then <math_exp> is an unbounded class of ordinals, and any such is necessarily a proper class. Since <math_exp> is not a set, it does not have a cardinality. '}\n",
      "1475 {'Id': '1475', 'Type': 'question', 'Title': 'Why can a Venn diagram for $4+$ sets not be constructed using circles?', 'Tags': ['combinatorics', 'geometry', 'logic', 'circles'], 'urls': ['http://math.gmu.edu/~eobrien/Venn4.html', 'http://en.wikipedia.org/wiki/Venn_diagram#Edwards.27_Venn_diagrams'], 'exp': ['$4$', '16', '4', '&lt;4', 'n', 'n=4'], 'Body': 'This page gives a few examples of Venn diagrams for <math_exp> sets. Some examples: <img src=\"https://i.stack.imgur.com/fHbmV.gif\" alt=\"alt text\">  <img src=\"https://i.stack.imgur.com/030jM.gif\" alt=\"alt text\"> Thinking about it for a little, it is impossible to partition the plane into the <math_exp> segments required for a complete <math_exp>-set Venn diagram using only circles as we could do for <math_exp> sets. Yet it is doable with ellipses or rectangles, so we don&#39;t require non-convex shapes as Edwards uses. So what properties of a shape determine its suitability for <math_exp>-set Venn diagrams? Specifically, why are circles not good enough for the case <math_exp>? '}\n",
      "1476 {'Id': '1476', 'Type': 'answer', 'ParentId': '1467', 'urls': ['http://math.uga.edu/~pete/settheorypart1.pdf'], 'exp': ['C', 'X', 'Y \\\\in C', 'X', 'Y'], 'Body': 'This is \"Fact 20\" on page 10 of http://math.uga.edu/~pete/settheorypart1.pdf These are notes on infinite sets from the most \"naive\" perspective (e.g., one of the facts is that every infinite set has a countable subset, so experts will see that some weak form of the Axiom of Choice is being assumed without comment.  But this is consistent with the way sets are used in mainstream mathematics).  It is meant to be accessible to undergraduates.  In particular, ordinals are not mentioned, although there are some further documents -- replace \"1\" in the link above with \"2\", \"3\" or \"4\" -- which describe such things a bit. But I don\\'t see why it is necessary or helpful to speak of ordinals (or universes!) to answer this question. Added: to be clear, I wish to recast the question in the following way: There is no set <math_exp> such that for every set <math_exp>, there exists <math_exp> and a bijection from <math_exp> to <math_exp>. This is easy to prove via Cantor\\'s diagonalization and it sidesteps the \"reification problem for cardinalities\", i.e., we do not need to say what a cardinality of a set is, only to know when two sets have the same cardinality.  I believe this is appropriate for a general mathematical audience. '}\n",
      "1477 {'Id': '1477', 'Type': 'answer', 'ParentId': '804', 'urls': [], 'exp': ['p', 'p', 'G', 'G', 'p', 'p'], 'Body': 'The class equation implies a nontrivial finite <math_exp>-group has a nontrivial center, or more generally that a nontrivial normal subgroup of a nontrivial finite <math_exp>-group <math_exp> contains a nontrivial element of the center of <math_exp>.  (Note: there are infinite groups where every element has <math_exp>-power order and the center is trivial, so the finiteness assumption on the group is important.) This has standard further consequences for finite <math_exp>-groups, although they are no longer direct consequences of the class equation. The class equation itself is a special case of a more general result: the orbit-stabilizer formula for group actions.  That has lots of uses. '}\n",
      "1479 {'Id': '1479', 'Type': 'answer', 'ParentId': '806', 'urls': [], 'exp': ['K', '[L:K] = n', 'O_K', 'O_L', 'K', ' 1', 'K', 'L/K', 'n', 'O_L', 'O_K', 'n', 'n', '\\\\text{SL}_2(O_K)', '{\\\\mathbf P}^1(K)', 'K', 'K', 'O', 'd', 'O', 'd', 'O = O_K', 'K', 'K'], 'Body': \"The class group of a number field <math_exp> can be used to parametrize other objects. 1) If <math_exp>, the possible <math_exp>-module structure of <math_exp> is described by the ideal classes of <math_exp>, although it is still an open question in general to show for each <math_exp> and each ideal class of <math_exp> that there's an extension <math_exp> with degree <math_exp> such that <math_exp> as an <math_exp>-module corr. to that ideal class.  (This is known for small <math_exp>, but not for general <math_exp>.) 2) The orbits of the action of <math_exp> on <math_exp> are in bijection with ideals classes in <math_exp>.  For instance, the action is transitive iff <math_exp> has class number 1. 3) When <math_exp> is a quadratic order with discriminant <math_exp>, the (narrow) class group of <math_exp> describes the primitive quadratic forms of discriminant <math_exp> up to proper equivalence.  Here we need a slightly more general concept than the usual ideal class group (unless <math_exp>). 4) Weierstrass equations for an elliptic curve over <math_exp> up to a standard change of variables are related to ideal classes in <math_exp> (see Silverman's first book on ell. curves, Chap. VIII). \"}\n",
      "1480 {'Id': '1480', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://www.maths.ed.ac.uk/~tl/msci/'], 'exp': [], 'Body': \"I'm also a fan of Tom Leinster's lecture notes, available on his webpage here. In difficulty level, I would say these are harder than Conceptual Mathematics but easier than Categories and Sheaves, and at a similar level as Categories for the Working Mathematician. \"}\n",
      "1481 {'Id': '1481', 'Type': 'answer', 'ParentId': '804', 'urls': ['http://en.wikipedia.org/wiki/Burnside'], 'exp': [], 'Body': \"http://en.wikipedia.org/wiki/Burnside's_lemma Burnside's lemma can be easily deduced from the class equation and is useful in combinatorics \"}\n",
      "1482 {'Id': '1482', 'Type': 'question', 'Title': 'problem with inequality', 'Tags': ['algebra-precalculus', 'inequality'], 'urls': ['https://math.stackexchange.com/questions/1543/tricky-inequality'], 'exp': ['0&lt;1−an/(mb^2)e^{−r(T−t)}&lt;1', 'r, a, b, T, t&gt;0', 'an\\\\leq mb^2', 'mb^2\\\\leq an\\\\leq mb^2e^{rT}', 't&lt; T − (\\\\ln(an) − \\\\ln(mb^2 ))/r', '0&lt;1−\\\\frac{an}{mb^2}e^{−r(T−t)}', 'an \\\\leq mb^2', 'e^{−r(T−t)}&gt;0', '1−\\\\frac{an}{mb^2}e^{−r(T−t)}&lt;1', '\\\\frac{an}{mb^2}e^{−r(T−t)}&gt;0', 'mb^2\\\\leq an\\\\leq mb^2e^{rT}', 't&lt; T − (\\\\ln(an) − \\\\ln(mb^2 ))/r', 'xy&lt;1', 'x,y&gt;0', '0&lt;1−\\\\frac{an}{mb^2}e^{−r(T−t)}&lt;1'], 'Body': 'Question: I want to solve <math_exp>, where <math_exp>. The solution is that either <math_exp> or <math_exp> and <math_exp>. My Attempt: My thoughts are that the first part <math_exp> gives me <math_exp> because <math_exp>, so I have the first part. The second part  <math_exp> does not give me useful information since <math_exp> always. How do I get the other half of the solution ( <math_exp> and <math_exp>)? I also realise that the problem I have to solve reduces to solving <math_exp> where both <math_exp>. Merged from: tricky inequality How do I go about solving <math_exp>, where a,b,T>t>0? I have been stuck here for some time now. '}\n",
      "1483 {'Id': '1483', 'Type': 'answer', 'ParentId': '529', 'urls': [], 'exp': ['\\\\mathbb C', '\\\\mathbb C', '\\\\mathbb R', '\\\\mathbb H', '\\\\mathbb R', '\\\\mathbb C', '\\\\mathbb R', '\\\\mathbb Q', '\\\\mathbb Q', '\\\\dim &gt; 1'], 'Body': \"There is a simple proof of Frobenius's theorem in Lam's book on noncommutative rings, pp. 208--209.  He attributes the argument to Palais. One should consider this theorem to be two theorems: (1) <math_exp> is the only <math_exp>-central division algebra and (2) <math_exp> and <math_exp> are the only <math_exp>-central division algebras. The reason there are so few choices is that <math_exp> is alg. closed and <math_exp> is nearly so. Division algebras with center equal to a particular field can be created using cyclic Galois extensions* and since <math_exp> has such extensions of arb. high degree there are <math_exp>-central division alg. of arb. high dimension. *There are further technical conditions to be satisfied on the cyclic extension in order for the construction of a division algebra to work, e.g., a finite field has a cyclic extension of each degree but there are no central div. alg. of <math_exp> over a finite field.  The relevant technical conditions are satisfied when the base field is the rational numbers. \"}\n",
      "1484 {'Id': '1484', 'Type': 'answer', 'ParentId': '301', 'urls': ['http://en.wikipedia.org/wiki/Post%27s_theorem'], 'exp': ['\\\\Sigma^0_1', '\\\\Pi^0_1', '\\\\Delta^0_1', '\\\\Delta^0_{n+1}', 'n', '\\\\emptyset^{(n)}'], 'Body': \"The general theorem giving the relationship between the arithmetical hierarchy and computability is known as Post's theorem [1]. In part, Post's theorem states that a set of natural numbers is <math_exp> if and only if it is recursively enumerable. If the set is also <math_exp> then its complement is recursively enumerable too. So a <math_exp> set of natural numbers must be computable. A more general consequence of Post's theorem is that a set of natural numbers is <math_exp> if and only if the set is computable from the <math_exp>th Turing jump of the empty set, <math_exp>. 1: http://en.wikipedia.org/wiki/Post%27s_theorem \"}\n",
      "1485 {'Id': '1485', 'Type': 'answer', 'ParentId': '1283', 'urls': ['http://en.wikipedia.org/wiki/Incomplete_gamma_function'], 'exp': ['\\\\Gamma', '\\\\frac{e^{x}\\\\Gamma(n+1,x)}{\\\\Gamma(n+1)}.', '\\\\Gamma(s,x) = \\\\int_x^{\\\\infty} t^{s-1}e^{-t}dt'], 'Body': \"I'm not sure you'll like this, but in terms of the incomplete <math_exp> function, one can get a closed form as <math_exp> The  function is defined as <math_exp>. \"}\n",
      "1486 {'Id': '1486', 'Type': 'answer', 'ParentId': '198', 'urls': ['http://en.wikipedia.org/wiki/G%C3%B6del%E2%80%93Gentzen_negative_translation'], 'exp': ['\\\\phi', '\\\\phi^N', '\\\\phi', '\\\\phi^N', '\\\\phi', 'N', '\\\\phi', '\\\\phi^N', '\\\\phi', 'A \\\\land \\\\lnot A', '\\\\phi^N = A^N \\\\land \\\\lnot A^N'], 'Body': 'Proof theorists have obtained several \"relative consistency\" proofs between classical and constructive theories. These show that if certain theories of classical mathematics are inconsistent, then corresponding theories of constructive mathematics are also inconsistent. These relative consistency results are proved constructively. They show that the consistency problem does not simply disappear if we switch to constructive mathematics. One of the more famous relative consistency techniques uses a \"double negation translation\". This method assigns each formula <math_exp> of a system a corresponding formula <math_exp> (the \"translation\" of <math_exp>). The exact definition of the translation varies from author to author, depending on the system at hand. But the name is somewhat accurate: the definition of <math_exp> involves adding additional negation symbols to <math_exp> in the right places. In 1933, Gödel proved there is a translation <math_exp> of formulas of Peano arithmetic so that whenever a formula <math_exp> is provable in Peano arithmetic, the corresponding formula <math_exp> is provable in the constructive system of Heyting arithmetic. Moreover, if <math_exp> is of the form <math_exp> then Gödel\\'s translation assigns it the formula <math_exp>, which is still contradictory. This means that if Peano arithmetic is inconsistent, so is its constructive counterpart Heyting arithmetic. Gödel\\'s proof is constructive, like you would hope. So if we were only worried about consistency, there would be no advantage to working in Heyting arithmetic instead of Peano arithmetic. But people who work in constructive mathematics do not do it only for the sake of consistency. Constructive proofs carry more information than classical proofs, so constructive provability is interesting even to classical mathematicians. You can read a little more about Gödel\\'s result at this wikipedia article. '}\n",
      "1487 {'Id': '1487', 'Type': 'question', 'Title': 'Connected simple cubic graph', 'Tags': ['graph-theory'], 'AcceptedAnswerId': '1520', 'urls': [], 'exp': [], 'Body': 'I am trying to understand this problem and yes this is from my assignment and I should be doing it myself, but I have been staring at it for 2 hours and not getting anywhere, so decided to post it here. Let G be a connected cubic simple graph that contains 2 edge-disjoint spanning trees show that |G| = 4. '}\n",
      "1488 {'Id': '1488', 'Type': 'answer', 'ParentId': '1487', 'urls': [], 'exp': [], 'Body': 'Express the number of edges in a connected cubic simple graph in terms of the number of vertices. Then do the same for a graph that contains 2 edge-disjoint spanning trees. Solve this equation for the number of vertices. '}\n",
      "1489 {'Id': '1489', 'Type': 'answer', 'ParentId': '1048', 'urls': [], 'exp': [], 'Body': 'To do this rigorously you have essentially no choice as to the logic of the proof. The first definition constructs (sin,cos) as a pair of functions on the unit circle X^2 + Y^2 = 1.  The functions are just Y and X respectively. The second definition constructs another pair (Sin,Cos), of functions on the real line.  The functions are specific power series. To show that these pairs are \"equal\" has a unique meaning: to construct some local identification of the two spaces (a locally invertible parametrization of the circle by the line, and vice versa, i.e., a covering map) such that under this identification (sin,cos) corresponds to (Sin,Cos). This identification is itself unique: the rotationally invariant angle measure in the plane (Haar measure on SO(2,R) in its conventional normalization to have total length 2*Pi).  If angle measure is already available by other means, such as length measurement on the circle, one gets a second construction of (sin,cos) through their inverse functions (using integrals) and one then has to check that arcsin^{-1} = Sin where the function on the left is an integral and the right is a power series. Otherwise we have only the map in one direction, (Cos(t),Sin(t)) from the line to the circle using power series, and one has to check that  Cos^2(t) + Sin^2(t) = 1  and the local invertibility (everywhere nonzero velocity vector) of the map.  This would define angle measure on the circle as \"t\", ie., the inverse of the power-series parametrization of the circle by the line. (Edited to remove Tex, click on the edit-history to see it with the TeX.) '}\n",
      "1490 {'Id': '1490', 'Type': 'question', 'Title': 'Collective Term for <span class=\"math-container\" id=\"12166\">XY, YZ</span> and <span class=\"math-container\" id=\"12167\">ZX</span> Planes', 'Tags': ['geometry', 'terminology', 'euclidean-geometry', 'analytic-geometry'], 'AcceptedAnswerId': '1494', 'urls': [], 'exp': ['XY, YZ', 'ZX', '3D'], 'Body': 'Is there a collective term for the <math_exp> and <math_exp> planes in <math_exp> co-ordinate geometry? I was thinking \"principal planes\" but I\\'m not sure where I heard that. '}\n",
      "1491 {'Id': '1491', 'Type': 'answer', 'ParentId': '1459', 'urls': [], 'exp': [], 'Body': \"What you have done is a sensible start. You have tried proving the result and you suspect it isn't true. In this situation you want to look for a counter example (and just one is enough). My suggestion is the category of non-commutative rings. You still have direct sum. However the product is the free product. (For commutative rings it is the tensor product.) \"}\n",
      "1492 {'Id': '1492', 'Type': 'answer', 'ParentId': '187', 'urls': [], 'exp': [], 'Body': 'Transitive action on the vertices is usually the definition of \"looking the same from each vertex\".   So maybe you have two different groups in mind: The automorphism group of the combinatorics of the tiling; the abstract structure of vertices, faces, edges, etc. The group of Euclidean motions that leave the tiling invariant. Motions meaning isometries of space, where one should also specify whether orientation-reversal is allowed or not.  Any instance of this is also an automorphism of the tiling combinatorics. If tilings that are vertex-transitive under group #1 are \"vertex-uniform\" and those that are vertex-transitive in the stricter sense of group #2 are \"vertex transitive\", then it is easy to give examples where some of the polyhedra are deformations of others (so not isometrically transitive in sense #2).  For example, a one-dimensional periodic tiling with several different sizes of interval will have all vertices equivalent combinatorially but a finite number of distinct vertex types under geometric equivalence. This is maybe too simple to be what Grunbaum had in mind, can you quote the book? '}\n",
      "1493 {'Id': '1493', 'Type': 'answer', 'ParentId': '1401', 'urls': [], 'exp': [], 'Body': 'Can\\'t leave comments yet, but the details of there being only one simple group of order 168, and why PSL(2,7) and PSL(3,2) are order 168 and simple, are spelled out on pages 141-147 in Smith and Tabachnikova\\'s \"Topics in Group Theory\". Steve '}\n",
      "1494 {'Id': '1494', 'Type': 'answer', 'ParentId': '1490', 'urls': [], 'exp': [], 'Body': '\"Principal planes\" (and principal axes, principal projections) are terms used in engineering, architecture and technical drawing when using orthogonal projections (usually several of them) to represent a 3-dimensional image.  Names of other fields where the same words could arise: descriptive geometry, perspective drawing, computer graphics, axonometry, surveying, geodesy, artistic perspective. In mathematics, \"coordinate planes\" is completely standard, as can be seen from the first comment and its multiple upvotes. Apart from being non-standard, use of \"principal\" to refer to the coordinate axes or planes is imprecise, and it conflicts with a web of other (useful because mutually consistent) terms such as principal axes of an ellipsoid or of a stress, principal component analysis in statistics, principal diagonal of a matrix.  One wants to keep the latter terms as they are because they all are expressing the same circle of concepts related to diagonalization of quadratic forms.  So \"coordinate planes\" it is. '}\n",
      "1495 {'Id': '1495', 'Type': 'question', 'Title': 'Learn Probability and Permutation and combination', 'Tags': ['probability', 'learning'], 'AcceptedAnswerId': '1524', 'urls': [], 'exp': [], 'Body': 'I am very weak in solving problem related to probability and permutation and combination. Can you please guide me the good content by which I can understand it better. Thanks in advance. '}\n",
      "1496 {'Id': '1496', 'Type': 'answer', 'ParentId': '250', 'urls': [], 'exp': [], 'Body': \"Every set can be well ordered, I once sat on the bar of my favourite place and ran into this girl I haven't seen for years. I explained a bit about AC, Zorn's Lemma and the Well-Ordering principle and that they are all equivalent. (In another time, my friend told me that if every set can be well-ordered I should tell him the successor of 0 in the real numbers, I answered that 1 is. He then argued that he means the real numbers, and not the natural numbers. I told him that my well-order puts the natural numbers with the usual ordering first, then the rationals and then the irrationals. But I can shift a finite number of positions if he wants me to.) \"}\n",
      "1497 {'Id': '1497', 'Type': 'answer', 'ParentId': '1446', 'urls': [], 'exp': ['L', '\\\\mathfrak{sl}(2)\\\\subset L', 'E', 'F', 'H=[E,F]', 'L', 'L=L_0\\\\oplus L_1\\\\otimes T \\\\oplus \\\\mathfrak{sl}(2)', 'L_1', 'L'], 'Body': 'One class of example arises in Lie theory. Take <math_exp> a simple Lie algebra. Then there is a particular <math_exp>, name take <math_exp> to be a highest root, <math_exp> a lowest root, and <math_exp>. Then decompose <math_exp> as a representation of this subalgebra. You get <math_exp>. Then <math_exp> is a ternary system. This satisfies a (complicated) identity. You can reconstruct <math_exp> from the ternary system and you need this identity for the Jacobi identity. '}\n",
      "1498 {'Id': '1498', 'Type': 'answer', 'ParentId': '1401', 'urls': [], 'exp': ['G=\\\\operatorname{PSL}_2(7)', 'X=P^1(\\\\mathbb{F}_7)', 'p\\\\in X', 'G_p=\\\\{g\\\\in G:g\\\\cdot p=p\\\\}', '\\\\binom{X\\\\setminus\\\\{p\\\\}}{3}', '3', 'X\\\\setminus\\\\{p\\\\}', '7', '7', '21', 'S(2,3,7)', 'P^2(\\\\mathbb{F}_2)', 'o\\\\in\\\\binom{X}{4}', 'G', '14', 'o', 'X', 'o', 'o', '\\\\operatorname{GL}_3(\\\\mathbb{F}_2)\\\\rtimes\\\\mathbb{F}_2^3', '\\\\mathbb{F}_2^3', 'p\\\\in X', '\\\\mathbb{F}_2^3', '3', '\\\\mathbb{F}_2', 'X', '14', \"o'\\\\in\\\\binom{X}{4}\", '3', '\\\\mathbb{F}_2', '\\\\operatorname{PGL}_2(7)', 'X'], 'Body': \"The group <math_exp> acts on <math_exp>. Fix <math_exp>, and consider the action of the stabilizer subgroup <math_exp> on the set <math_exp> of <math_exp>-element subsets of <math_exp>. It has three orbits, of sizes <math_exp>, <math_exp> and <math_exp>; this can be checked by considering cross-ratios. Pick one of the small ones: one can check that it is a triple Steiner system <math_exp>, so it is isomorphic as a design, to <math_exp>. Playing a bit with this construction can be used to realize the isomorphism explicitely. Later. An observation, which helps explainwhy this works, is that if <math_exp> is one of the <math_exp>-orbits of size <math_exp>, then the automorphism group of <math_exp> (that is, the set of permutations of <math_exp> which map elements of <math_exp> to elements of <math_exp>) is <math_exp>, the group of affine maps of <math_exp> (here  \\\\rtimes is supposed to mean crossed product). Fixing an element <math_exp>, as I did above, amounts to picking a 'zero' in <math_exp> viewed as an affine space, that is, looking at it as a vector space. (This puts a structure of affine <math_exp>-space over <math_exp> on <math_exp>, and if we start with the other <math_exp>-element orbit <math_exp> we get another structure of affine <math_exp>-space over <math_exp>; <math_exp> is precisely the set of permutations of <math_exp> which preserves both affine structures) \"}\n",
      "1499 {'Id': '1499', 'Type': 'question', 'Title': 'Proper Measurable subgroups of <span class=\"math-container\" id=\"12171\">\\\\mathbb R</span>', 'Tags': ['real-analysis'], 'AcceptedAnswerId': '1517', 'urls': [], 'exp': ['(\\\\mathbb{R},+)', 'H', '\\\\mathbb{R}', 'H'], 'Body': 'If <math_exp> is a group and <math_exp> is a proper subgroup of <math_exp> then prove that <math_exp> is of measure zero. '}\n",
      "1501 {'Id': '1501', 'Type': 'answer', 'ParentId': '1499', 'urls': ['http://unapologetic.wordpress.com/2010/04/23/lebesgue-measurable-sets/'], 'exp': [], 'Body': 'Your title talks about \"proper measurable subgroups of R\", but the body of your post doesn\\'t require that H be measurable.  The following outline shows that if H is a proper subgroup of R and is measurable, then it must be measure 0.  I\\'m not sure if every subgroup must be measurable or not.... Here is a rough outline of the proof: Lemma 1:  If H is a subset of R and has positive measure, then H-H = {a-b| a,b, in H} contains an interval around 0. Proof:  See http://unapologetic.wordpress.com/2010/04/23/lebesgue-measurable-sets/ For Lemma 2 and 3, I\\'ll leave the proofs to you (but I can add details if you need them). Lemma 2:  If H is a subgroup of G, then H-H=H. Lemma 3:  If H is a subgroup of the real numbers R and contains an interval around 0, then H = R. '}\n",
      "1502 {'Id': '1502', 'Type': 'question', 'Title': 'Definition of a set', 'Tags': ['set-theory', 'definition'], 'AcceptedAnswerId': '1505', 'urls': ['http://en.wikipedia.org/wiki/Russell%27s_paradox'], 'exp': [], 'Body': \"What is a set? I know that results such as Russell's paradox mean that the definition isn't as straight forward as one might expect. \"}\n",
      "1503 {'Id': '1503', 'Type': 'answer', 'ParentId': '1446', 'urls': [], 'exp': ['Q_1', 'Q_2', 'Q_3', \"Q_1(x,y)Q_2(x',y') = Q_3(B,B')\", 'B', \"B'\", \"xx', xy', yx', yy'\", 'ghk = 1'], 'Body': 'So far nobody is actually giving properties, but just examples.  I\\'ll continue that theme. When Gauss defined composition of quadratic forms, on the level of quadratic forms what he defined was not really a law of composition but a ternary relation (three quadratic forms <math_exp>, <math_exp>, and <math_exp> are \"in composition\" if <math_exp> where <math_exp> and <math_exp> are linear in <math_exp>). At the level of proper equivalence classes of quadratic forms this becomes a group law. You could say any group law is defined by a ternary relation <math_exp> on the group.  This fits the geometric description and addition of points on an elliptic curve or Bhargava\\'s interpretation of Gauss\\'s composition. '}\n",
      "1505 {'Id': '1505', 'Type': 'answer', 'ParentId': '1502', 'urls': ['http://plato.stanford.edu/entries/frege-logic/#S2', 'http://mathworld.wolfram.com/Burali-FortiParadox.html', 'http://mathworld.wolfram.com/Zermelo-FraenkelSetTheory.html', 'http://mathworld.wolfram.com/AxiomofChoice.html', 'http://mathworld.wolfram.com/ContinuumHypothesis.html'], 'exp': [], 'Body': 'The intuitive notion of a set is of a collection whose identity is entirely determined by its members. Russell\\'s paradox resulted, in effect, from taking any condition at all to determine a collection of members and thus a set. (Russell formulated his paradox against Gottlob Frege\\'s Basic Law V which has, as a consequence, that every condition determines a set.) In a way, there is no \"one notion of a set.\" In response to Russell\\'s Paradox (and others, such as the Burali-Forti Paradox), various axiomatizations of set theory have been developed to try to capture and render precise the notion of a set. Results due to Goedel and Cohen show that widely accepted principles governing sets as captured by the Zermelo–Fraenkel axiomatization fail to decide some interesting and contentious \"higher\" claims about sets (the Axiom of Choice and the Continuum Hypothesis). So, in a real way, there is not consensus in the mathematical community about what exactly a set is nor about what principles give a full description of sets. (In practice, most mathematicians use Zermelo–Fraenkel set theory and make appeal to the Axiom of Choice, though many prefer to avoid the later when possible. Some refuse to accept it outright.) '}\n",
      "1506 {'Id': '1506', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://en.wikipedia.org/wiki/Category_theory#External_links', 'http://katmat.math.uni-bremen.de/acc/acc.pdf', 'http://www.math.uchicago.edu/~may/VIGRE/VIGRE2009/Awodey.pdf'], 'exp': [], 'Body': \"Wikipedia has some nice free texts linked at the bottom. There's an online version of Abstract and Concrete Categories, for example. Steve Awodey has some lecture notes available online too. (Awodey's newish book is expensive, but probably rather good) Patrick Schultz's answer, and BBischoff's comment on an earlier answer also have good links to freely available resources. \"}\n",
      "1507 {'Id': '1507', 'Type': 'answer', 'ParentId': '1057', 'urls': ['http://www.artofproblemsolving.com/Forum/viewtopic.php?f=61&amp;t=346228'], 'exp': [], 'Body': 'I wrote a short proof here . Steve '}\n",
      "1508 {'Id': '1508', 'Type': 'answer', 'ParentId': '1475', 'urls': ['http://www.ams.org/notices/200611/ea-wagon.pdf', 'http://en.wikipedia.org/wiki/Planar_graph#Euler.27s_formula', 'http://wiki.answers.com/Q/How_do_you_solve_a_four_circle_venn_diagram', 'http://mathworld.wolfram.com/VennDiagram.html', 'http://www.brynmawr.edu/math/people/anmyers/PAPERS/Venn.pdf'], 'exp': [], 'Body': 'The short answer, from a paper by Frank Ruskey, Carla D. Savage, and Stan Wagon is as follows: ... it is impossible to draw a Venn diagram with circles that will represent all the possible intersections of four (or more) sets. This is a simple consequence of the fact that circles can finitely intersect in at most two points and Euler’s relation F − E + V = 2 for the number of faces, edges, and vertices in a plane graph. The same paper goes on in quite some detail about the process of creating Venn diagrams for higher values of n, especially for simple diagrams with rotational symmetry. For a simple summary, the best answer I could find was on WikiAnswers: Two circles intersect in at most two points, and each intersection creates one new region. (Going clockwise around the circle, the curve from each intersection to the next divides an existing region into two.) Since the fourth circle intersects the first three in at most 6 places, it creates at most 6 new regions; that\\'s 14 total, but you need 2^4 = 16 regions to represent all possible relationships between four sets. But you can create a Venn diagram for four sets with four ellipses, because two ellipses can intersect in more than two points. Both of these sources indicate that the critical property of a shape that would make it suitable or unsuitable for higher-order Venn diagrams is the number of possible intersections (and therefore, sub-regions) that can be made using two of the same shape. To illustrate further, consider some of the complex shapes used for n=5, n=7 and n=11 (from Wolfram Mathworld): <img src=\"https://i.stack.imgur.com/JnpeY.jpg\" alt=\"Venn diagrams for n=5, 7 and 11\"> The structure of these shapes is chosen such that they can intersect with each-other in as many different ways as required to produce the number of unique regions required for a given n. See also: Are Venn Diagrams Limited to Three or Fewer Sets? '}\n",
      "1509 {'Id': '1509', 'Type': 'answer', 'ParentId': '370', 'urls': [], 'exp': [], 'Body': \"Awodey's new book, while pricey, is a really pleasant read and a good tour of Category Theory from a logician's perspective all the way up to topos theory, with a more up to date view on categories than Mac Lane. \"}\n",
      "1510 {'Id': '1510', 'Type': 'answer', 'ParentId': '370', 'urls': ['http://www.tac.mta.ca/tac/reprints/'], 'exp': [], 'Body': 'Barr and Wells, in addition to Toposes, Triples and Theories, have written Category Theory for the Computing Sciences, a comprehensive tome which goes through most of the interesting aspects of category theory with a constant explicit drive to relate everything to computer science whenever possible. Both books are available online as TAC Reprints. '}\n",
      "1512 {'Id': '1512', 'Type': 'question', 'Title': 'How to describe the Galois group of the compositum of all quadratic extensions of Q?', 'Tags': ['number-theory', 'group-theory', 'galois-theory'], 'AcceptedAnswerId': '1556', 'urls': ['http://www.colby.edu/personal/fqgouvea/deform.dvi'], 'exp': [], 'Body': 'This is Problem 1.7 from Gouvea\\'s lecture notes on deformations of Galois representations. In particular, he asks you to show that it has many subgroups of finite index which are not closed. So here\\'s what I\\'ve got so far, which may be wrong. I can write the compositum as F = Q[&radic;–1, &radic;2, &radic;3, &radic;5, &radic;7 ...] (can I?) and then the Galois group G = Gal(F/Q)  is isomorphic to a direct product &Pi;p (Z/2Z) where the product is taken over all primes p, as well as p=-1, and the pth component is generated by the conjugation &sigma;p defined by &radic;p -> –&radic;p. An example of a subgroup which isn\\'t closed would be the subgroup H consisting of finite products of conjugations, since for example, H contains the sequence &sigma;2, &sigma;2&sigma;3, &sigma;2&sigma;3&sigma;5, &sigma;2&sigma;3&sigma;5&sigma;7... which converges to the automorphism \"conjugate everything\", and this automorphism is not contained in H. However this subgroup is nowhere near being finite index--it has the cardinality of the natural numbers, whereas G has the cardinality of the reals. The only finite index subgroups I can think of take are of the form Gal(F/K) where K is a finite extension of Q, but of course these are by definition all closed. So I guess I\\'ve stuffed up somewhere, and I\\'d be really grateful for any help?! In know this may seem a bit \"homework questiony\" but it\\'s not, it\\'s just something that\\'s really bugging me! '}\n",
      "1513 {'Id': '1513', 'Type': 'answer', 'ParentId': '1502', 'urls': [], 'exp': ['\\\\alpha', 'V_\\\\alpha', 'V_0 = \\\\emptyset', ' V_\\\\alpha = \\\\bigcup_{\\\\beta &lt; \\\\alpha} P(V_\\\\beta) ', '\\\\alpha', 'V_\\\\alpha', 'V = \\\\bigcup_\\\\alpha V_\\\\alpha', '( V_\\\\alpha )', 'V', 'V', 'V'], 'Body': 'In very naive set theory (say in the late 19th century), a set was taken to be an arbitrary collection of objects. The difficulty is in telling which things that seem like they should be collections actually are well-defined collections.  The paradoxes show that it\\'s unclear whether this concept of set is coherent, although it is the natural-language meaning of the word \"set\". Because the concept seems poorly defined, almost all contemporary set theory deals with a more restrictive notion: pure, well-founded sets. These are the sets that can be constructed starting with the empty set and taking powersets and subsets. The only elements of these sets are other sets. These sets are defined in stages. At the first stage, you only have the empty set. At every larger stage, you add the powerset of every set that has already been constructed. There is one stage for every ordinal number, and the collection of all sets available after stage <math_exp> is named <math_exp>. Symbolically, we have <math_exp> and, in general,  <math_exp> For each ordinal <math_exp>, <math_exp> is a set. The union <math_exp> is a proper class. The sequence <math_exp>, indexed by ordinals, is known as the cumulative hierarchy. The \"sets\" that mathematicians study and that are formalized in Zermelo-Fraenkel set theory are exactly the sets in <math_exp>.  Moreover, the formalization of mathematics into set theory does not require any other sets than those in <math_exp> (which is why essentially all mathematics can be formalized into ZFC). So, for all practical mathematical purposes (outside of set theory), the answer to \"what is a set\" is \"a set is an element of <math_exp>\". '}\n",
      "1514 {'Id': '1514', 'Type': 'question', 'Title': 'A high school competition-level problem concerning sum and sequence', 'Tags': ['sequences-and-series', 'contest-math'], 'AcceptedAnswerId': '1516', 'urls': [], 'exp': ['N', 'A', 'S_{n} = n^{2} + 3n + 4', 'A_{1} + A_{3} + \\\\cdots + A_{21}'], 'Body': 'Given the sum of first <math_exp> elements of sequence <math_exp>: <math_exp>. Compute <math_exp>. I know this problem can be tackled by carefully calculating each value of the sequence. But I wonder what are the better ways to solve it. Thanks in advance! '}\n",
      "1515 {'Id': '1515', 'Type': 'answer', 'ParentId': '162', 'urls': [], 'exp': [], 'Body': 'How about a set that contains everything with the exception of itself? '}\n",
      "1516 {'Id': '1516', 'Type': 'answer', 'ParentId': '1514', 'urls': [], 'exp': ['4+8+\\\\dots+44'], 'Body': '\\\\begin{equation*} A_n=S_n-S_{n-1}=2n+2.  \\\\end{equation*} Now one has to find a sum of an arithmetic progression <math_exp>. '}\n",
      "1517 {'Id': '1517', 'Type': 'answer', 'ParentId': '1499', 'urls': [], 'exp': ['\\\\mathbb{R}', '\\\\mathbb{Q}', '\\\\mathbb{R}', '\\\\mathbb{R}', 'H+q e = \\\\{h+qe, h\\\\in H\\\\}', 'q\\\\in Q', '\\\\mathbb{R}', '\\\\mathbb{R} = \\\\cup_{q\\\\in\\\\mathbb{Q}}(H+qe)', 'm(\\\\mathbb{R})=0', '\\\\mathbb{R}', 'H_0 = H\\\\cap [0,z)', 'm(H_0)=0', 'm(H)=0', 'm(H_0)=\\\\delta&gt;0', '\\\\delta N &gt; z+1', 'x\\\\notin H', 'n=1,\\\\dots,N', 'H_0, H_0+y, \\\\dots, H_0+(N-1)y', '[0, 1+z)', '\\\\displaystyle 1+z = m\\\\Big( [0, 1+z) \\\\Big) \\\\geq m\\\\Bigg(\\\\bigcup_{n=0}^{N-1} (H_0 + n y)\\\\Bigg) = N \\\\delta.'], 'Body': \"As was noted by Jason DeVito if H is measurable then measure of H is 0. From the other hand, if we suppose, that the axiom of choice holds, there is a possibility, that H is not measurable. The proof is quite simple. <math_exp> is a vector space over <math_exp>. Therefore there is some basis. Suppose e is one of it's elements and H is a subspace in <math_exp>, generated by others. Then H is a subgroup of (<math_exp>, +). Lemma: H is not measurable. Suppose H is measurable. Then as was noted above, it's measure m(H)=0. Then every set of the form <math_exp>, where <math_exp>, has measure 0 (because it is just a shift of H). But <math_exp> is equal to union of countable many sets with measure 0: <math_exp>. Therefore <math_exp>. We have come to a contradiction. Also there is simple proof of the fact, that if H is measurable, then H is of measure 0. Lemma: If H is measurable proper subgroup of <math_exp>, then m(H)=0. If H={0} then proposition of the lemma is obvious. Otherwise we can find positive element z in H. Suppose, <math_exp>. If <math_exp> then <math_exp>. Otherwise <math_exp>. Let's take integer N, such that <math_exp> (we will see later, why). Note that if x is not in H, then x/n (for every positive integer n) and -x are also not in H. Therefore, using the fact that H is proper, we can find positive x&lt;1, such that <math_exp>. Suppose y=x/N!. Then for <math_exp> number ny obeys the following properties: 1. 1>ny>0. 2. ny is not in H. Then sets <math_exp> are disjoint subsets of <math_exp>. Therefore, <math_exp> Here we have a contradiction with definition of N. \"}\n",
      "1519 {'Id': '1519', 'Type': 'question', 'Title': 'Visualising <span class=\"math-container\" id=\"12271\">\\\\mathbb CP^2</span>: a problem of attaching cells with a dimension gap >1', 'Tags': ['intuition', 'general-topology', 'algebraic-topology'], 'urls': ['http://en.wikipedia.org/wiki/Morse_theory', 'http://en.wikipedia.org/wiki/Homology_%28mathematics%29', 'http://en.wikipedia.org/wiki/Cellular_homology', 'http://en.wikipedia.org/wiki/Hopf_map', 'http://en.wikipedia.org/wiki/Dehn_surgery'], 'exp': ['\\\\mathbb{D}^n', 'n', '\\\\mathbb{T}^2', '\\\\mathbb{D}^0=', '\\\\mathbb{D}^1=', '\\\\mathbb{D}^2=', '\\\\mathbb{CP}^2', '\\\\partial \\\\mathbb{D}^4=S^3 \\\\to \\\\mathbb{CP}^1=S^2 '], 'Body': 'Morse theory, as many other early algebraic-topology widgets, leads to a picture of smooth manifolds as being built up from \\'cells\\', copies of <math_exp> for varying <math_exp>, \\'glued\\' to each other by the usual topological tools; giving rise to (in some sense) a more natural picture of homology as \\'coming from\\' cellular homology. As an example, consider the torus <math_exp>: we begin with empty space, attach a 0-cell (<math_exp> a point), attach a 1-cell (<math_exp> a line) to your point (both ends of the line are attached to the point, creating a circle), attach another 1-cell (in the same way, to the same point, creating a sort of figure 8). The hardest bit to visualise is next: attaching a 2-cell (<math_exp> a disk, which we will think of as its homeomorphism equivalent, a square). Begin by twisting your figure 8 so that one circle is in the xy-plane, the other in the xz-plane, now attach the top and bottom of your square (coloured red in picture below) to the xy circle (creating a \\'curling round\\' tube) and the left and right edges (coloured blue below) of your square (now a tube) either side of the xz circle, completing the torus. <img src=\"https://i.stack.imgur.com/j7jsK.jpg\" alt=\"Torus cell decomposition\"> The above takes some thinking, but a little reading around shows that this is fairly easy to see. What makes it so easy is that the cells we are attaching are of adjacent dimensions, that is; we may easily identify the boundary of one with the entirety of another. Where it gets harder to visualise is when the dimensions of the cells we are attaching to one another differ by >1- the canonical example of this is the complex projective plane <math_exp>, a 4-manifold built by attaching a disk to a point (making a sphere) and then attaching a 4-ball to that sphere. The latter attaching map (wherein points are identified with their images), I know, may be thought of as the Hopf fibration <math_exp>, but I have no way of visualising this, particularly with regard to the interior of the 4-disk. How does a 4 cell wrap around a 2 cell without producing a singularity of some kind? Is this analagous in some sense to Dehn surgery in which one uses a thickening? Is there a right way to think about this or can it only really be thought of \\'intellectually\\'? '}\n",
      "1520 {'Id': '1520', 'Type': 'answer', 'ParentId': '1487', 'urls': [], 'exp': ['G', 'n', 'm', 'G', '3n = 2m.', 'n-1', 'G', 'm \\\\ge 2n - 2.', 'n \\\\le 4'], 'Body': 'Let <math_exp> be a simple cubic graph, with two edge-disjoint spanning trees, that has <math_exp> vertices and <math_exp> edges. Since <math_exp> is regular of degree 3, we have <math_exp> Now any spanning tree has <math_exp> edges, and <math_exp> has two disjoint spanning trees. So we have <math_exp> Combining the first equation and the second inequality, we get <math_exp>. Now there are no cubic simple graphs on 1, 2 or 3 vertices, so the result follows. (It is debatable whether there is one on 0 vertices.) '}\n",
      "1521 {'Id': '1521', 'Type': 'answer', 'ParentId': '1519', 'urls': [], 'exp': ['S^2', '\\\\mathbb{C}', '\\\\mathbb{C}^2', '\\\\mathbb{C}P^2\\\\cong(\\\\mathbb{C}^3\\\\setminus{0})/\\\\mathbb{C}^{\\\\times}', 'S^2=\\\\mathbb{C}P^1', '\\\\mathbb{C}^3', 'S^2', '\\\\mathbb{C}P^2', 'S^2', 'S^2'], 'Body': \"1) Near a point of <math_exp> the picture looks like <math_exp> sitting inside, naturally <math_exp> (because <math_exp> and <math_exp> is the image of a hyperplane in <math_exp>). 2) Think first about <math_exp>: one attaches a disk to a point, contracting the circle on the disk's boundary. Now, to make <math_exp> one takes 4-disk and do pretty much the same but not for one circle but for all, well, fibers of the Hopf fibration at once — or, in other words, for all points of <math_exp> at once. So, since there were no singularities after gluing <math_exp>, there will be no singularities here either. (Not sure if it's an answer, but only hope it helps.) \"}\n",
      "1522 {'Id': '1522', 'Type': 'answer', 'ParentId': '1459', 'urls': [], 'exp': ['A,B,C:=\\\\mathbb{Z}', '\\\\mathbb{Z}\\\\times (\\\\mathbb{Z} * \\\\mathbb{Z})', '(\\\\mathbb{Z}\\\\times \\\\mathbb{Z})*(\\\\mathbb{Z}\\\\times \\\\mathbb{Z})', '\\\\mathbb{Z}^3', '\\\\mathbb{Z}^4', 'A,B,C', '\\\\times\\\\neq \\\\otimes'], 'Body': \"The category of of all groups is also a counterexample. Letting <math_exp> , the LHS is <math_exp> and the RHS is <math_exp>, which are not isomorphic, as their Abelianizations are <math_exp>, resp. <math_exp>. Another example is the category of Vector spaces. If <math_exp> are just one dimensional vector spaces the dimension of the LHS and the RHS don't agree (Note <math_exp>). \"}\n",
      "1523 {'Id': '1523', 'Type': 'answer', 'ParentId': '1459', 'urls': [], 'exp': ['(A\\\\times B)\\\\times(A\\\\times C)'], 'Body': \"Note that in your example (as far as I understand it, you have a category with only 8 objects) your category does not have products and coproducts (you have them only for some special pairs of objects (for example, you don't have product <math_exp>. So if your exercise was about categories with products and coproducts, then your example is not an answer on it. The simplest way to construct a counterexample is to check some known categories. It will be quite useful for studying category theory, because you will check, what do abstract notions mean in concrete examples. In this case this works, you can check for example category of vector spaces over fixed field. Another method is to use some method to construct new category by those you know (for example, as was it was shown by Robin Chapman). Another method is just create some simple category --- it is the method you have tried. \"}\n",
      "1524 {'Id': '1524', 'Type': 'answer', 'ParentId': '1495', 'urls': ['http://www.artofproblemsolving.com/', 'https://math.stackexchange.com/'], 'exp': [], 'Body': 'You learn to swim by swimming, learn to talk by talking and you learn combinatorics by solving problems. You can get problems at , try to solve as many \"different\" problems as you can. If you need help you know  ;) May the force be with you! '}\n",
      "1525 {'Id': '1525', 'Type': 'answer', 'ParentId': '967', 'urls': ['http://worrydream.com/AlligatorEggs/'], 'exp': [], 'Body': '<img src=\"https://i.stack.imgur.com/8E8Sp.png\" alt=\"alligators\">  is a cool way to learn lambda calculus. Also learning functional programming languages like Scheme, Haskell etc. will be added fun. '}\n",
      "1526 {'Id': '1526', 'Type': 'question', 'Title': 'Need faster division technique for <span class=\"math-container\" id=\"12317\">4</span> digit numbers.', 'Tags': ['number-theory', 'arithmetic', 'numerical-methods'], 'AcceptedAnswerId': '1536', 'urls': [], 'exp': ['2860', '3186', '2', '2', '2860/3186', '6/7', '7/8', '8/9', '9/10'], 'Body': \"I have to divide <math_exp> by <math_exp>. The question gives only <math_exp> minutes and that division is only half part of question. Now I can't possibly make that division in or less than <math_exp> minutes by applying traditional methods, which I can't apply on that division anyways. So anyone can perform below division using faster technique? <math_exp> Thanks for reading, hoping to get some answers. :) This is a multiple choice question, with answers <math_exp>, <math_exp>, <math_exp>, and <math_exp>. \"}\n",
      "1527 {'Id': '1527', 'Type': 'answer', 'ParentId': '1378', 'urls': [], 'exp': ['\\\\rightarrow', '\\\\frac{(a  60 + b  35)}{(a 100 + b  100)} = \\\\frac{50}{100}', '\\\\frac{a}{b} = \\\\frac{3}{2}'], 'Body': \"Mixture1 : 60% Mixture2 : 35% Let us say you take mixture using a pot of 100 units. also you take Mixture1 'a' times and Mixture2 'b' times using the pot. <math_exp> <math_exp> solving this you get <math_exp> -> 300 units of Mixture1 and 200 units of Mixture2 will give you 500 units of mixture with alcohol concentration of 50% because 3 x 60 + 2 x 35 = 250. \"}\n",
      "1528 {'Id': '1528', 'Type': 'answer', 'ParentId': '1526', 'urls': ['http://en.wikipedia.org/wiki/Euclidean_algorithm'], 'exp': [], 'Body': \"You could maybe use Euclid's Algorithm to find the GCD of these two numbers, and use this to reduce the fraction. With luck, the result will be easier to manage. \"}\n",
      "1529 {'Id': '1529', 'Type': 'answer', 'ParentId': '1526', 'urls': [], 'exp': ['\\\\frac{2.9}{3.2}'], 'Body': 'If this problem appears on a standardized test you may want to take into account some broader considerations...  How many of these digits are even significant (relative to other quantities you will be computing with in \"second half\" of the problem)? What further operations will you be performing with this quantity - i.e. will you be significantly amplifying your error with nonlinear operations? At this point you may find that you can simply approximate the quantity as <math_exp>, which can quickly be converted to the decimal 0.906, or so.  Seeing that the original number is approximately 0.898, you should be good to go. '}\n",
      "1530 {'Id': '1530', 'Type': 'answer', 'ParentId': '1514', 'urls': [], 'exp': ['A(n)', 'S(n) - S(n-1) = 2n + 2 '], 'Body': 'Determining a nice form for <math_exp> is a good start.  This is <math_exp>. Now the desired sum is \\\\begin{equation*} \\\\sum_{k= 1}^{10} A(2k-1) = \\\\sum_{k=1}^{10} 4k = 4 \\\\ast 10 \\\\ast 11 / 2 = 220 .~_{\\\\square} \\\\end{equation*} '}\n",
      "1531 {'Id': '1531', 'Type': 'answer', 'ParentId': '1526', 'urls': [], 'exp': ['\\\\frac{2860}{3186}=\\\\frac{a}{b} \\\\iff 2860b= 3186a', '\\\\frac{a}{b}', '2860b- 3186a'], 'Body': 'If you are given options like those in your comment (ie. where the numerators/denominators are of comparable size) you can use the fact that: <math_exp> To see which <math_exp> is best, calculate <math_exp> for each starting from one of the two midsize fractions and repeating for larger or smaller fractions depending on the sign of the result (if your answer is positive a larger fraction would approximate better, if negative: a smaller one). The choice of a and b giving the smallest answer is the correct one. '}\n",
      "1532 {'Id': '1532', 'Type': 'answer', 'ParentId': '201', 'urls': ['http://www.matrix67.com/blog/archives/2700'], 'exp': ['\\\\log(n)', '\\\\log_2(n)', 'N', 'n', 'H(N) = \\\\log(n)', 'H(x)', 'N', 'm', 'X_1,\\\\ldots,X_m', 'N = p_i^{X_i}', 'm', 'n', 'H(N) = H(X_1, X_2, \\\\ldots, X_m)', 'H(N) \\\\leq H(X_1)+H(X_2)+\\\\cdots+H(X_m)', 'X_i \\\\leq \\\\log(N)', '2^{\\\\log_2(N)} = N', 'H(N) \\\\leq m H(\\\\log(N))', '\\\\frac{\\\\log(n)}{\\\\log(\\\\log(n)+1)} \\\\leq m', 'n'], 'Body': 'I found this proof on this Chinese website. Here is the translation. This is a proof with information theory (the proof can be done without information theory, but this is just cool). <math_exp> always means <math_exp> Uniformly pick a integer <math_exp> between 1 and <math_exp>. We have <math_exp> Where <math_exp> is the entropy function. <math_exp> can be represented by <math_exp> integers <math_exp> uniquely as <math_exp> where <math_exp> is the amount of primes no larger than <math_exp>. <math_exp> <math_exp> Since <math_exp> (<math_exp>), we have <math_exp> <math_exp> The lhs can increase indefinitely. This even give a bound for the amount of primes smaller than <math_exp> '}\n",
      "1533 {'Id': '1533', 'Type': 'question', 'Title': 'What is the best book for studying discrete mathematics?', 'Tags': ['discrete-mathematics', 'reference-request', 'big-list', 'book-recommendation'], 'urls': [], 'exp': [], 'Body': 'As a programmer, mathematics is important basic knowledge to study some topics, especially Algorithms. Many websites, and my fellows suggest me to study Discrete Mathematics before going to Algorithms, so I want to know which Discrete Mathematics book is suitable for my needs? '}\n",
      "1534 {'Id': '1534', 'Type': 'answer', 'ParentId': '262', 'urls': ['http://en.wikipedia.org/wiki/Men_of_Mathematics'], 'exp': [], 'Body': 'Men of Mathematics by E T Bell <img src=\"https://i.stack.imgur.com/5daFB.jpg\" alt=\"picture of book cover\"> '}\n",
      "1535 {'Id': '1535', 'Type': 'answer', 'ParentId': '1533', 'urls': [], 'exp': [], 'Body': 'Concrete Mathematics: A Foundation for Computer Science, By Donald Knuth himself! <img src=\"https://images-na.ssl-images-amazon.com/images/I/518GBVWEBYL._SL500_AA300_.jpg\" alt=\"Book Cover\"> '}\n",
      "1536 {'Id': '1536', 'Type': 'answer', 'ParentId': '1526', 'urls': [], 'exp': [], 'Body': \"if an approximate answer is enough, you may just throw out digits at the right of the two numbers; so 2860/3186 is more or less 28/31 (it's better to leave two digits, especially because 2860 is much more than 2000 while 3186 is not so much more than 3000) A second kind of approximation consists in adding or subtracting from both sides two numbers whose ratio is more or less what you expect the answer is. If you start from 2860/3186, which is near 1/1, you may subtract 186 from both sides, ending up with 2674/3000 ~ .891; if you are quick in additions and subtractions you may first add 14 to both sides, obtaining 2874/3200, then subtracting 180 and 200 (whose ratio is .9) obtaining 2694/3000 ~ .896. \"}\n",
      "1537 {'Id': '1537', 'Type': 'question', 'Title': 'Why is Euler\\'s Gamma function the \"best\" extension of the factorial function to the reals?', 'Tags': ['real-analysis', 'education', 'gamma-function', 'special-functions'], 'AcceptedAnswerId': '1539', 'urls': [], 'exp': ['\\\\Gamma (z) = \\\\int_0^\\\\infty t^{z-1} e^{-t} dt'], 'Body': 'There are lots (an infinitude) of smooth functions that coincide with f(n)=n! on the integers. Is there a simple reason why Euler\\'s Gamma function <math_exp> is the \"best\"?  In particular, I\\'m looking for reasons that I can explain to first-year calculus students. '}\n",
      "1539 {'Id': '1539', 'Type': 'answer', 'ParentId': '1537', 'urls': ['http://en.wikipedia.org/wiki/Bohr%E2%80%93Mollerup_theorem', 'http://en.wikipedia.org/wiki/Euler%27s_constant'], 'exp': ['f(z+1)=z f(z),\\\\qquad f(1)=1,', 'z=-n', 'n=0,1,2\\\\dots', 'f(z)=\\\\exp{(-g(z))}\\\\frac{1}{z\\\\prod\\\\limits_{m=1}^{\\\\infty} \\\\left(1+\\\\frac{z}{m}\\\\right)e^{-z/m}},', 'g(z)', 'g(z+1)-g(z)=\\\\gamma+2k\\\\pi i,\\\\quad k\\\\in\\\\mathbb Z, ', '\\\\gamma', 'g(z)=\\\\gamma z'], 'Body': \"The Bohr–Mollerup theorem shows that the gamma function is the only function that satisfies the properties The condition of log-convexity is particularly important when one wants to prove various inequalities for the gamma function. By the way, the gamma function is not the only meromorphic function satisfying <math_exp> with no zeroes and no poles other than the points <math_exp>, <math_exp>. There is a whole family of such functions, which, in general, have the form <math_exp> where <math_exp> is an entire function such that <math_exp> (<math_exp> is Euler's constant). The gamma function corresponds to the simplest choice <math_exp>. Edit: corrected index in the product. \"}\n",
      "1542 {'Id': '1542', 'Type': 'answer', 'ParentId': '1467', 'urls': [], 'exp': ['\\\\mathbb{Z}', '\\\\mathbb{N}', '|C| \\\\in C', '\\\\{1\\\\}'], 'Body': \"I may be wrong but it seems that C is not a set of all sets or anything similar. It has IMHO a trivial bijection into <math_exp> and therefore into <math_exp>. I haven't met any paradox to forbid <math_exp> (<math_exp> is such set). \"}\n",
      "1543 {'Id': '1543', 'Type': 'question', 'Title': 'tricky inequality', 'Tags': ['algebra-precalculus'], 'urls': ['https://math.stackexchange.com/questions/1482/problem-with-inequality'], 'exp': ['0&lt;1−\\\\frac{an}{mb^2}e^{−r(T−t)}&lt;1'], 'Body': 'Possible Duplicate:   problem with inequality How do I go about solving <math_exp>, where a,b,T>t>0? I have been stuck here for some time now. '}\n",
      "1544 {'Id': '1544', 'Type': 'answer', 'ParentId': '1454', 'urls': [], 'exp': ['(v_s-p_s)x_s \\\\geq (v_s-p_{S+1})x_{S+1}', '(v_s-p_s)x_s \\\\geq (v_{S+1}-p_{S+1})x_{S+1}'], 'Body': 'The notion \"symmetric equilibrium\" (the one from Wikipedia article) is not applicable here, because the game is not symmetric (different players have different \"profits per click\"). I\\'ve a look at the paper and I think, that \"symmetric Nash equilibrium\" in your case is nothing but technically convenient case of Nash equilibrium (the latter is not unique in your case). Also I\\'ve noted a probable misprint in the proof of the \"Fact 1\". It should be <math_exp> instead of <math_exp>. '}\n",
      "1545 {'Id': '1545', 'Type': 'answer', 'ParentId': '1537', 'urls': ['https://i.stack.imgur.com/ILNah.gif', 'https://i.stack.imgur.com/Z3ECh.gif', 'https://i.stack.imgur.com/gAEgA.gif', 'http://www.luschny.de/math/factorial/hadamard/HadamardsGammaFunction.html'], 'exp': [], 'Body': \"Actually there are other (less-frequently) used extensions to the factorial, with different properties from the gamma function which may be desirable in some contexts.  Euler's Gamma Function  Hadamard's Gamma function  Luschny's factorial function See here for more information. \"}\n",
      "1546 {'Id': '1546', 'Type': 'answer', 'ParentId': '1519', 'urls': ['http://www.dimensions-math.org/Dim_E.htm', 'http://www.dimensions-math.org/Dim_tour_E.htm'], 'exp': [], 'Body': 'Concerning the Hopf fibration, the film \"Dimensions\" does a superb job of visualizing it (chapters 7-8 in the table of contents). Concerning the visualization of the glueing, maybe it helps to visualize other, similar constructions to get at least some intuition. For instance, you could try to get a picture of how the torus looks like after every \"meridian\" (blue circle in your picture) has been squashed to a point. '}\n",
      "1547 {'Id': '1547', 'Type': 'answer', 'ParentId': '198', 'urls': [], 'exp': ['\\\\left\\\\{X_i\\\\right\\\\}_{i\\\\in I}', '\\\\prod_{i\\\\in I} X_i', 'X', 'Y', 'X \\\\times Y', 'X', 'x \\\\in X', 'y \\\\in Y', '(x,y) \\\\in X\\\\times Y', 'X_1, \\\\dots , X_n', 'x_1 \\\\in X_1', 'x_2', 'X_2', 'x_3', 'X_3', 'x_n \\\\in X_n', '(x_1, \\\\dots , x_n) \\\\in X_1 \\\\times \\\\dots \\\\times X_n', 'X_1, X_2, \\\\dots , X_n, \\\\dots', '\\\\prod_{n=1}^{\\\\infty} X_n', 'x_1 \\\\in X_1', 'x_2 \\\\in X_2', 'x_n \\\\in X_n', '(x_n)', '\\\\prod_{n=1}^{\\\\infty} X_n', 'X_n', 'n=1, 2, \\\\dots ', 'X_i', 'i', 'I', 'I', 'I', '\\\\mathbb{R}', 'X_i = \\\\mathbb{R}', 'i \\\\in \\\\mathbb{R}', '\\\\prod_{i\\\\in \\\\mathbb{R}} X_i = \\\\mathbb{R}^\\\\mathbb{R}', '\\\\mathbb{R}', '\\\\mathbb{R}', '\\\\mathbb{R}^\\\\mathbb{R}', 'f(x) = x', 'x\\\\in \\\\mathbb{R}', '\\\\prod_{i\\\\in I}X_i', '\\\\ i \\\\in I', 'i', 'I', 'i', '\\\\prod_{i\\\\in I} X_i', 'x_i \\\\in X_i', 'I', '(x_i)_{i\\\\in I}', 'X_i \\\\neq \\\\emptyset', '\\\\prod_{i\\\\in I} X_i', 'I', 'x_i \\\\in X_i', 'i\\\\in I'], 'Body': 'Here is one elementary result you can\\'t prove without the Axiom of Choice (that I used to think was not part of constructive mathematics): let <math_exp> be an arbitrary family of non-empty sets. Then its product <math_exp> is a non-empty set. Everybody knows how to prove that for a family of two sets. If two sets <math_exp> and <math_exp> are non-empty, then their cartesian product <math_exp> is non-empty too: since <math_exp> is non-empty, you can choose one element from it: <math_exp>. For the same reason, you can choose also one <math_exp>. So you have an element <math_exp> and therefore it is non-empty. Right? For a finite family of non-empty sets <math_exp> this procedure still works: you choose one element in each set starting from the first: <math_exp>. And then keep on doing the same for the rest: choose <math_exp> in the second set <math_exp>, <math_exp> in the third one <math_exp>,... And so on, till you reach <math_exp>. You have thus produced an element of the cartesian product: <math_exp>. Even if you had an infinite countable family of non-empty sets <math_exp>, you could produce an element of their product <math_exp> this way: choose successively elements <math_exp>, <math_exp>,..., <math_exp>... And you would obtain your element <math_exp> of your infinite cartesian product. Hence <math_exp> is non-empty too if all the <math_exp>, <math_exp> are non-empty. Now, try to do the same with an arbitrary family of sets <math_exp>, with the indexes <math_exp> belonging to an arbitrary indexing set <math_exp>. That is, set <math_exp> can be infinite and not  countable. This kind of cartesian products do exist in \"nature\". For instance: <math_exp> could be the set of real numbers <math_exp>. Take <math_exp> for all <math_exp> too. This cartesian product <math_exp> is the same as the set of all (not necessarily continuous) functions from <math_exp> to <math_exp>. Of course, you can prove that <math_exp> is non-empty by showing an element of it (for instance, <math_exp> for all <math_exp>), but how can you prove, in general, that the product of an arbitrary family of non-empty sets <math_exp> is non-empty too? If you try to imitate what you have just done in the finite or countable cases, you\\'ll find asking yourself: where do I start? Which is the first  <math_exp>? Assuming there is a first <math_exp>, which is the next one? The answer is that, for an infinite, non-countable, set of indexes <math_exp> there is in general no such a thing as a first index <math_exp>, nor a next one. So you can\\'t use the previous algorithm in order to produce an element of the product <math_exp>... Except you have something that allows you to choose, one particular element of each set <math_exp> -no matter how, even with no precise constructive algorithm as in the finite or countable cases. If you were able to do that for no matter which set of indexes <math_exp>, then you would have your element <math_exp> and you would have proved that, if every <math_exp>, so is their cartesian product <math_exp>. What do allow you for an infinite, non-countable, set of indexes <math_exp> to choose those <math_exp> for every <math_exp>? Well, this is exactly what the Axiom of Choice says. '}\n",
      "1548 {'Id': '1548', 'Type': 'question', 'Title': 'G a finite group, M a maximal subgroup; M abelian implies G solvable?', 'Tags': ['group-theory'], 'AcceptedAnswerId': '1625', 'urls': [], 'exp': ['G', 'M', 'G', 'M', 'G', 'Z(G)=1'], 'Body': \"Here is a classic theorem of Herstein: <math_exp> is a finite group, <math_exp> a maximal subgroup, which is abelian.  Then <math_exp> is solvable. The proof is pretty easy, but it uses character theory (specifically, Frobenius' theorem on Frobenius groups).  Is there a character-theory-free proof? To get things going, note that we can reduce to the case where: i) <math_exp> is core-free and a Hall subgroup of <math_exp>; ii) <math_exp>. Steve \"}\n",
      "1549 {'Id': '1549', 'Type': 'answer', 'ParentId': '1537', 'urls': ['http://en.wikipedia.org/wiki/N-sphere#Volume_and_surface_area', 'http://en.wikipedia.org/wiki/Riemann_zeta_function#The_functional_equation'], 'exp': ['a, b', '\\\\displaystyle \\\\int_{0}^1 t^a (1 - t)^b \\\\, dt = \\\\frac{a! b!}{(a+b+1)!}.', 'a', 'b', '\\\\displaystyle \\\\int_0^1 t^a (1 - t)^b \\\\, dt = \\\\frac{\\\\Gamma(a+1) \\\\Gamma(b+1)}{\\\\Gamma(a+b+2)}.'], 'Body': 'For whatever reason, Nature (by which I mean integrals) seems to prefer the Gamma function as the \"correct\" substitute for the factorial in various integrals, which seems to come more or less from its integral definition.  For example, for non-negative integers <math_exp>, it\\'s not hard to show (and there\\'s a really nice probabilistic argument) that <math_exp> For (non-negative?) real values of <math_exp> and <math_exp> the correct generalization is <math_exp> And, of course, integrals are important, so the Gamma function must also be important.  For example, the Gamma function appears in the general formula for the volume of an n-sphere.  But the result that, for me, really forces us to take the Gamma function seriously is its appearance in the functional equation for the Riemann zeta function. '}\n",
      "1550 {'Id': '1550', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'Gödel, Escher, Bach: An Eternal Golden Braid  by Douglas Hofstadter: <img src=\"https://i.stack.imgur.com/5vbib.jpg\" alt=\"alt text\"> '}\n",
      "1551 {'Id': '1551', 'Type': 'answer', 'ParentId': '1408', 'urls': [], 'exp': ['\\\\varinjlim_i X_i', 'x \\\\in \\\\varinjlim_i X_i', 'i', 'I', 'U', 'i', 'x_i \\\\in X_i', 'x', 'X_i \\\\longrightarrow \\\\varinjlim_i X_i', 'O_{X,p}', 'f \\\\in O_X(U)', 'U', 'f: A \\\\longrightarrow B', 'g: A \\\\longrightarrow C', 'B \\\\oplus_A C', '(b,c) \\\\in B\\\\oplus C', '(f(a), 0) - (0, g(a))', 'a\\\\in A', '(f(a),0) = (0,g(a))', 'B\\\\oplus_A C', 'B\\\\oplus_A C', 'B', 'C', '(b,0)', '(0,c)', '(b,c) \\\\in B\\\\oplus_A C', 'b \\\\in B', 'c\\\\in C'], 'Body': 'I think there is a missing word in Akhil Mathew\\'s answer: and it\\'s \"filtered\". You can do that because stalks are filtered colimits (aka \"direct limits\"). For filtered colimits, <math_exp>, you can take representatives of elements <math_exp> for some <math_exp> belonging to the set of indexes <math_exp> (in our case, the open sets <math_exp>). That is, you can find some <math_exp> and <math_exp> that goes to your <math_exp> through the universal arrow <math_exp>. For instance, every element of the stalk <math_exp> can be represented by a section <math_exp> for some open set <math_exp>. But this is not true for other kinds of colimits. For instance, take the push-out of two arrows <math_exp> and <math_exp> in the category of, say, abelian groups. Elements of this push out <math_exp> are classes of pairs <math_exp>, where you quotient out elements of the form <math_exp>, for all <math_exp>. That is, <math_exp> in <math_exp>. Elements of <math_exp> cannot be represented, in general, by elements coming from just <math_exp> or <math_exp>, which are of the form <math_exp> or <math_exp>, respectively: so, for a general <math_exp> there is no <math_exp>, nor <math_exp>, that represents it. '}\n",
      "1552 {'Id': '1552', 'Type': 'answer', 'ParentId': '1512', 'urls': [], 'exp': ['Gal(F/\\\\mathbb{Q})'], 'Body': \"The finite index subgroups that you say that you can think of are precisely the closed (equivalently open) finite index subgroups of <math_exp>  with respect to the Krull topology on the group. These are really the only relevant ones in terms of Galois theory. But the group has non-closed subgroups too, but their existence requires some non-constructive principle such as Zorn's lemma. These subgroups won't be open or closed in the Krull topology. \"}\n",
      "1553 {'Id': '1553', 'Type': 'answer', 'ParentId': '1482', 'urls': [], 'exp': ['\\\\alpha = \\\\frac{an}{mb^2}.\\\\qquad(1)', ' 0 &lt; 1 - \\\\alpha e^{-r (T - t)} &lt; 1 ', ' 0 '], 'Body': \"There are many unnecessary variables. Let <math_exp> Then the inequality becomes <math_exp> The first obvious step is perform “1 &minus;” on every parts, <math_exp> Since the exponential function's range is positive and &lt; 1 (since r > 0 and T > t > 0), we can ensure &alpha; is positive. If 0 &lt; &alpha; ≤ 1, then every t will satisfy the inequality (the first solution). So assume &alpha; > 1. Now it's pretty obvious on how to solve t in terms of r, T and &alpha;. Substitute (1) again to get back a, n, m and b. Things you may consider: \"}\n",
      "1555 {'Id': '1555', 'Type': 'question', 'Title': 'Finding an addition formula without trigonometry', 'Tags': ['calculus'], 'AcceptedAnswerId': '1560', 'urls': [], 'exp': ['\\\\int_0^a \\\\frac{\\\\mathrm{d}x}{\\\\sqrt{1-x^2}} + \\\\int_0^b \\\\frac{\\\\mathrm{d}x}{\\\\sqrt{1-x^2}} = \\\\int_0^{a\\\\sqrt{1-b^2}+b\\\\sqrt{1-a^2}} \\\\frac{\\\\mathrm{d}x}{\\\\sqrt{1-x^2}}', 'a\\\\sqrt{1-b^2}+b\\\\sqrt{1-a^2}', '\\\\sin(t) = \\\\sqrt{1 - \\\\cos^2(t)}'], 'Body': \"I'm trying to understand better the following addition formula: <math_exp> The term <math_exp> can be derived from trigonometry (since <math_exp>) but I have not been able to find any way to derive this formula without trigonometry, how could it be done? edit: fixed a mistake in my formula. \"}\n",
      "1556 {'Id': '1556', 'Type': 'answer', 'ParentId': '1512', 'urls': [], 'exp': ['G = \\\\mathbb{Z}/(2)^\\\\mathbb{N}', 'G', '\\\\mathbb{N}', '(a_i)', 'A', 'n \\\\in A', 'a_n = 1', 'G', \"F'\", '\\\\mathbb{N}', 'F', 'F', '2', 'F', 'F', 'F', 'f = \\\\chi_F \\\\colon G = \\\\mathcal{P}(\\\\mathbb{N}) \\\\to \\\\mathbb{Z}/(2)', 'A, B \\\\in G', 'f(A) = f(B) = 0', 'A', 'B', 'F', 'A^c \\\\cap B^c \\\\in F', 'A \\\\cup B \\\\notin F', 'A \\\\Delta B \\\\notin F', 'f(A \\\\Delta B) = 0'], 'Body': \"Let's forget the Galois group structure: it is enough to consider the infinite product <math_exp> with its profinite topology and exhibit non-closed subgroups of finite index. An element of <math_exp> is naturally identified with a subset of <math_exp>: the string <math_exp> corresponds to the subset <math_exp> such that <math_exp> if and only if <math_exp>. With this interpretation, the operation on <math_exp> just becomes the symmetric difference of sets. Now consider the filter <math_exp> of all cofinite subsets of <math_exp>, and let <math_exp> be any ultrafilter containing it. I show that we can associate to <math_exp> a subgroup of index <math_exp>. The <math_exp> will be non closed since <math_exp> is not a principal ultrafilter (indeed any non-principal ultrafilter will do). Consider the characteristic function of <math_exp>: <math_exp>. I claim that this is a homomorphism, and its kernel is the desired subgroup. This is easy to verify directly. Let <math_exp> and assume <math_exp>. This means that neither <math_exp> nor <math_exp> belong to <math_exp>, so their complementary sets do. It follows that <math_exp>, so that <math_exp>. A fortiori the symmetric difference <math_exp>, whic means <math_exp>. The other cases are similar. \"}\n",
      "1557 {'Id': '1557', 'Type': 'answer', 'ParentId': '1526', 'urls': [], 'exp': ['\\\\displaystyle \\\\frac{n-1}{n} = 1 - \\\\frac{1}{n}', 'n', '1/n'], 'Body': \"Another very specific trick, based on the multiple choices: since all the choices are of the form <math_exp>, you're trying to find <math_exp> for which <math_exp> is closest to 1&minus;(your fraction). So you'd consider (denominator&minus;numerator)/denominator = (3186&minus;2860)/3186 which is (around 320)/3186, clearly closest to 1/10. \"}\n",
      "1560 {'Id': '1560', 'Type': 'answer', 'ParentId': '1555', 'urls': [], 'exp': ['-a', '0', '-a,0,b', 'c = a\\\\sqrt{1-b^2} + b \\\\sqrt{1-a^2}', 'x', '[-a,0]', '[b,c]', 'dx/y', 'x^2 + y^2 = 1', 'x \\\\to', 'x \\\\to', '\\\\to x', '[-1,1]'], 'Body': 'Replace the first integral by the same thing from <math_exp> to <math_exp>, and consider the points W,X,Y,Z on the unit circle above <math_exp> and <math_exp>.  Draw the family of lines parallel to XY (and WZ). This family sets up a map from the circle to itself; through each point, draw a parallel and take the other intersection of that line with the circle. Your formula says that this map [edit: or rather the map it induces on the <math_exp>-coordinates of points on the circle] is a change of variables converting the integral on <math_exp> to the same integral on <math_exp>.  Whatever differentiation you perform in the process of proving this, will be the verification that <math_exp> is a rotation-invariant differential on the circle <math_exp>. [The induced map on x-coordinates is: <math_exp> point on semicircle above <math_exp> corresponding point on line parallel to XY <math_exp>-coordinate of the second point.  Here were are just identifying <math_exp> with the semicircle above it.] '}\n",
      "1562 {'Id': '1562', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'I\\'ll recommend two, which are similar in that they take fairly elementary mathematical problems and give very thorough and careful \"talking out loud\" illustrations of how a proper mathematician would go about thinking them through - what\\'s really going on, what\\'s a good example, what\\'s a definitive counterexample, how to generalise, how to realise you\\'ve reached a dead end, and so on.  \"Proofs and refutations\" by Imre Lakatos (just one, geometrical, problem, in glorious detail).  \"Mathematics and plausible reasoning Vol 1\" by G. Polya (a little more advanced, and much more satisfying, than \"How to solve it\"). '}\n",
      "1563 {'Id': '1563', 'Type': 'question', 'Title': 'Example of non-isomorphic structures which are elementarily equivalent', 'Tags': ['logic', 'model-theory'], 'urls': [], 'exp': [], 'Body': 'I just started learning model theory on my own, and I was wondering if there are any interesting examples of two structures of a language L which are not isomorphic, but are elementarily equivalent (this means that any L-sentence satisfied by one of them is satisfied by the second). I am using the notaion of David Marker\\'s book \"Model theory: an introduction\". '}\n",
      "1564 {'Id': '1564', 'Type': 'answer', 'ParentId': '1563', 'urls': ['http://en.wikipedia.org/wiki/Real_closed_field'], 'exp': [], 'Body': \"Take a look at the Wikipedia article on Real closed fields. Briefly, they are fields that are first-order equivalent to the field of the real numbers. An example is the field of real numbers that are roots of polynomials with rational coefficients. That's pretty much all I know about them, though. \"}\n",
      "1565 {'Id': '1565', 'Type': 'answer', 'ParentId': '1563', 'urls': [], 'exp': [], 'Body': 'There are a real numbers worth of countable models of Peano arithmetic, all elementarily equivalent to the usual model, but all pairwise nonisomorphic. Every nonstandard example (nonstandard means not isomorphic to the usual model) is characterized by the fact that it contains \"infinitely large\" numbers.  That is, each model M has a canonical copy of N = usual naturals inside.  However, each nonstandard model M has an element p (in fact, countably many such elements) which the model things is bigger than everything in its copy of N. They are created by standard compactness arguments paired with the downward Lowenheim-Skolem theorem. Let PA denote the axioms for Peano Arithmetic and set T = Th(N), the theory of N (i.e., the set of all first order statements which are true in the usual interpretation. Finally, add a constant c to the language and let phi_n be the statement c > n, or more appropriately, c > SSSS.....S(0), where S is the successor function of PA and there are n S\\'s in the expression. Now, consider the set of axioms PA union T union phi_n for every n.  By a standard compactness argument, this set of axioms is consistent, and so by the Godel completeness theorem, there is a model M of all the axioms simultaneously.  By downward Lowenheim-Skolem, we can assume M is countable. Now, ask yourself, what can c be?  Well, since M satisfies phi_n for every n, we must have c > n for each \"usual\" natural number in M, i.e., c is infinite! What\\'s really cool is that by playing around with different kinds of phi_n, one can find models that have elements which are divisible by NO \"standard\" prime number.  One can also find models that have elements which are simultaneously divisible by EVERY \"standard\" prime number! '}\n",
      "1566 {'Id': '1566', 'Type': 'answer', 'ParentId': '262', 'urls': ['http://rads.stackoverflow.com/amzn/click/0471608394'], 'exp': [], 'Body': \"Geometric Algebra by Emil Artin. Though not for the beginner, it can do wonders for an intermediate undergraduate in terms of expanding their horizons and helping them appreciate the beauty and interconnectedness of mathematics. It did for me and I think convinced me that I'm a geometer at heart. \"}\n",
      "1567 {'Id': '1567', 'Type': 'answer', 'ParentId': '1563', 'urls': [], 'exp': ['L', 'L', 'L', '$2^{\\\\max\\\\{|L|,\\\\aleph_0\\\\}}$', 'L', 'L'], 'Body': 'Laws, yes[1]: if there weren\\'t such examples, there wouldn\\'t be a subject called model theory and therefore Marker\\'s book would not exist. There are, however, not very many explicit examples which can be given, with proof, right after the definition of elementary equivalence, a pedagogical problem that I encounered recently when I taught a short summer course on model theory.  When I first gave the definition, all I was able to come up with was the following argument: for any language <math_exp>, the class of <math_exp>-structures is a proper class [any nonempty set can be made into the \"universe\", or underlying set of, an <math_exp>-structure] whereas since there are at most  <math_exp> different theories in the language <math_exp>, the <math_exp>-structures up to elementary equivalence must form a set. But if you just want examples without proof, sure, here goes: 1) In the empty language, two sets are elementarily equivalent iff they are both infinite or both finite of the same cardinality. 2) Any two dense linear orders without endpoints are elementarily equivalent.  [The same holds for DLOs with endpoints, but the two classes of structures are not elementarily equivalent to each other.] 3) Any two algebraically closed fields of the same characteristic are elementarily equivalent. 4) Any two real-closed fields are elementarily equivalent. 5) Any two infinite models of the theory of finite fields are elementarily equivalent. (Nope! See Dave Marker\\'s answer.) In each case, such structures exist of every infinite cardinality, so the class of isomorphism classes of such structures is a proper class, not a set. [1]: \"M-O-O-N, that spells model theory!\" '}\n",
      "1568 {'Id': '1568', 'Type': 'question', 'Title': 'Derivatives of random variables', 'Tags': ['probability-theory', 'probability'], 'AcceptedAnswerId': '1574', 'urls': ['https://math.stackexchange.com/questions/1416/algebra-of-random-variables'], 'exp': ['z = \\\\frac{dx}{dy}', 'x', 'y', 'x \\\\in f(x)', 'y \\\\in g(y)', 'h(x,y) = f(x)g(y)', 'z \\\\in q(z)', 'q(z) = \\\\int f(x) g( \\\\int{[z dx]} ) ( \\\\frac{d}{dz} \\\\int{[z dx]} ) dx'], 'Body': \"As a follow up to algebra-of-random-variables, is it possible to compute: <math_exp> Where <math_exp> and <math_exp> are drawn from a known, independent probability distribution <math_exp>, <math_exp>? Geometrically  the problem makes sense, one has a function <math_exp> that is the joint probability distribution and we are essentially asking for the gradient along one direction. I can get this to the expression (<math_exp>): <math_exp> But I'm not quite sure what to make of this, or even how to compute it given specific PDF's. EDIT: For simplicity, assume all the PDF's considered are continuous and smooth. \"}\n",
      "1570 {'Id': '1570', 'Type': 'answer', 'ParentId': '847', 'urls': [], 'exp': ['cos(z)=cosh(iz)', 'x^2 + y^2 - w^2 - u^2', '0'], 'Body': 'This is not a directly a matter of hyperbolic geometry but of complex Euclidean geometry.  The construction of \"impossible\" triangles is the same as the construction of square roots of negative numbers, when considering the coordinates the vertices of those triangles must have.  If you calculate the coordinates of a triangle with sides 1,3,5 or 3,4,8 you get complex numbers.  In ordinary real-coordinate Euclidean geometry this means there is no such triangle.  If complex coordinates are permitted, the triangle exists, but not all its points are visible in drawings that only represent the real points. In plane analytic geometry where the Cartesian coordinates are allowed to be complex, concepts of point,line, circle, squared distance, dot-product, and (with suitable definitions) angle and cosine can be interpreted using the same formulas.  This semantics extends the visible (real-coordinate) Euclidean geometry to one where any two circles intersect, but possibly at points with complex coordinates.  We \"see\" only the subset of points with real coordinates, but the construction that builds a triangle with given distances between the sides continues to work smoothly, and some formulations of the law of Cosines will continue to hold. There are certainly relations of this picture to hyperbolic geometry.  One is that <math_exp> so you can see the hyperbolic cosine and cosine as the same once complex coordinates are permitted.  Another is that the Pythagorean metric on the complex plane, considered as a 4-dimensional real space, is of the form <math_exp>, so that the locus of complex points at distance <math_exp> from the origin contains copies of the hyperboloid model of hyperbolic geometry.  But there is no embedding of the hyperbolic plane as a linear subspace of the complex Euclidean plane, so we don\\'t get from this an easier way of thinking about hyperbolic geometry. To help visualize what is going on it is illuminating to calculate the coordinates of a triangle with sides 3,4,8 or other impossible case, and the dot-products of the vectors involved. '}\n",
      "1571 {'Id': '1571', 'Type': 'answer', 'ParentId': '1568', 'urls': [], 'exp': ['x', 'x', 'x', '\\\\frac{dx}{dy}'], 'Body': \"You can take a derivative of a function <math_exp> with respect to one of its variables and the function has to be smooth w.r.t. this parameter.   If <math_exp> is being drawn from a set of smooth functions then it is certainly possible to consider the derivative of a random variable <math_exp>. However, I'm really not sure what <math_exp> would even mean in your case. \"}\n",
      "1572 {'Id': '1572', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://dl.dropbox.com/u/190212/public/speq.png', 'http://www.speqmath.com/index.php?id=1'], 'exp': [], 'Body': \"SpeQ is really nice. If you close it's left/right areas it's just like Notepad, but a Notepad which can compute stuff. You can change any line displayed, press Enter at it's end and the line will be recomputed. Just like in Notepad, you can select any part of what's displayed and delete it. You can also paste non-mathematical text into it. It will also remember what's displayed inside it the next time you start it. It has an option to clear it's content on Exit. A lot of people recommended Python. It's is my main programming language and I love it, but SpeQ it's MUCH more powerful and easy to use than Python as a calculator. alt text http://dl.dropbox.com/u/190212/public/speq.png http://www.speqmath.com/index.php?id=1 \"}\n",
      "1573 {'Id': '1573', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'When I was in my fourth year of high school I got a copy of What is Mathematics? by Courant and Robbins. That book showed to me that Mathematics is far more than a \"boring tool\" to do Physics and opened up new worlds. I would recommend it to any bright high school kid with an interest in math and sciences. '}\n",
      "1574 {'Id': '1574', 'Type': 'answer', 'ParentId': '1568', 'urls': [], 'exp': ['x', 'y', 'h(x,y)=f(x)g(y)', 'x', 'y', 'x'], 'Body': 'Radon-Nikodym derivative of the measures represented by variables <math_exp> and <math_exp> is one answer to your question, and is somewhat similar to the reasoning you sketched about a product density <math_exp>.   For \"change in <math_exp> per change in <math_exp>\" you perhaps want the correlation coefficient (multiplied by standard deviation of <math_exp>). '}\n",
      "1576 {'Id': '1576', 'Type': 'question', 'Title': \"'(Pseudo)-random functions' by seeding of PRNGs?\", 'Tags': ['number-theory', 'algorithms', 'computer-science', 'numerical-methods'], 'AcceptedAnswerId': '1602', 'urls': [], 'exp': ['\\\\mathbb{Z}^2', '\\\\mathbb{Z}^3', '2^{32}', '(x,y)', 'x', 'y', '2^{32}', 'f', 'f(0, 0)', 'f(437, 61)', 'f(-23, 129)', 'f(5,3)', 'f(1,0)', 'f(5,3)', 'f', '(5,3)', 'f', 'f(x,y)', 'x+y'], 'Body': \"I have an application that wants controllable random functions from <math_exp> and <math_exp> to <math_exp> , where by controllable I basically mean seedable by some parameters (say, on the order of 3 to 5 32-bit integers) such that the same seeds will always produce the same functions.  The most obvious way of doing this (for the two-dimensional case, say) would seem to be computing the value at some point <math_exp> by using <math_exp>, <math_exp>, and the seed parameters as seeds for something like an LFSR generator or a Mersenne Twister, then running the RNG for some fixed number of steps and taking the resultant value as the value of the function at that point. My question is, how can I be certain that this procedure won't keep too much correlation between adjacent 'seed points', and is there either a straightforward analysis or even just some general guideline for how many iterations would be necessary to eliminate that correlation?  My first back-of-the-envelope guess would be that each iteration roughly doubles the decorrelation between given seed values, so that 32 iterations would be necessary to achieve the requisite decorrelation over a range of <math_exp> values (and in practice I'd probably double it to 64 iterations), but that's strictly a guess and any proper analysis would be welcome! Edited for clarification: To further outline the issue, I may be sampling this random function <math_exp> (for some given seed parameters) at arbitrary values, and need those samples to be identical between passes; so for instance, if a first application computes <math_exp>, <math_exp>, <math_exp>, and then <math_exp>, and a second (potentially concurrent) application computes <math_exp> and then <math_exp>, both passes need to find the same value of <math_exp> at <math_exp>.  I may also be sampling <math_exp> at arbitrary points, so I'd like the evaluation to take constant time (and in particular, evaluating <math_exp> shouldn't take time linear in <math_exp>). \"}\n",
      "1577 {'Id': '1577', 'Type': 'question', 'Title': 'disjoint union of Baire spaces which is a Baire space', 'Tags': ['general-topology', 'baire-category'], 'AcceptedAnswerId': '1578', 'urls': [], 'exp': ['A_\\\\alpha', 'A_\\\\alpha', '\\\\bigcup_{\\\\alpha} A_\\\\alpha', '\\\\beta', '\\\\alpha', '\\\\alpha', '\\\\alpha', '\\\\bigcup_{\\\\beta&lt;\\\\alpha} A_\\\\alpha', '\\\\beta', 'U', 'U', '\\\\beta', 'U \\\\in A_\\\\beta', '\\\\beta'], 'Body': 'Say we have a family {<math_exp>} of disjoint Baire spaces. Also suppose that each <math_exp> is disjoint from the closure of the union of the other sets. Show that <math_exp> is a Baire space. I think we can prove this by transfinite induction. Suppose the property holds for all <math_exp> &lt; <math_exp>, show it holds for <math_exp>. If <math_exp> is a limit ordinal: we want to show that for every sequence of open dense subsets of <math_exp> their intersection is open dense. This sequence of open dense sets in the union is defined as the union of all open dense sets in each of the sets since they all are Baire spaces and by induction hypothesis the union up to <math_exp> is a Baire space. Suppose this is not the case. Then there exists an open set <math_exp> such that the intersection of these open dense sets misses <math_exp>. There is some ordinal <math_exp> such that <math_exp>, but this is a contradiction since we supposed that the union up to <math_exp> was a Baire space. Now how do I handle the successor case? it seems trickier. '}\n",
      "1578 {'Id': '1578', 'Type': 'answer', 'ParentId': '1577', 'urls': [], 'exp': ['A_\\\\alpha', 'U_1, \\\\dots,', 'U_i \\\\cap A_\\\\alpha', 'A_\\\\alpha', 'A_\\\\alpha', 'F', '\\\\bigcup A_\\\\alpha', 'A_\\\\alpha', 'F \\\\cap A_\\\\alpha', 'F'], 'Body': \"I'm pretty sure it's simpler and doesn't require transfinite induction or anything nearly as difficult. In this case the <math_exp> are open sets (as the complement of the closure of the union of the others). So suppose <math_exp> are open dense sets in this union. Then each intersection <math_exp> is open and dense in <math_exp>. The countable collection has intersection which is dense in each <math_exp> by the Baire property. This means that the countable intersection, call it <math_exp>, is dense in the union <math_exp>. (Any point in the union belongs one of the <math_exp>, and there is a point of <math_exp> nearby, hence a point of <math_exp> nearby.) \"}\n",
      "1579 {'Id': '1579', 'Type': 'question', 'Title': 'n-ary version of gcd<span class=\"math-container\" id=\"12635\">(a,b)\\\\space </span>lcm<span class=\"math-container\" id=\"12636\">(a,b)</span> = <span class=\"math-container\" id=\"12637\">ab</span>', 'Tags': ['number-theory', 'gcd-and-lcm'], 'AcceptedAnswerId': '1581', 'urls': ['https://math.stackexchange.com/questions/1442/is-there-a-direct-proof-of-this-lcm-identity'], 'exp': ['\\\\gcd(1,6,15) = 1', '\\\\operatorname{lcm}(1,6,15)=30', '1\\\\cdot 6\\\\cdot 15 = 90', '(2,6,15)', 'n', '\\\\gcd', '\\\\operatorname{lcm}'], 'Body': 'This question was motivated by pondering this lcm identity. Consider that <math_exp>, but <math_exp>, but <math_exp>.  <math_exp> shows a similar phenomenon. So what is the correct identity for <math_exp>-ary <math_exp>/<math_exp>? '}\n",
      "1580 {'Id': '1580', 'Type': 'answer', 'ParentId': '29', 'urls': [], 'exp': [], 'Body': \"If I am at school (and in my lab) I will just have Maple running for symbolic stuff. It's pretty good at it. Other then that, for simple calculations I use windows calculator (its a hotkey on my keyboard) and for more advanced stuff, I'll use WolframAlpha. However, my plan is to learn mathematica. \"}\n",
      "1581 {'Id': '1581', 'Type': 'answer', 'ParentId': '1579', 'urls': [], 'exp': ['n \\\\ge 2', 'n+1', 'a_1 ... a_n = a_1 ... a_n', 'P = a_1 ... a_n', '\\\\displaystyle a_1 ... a_n = \\\\text{gcd}(a_1, a_2, ... a_n) \\\\text{lcm} \\\\left( \\\\frac{P}{a_1}, \\\\frac{P}{a_2}, ... \\\\right).', '\\\\displaystyle a_1 ... a_n = \\\\text{gcd}(a_1 a_2, a_1 a_3, ..., a_{n-1} a_n) \\\\text{lcm} \\\\left( \\\\frac{P}{a_1 a_2}, \\\\frac{P}{a_1 a_3}, ..., \\\\frac{P}{a_{n-1} a_n} \\\\right).', 'p', '\\\\text{min}(a, b) + \\\\text{max}(a, b) = a + b', 'a, b'], 'Body': 'For <math_exp> there isn\\'t just one identity; there are actually <math_exp> identities, half of which are dual to the other half, and two of which are the trivial identity <math_exp>.  The first two are as follows.  Let <math_exp>.  Then <math_exp> <math_exp> The other identities are similar.  This is because \"pointwise\" (e.g. at a particular prime <math_exp>) the two-variable identity is just the identity <math_exp> for non-negative integers <math_exp>, and the above identities are the natural generalization of this one. '}\n",
      "1582 {'Id': '1582', 'Type': 'answer', 'ParentId': '1533', 'urls': ['http://rads.stackoverflow.com/amzn/click/0201199122'], 'exp': [], 'Body': 'Discrete Math knowledge is needed to become adept in proving the correctness and deriving the complexity of algorithms and data structures. You will be taught those in Algo/DS books, but you can only get the mathematical proficiency by practicing just discrete math. Knuth book is very good for that. But IMHO, you will only need it if you for doing advanced proofs in DS/Algorithms. For a beginner, it would be great to go over \"Grimaldi\" http://www.amazon.com/Discrete-Combinatorial-Mathematics-Applied-Introduction/dp/0201199122 and then quickly move to Algorithms. Otherwise, you will continue going deep in Discrete Math and never get to Algorithms/DS. Remember, Discrete Math does not teach you how to design algorithms or Data structures. That will come only by practicing Algorithm problems @ topcoder, acm icpc , spoj etc and reading books on Algos/DS or courses on those. My 2 cents. '}\n",
      "1583 {'Id': '1583', 'Type': 'answer', 'ParentId': '29', 'urls': ['http://www.sagenb.org', 'http://sidk.info/wp-content/uploads/2007/11/sage_screenshot2.jpeg'], 'exp': [], 'Body': 'A few people have mentioned Sage, but I think deserves a bit more of a plug. Firstly someone mentioned that Sage is quite large to install. This is true, the binary downolad comes in at around 1 gig, however you can use it for free online, without having to install a thing! Head on over to   sagenb.org , and sign up for a free account, which will allow you to create and save your own worksheets. I think this is really nice--it feels like it\\'s your \"gmail\" but for maths, so you can just log in from anywhere and \"check your maths\" if you want :-D This is a very popular option--I believe that in the last 9  months around 30,000 people have created accounts! It\\'s perfectly suited for high school students and undergraduates, since no installation is necessary, and hence it can be used straight away in any computer lab with internet access. Here\\'s a screenshot of what the notebook looks like. (I just googled \"sage notebook screenshot.\") It\\'s been designed to look and work like the Mathematica notebook system, and it\\'s surprisingly slick and easy to use. Sage screenshot http://sidk.info/wp-content/uploads/2007/11/sage_screenshot2.jpeg. There really is a lot to recommend it. I used the online notebook for around 6 months before I installed a copy on my own computer, and this was because I became interested in developing Sage. Although Sage is based on Python (so in particular any Python syntax will work), you really don\\'t need to know Python to get started using Sage--I didn\\'t when I started. Sage is quite powerful too. Aside from using Maxima for symbolic integration, I belive Sage uses Pari for highly optimised polynomial arithmatic, and I have used Sage for some pretty intense linear algebra computations (over the integers as well as over finite fields), although for this kind of thing you\\'ll probably not want to rely on the online server which can be a little slow. And let\\'s not forget that you also get the moral boost of being part of something really positive--an movement to create an open source alternative to costly and \"closed\" alternatives like Mathematica, Maple and Matlab. (Disclaimer: I just came back from the Sage Days 23 sage developers workshop in Leiden, and I\\'m a bit pumped!) '}\n",
      "1584 {'Id': '1584', 'Type': 'answer', 'ParentId': '1563', 'urls': [], 'exp': ['T', 'T', 'T'], 'Body': 'Take any complete theory <math_exp> that has two models of of different cardinalities. Then the models are elementary equivalent (they both model <math_exp>) but they cannot be isomorphic because isomorphisms preserve cardinality. By the Lowenheim-Skolem theorem, every theory with infinite models has models of different cardinalities, so really any complete theory with infinite models will work for <math_exp>. Personal note: Whenever I teach students about cardinality, I always point out this sort of application. Because cardinality is preserved by bijections, showing two objects (groups, rings, models, etc) have different cardinalities is an easy way to show they are not isomorphic.  I find this a very compelling reason to care about cardinality outside of set theory. '}\n",
      "1585 {'Id': '1585', 'Type': 'answer', 'ParentId': '1533', 'urls': ['http://diestel-graph-theory.com/', 'http://www.math.upenn.edu/~wilf/DownldGF.html', 'http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2005/readings/'], 'exp': [], 'Body': 'Theres many different areas to discrete math, and many good books. theres Graph Theory by Diestel, which has a free pdf version available at diestel-graph-theory.com theres generatingfunctionology by wilf, free pdf version at math.upenn.edu/~wilf/DownldGF.html Other books that are good include Enumerative combinatorics 1 and 2 by Richard P Stanley (a book which is sufficiently dense that having at least 1 analysis and algebra course each will help). that being said, for more introductory expositions in terms of expected mathematical maturity, I\\'d suggest googling around and looking at various lecture notes of the \"intro to combinatorics\" or \"mathematics for computer scientists\" sorts. I found that MIT OCW\\'s \"mathematics for Computer Scientists\" notes were quite nice when I looked at them several years ago. ocw.mit.edu/courses/electrical-engineering-and-computer-science... has a link to the lecture notes. There are some really funny asides in it. One of my favorites \"... anyone who says that is wrong, and you should make fun of them until they cry\". Also, If you want to dig even deeper into discrete math/ combinatorics, the value of building up a wee bit of mathematical basics in other areas of math. Complex Analysis, real analysis (at the level of at least baby rudin, and perhaps even up to functional analysis), maybe some probability up to its measure theory formulation level, and at least a smidge of abstract algebra.  Then you can do stuff like look at the combinatorics of random processes (great for analyzing randomized algorithms) and look at cool problems like percolation. theres probably other things I should suggest, but the point is discrete math is accessible without that much of a background, but is also rewards you for enriching that mathematics background with some amazingly beautiful stuff thats 1) awesome and fun 2) useful. '}\n",
      "1586 {'Id': '1586', 'Type': 'answer', 'ParentId': '1512', 'urls': [], 'exp': ['\\\\mathbf Z/2\\\\mathbf Z', 'V', '\\\\mathbf F_2', 'f:V \\\\to \\\\mathbf F_2', 'V', 'f'], 'Body': \"One way to think of the situation is that the countable product of <math_exp>s that you are considering is a vector space <math_exp> over <math_exp> that is uncountable, and thus has an (even more) uncountable dual space.  If <math_exp> is a non-zero element of the dual space, its kernel is an index 2 subgroup of <math_exp>. Now the open subgroups that you know about correspond to (i.e. are the kernels of) the projections onto the various factors in the product, and these are countable in number.  So there is a massive host of other index 2 subgroups, corresponding to all the other non-zero elements <math_exp> of the dual space. As others have noted, you won't be able to write these down explicitly though. \"}\n",
      "1587 {'Id': '1587', 'Type': 'answer', 'ParentId': '903', 'urls': [], 'exp': [], 'Body': 'As BBischof says: Willard is nice. I also learned a lot from Mendelson, a little dover book. I think that you should use whatever gets you through the subject \"quickest\". Point Set topology is now just basic language that is very necessary, but it seems there is not much research in the field. My dad did not go into math because of a course out of munkres, he had no intuition for the topic the way it was taught. I dont see this as a criticism of Munkres because I think one of the major goals of any early course is to untrain the student. There are spaces that are pathological that you have to deal with. You have to be rigorous. I think that is one of the main points of such a course, that and preparing the student for what comes next. Which is why mariano asks what your goals are. One thing about willard though is that you should make sure you do the exercises, they contain key facts! Edit: I felt it pertinent to add that I did go through a course from munkres (by the time I did it was pointless for me, I had already been using the relevant stuff for my field daily) and it was quite well done I thought. It is the classic, I don\\'t think you can go wrong with it. '}\n",
      "1588 {'Id': '1588', 'Type': 'answer', 'ParentId': '903', 'urls': ['http://uob-community.ballarat.edu.au/~smorris/topology.htm'], 'exp': [], 'Body': 'Good and Free <br />  '}\n",
      "1589 {'Id': '1589', 'Type': 'question', 'Title': 'Toy sheaf cohomology computation', 'Tags': ['algebraic-geometry', 'algebraic-topology', 'homology-cohomology'], 'AcceptedAnswerId': '1591', 'urls': ['https://mathoverflow.net/questions/32689/how-should-a-homotopy-theorist-think-about-sheaf-cohomology'], 'exp': ['H^*(CP^{\\\\infty})'], 'Body': 'I asked this question a while back on MO : One thing that really helped in learning the Serre SS was doing particular computations (like <math_exp>) I am curious, as a sort of followup if anyone can suggest: In short, please suggest a space and a sheaf on it that I should work on computing the sheaf cohomology of. PS: I of course welcome any other suggestions for understanding how to compute sheaf cohomology. '}\n",
      "1590 {'Id': '1590', 'Type': 'answer', 'ParentId': '1519', 'urls': [], 'exp': ['S^n', '0', 'D^n'], 'Body': 'I think you understand attaching cells that have dimension gap better than you think! Consider any <math_exp> for <math_exp>, we can think of this as being the attachment of <math_exp> to a point which is 0-dimensional. Also, I would recommend that you think of this attaching cells thing a bit more carefully. if you understand the hopf fibration then you do understand the attaching of the cell. The hopf fibration is not easy to picture (as far as I know). I would recommend thinking about the map in terms of what happens when you work with coordinates. I will be back later to add more... I promise. '}\n",
      "1591 {'Id': '1591', 'Type': 'answer', 'ParentId': '1589', 'urls': [], 'exp': ['\\\\mathbb{A}^2', 'd', '\\\\mathbb{P}^2', '\\\\mathbb{P}^n'], 'Body': 'Any de Rham cohomology (or Dolbeault cohomology) computation is a computation in sheaf cohomology. Actually --- any computation in singular cohomology is a computation in sheaf cohomology!! ;-) We\\'re just taking different resolutions of the appropriate constant sheaf. IIRC, there are some good Cech cohomology computations and examples in Bott-Tu. Also, have you read section 3.H of Hatcher\\'s algebraic topology book, on \"local coefficients\"? For a simple example from algebraic geometry, compute the cohomology of the structure sheaf of <math_exp> minus a point. I seem to recall an exercise or an example in Hartshorne in which the genus of a degree <math_exp> curve in <math_exp> is computed using Cech cohomology. The section in Hartshorne on the cohomology of <math_exp> uses Cech cohomology, and I remember finding it pretty instructive. Eisenbud\\'s commutative algebra book probably has lots of good examples. '}\n",
      "1593 {'Id': '1593', 'Type': 'answer', 'ParentId': '1576', 'urls': [], 'exp': [], 'Body': 'This does not strictly answer your question, but might be too long for a comment. =) I believe you should rethink your approach. Most random number generators are designed to be seeded once, and than return a sequence of \"random\" numbers, as close to independent and uniformly distributed as possible. I don\\'t think that there are many results about the correlation of the 64th sequence element for different seeds. Also, (at least in the implementations I know) the period of the algorithm is a lot longer than the number of possible seeds. If you have to reuse seeds in your case, you could introduce correlations that are avoidable. Instead of precomputing the entire grid, you could get a random value just-in-time and store it for future reference, saving memory if you don\\'t access every node. '}\n",
      "1594 {'Id': '1594', 'Type': 'answer', 'ParentId': '709', 'urls': ['https://i.stack.imgur.com/Sg9sf.gif', 'http://hotmath.com/images/gt/lessons/genericalg1/parabs.gif', 'https://www.varsitytutors.com/hotmath/hotmath_help/topics/vertex-of-a-parabola.html'], 'exp': ['y = ax^2 + bx + c', 'y = m(x-h)^2 + k = \\\\underline{m}x^2 + \\\\underline{(-2mh)}x+ \\\\underline{mh^2 +k}', '\\\\rightarrow', 'a = m', 'b = -2mh = -2ah \\\\rightarrow h = -b/2a', 'c = mh^2 + k = ah^2 + k \\\\rightarrow k = c - b^2/4a'], 'Body': ' (source: hotmath.com) <math_exp><br /> for parabola written in Vertex Form vertex is (h,k). <math_exp> <math_exp> <math_exp> <math_exp> <math_exp>  '}\n",
      "1595 {'Id': '1595', 'Type': 'answer', 'ParentId': '1512', 'urls': [], 'exp': ['(p_1,p_2,p_3,p_4,\\\\dots):=(-1,2,3,5,\\\\dots)', 'K_0,K_1,\\\\dots', 'K_0:=\\\\mathbb Q', 'K_n:=K_{n-1}(\\\\sqrt{p_n})', 'n\\\\ge 1', 'K_n', '\\\\sqrt{p_n}', 'K_{n-1}', 'n', 'a', 'p_k^{e(k)}', 'k', '\\\\ge n', 'e(k)', '0', 'k', '\\\\sqrt a\\\\in K_{n-1}', 'e(k)', '\\\\mathbb Q^{\\\\times}/\\\\mathbb Q^{\\\\times 2}'], 'Body': \"Here is an easy point which is implicit in the other answers, but which is part of the question (as I read it). Put <math_exp>, and define the fields <math_exp> by <math_exp> and <math_exp> for <math_exp>. [It's clear that any quadratic field is contained into some <math_exp>.] To make sure that the Galois group in question is the indicated one, we need to check that <math_exp> is not in <math_exp>. To do that we prove the following apparently stronger claim. Let <math_exp> be a positive integer and <math_exp> the product of the <math_exp>, where <math_exp> runs over the integers <math_exp> and <math_exp> is an integer equal to <math_exp> for almost all <math_exp>. Assume <math_exp>. Then all the <math_exp> are even. This is easily proved by induction. EDIT. Things can be described in this way: The obvious morphism from <math_exp> into our Galois group induces an isomorphism between the profinite completions. [It's nice to obtain a Galois group by applying a simple construction to the base field.] \"}\n",
      "1596 {'Id': '1596', 'Type': 'question', 'Title': 'Non-Linear Transformation', 'Tags': ['calculus', 'transformation'], 'urls': [], 'exp': [], 'Body': \"Can someone explain to me in simple terms what a non-linear transformation is in maths? I know some single-variable calculus, but I read it has to do with multi-variable calculus, which I'm not familiar with. If someone could explain it in simple words, that would be helpful. \"}\n",
      "1597 {'Id': '1597', 'Type': 'answer', 'ParentId': '1537', 'urls': ['https://mathoverflow.net/questions/23229/importance-of-log-convexity-of-the-gamma-function'], 'exp': ['f'], 'Body': \"Wielandt's theorem says that the gamma-function is the only function <math_exp> that satisfies the properties: (See also the related MathOverflow thread Importance of Log Convexity of the Gamma Function, where I learned about the above theorem.) \"}\n",
      "1598 {'Id': '1598', 'Type': 'answer', 'ParentId': '903', 'urls': [], 'exp': [], 'Body': \"Sutherland's Metric and topological spaces is nice. I know a guy who swears by Kelley's General topology but I've not read it myself. \"}\n",
      "1599 {'Id': '1599', 'Type': 'answer', 'ParentId': '1596', 'urls': ['http://en.wikipedia.org/wiki/Vector_space', 'http://en.wikipedia.org/wiki/Field_%28mathematics%29'], 'exp': ['V_1, V_2', 'F', 'T: V_1 \\\\to V_2', 'x, y \\\\in V_1', '\\\\alpha \\\\in F', 'T(x + \\\\alpha y) = T(x) + \\\\alpha T(y)', 'x, y, \\\\alpha'], 'Body': 'Let <math_exp> be two vector spaces over the field <math_exp>. A transformation <math_exp> is linear if for every <math_exp> and every <math_exp> it is true that (*) <math_exp> T is not a linear transformation if there are some <math_exp> such that (*) is not true. '}\n",
      "1600 {'Id': '1600', 'Type': 'answer', 'ParentId': '1537', 'urls': [], 'exp': [], 'Body': 'This is a comment posted as an answer for lack of reputation. Following Qiaochu Yuan, the gamma function shows up in the functional equation of the zeta function as the factor in the Euler product corresponding to the \"prime at infinity\", and it occurs there as the Mellin transform of some gaussian function. (Gaussian functions occur in turn as eigenvectors of the Fourier transform.) This is at least as old as Tate\\'s thesis, and a possible reference is Weil\\'s Basic Number Theory. EDIT. Artin was one of the first people to popularize the log-convexity property of the gamma function (see his book on the function in question), and also perhaps the first mathematician to fully understand this Euler-factor-at-infinity aspect of the same function (he was Tate\\'s thesis advisor). I thought his name had to be mentioned in a discussion about the gamma function. '}\n",
      "1601 {'Id': '1601', 'Type': 'question', 'Title': 'Lower bound for finding second largest element', 'Tags': ['algorithms', 'computer-science', 'searching'], 'AcceptedAnswerId': '1702', 'urls': [], 'exp': ['n - 1', 'n - 1'], 'Body': 'In a recent discussion, I came across the idea of proving a lower bound for the number of comparisons required to find the largest element in an array. The bound is <math_exp>. This is so because the set of comparisons performed by every such algorithm looks like a tournament tree, which always has <math_exp> internal nodes. The obvious question is then, What should be the lower bound for finding the 2nd largest element in an array. '}\n",
      "1602 {'Id': '1602', 'Type': 'answer', 'ParentId': '1576', 'urls': ['http://www.newwaveinstruments.com/resources/articles/m_sequence_linear_feedback_shift_register_lfsr.htm'], 'exp': ['+1\\\\;/-1', 'i = 0', 'N-1', 'a[i]a[i+k]', 'N', 'k', 'N', '-1', '\\\\mathcal{O}(log(k))', '\\\\mathcal{O}(k)', '2', '3', 'k'], 'Body': 'Note that when an LFSR is used to generate a sequence of <math_exp> values, the correlation between that sequence and any time-shifted version of that sequence is near zero: (sum from <math_exp> to <math_exp> of <math_exp>) is either <math_exp> if <math_exp> is a multiple of <math_exp>, or <math_exp> otherwise. The initial state of an LFSR has a one-to-one mapping to the time shift -- this is analogous to a discrete logarithm problem in Galois fields; given a time shift k, it\\'s easy to find the initial state of an LFSR (<math_exp>, I think, as it\\'s just computing exponentiation in the appropriate Galois field), but given the initial state of an LFSR, it\\'s hard to find the time shift k in anything shorter than <math_exp>. So your PRNG could be an LFSR with very long period e.g. 64 bits or greater, and the \"seed\" could be a time shift used to derive a different initial state. As far as <math_exp> - or <math_exp> - or <math_exp>-dimensional mapping, interleave the bits of the coordinates, and use a hash function to map those bits to an integer for use in deriving LFSR seeds. I\\'m not sure what kind of correlation you\\'re looking to avoid between functions. (e.g. correlation in a statistical sense, or cryptographic independence e.g. one PRNG cannot be used to predict future output of another). '}\n",
      "1603 {'Id': '1603', 'Type': 'answer', 'ParentId': '1596', 'urls': [], 'exp': ['f(x,y) = x^2y', 'f: \\\\mathbb{R}^2 \\\\longrightarrow \\\\mathbb{R}', '  f(2x,2y) = 4x^22y \\\\neq 2x^2y = 2f(x,y) \\\\ .  ', 'f: \\\\mathbb{R}^m \\\\longrightarrow \\\\mathbb{R}^n', '  f(x_1, \\\\dots , x_m) = (a_1^1 x_1 + \\\\dots + a_1^m x_m , \\\\dots , a_n^1 x_1 + \\\\dots + a_n^m x_m)  ', 'a^i_j'], 'Body': 'In addition to the definition of linear map that Tomer remind you, here are two examples. For instance, <math_exp> is not a linear map <math_exp> because <math_exp> More generally, the linear maps <math_exp> are necessarily of the form <math_exp> with <math_exp> constant coefficients. So, two more examples: '}\n",
      "1604 {'Id': '1604', 'Type': 'answer', 'ParentId': '1589', 'urls': [], 'exp': [], 'Body': 'Rotman does some very elementary explicit computations of Cech cohomology in his book Homological Algebra. If I remember correctly he does these computations using resolutions, spectral sequences, and by just starting with some sequence. As a complete beginner to this material, I was able to understand his treatment and compute some specific examples on my own. I hope this helps :) P.S. I just looked at the first edition, and it seems to be different slightly. For your information I used the second edition. '}\n",
      "1605 {'Id': '1605', 'Type': 'answer', 'ParentId': '198', 'urls': [], 'exp': [], 'Body': '\"Taking the principle of excluded middle from the mathematician would be the same, say, as proscribing the telescope to the astronomer or to the boxer the use of his fists. To prohibit existence statements and the principle of excluded middle is tantamount to relinquishing the science of mathematics altogether.\" -David Hilbert '}\n",
      "1606 {'Id': '1606', 'Type': 'answer', 'ParentId': '1453', 'urls': ['http://en.wikipedia.org/wiki/Cook%E2%80%93Levin_theorem', 'http://en.wikipedia.org/wiki/Boolean_satisfiability_problem', 'http://en.wikipedia.org/wiki/Np_complete', 'http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/combpcp.pdf'], 'exp': ['x\\\\in L'], 'Body': 'A detailed explanation can be found in many places. I\\'ll to provide an intuitive one. By Cook-Levin theorem, the Boolean satisfiability problem is NP-complete - ie. every decision problem in NP can be reduced to it. We will ask of our prover to supply the input and output of every gate in the circuit and consider this as the proof that <math_exp>. If we were to stop there, then the prover could cheat by changing a single bit in his proof, which would require us to check a large (non-constant) number of bits in it. So we must ask something more from the prover. This something more will be the encoding of his proof using some (special) error-correcting code. Intuitively, this \"smudges\" the false bits onto a large number of bits in the code-word. The verifier receives a word from the prover, there are 2 ways in which the prover might try and cheat, this word might not be a code-word in the chosen code, or it might be the encoding of something which isn\\'t a proof. We\\'ll examine the (slightly different) separation of cases of the word either being far from every codeword (a large number of bits must be changed to reach a codeword) or that it is close to codeword which isn\\'t an encoding of a proof. To be able to detect these we demand that our code has the following 2 properties: 1) locally checkable - we can, by reading a constant number of bits, detect w.h.p if a word is far from any codeword. 2) locally decodable - we can, by reading a constant number of bits, decode w.h.p a bit from the encoded word. (finding such codes is hard, hadamard has this properties but the code-word\\'s size is exponential, RM codes have these properties as well (to a lesser degree) and they are the ones generally used. So the verifier checks if the word is close to being a code-word (using 1), and if the test succeeds, picks a random gate and decodes it\\'s outputs and output (using 2). this is sufficient to achieve constant query complexity and logarithmic randomness. It should be mentioned that  (Dinur) has a combinatorial proof of PCP which is quite different from what i\\'ve discribed, but her version is (for me, at least) less intuitive. '}\n",
      "1607 {'Id': '1607', 'Type': 'answer', 'ParentId': '1601', 'urls': ['http://en.wikipedia.org/wiki/Selection_algorithm'], 'exp': ['f(n)=min f(k)+f(n-k)+2', ' 0 &lt; k &lt; n '], 'Body': 'An observation: it will always require at least n-1. Draw a graph. If we test two nodes, then we draw an arrow from the smaller to the larger. We know a is greater than b if and only if there is a path from a to b along the arrows. We will always need at least n-1 arrows to connect the graph and hence n-1 comparisons to find a lower bound. It is easy enough to see that this isn\\'t the minimal upper bound though - we will only know the second highest element if our arrows form a line (ie. we can go from a \"start\" node to an \"end\" node by passing through each node and each arrow). Another fact - there are algorithms for finding the kth largest number in guaranteed linear time. That could give you some information, but there could always be a special algorithm for finding the second largest element. I don\\'t have a solution for the problem yet. But one thing we can try is breaking the problem into two groups and combining them. Suppose we have two groups, A and B, where we know both the largest and second largest elements, but no knowledge of any relationships between the groups. Then it takes two arrows to find the second highest element. First we compare the highest elements in A and B. Without loss of generality assume A has the highest element. Then we would compare B with the highest element in A. Showing that the groups can\\'t be combined with a single comparison is trivial. This gives us a recursion relation for the time taken to solve by combining groups which have been solved separately: <math_exp> where <math_exp>. '}\n",
      "1608 {'Id': '1608', 'Type': 'question', 'Title': 'Get length of cathetus/i from length of hypothenuse and ratio between catheti?', 'Tags': ['geometry'], 'AcceptedAnswerId': '1616', 'urls': ['http://en.wikipedia.org/wiki/Cathetus'], 'exp': ['5', '4:3'], 'Body': 'How can I calculate the length of the cathetus in a triangle if I know the length of the hypotenuse and the ratio between the two catheti? For example: The hypotenuse is <math_exp>cm long, and the ratio between the catheti is <math_exp> - how long are the catheti or either cathetus? '}\n",
      "1609 {'Id': '1609', 'Type': 'question', 'Title': 'Intuitive explanations for the concepts of divisor and genus', 'Tags': ['intuition', 'algebraic-geometry', 'algebraic-curves'], 'AcceptedAnswerId': '1635', 'urls': [], 'exp': [], 'Body': 'When trying to explain AG-codes to computer scientists, the major points of contention I am faced with are the concepts of divisors, Riemann-Roch space and the genus of a function field. Are there any intuitive explanations for these concepts, preferably explanations that are less dependent on knowledge of algebraic-geometry/topology? '}\n",
      "1610 {'Id': '1610', 'Type': 'answer', 'ParentId': '1608', 'urls': [], 'exp': [], 'Body': \"You can call one cathetus 4*x*, the other 3*x*, and apply Pythagora's theorem: (4*x*)2+(3*x*)2=52. You will obtain 25*x*2=25, which yields x=1. So one cathetus is 4cm, and the other 3cm. (Remember that 3,4,5 is a Pythagorean triple) \"}\n",
      "1611 {'Id': '1611', 'Type': 'answer', 'ParentId': '1609', 'urls': [], 'exp': ['k(x)', 'k(x,y)', 'y^2', 'x', '\\\\infty'], 'Body': 'Well, the concepts of genus and divisors come from geometry/topology intuition originally, so those explanations are generally going to be the fastest. However, we can give very rough versions without needing to say \"genus is the number of holes in the surface that the curve looks like.\"  You can think of genus as a measure of how complicated the function field is.  Naturally <math_exp> is going to be the simplest possible one, and it happens to be genus zero.  Genus one function fields look like <math_exp> but where <math_exp> is a cubic in <math_exp>.  Now, this isn\\'t very precise, but it\\'s a rough thing. As for divisors, the topology/geometry of them is \"linear combinations of points, modulo zero-<math_exp> loci of functions,\" but to handle it purely algebraically, you\\'ll want to change the word \"point\" to \"discrete valuations\", so these are integer linear combinations of the various ways to describe the order of an element of the function field, modulo some set of trivial ones. '}\n",
      "1612 {'Id': '1612', 'Type': 'answer', 'ParentId': '1609', 'urls': ['http://en.wikipedia.org/wiki/Riemann-Roch_Theorem'], 'exp': ['\\\\mathbb{Z}', 'D', 'div(f)+ D', 'L(D)', 'l(D)', 'l(D)', '|D|', 'L(D)', 'D', 'f'], 'Body': 'Charles has already explained the notion of genus (which comes from topology--there\\'s also a purely algebraic notion of genus as a first cohomology group, but I at least would find it less intuitive). So I\\'ll talk about divisors. First of all, the analytic notion of a nonsingular projective curve is a compact Riemann surface. \"Nonsingular\" translates to \"complex manifold,\" \"curve\" translates to \"dimension 1,\" and \"projective\" translates to \"compact.\" So I will talk about Riemann surfaces.* If you have a Riemann surface, it\\'s locally the same as the complex plane. And in the complex plane, you have a way to measure the order of the zero of a holomorphic function. More generally, you can measure (in <math_exp>) the order of a zero of a meromorphic function (which is negative if it has a pole, zero if it is nonvanishing and analytic, positive if it has a zero).  So to any meromorphic function (of course, we don\\'t restrict to holomorphic ones---they\\'re all constant) on a compact Riemann surface, we can assign a finite set of points with multiplicities consisting of the places where the order is nonzero (i.e., the zeros and poles). A divisor is more general. It\\'s just a formal sum of points on the Riemann surfaces with multiplicities.  A divisor may come from a function as above; then it\\'s called principal. In general, however, it does not. Yet given a divisor <math_exp>, we can associate a vector space of meromorphic functions such that <math_exp> is a divisor with nonnegative coefficients. This is denoted <math_exp> and its dimension by <math_exp>. The Riemann-Roch theorem is then a statement about the dimensions <math_exp>. One of its consequences, in particular, is that if <math_exp> (the sum of the multiplicities) is large, then you can always find a (nonzero) element of <math_exp>. This is intuitively easy to understand.  If <math_exp> is large, then you are allowing lots of leeway for <math_exp>, potentially having poles at many places. *I\\'m being informal here, but there is a general \"GAGA\" theory about these equivalences that goes into much more detail, which I know next to nothing about. '}\n",
      "1613 {'Id': '1613', 'Type': 'answer', 'ParentId': '1589', 'urls': ['http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.rmjm/1250128841', 'https://amathew.wordpress.com/2010/08/05/the-paper-of-kempf/'], 'exp': [], 'Body': \"This is rather scheme-y, but there's  a really nice paper by Kempf (hopefully you have institutional access :() that gives a very basic and elementary proof that the higher cohomology of a quasi-coherent sheaf on an affine scheme is trivial. The first part of the paper uses nothing more than the basic properties  (e.g. long exact sequence) of cohomology, and might be fun. I thought it was fun, anyway; it's also nice because it shows that Hartshorne is unnecessarily restrictive in sticking to noetherian affine schemes in chapter III (even if one wants to avoid anything fancy). OK, update: here is the proof explained (admittedly by a beginner :)). \"}\n",
      "1614 {'Id': '1614', 'Type': 'question', 'Title': \"Is this a counter example to Stromquist's Theorem?\", 'Tags': ['geometry'], 'AcceptedAnswerId': '1622', 'urls': ['http://www.freeimagehosting.net/uploads/5b289e6824.png'], 'exp': [' (0.2,0),\\\\    (1,0),\\\\ (1,1),\\\\ (0,1),\\\\ (0,0.2),\\\\ (-0.2, -0.2),\\\\ (0.2,0).'], 'Body': 'Stromquist\\'s Theorem:  If the simple closed curve J is \"nice enough\" then it has an inscribed square. \"Nice enough\" includes polygons. Read more about it here:  www.webpages.uidaho.edu/~markn/squares An \"inscribed square\" means that the corners of a square overlap with the curve. I would like to suggest a counter-example: The curve connected by the points<math_exp> Link to plot: http://www.freeimagehosting.net/uploads/5b289e6824.png Can this curve be incribed by a square? (An older version of this question had another example: a triangle on top of a square (without their mutual side.) ) '}\n",
      "1615 {'Id': '1615', 'Type': 'answer', 'ParentId': '1614', 'urls': [], 'exp': [], 'Body': \"What about the square on the bottom- with it and the triangle's mutual side returned? Edit: Btw. Don't be disheartened that your counterexample didn't come through- looking for counterexamples to proved theorems will never give a genuine counterexample (unless ZFC turns out to be inconsistent!!;)), but it can provide valuable insight into what the theorem means. Here you can see from experience that 'most of the square' may be contained within the curve- this is something that may not be obvious looking at the examples of Stormquist in action in books on the topic. Trying to find counterexamples, IMHO, is a great way to develop as a mathematician- just do not be afraid to be wrong ;) \"}\n",
      "1616 {'Id': '1616', 'Type': 'answer', 'ParentId': '1608', 'urls': ['http://en.wikipedia.org/wiki/Pythagorean_theorem'], 'exp': ['x^2+r^2*x^2=h^2', 'x=\\\\sqrt{{h^2}/(1+r^2)}'], 'Body': 'Let the ratio be 1:r and the hypotenuse be h. Then the sides are then x and rx for some x. By the Pythagorean Theorem we get <math_exp>. So <math_exp>. We can then calculate rx, the other side. '}\n",
      "1617 {'Id': '1617', 'Type': 'question', 'Title': 'Symmetric matrices and orthogonal diagonalization.', 'Tags': ['linear-algebra'], 'urls': [], 'exp': ['3\\\\times 3', 'A = \\\\begin{pmatrix} 2 &amp; -1 &amp; 1\\\\\\\\ -2 &amp; 3 &amp; -2\\\\\\\\-1 &amp; 1 &amp; 0\\\\end{pmatrix}', '\\\\lambda^2 - 4\\\\lambda + 3 = 0', '\\\\lambda_1 = 1', '\\\\lambda_2 = \\\\frac{-1 + \\\\sqrt{13}}{2}', '\\\\lambda_3 = \\\\frac{-1 - \\\\sqrt{13}}{2}', '\\\\vec{x}_1', '\\\\operatorname{span}\\\\{\\\\left&lt;\\\\sqrt{2}/2, \\\\sqrt{2}/2, 0\\\\right&gt;, \\\\left&lt; -\\\\sqrt{2}/2, 0, \\\\sqrt{2}/2\\\\right&gt;\\\\}', 'P', '3\\\\times 4', '\\\\begin{pmatrix} \\\\sqrt{2}/2 &amp; -\\\\sqrt{2}/2 &amp; 0 &amp; 0\\\\\\\\ \\\\sqrt{2}/2 &amp; 0 &amp; 0 &amp; 0\\\\\\\\0 &amp; \\\\sqrt{2}/2 &amp; 0 &amp; 0 \\\\end{pmatrix}', 'P', 'P^{-1} = P^T'], 'Body': \"A <math_exp> matrix <math_exp> produces this  characteristic equation: <math_exp>, these eigenvalues: <math_exp>, <math_exp>, <math_exp>, and these eigenvectors: After normalizing the already orthogonal <math_exp>, I get <math_exp>. Setting up my <math_exp> matrix for diagonalization, it appears to be a <math_exp> matrix that is singular: <math_exp> Questions: Why do I have four vectors, and why are two of them zero vectors? Symmetric matrices are always diagonalizable, where there's a <math_exp> such that <math_exp>. \"}\n",
      "1618 {'Id': '1618', 'Type': 'question', 'Title': 'Is the natural map <span class=\"math-container\" id=\"12855\">L^p(X) \\\\otimes L^p(Y) \\\\to L^p(X \\\\times Y)</span> injective?', 'Tags': ['linear-algebra', 'real-analysis', 'functional-analysis'], 'AcceptedAnswerId': '1628', 'urls': [], 'exp': ['X,Y', '\\\\sigma', 'L^p(X) \\\\otimes L^p(Y)', 'L^p(X \\\\times Y)', '\\\\sum a_{ij} f_i \\\\otimes g_j', 'F(x,y) = \\\\sum a_{ij} f_i(x) g_j(y)', '\\\\sum a_{ij} f_i(x) g_j(y) = 0'], 'Body': \"Let <math_exp> be <math_exp>-finite measure spaces, and let <math_exp> be the algebraic tensor product.  The product has a natural map into <math_exp> which takes <math_exp> to the function <math_exp>.  A moment's thought shows that this map is well-defined.  Is it also injective? It seems that this should be true, but I can't see how to prove it.  Intuitively, one needs to show that if <math_exp> a.e., then one should be able to cancel all the terms in the sum using bilinearity.  It is not quite clear how to do this without knowing anything about the terms. \"}\n",
      "1619 {'Id': '1619', 'Type': 'answer', 'ParentId': '1617', 'urls': [], 'exp': ['\\\\lambda_4', '\\\\lambda_2,\\\\lambda_3'], 'Body': \"Well, first off, a vector isn't orthogonal, a set of vectors is.  Remember that for two vectors to be orthogonal, it means that they are at right angles to each other, and orthonormal means that plus they're unit vectors. Now, if I had to say where I think your first error is, you took a 3 x 3 matrix and got a quadratic equation somehow, but you should have a cubic.  And also, as for your eigenvectors, where did <math_exp> come from? What is it? The symbol just appears from nowhere.  Perhaps you have typos and it's <math_exp> rather than 3 and 4, and each of them must have a nonzero eigenvector, because they are eigenvalues of multiplicity 1 (though with the equation error, they might not be eigenvalues), and so you would have to be incorrect about having a two dimensional eigenspace in the first place. However, both of the eigenvectors for 1 check out, which means that you've incorrectly calculated the eigenvalues. (instead of just an answer, I put in all the thinking I did to get there, because I thought it might help clarify how to check your work in the future) \"}\n",
      "1620 {'Id': '1620', 'Type': 'answer', 'ParentId': '790', 'urls': ['http://en.wikipedia.org/wiki/There_is_no_infinite-dimensional_Lebesgue_measure'], 'exp': ['0', '\\\\infty', '\\\\mathbb{R}^n', '\\\\mathbb{R}^n'], 'Body': 'In an infinite-dimensional normed space with a translation-invariant measure, the measure of a ball must be either <math_exp> or <math_exp>.  This fact is sometimes summarized as \"there is no infinite-dimensional Lebesgue measure.\"  So unless you have some other notion of \"volume\" in mind, only the finite-dimensional case (i.e. <math_exp> with some other norm) has any content.  And with regard to Hilbert spaces, the only finite-dimensional Hilbert spaces are <math_exp> with the Euclidean inner product, so of course we know about this. '}\n",
      "1622 {'Id': '1622', 'Type': 'answer', 'ParentId': '1614', 'urls': ['http://www.imgftw.net/img/326639277.png'], 'exp': [], 'Body': 'Regarding your edit: (0.2, 0) — (1, 0.2) — (0.8, 1) — (0, 0.8) (and many others) http://www.imgftw.net/img/326639277.png '}\n",
      "1623 {'Id': '1623', 'Type': 'answer', 'ParentId': '1526', 'urls': ['http://en.wikipedia.org/wiki/Continued_fraction'], 'exp': [], 'Body': 'Continued fractions give the best approximations to a number using smaller terms in the fraction. This may take a bit of practice to calculate at speed, but for the number in question there are only 8 terms: with convergents: 0, 1, 8/9, 9/10, 35/39, 79/88, 193/215, 1430/1593 One can clearly see that the option 9/10 is the best one from the choices on offer. Note the convergents get progressively more accurate and oscillate between being over-estimates and under-estimates. The final term is, of course, the original fraction itself in its simplest form. '}\n",
      "1624 {'Id': '1624', 'Type': 'answer', 'ParentId': '1617', 'urls': [], 'exp': [], 'Body': \"Your characteristic polynomial is incorrect. Go to wolframalpha.com and enter: {{2, -1, 1},{-2, 3, 2}, {-1, 1, 0}} www.wolframalpha.com/input/?i={{2,+-1,+1},{-2,+3,+2},+{-1,+1,+0}} You will see that the characteristic polynomial is of 3rd degree. It must be of 3rd degree because your matrix is 3x3. Here is what you'll get: p(x)=-x^3+5 x^2-3 x-1 lambda_1 = 4.23607 lambda_2 = 1 lambda_3 = -0.236068 And you will also get 3 eigenvectors. (Never 4 for a 3x3 matrix). \"}\n",
      "1625 {'Id': '1625', 'Type': 'answer', 'ParentId': '1548', 'urls': ['http://groupprops.subwiki.org/wiki/Burnside%27s_normal_p-complement_theorem'], 'exp': ['N', 'M', 'm', 'M', 'C_N(m)', 'm', 'N', 'M', 'm', 'm', 'N', 'C_N(m)', 'M', 'N', 'C_N(m) = N', 'm \\\\in Z(G)', 'M &lt; C_N(m)M &lt; G', 'M', 'N', 'm', 'N', 'G'], 'Body': \"You can apply -complement theorem to get a normal complement <math_exp> of <math_exp>. Then take an element <math_exp> of <math_exp> with prime order. Case 1: The centralizer <math_exp> of <math_exp> in <math_exp> is nontrivial. As <math_exp> centralizes <math_exp>, it acts on the fixed points of <math_exp> (in the action by conjugation on <math_exp>), i.e., <math_exp> is an <math_exp>-invariant subgroup of <math_exp>. If <math_exp>, then <math_exp> contradicts ii). Otherwise <math_exp> contradicting the maximality of <math_exp>. Case 2: <math_exp> has the fixed point free automorphism <math_exp> of prime order. By Thompson's thesis <math_exp> is nilpotent, hence <math_exp> solvable. \"}\n",
      "1626 {'Id': '1626', 'Type': 'question', 'Title': 'Modular exponentiation using Euler’s theorem', 'Tags': ['number-theory', 'modular-arithmetic'], 'AcceptedAnswerId': '1634', 'urls': ['http://en.wikipedia.org/wiki/Euler%27s_theorem'], 'exp': ['27^{41}\\\\ \\\\mathrm{mod}\\\\ 77', '27^{60}\\\\ \\\\mathrm{mod}\\\\ 77 = 1', ' a^{\\\\phi(n)}\\\\ \\\\mathrm{mod}\\\\ n = 1 ', ' \\\\phi(77) = \\\\phi(7 \\\\cdot 11) = (7-1) \\\\cdot (11-1) = 60 ', '27^{10} \\\\mathrm{mod}\\\\ 77 = 1', ' 27^{41}\\\\ \\\\mathrm{mod}\\\\ 77 = 27^{10} \\\\cdot 27^{10} \\\\cdot 27^{10} \\\\cdot 27^{10} \\\\cdot 27^{1}\\\\ \\\\mathrm{mod}\\\\ 77 = 1 \\\\cdot 1 \\\\cdot 1 \\\\cdot 1 \\\\cdot 27 = 27 ', '27^{41}\\\\ \\\\mathrm{mod}\\\\ 77', '27^{60}\\\\ \\\\mathrm{mod}\\\\ 77 = 1'], 'Body': 'How can I calculate <math_exp> as simple as possible? I already know that <math_exp> because of Euler’s theorem: <math_exp> and <math_exp> I also know from using modular exponentiation that <math_exp> and thus <math_exp> But can I derive the result of <math_exp> using <math_exp> somehow? '}\n",
      "1627 {'Id': '1627', 'Type': 'question', 'Title': 'Thin categories and graphs (isomorphism of categories)', 'Tags': ['category-theory'], 'AcceptedAnswerId': '1636', 'urls': ['http://katmat.math.uni-bremen.de/acc/acc.pdf'], 'exp': ['(O, H, id, \\\\cdot)', 'O', 'H', 'id', '\\\\cdot'], 'Body': 'I try to learn the theory of category from The Joy of Cats. I got stacked with the first exercise (3A a). If we have a simple graph with one vertex and 2 nodes all we know is that in category there is 2 objects and one homomorphism (beside identities). Hence: \\\\[C = (\\\\{\\\\{x, y\\\\}, \\\\{1, 2\\\\}\\\\}, \\\\{f, id\\\\}, id, \\\\cdot), f = \\\\{(x, 1), (y, 2)\\\\}\\\\] and \\\\[C = (\\\\{\\\\{x\\\\}, \\\\{1\\\\}, \\\\{f, id\\\\}, id, \\\\cdot), f = \\\\{(x, 1)\\\\}\\\\] Should have the same graph but are they isomorphic? Note about notation: I treat category as quadruple <math_exp> where <math_exp> is class of objects (here is set), <math_exp> is class of homomorphisms (in example set). <math_exp> is identity and <math_exp> is composition of homomorphisms. '}\n",
      "1628 {'Id': '1628', 'Type': 'answer', 'ParentId': '1618', 'urls': ['https://math.stackexchange.com/questions/1618/is-the-natural-map-lpx-otimes-lpy-to-lpx-times-y-injective/1678#1678', 'https://math.stackexchange.com/questions/1618/is-the-natural-map-lpx-otimes-lpy-to-lpx-times-y-injective/1678#1678', 'https://math.stackexchange.com/questions/1618/is-the-natural-map-lpx-otimes-lpy-to-lpx-times-y-injective/1678#1678'], 'exp': ['U,V,W', 'K', '\\\\alpha:U\\\\otimes V\\\\to W', 'u_0', 'U', '\\\\beta:W\\\\to V', '\\\\gamma:U\\\\to K', '\\\\gamma(u_0)\\\\not=0', '\\\\beta(\\\\alpha(u\\\\otimes v))=\\\\gamma(u)v\\\\ \\\\forall\\\\ u,v.', '\\\\alpha', 'U,V,W', 'K', 'X,Y,X\\\\times Y', '\\\\beta,\\\\gamma', '(\\\\beta(w))(y)=w(x_0,y)', '\\\\gamma(u)=u(x_0)', 'x_0', 'u_0(x_0)\\\\not=0', 'X', '\\\\mathcal V(X)', 'X', '\\\\mathcal N', 'V(X)', 'Y', '\\\\mathcal V(X)\\\\times\\\\mathcal V(Y)', '\\\\mathcal V(X\\\\times Y)', '(f,g)', '(x,y)\\\\mapsto f(x)g(y)', '\\\\Phi', 'V(X)\\\\otimes V(Y)', 'V(X\\\\times Y)', 'X', 'Y', '\\\\sigma', '\\\\Phi', 'f_i,\\\\dots,f_n', '\\\\mathcal V(X)', 'S\\\\subset X^n', 'x\\\\in X^n', '\\\\det(f_i(x_j))\\\\not=0', 'f_i', 'V(X)', 'S', '|A|', 'A', 'S_g', 'g', 'd(x):=\\\\det(f_i(x_j))', 'x\\\\in X^n', 'S', 'S_d', 'f_i', 'V(X)', '|S_d|=0', '\\\\lambda', '\\\\mathbb C^n', 'g:=\\\\sum \\\\lambda_i f_i', '|S_g|=0', '\\\\lambda', 'n', '\\\\lambda_i', 'A(x)', '(f_i(x_j))', 'x', '(S_g^c)^n', '\\\\lambda A(x)=0', 'A(x)', 'd(x)=0', 'S_d', '((S_g^c)^n)^c', 'X^n', '|S_d|=0', 'f_i', 'V(X)', 'd(x)', \"d(x)=\\\\sum\\\\ d_i(x')\\\\ f_i(x_n)\", \"x':=(x_1,\\\\dots,x_{n-1})\", '|S_{d_i}|&gt;0', 'i', 'U', 'S_{d_i}', \"d(x',\\\\bullet)\", 'x_n\\\\mapsto d(x)', \"S_d=\\\\bigcup_{x'\\\\in X^{n-1}}\\\\ \\\\{x'\\\\}\\\\times S_{d(x',\\\\bullet)}.\", \"0=|S_d|=\\\\int_{X^{n-1}}\\\\ |S_{d(x',\\\\bullet)}|\\\\ dx'.\", 'f_i', 'V(X)', \"|S_{d(x',\\\\bullet)}|&gt;0\", \"x'\", 'U', 'i,j,k', 'm', 'p,q,r', 'n', 'f_1,\\\\dots,f_m', '\\\\mathcal V(X)', 'g_1,\\\\dots,g_n', '\\\\mathcal V(Y)', 'F_{ip}(x,y):=f_i(x)g_p(y)', 'f_i', 'g_p', 'V(X)', 'V(Y)', 'F_{ip}', 'V(X\\\\times Y)', 'S\\\\subset X^m', 'T\\\\subset Y^n', '\\\\det(f_i(x_j))\\\\not=0', '\\\\det(g_p(y_q))\\\\not=0', 'U:=S^n\\\\times T^m', '(X\\\\times Y)^{mn}', '(i,p)', 'z\\\\in(X\\\\times Y)^{mn}', 'z_{ip}=(x_{pi},y_{ip})', 'z', 'U', 'x_p\\\\in S', 'y_i\\\\in T', '(i,p)', '\\\\det\\\\Big(f_j(x_{pk})\\\\Big)_{jk}\\\\not=0\\\\not =\\\\det\\\\Big(g_q(y_{ir})\\\\Big)_{qr}', '(i,p)', '(i,p)', '\\\\det\\\\Big(f_i(x_{qj})\\\\ g_p(y_{jq})\\\\Big)_{(i,p)(j,q)}\\\\not=0.', 'X', 'Y', 'X', 'Y'], 'Body': 'The previous version of this answer was incorrect. It has generously been accepted by Nate Eldredge on the ground that \"it definitely contains the right idea\". This previous version has better remain forgotten forever. The reader should consult Nate\\'s answer, which is correct and complete. I\\'ll just try here to spell out the underlying linear algebra lemma (which is trivial, but which I failed to see). UNDERLYING LEMMA. Let <math_exp> vector spaces over some field <math_exp>, and <math_exp> a linear map. Assume that for each nonzero vector <math_exp> in <math_exp> there are linear maps <math_exp> and <math_exp> such that <math_exp> and    <math_exp> Then <math_exp> is injective. The proof can be very easily extracted from Nate\\'s answer. Here is the most standard application: APPLICATION. Let <math_exp> be respectively the spaces of <math_exp>-valued functions on the sets <math_exp>, and <math_exp> are given by <math_exp>, <math_exp> where <math_exp> is chosen so that <math_exp>. EDIT. Here is a mild generalization. Let <math_exp> be a measure space, let <math_exp> be the vector space of all measurable complex valued functions on <math_exp>, let <math_exp> be the subspace of functions vanishing almost everywhere, and let <math_exp> be the quotient. [Is there a standard notation for these spaces?] Let <math_exp> be another measure space. The bilinear map from <math_exp> to <math_exp> sending <math_exp> to <math_exp> induces a linear map <math_exp> from <math_exp> to <math_exp>. THEOREM. Assume <math_exp> and <math_exp> are <math_exp>-finite. Then <math_exp> is injective. This slightly more general than the statement proved in Nate\\'s answer because no integrability assumptions are made on the functions. LEMMA. Let <math_exp> be in <math_exp>, and <math_exp> the set of <math_exp> such that <math_exp>. Then the <math_exp> are linearly independent in <math_exp> if and only if the measure of <math_exp> is positive. Denote by <math_exp> the measure of <math_exp>, and by <math_exp> the support of the function <math_exp>, that is the set of points where it is nonzero. Put <math_exp> for <math_exp>. The above set <math_exp> is now <math_exp>. To prove the if part of the lemma, assume the <math_exp> are linearly dependent in <math_exp> and check <math_exp> as follows. Let <math_exp> be a nonzero vector of <math_exp> such that <math_exp> satisfies <math_exp>. Denote again by <math_exp> the 1 by <math_exp> matrix formed by the <math_exp>, and by <math_exp> the matrix <math_exp>. For <math_exp> in <math_exp>, where the superscript c mean \"complement\", we have <math_exp>. Multiplying on the right by the adjugate of <math_exp> we get <math_exp>. This shows that <math_exp> is contained in the measure zero subset <math_exp> of <math_exp>. To prove the only if part of the lemma, assume <math_exp> and check that the <math_exp> are linearly dependent in <math_exp> as follows. Expand <math_exp> as <math_exp> with <math_exp>. Arguing by induction, we can assume <math_exp> for all <math_exp>. Let <math_exp> be the union of the <math_exp>. Denoting by <math_exp> the function <math_exp>, we have <math_exp> Fubini yields <math_exp> If the <math_exp> were linearly independent in <math_exp>, we would have <math_exp> for all <math_exp> in <math_exp>, a contradiction. The lemma is proved. Let\\'s prove the theorem. In the next lines, <math_exp> will lie between 1 and <math_exp>, whereas <math_exp> will lie between 1 and <math_exp>. Let <math_exp> be in <math_exp>; let <math_exp> be in <math_exp>; and put <math_exp>. The theorem is equivalent to the statement that the linear independence of the <math_exp> and <math_exp> in <math_exp> and <math_exp> implies that of the <math_exp> in <math_exp>. Let <math_exp> and <math_exp> be defined by the conditions <math_exp> and <math_exp>. Embedd <math_exp> into <math_exp> in the obvious way, and denote the <math_exp>-th coordinate of <math_exp> by <math_exp>. Then <math_exp> is in <math_exp> iff <math_exp> and <math_exp> for all <math_exp>, that is iff <math_exp> for all <math_exp>. We claim that the validity of this double nonequality for all <math_exp> implies the nonequality <math_exp> In view of the lemma, the theorem is equivalent to the claim. But the claim doesn\\'t depend on the measures defined on <math_exp> and <math_exp>. As the theorem holds when <math_exp> and <math_exp> are equipped with the counting measure (see the application of the \"underlying lemma\" at the beginning of the post), we are done. '}\n",
      "1629 {'Id': '1629', 'Type': 'answer', 'ParentId': '653', 'urls': ['http://rmp.aps.org/abstract/RMP/v51/i3/p591_1'], 'exp': ['\\\\pi_k(X)', 'k+1', 'X', 'R^d', 'X'], 'Body': 'Homotopy groups and homotopy classes of maps are used in physics to study topological defects.  Roughly speaking, <math_exp> classifies codimension <math_exp> defects in textures modeled on <math_exp> (maps from <math_exp> to <math_exp> continuous except at the loci of defects).  Other homotopy classes of maps can be used to study linked defects. See the following beautiful review paper by N.D. Mermin for an introduction: http://rmp.aps.org/abstract/RMP/v51/i3/p591_1 '}\n",
      "1630 {'Id': '1630', 'Type': 'answer', 'ParentId': '1626', 'urls': ['http://en.wikipedia.org/wiki/Exponentiation_by_squaring'], 'exp': ['27^{41} = 27^{32+8+1} = 27 \\\\cdot 27^8 \\\\cdot (27^8)^4 = (*)', '\\\\big[ 27^8 = ((27^2)^2)^2 = (36^2)^2 = 64^2 = 15 \\\\big]', '(*) = 27 \\\\cdot 15 \\\\cdot 15^4 = 27 \\\\cdot 15 \\\\cdot (15^2)^2 = 27 \\\\cdot 15 \\\\cdot 71^2 = 27 \\\\cdot 15 \\\\cdot 36 = 27'], 'Body': 'You can use exponentiation by squaring (all operations are modulo 77): <math_exp> <math_exp> <math_exp> This uses only 7 multiplications instead of 41. '}\n",
      "1631 {'Id': '1631', 'Type': 'answer', 'ParentId': '709', 'urls': ['http://en.wikipedia.org/wiki/Parabola'], 'exp': ['(x,y)=\\\\left( 0,0\\\\right) ', 'y=ax^{2}', 'X=x+\\\\dfrac{b}{2a},Y=y+\\\\dfrac{b^{2}-4ac}{4a}', 'y=ax^{2}+bx+c', 'Y=aX^{2}', '(X,Y)=(0,0)', '(x,y)=\\\\left(-\\\\dfrac{b}{2a},-\\\\dfrac{b^{2}-4ac}{4a}\\\\right)', 'x', 'y', '-\\\\dfrac{b}{2a},-\\\\dfrac{b^{2}-4ac}{4a}', 'x', '-\\\\dfrac{b}{2a}', 'y=2x^2-x+4', 'X=x-1/4,Y=y-31/8'], 'Body': 'The vertex of a parabola is the point on the axis of symmetry that intersects it (Wikipedia). The point <math_exp> is the vertex of the parabola given by <math_exp>. By making the change of coordinates <math_exp>, equation <math_exp> is transformed into <math_exp>, the vertex of which is the point <math_exp>, i.e. <math_exp>. (This change of coordinates is a translation of both axes <math_exp>, <math_exp>, respectively, by <math_exp>.) Hence, the <math_exp>-coordinate of the vertex of the parabola is <math_exp>. Edit. Example. Plot of <math_exp>; blue axes: <math_exp> <img src=\"https://problemasteoremas.files.wordpress.com/2010/08/parabola.jpg?w=490&amp;h=392\" alt=\"alt text\"> '}\n",
      "1632 {'Id': '1632', 'Type': 'question', 'Title': 'Analysing an optics model in discrete and continuous forms', 'Tags': ['analysis', 'signal-processing', 'physics'], 'urls': ['https://math.stackexchange.com/questions/1130/preserving-the-extrema-of-one-function-after-applying-another'], 'exp': ['I(r) = \\\\sum_i e_i P(r - r_i)', 'e_i', 'r_i', 'P', 'P', 'P(x) = 0', '|x| &gt; p', 'e_i', 'I(r)^2 = \\\\sum_{i,j} e_i e_j P(r - r_i) P(r - r_j) \\\\cos (r_i - r_j)', '^2', 'p \\\\sim \\\\pi', 'p \\\\ll \\\\pi', 'p \\\\gg \\\\pi', '\\\\cos (r_i - r_j)', 'O', 'e_i', 'I(r) = \\\\int O(s) \\\\: P(r - s) \\\\; \\\\mathrm{d}s', 'I(r)^2 = \\\\int \\\\int  O(s) \\\\; O(t) \\\\; P(r-s) \\\\; P(r-t) \\\\; \\\\cos (s - t) \\\\;\\\\mathrm{d}s\\\\, \\\\mathrm{d}t'], 'Body': 'A discrete one-dimensional model of optical imaging looks like this: <math_exp> Here, the <math_exp> are point light sources at locations <math_exp> in the object and <math_exp> is a point spread function that blurs each point. We can assume that <math_exp> is even, non-negative and has a finite extent, ie <math_exp> for <math_exp>. The <math_exp> are all positive. A more complex imaging process instead produces an image like this: <math_exp> (The <math_exp> is a consequence of the reconstruction method. In practice we normally take the square root, but per my earlier question here, I think the salient structural features of the resulting image should be unchanged by this transformation.) Numerical simulations suggest that the latter method allows better resolution of the points when <math_exp>, and is not materially different when <math_exp>. (We ignore the case where <math_exp> as not physically tenable.) Intuitively, this is because the \"trough\" in the <math_exp> term reduces the interaction between points at some intermediate separations. I\\'d like to be able to be more analytical about this. We can readily extend the above to a continuous model with an object function <math_exp> in place of the <math_exp>. The ordinary image becomes a convolution integral <math_exp> for which there are standard analytical approaches available. However, the more complex reconstruction looks something like this: <math_exp> This looks fairly intractable to me, and I\\'m not really sure where to begin with it. So, my first question is: is there any point in trying to look at the continuous model, or should I just concentrate on the discrete? On the one hand, every single practical use case for the technique will actually be using discrete measurements, so in that sense the discrete model is more realistic. On the other, it may be that there are things one could demonstrate using the continuous model that are not available otherwise. (But even if that\\'s the case, I\\'m probably not capable of doing so without at least being nudged in the right direction!) And I guess my second question is: continuous or not, does anyone have any other suggestions of useful ways to analyse this model? '}\n",
      "1634 {'Id': '1634', 'Type': 'answer', 'ParentId': '1626', 'urls': [], 'exp': ['27^{10} \\\\equiv 1 \\\\mod 11', '27 \\\\equiv -1 \\\\mod 7', '27^{10} \\\\equiv (-1)^{10} \\\\equiv 1\\\\mod 7', '27^{10} \\\\equiv 1 \\\\mod 77', '27^{41} = 27^{40+1} \\\\equiv 27 \\\\mod 77', '27^{41} \\\\equiv 27 \\\\equiv 5 \\\\mod 11', '27^{6} \\\\equiv 1 \\\\mod 7', '27^{41} = 27^{42-1} \\\\equiv 27^{-1} \\\\equiv -1 \\\\mod 7', 'n', 'n'], 'Body': \"As suggested in the comment above, you can use the Chinese Remainder Theorem, by using Euler's theorem / Fermat's theorem on each of the primes separately. You know that <math_exp>, and you can also see that modulo 7, <math_exp>, so <math_exp> as well. So <math_exp>, and <math_exp>. (We've effectively found the order of 27 as 10, but a more mechanical procedure may be to use that <math_exp> and <math_exp> to see that <math_exp>, and put 5 mod 11 and -1 mod 7 together to get 27.) This is if you're doing it by hand, for this case. In general, algorithmically, you would just use repeated squaring to exponentiate numbers. You don't gain much by using Euler's theorem, since finding the order of a number mod <math_exp> is as hard as factoring <math_exp>. \"}\n",
      "1635 {'Id': '1635', 'Type': 'answer', 'ParentId': '1609', 'urls': [], 'exp': ['k(x,y),', 'x', 'y', 'f(x,y) = 0', 'f', 'f(x,y) = 0', 'g(x,y) = 0', 'f(x,y) = 0', 'd', 'd', 'V_n', '\\\\leq n', 'x', 'y', 'n \\\\geq d', 'n', 'V_n', '(n+2)(n+1)/2', 'V_n', 'f', 'V_{n-d}', 'n-d', 'f', 'f V_{n-d}', 'V_n', '(n-d+2)(n-d+1)/2', 'V_n/fV_{n-d},', 'n d + 1 - (d-1)(d-2)/2', 'g \\\\in V_n', 'V_n/fV_{n-d}', 'n', 'f', 'f(x,y) = 0', 'g(x,y) = 0', 'f(x,y) = 0', 'n d', 'n d', 'V_n/f V_{n-d}', 'n d', 'f(x,y) = 0', 'n', 'g', 'g(x,y) = 0', 'n d', 'f(x,y) = 0', 'n', 'n d - (d-1)(d-2)/2.', 'n d', 'n d', 'n d', 'n d', 'n d', 'n d', 'n', 'n d -(d-1)(d-2)/2', '(d-1)(d-2)/2', 'n d', 'f(x,y) = 0', 'd', '1', '2', 'n d', 'n', 'd = 2', 'd = 3', 'f(x,y) = 0', 'n =3', 'V_{n}/f V_{n-d}', 'n d + 1 - ', 'f', 'g', 'f(x,y) = 0', '(d-1)(d-2)/2', 'd'], 'Body': \"If you want to avoid algebraic geometry and topology, then you will probably be forced to use algebra.  This is okay, because the things you ask about have algebraic interpretations. First of all, it probably helps to write the function field as <math_exp> where <math_exp> and <math_exp> are related by some equation <math_exp>.  One problem is going to be that <math_exp> can't be chosen to be smooth in general (because not every curve can be embedded as a smooth plane curve), but it's probably best to ignore this; if it becomes important, then probably your colleagues will be delving deeper into the theory anyway, and so the whole level of explanation can be ramped up. Now to explain divisors, you can have them imagine intersecting <math_exp> with some other curve <math_exp>; the intersections will be a bunch of points, possibly with multiplicity. This is a divisor.   So divisors are just natural ways of encoding how curves intersect one another.  (Again, I am ignoring here the issue of points at infinity; you will have to decide whether that is appropriate, or whether you need a higher level of precision in your explanations.) As for the genus, it's a little more complicated to explain algebraically, but possible; here goes: Suppose that the equation <math_exp> has degree <math_exp>, so your function field corresponds to a degree <math_exp> curve in the plane.  Now let <math_exp> be the vector space of all polynomials of degree <math_exp> in <math_exp> and <math_exp>.  Let's suppose that <math_exp> (and in general, we should think that <math_exp> is large). A simple computation shows that <math_exp> has dimension <math_exp>.  Inside <math_exp>, we have a subspace consisting of all the multiples of <math_exp>.  This subspace is obtained by taking the elements of <math_exp> (i.e. polynomials of degree at most <math_exp>) and multiplying them by <math_exp>, i.e. it is the subspace <math_exp> of <math_exp>, and so has dimension <math_exp>.  If we look at the quotient <math_exp> this then has dimension  <math_exp>. What is the meaning of this quotient? If <math_exp> represents a non-zero element of <math_exp>, then it is a degree <math_exp> equation that is not divisible by <math_exp>, so it doesn't vanish identically on <math_exp>, so by Bezout's theorem, the intersection of <math_exp> and <math_exp> is <math_exp>  points, i.e. a divisor of degree <math_exp>.  So we see that non-zero elements of  <math_exp> correspond to those divisors of degree <math_exp> that are obtained by  intersecting <math_exp> with a degree <math_exp> curve.  (In fact, we should think about non-zero elements up to scaling, because if we multiply <math_exp> by a non-zero scalar, the curve <math_exp> doesn't change.) Thus among all the degree <math_exp> divisors on <math_exp>, those that come by intersecting with a degree <math_exp> curve form a space of dimension <math_exp>  (Here I have subtracted 1, because rescaling accounts for 1 of the dimensions in the above formula.) Now what is the dimension of the space of all divisors of degree <math_exp> (with non-negative coefficients, which are the only kind that can possibly arise as intersections; divisors with non-negative coefficients are called effective)?  Well, we just have to choose <math_exp> points and add them together.  We are choosing the points from a curve, which is one-dimensional, so that means there is an <math_exp>-dimensional space of effective divisors of degree <math_exp>. Thus we see that, in the <math_exp>-dimensional space of all effective divisors of degree <math_exp>, those that arise by intersecting with a degree <math_exp> curve are a subspace of dimension <math_exp>. The quantity <math_exp> is precisely the genus. So what we see is that the bigger the genus is, the harder it is for a degree <math_exp> divisor on <math_exp> to be obtained by intersecting with another curve. For example, if the degree <math_exp> is <math_exp> or <math_exp>, then the genus vanishes, and  every degree <math_exp> divisor comes from intersecting with a degree <math_exp> curve. E.g. on a conic (i.e. when <math_exp>) any two points come from intersecting with a line (the line that passes through those two points), any four points come from intersecting  with a conic, and so on. On the other hand, if the degree <math_exp>, then not all triples of points on <math_exp> come from intersecting with a line: in fact if you give yourself two points, then they determine a line, which in turn determines the 3rd point of intersection. Similarly (going up to the case <math_exp>), a general set of 9 points on the cubic doesn't come from intersecting with another cubic; instead, if you give yourself 8 points on the cubic curve, you can find another cubic passing through these 8 points, and its 9th point of intersection is uniquely determined by the given 8 points.  (Since 9 = 8 + 1, this  is a particular manifestation of the fact that our cubic curve has genus 1.) As you can see, I've lapsed into a more geometrical way of thinking (using notions such as dimension), but I don't think one can avoid this completely: the concept of genus arose historically from essentially the kind of computations that I've just been making, and at some point you have to think about spaces of divisors and their dimensions if you want to understand it.  Still, I hope that this gives you an avenue to explaining the concept which is more algebraic, and so more accessible to your colleagues. One more technical remark: The space <math_exp> is an example of a Riemann--Roch space, and the formula for its dimension (<math_exp> the genus) is a special case of the Riemann-Roch formula. [Technical remark added in response to a question of T. in the comments below; feel free to ignore it if it is at too high a level:]  Note that the above discussion works even if <math_exp> is allowed to be singular (and irreducible, say, so that we can't have polynomials <math_exp> which vanish on one component of <math_exp> without vanishing  on the whole curve).  The reason is that <math_exp> is always the arithmetic genus of a plane curve of degree <math_exp>, whether or not the curve is smooth, and it is the arithmetic genus that intervenes in the Riemann--Roch formula.  (I believe that this is the origin of the adjective arithmetic in arithmetic genus: it is this version of the genus for singular curves that comes up when you make Riemann--Roch-type calculations of the dimensions of various spaces of divisors.) \"}\n",
      "1636 {'Id': '1636', 'Type': 'answer', 'ParentId': '1627', 'urls': [], 'exp': ['F', '\\\\{x, y\\\\}', '\\\\{ x \\\\}', '\\\\{ 1, 2 \\\\}', '\\\\{ 1 \\\\}', 'f', 'f'], 'Body': 'It is a bit hard to understand your notation. If understand correctly, the isomorphism is realized by the functor <math_exp> which sends <math_exp> to <math_exp> and <math_exp> to <math_exp> (on objects) and send the unique non trivial morphism <math_exp> to <math_exp>. '}\n",
      "1637 {'Id': '1637', 'Type': 'answer', 'ParentId': '764', 'urls': [], 'exp': [], 'Body': 'In Stieltjes integral you assign different importance to different parts of the set you are integrating over. The usefulness of this would become clearer when you know the theory of Lebesgue measure, which generalizes this even further. For example, there is the Dirac measure which when used for integrating functions cares only about the value of a function at a point (typically the origin). Understanding it in the Stieltjes form is not any harder. Moreover, it will pave the way for measure theory. '}\n",
      "1638 {'Id': '1638', 'Type': 'answer', 'ParentId': '998', 'urls': [], 'exp': [], 'Body': 'Nobody has mentioned the book of Shafarevich; so I mention myself. '}\n",
      "1639 {'Id': '1639', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': 'Anybody who wants to be a serious mathematician better read W. Rudin\\'s \"Principles of mathematical Analysis\". It gives a rigorous foundation to the basic notions analysis and introduces the reader to the world of rigor, after the sloppy days of calculus courses. One must learn the notion of rigor properly if one wants to be a mathematician. More than anything else, it is an exercise in the rectitude of thought. No other book is so universally used that would teach this notion, than Rudin. '}\n",
      "1640 {'Id': '1640', 'Type': 'answer', 'ParentId': '1413', 'urls': [], 'exp': [], 'Body': 'I do not want to be mean or rude here; but my personal belief is that if you do not appreciate number theory, you are missing the most important topic in mathematics. It is literally the heart of mathematics. If you really need a concrete example, I can give one from the point of view of algebra. To have an example of a purely non-separable polynomial, you go to char p. '}\n",
      "1641 {'Id': '1641', 'Type': 'answer', 'ParentId': '457', 'urls': [], 'exp': [], 'Body': 'The Fourier transform of the Dirac delta function is a constant function. Therefore whatever frequency your radio is turned to during a storm, or your TV, you will receive a noise. '}\n",
      "1642 {'Id': '1642', 'Type': 'answer', 'ParentId': '450', 'urls': ['http://en.wikipedia.org/wiki/List_of_canonical_coordinate_transformations'], 'exp': ['\\\\mathrm{d}A = J\\\\;\\\\mathrm{d}r\\\\,\\\\mathrm{d}\\\\theta = r\\\\,\\\\mathrm{d}r\\\\,\\\\mathrm{d}\\\\theta', '\\\\iint_\\\\mathbf{R} f(r,\\\\theta)\\\\,\\\\mathrm{d}A = \\\\int_a^b \\\\int_0^{r(\\\\theta)} f(r,\\\\theta) r\\\\,\\\\mathrm{d}r\\\\,\\\\mathrm{d}\\\\theta', 'f(r, \\\\theta)', 'r'], 'Body': 'As others have already mentioned, the Jacobian determinant transforms one coordinate system to another by relating infinitesimal areas (or volumes) from one system to another. Consider going from Cartesian to Polar coordinates: \\\\begin{align} J &amp;= \\\\det\\\\frac{\\\\partial(x,y)}{\\\\partial(r,\\\\theta)} =\\\\begin{vmatrix}  \\\\frac{\\\\partial x}{\\\\partial r} &amp; \\\\frac{\\\\partial x}{\\\\partial \\\\theta} \\\\\\\\\\\\\\\\  \\\\frac{\\\\partial y}{\\\\partial r} &amp; \\\\frac{\\\\partial y}{\\\\partial \\\\theta} \\\\\\\\\\\\\\\\ \\\\end{vmatrix} \\\\\\\\&amp;=\\\\begin{vmatrix}  \\\\cos\\\\theta &amp; -r\\\\sin\\\\theta \\\\\\\\\\\\\\\\  \\\\sin\\\\theta &amp; r\\\\cos\\\\theta \\\\\\\\\\\\\\\\ \\\\end{vmatrix} =r\\\\cos^2\\\\theta + r\\\\sin^2\\\\theta = r \\\\end{align} This is useful because: <math_exp> <math_exp> Which tells you that if you have a function <math_exp> you can compute the integral as long as you remember to add a factor of <math_exp>. The common transformations have all been worked out and can be found here on Wikipedia. '}\n",
      "1643 {'Id': '1643', 'Type': 'answer', 'ParentId': '903', 'urls': ['http://www.pearson.ch/HigherEducation/MathematicsStatistics/AdvancedMathematics/1471/9780131848696/Introduction-to-Topology-Pure-and.aspx'], 'exp': [], 'Body': \"I would like to recommend Topology: Pure and Applied, by Colin Adams and Robert Franzosa. Anyone familiar with Colin Adams's The Knot Book will expect this to be equally accessible. And they will not be disappointed.  In terms of your criteria&mdash;clarity and motivation&mdash;I find it unsurpassed. \"}\n",
      "1644 {'Id': '1644', 'Type': 'answer', 'ParentId': '1626', 'urls': ['http://en.wikipedia.org/wiki/Carmichael_function'], 'exp': ['27^{41} = 3^{123}', '3^{30} \\\\equiv 1 \\\\bmod 77', '3^{123} \\\\equiv 3^3 \\\\equiv 27 \\\\bmod 77'], 'Body': 'If you are serious about \"as simple as possible\" then observe that <math_exp> and use  Carmichael\\'s theorem (a strengthening of Euler\\'s theorem which actually gives a tight bound) to deduce that <math_exp> and hence <math_exp>.  But I do not think this is the right question to ask; you should really be asking \"as general as possible,\" and then the answer you have accepted is most appropriate. (Of course it suffices to use Euler\\'s theorem for the above computation, but few people seem to learn Carmichael\\'s theorem and I always like to point it out when I can.) '}\n",
      "1645 {'Id': '1645', 'Type': 'answer', 'ParentId': '262', 'urls': [], 'exp': [], 'Body': \"I've been rereading Littlewood's Miscellany recently. It's a very readable collection of the writings of J. E. Littlewood, carefully edited by Béla Bollobás. Any budding mathematician will draw much inspiration from it. I like A Mathematician's Apology, but if I was forced into choosing only one book, it would be Littlewood's Miscellany. \"}\n",
      "1646 {'Id': '1646', 'Type': 'answer', 'ParentId': '450', 'urls': ['http://en.wikipedia.org/wiki/Transformation_matrix'], 'exp': ['n', '(n+1)', '(n+1)'], 'Body': 'You can represent some non-linear transforms (like translation) of an <math_exp>-dimensional vector with an <math_exp>-dimensional matrix. However, converting the vector to its <math_exp>-dimensional homogeneous version and back is not a linear transformation and also not representable as a matrix. More is explained here. '}\n",
      "1647 {'Id': '1647', 'Type': 'question', 'Title': 'How do I calculate a specific variation for a known value of the normal distribution function?', 'Tags': ['algorithms', 'computer-science'], 'urls': [], 'exp': ['x = 10', '.001 = \\\\frac{1}{2 \\\\pi v^2}e^{-\\\\frac{x^2}{2v^2}}'], 'Body': 'I am writing a Gaussian blur filter in graphics shader code.  I want to make the blur parameterized by radius from the users perspective.  The best method I can figure to do this is to pick a suitable stopping point for y, say .001, and solve for the variance to plug into the normal distribution function that will achieve that value of y. Unfortunately I cannot for the life of me solve this equation for v... <math_exp> (blur radius) <math_exp>. '}\n",
      "1648 {'Id': '1648', 'Type': 'question', 'Title': 'How do I figure out what kind of distribution this is?', 'Tags': ['probability', 'gamma-function', 'average'], 'urls': ['https://i.stack.imgur.com/oqxik.png', 'https://i.stack.imgur.com/S9Tz8.png', 'https://i.stack.imgur.com/kqkUU.png', 'https://i.stack.imgur.com/iNgJH.png', 'https://i.stack.imgur.com/LWhyH.png'], 'exp': [], 'Body': 'I\\'ve sampled a real world process, network ping times. The \"round-trip-time\" is measured in milliseconds. Results are plotted in a histogram:  Ping times have a minimum value, but a long upper tail. I want to know what statistical distribution this is, and how to estimate its parameters. Even though the distribution is not a normal distribution, I can still show what I am trying to achieve. The normal distribution uses the function:  with the two parameters The formulas for estimating the two parameters are: <img src=\"https://upload.wikimedia.org/math/3/2/4/324081bc08a66fbeba64f43a3dbd5e1a.png\" alt=\"alt text\"> Applying these formulas against the data I have in Excel, I get: With these parameters I can plot the \"normal\" distribution over top my sampled data:  Obviously it\\'s not a normal distribution. A normal distribution has an infinite top and bottom tail, and is symmetrical. This distribution is not symmetrical. What principles would I apply, what flowchart, would I apply to determine what kind of distribution this is? And cutting to the chase, what is the formula for that distribution, and what are the formulas to estimate its parameters? I want to get the distribution so I can get the \"average\" value, as well as the \"spread\":  I am actually plotting the histrogram in software, and I want to overlay the theoretical distribution:  Tags: sampling, statistics, parameter-estimation, normal-distribution '}\n",
      "1649 {'Id': '1649', 'Type': 'question', 'Title': 'Finding the Extrema Non-differentiable Functions.', 'Tags': ['calculus', 'sequences-and-series', 'discrete-optimization'], 'AcceptedAnswerId': '1657', 'urls': [], 'exp': [], 'Body': 'Are there any examples of solving for the global maximum of a non-differentiable function where you: For all I know, the procedure above is fatally flawed (or there are trivial examples, I would be most interested in non-trivial examples) in some way, if that is the case let me know. I am specifically interested in examples involving absolute values. '}\n",
      "1650 {'Id': '1650', 'Type': 'answer', 'ParentId': '1648', 'urls': ['http://en.wikipedia.org/wiki/Exponential_distribution', 'http://stats.stackexchange.com', 'https://stats.stackexchange.com/questions/1315/how-do-i-figure-out-what-kind-of-distribution-this-is/1316#1316', 'http://en.wikipedia.org/wiki/Gamma_distribution'], 'exp': [], 'Body': \"The go-to distribution for things like wait times is the Exponential. Yours doesn't look exactly the same because of the tiny lower tail, but I would be inclined to attribute that to noise/measurement error. (The assumption of independence of events is almost certainly wrong for ping times, but it's probably still your best choice.) Also, you would probably be better off asking this sort of question on the stats site. Edit: As pointed out by Srikant Vadali, the Gamma distribution is more general and can account for a non-negligible short tail, so may be a more appropriate choice. It's easier to estimate the parameter for the exponential, though. \"}\n",
      "1651 {'Id': '1651', 'Type': 'question', 'Title': 'Distinct Hamiltonian cycles of the icosahedron and dodecahedron', 'Tags': ['geometry', 'reference-request', 'euclidean-geometry', 'graph-theory', 'discrete-geometry'], 'urls': ['http://mathworld.wolfram.com/HamiltonianCycle.html'], 'exp': [], 'Body': 'I am seeking a listing of the distinct Hamiltonian cycles following the edges of the icosahedron and the dodecahedron.  By distinct I mean they are not congruent by some symmetry of the icosahedron or dodecahedron (respectively). So they do not make the same sequence of angular turns. For example (as Gerhard corrected me in the comments), there is just one distinct Hamiltonian cycle on the cube. Hamiltonian cycles of the Platonic solids are all over the web, but I am not finding a definitive list of the number and a description of each.  Thanks to anyone who can point me in the right direction! '}\n",
      "1652 {'Id': '1652', 'Type': 'answer', 'ParentId': '1647', 'urls': ['https://gamedev.stackexchange.com/'], 'exp': [], 'Body': \"I don't think the radius has to involve the variance. You could just use the radius to scale the x-axis. So you could sample the distribution from -1 to 1, always. If the radius is 4, would sample 9 regions. For 10 you sample 21 times between -1 and 1. The variance can stay the same. You might want to try this question here too https://gamedev.stackexchange.com/. \"}\n",
      "1653 {'Id': '1653', 'Type': 'answer', 'ParentId': '1651', 'urls': ['http://mathworld.wolfram.com/IcosahedralGraph.html'], 'exp': [], 'Body': 'There is a listing of a hamilton cycle count of 2560 at this website: http://mathworld.wolfram.com/IcosahedralGraph.html. '}\n",
      "1654 {'Id': '1654', 'Type': 'answer', 'ParentId': '1388', 'urls': ['http://books.google.com/books?id=0bH6SUHSvloC&amp;pg=PA24'], 'exp': ['x^{11} = 1'], 'Body': \"Solving <math_exp> in radicals was first accomplished by Vandermonde, whose method is described on p.24-26 of Edwards' book on Galois theory \"}\n",
      "1655 {'Id': '1655', 'Type': 'answer', 'ParentId': '1442', 'urls': ['http://en.wikipedia.org/wiki/Leibniz_harmonic_triangle', 'http://upload.wikimedia.org/math/5/8/1/581098eb5e9213bf6c66e932ed218e08.png'], 'exp': ['\\\\frac{1}{n}', '\\\\frac{1}{(n+1)\\\\binom{n}{m}}', '1/1,1/2,\\\\dots,1/n', 'lcd(1/1,\\\\dots,1/n)'], 'Body': \"Consider Leibniz harmonic triangle — a table that is like &laquo;Pascal triangle reversed&raquo;: on it's sides lie numbers <math_exp> and each number is the sum of two beneath it (see the picture). One can easily proove by induction that m-th number in n-th row of Leibniz triangle is <math_exp>. So LHS of our identity is just lcd of fractions in n-th row of the triangle. But it's not hard to see that any such number is an integer linear combination of fractions on triangle's sides (i.e. <math_exp>) — and vice versa. So LHS is equal to <math_exp> — and that is exactly RHS. \"}\n",
      "1656 {'Id': '1656', 'Type': 'question', 'Title': 'Space of inscribed <span class=\"math-container\" id=\"13133\">n</span>-gons modulo projective transformations.', 'Tags': ['geometry', 'projective-geometry'], 'AcceptedAnswerId': '1671', 'urls': [], 'exp': ['P \\\\sim Q', 'P', 'Q', 'f', 'f(P) = Q', '\\\\sim', 'n', 'n-3'], 'Body': 'Say <math_exp> (<math_exp> and <math_exp> are «projectively equivalent») iff there is a projective transformation <math_exp> such that <math_exp>.  Then <math_exp> is an equivalence relation.  I read that the space of inscribed <math_exp>-gons modulo projective equivalence has dimension <math_exp>.  Why is this?  Also, are there any related results? '}\n",
      "1657 {'Id': '1657', 'Type': 'answer', 'ParentId': '1649', 'urls': [], 'exp': ['F_n(x) = \\\\sqrt{x^2+2^{-n}}', 'F_n(x) \\\\to \\\\sqrt{x^2} = |x|', 'F_n'], 'Body': \"A simple example: Let <math_exp>. It is not hard to show that <math_exp>. Every <math_exp> is differentiable and has a local minimum at 0, and indeed so does |x|. Let me know if this is what you're looking for. \"}\n",
      "1658 {'Id': '1658', 'Type': 'question', 'Title': 'Function behavior with very large variables', 'Tags': ['functions', 'special-functions'], 'AcceptedAnswerId': '1661', 'urls': [], 'exp': [], 'Body': 'Whenever I think about how a function behaves, I always try to identify a general pattern of behavior with some common numbers (somewhere between 5 and 100 maybe) and then I try to see if anything interesting happens around 1, 0 and into negative numbers if applicable. If that all works out, I essentially assume that I know that the function is going to behave similarly for very large numbers as it does for those relatively small numbers. Are there notable (famous, clever or common) functions where very large numbers would cause them to behave significantly differently than would initially be thought if I followed my regular experimental pattern? If so, are there any warning signs I should be aware of? '}\n",
      "1659 {'Id': '1659', 'Type': 'question', 'Title': 'Locating the quadrant containing a point on an n-sphere', 'Tags': ['geometry'], 'AcceptedAnswerId': '1663', 'urls': ['http://en.wikipedia.org/wiki/N-sphere'], 'exp': ['x \\\\in \\\\mathbb{R}^n', 'n', 'x'], 'Body': 'Suppose I have a point <math_exp> on an n-sphere. Suppose I divide the n-sphere into 4 sections (I think this makes sense in <math_exp> dimensions), how do I know which section <math_exp> lies on? '}\n",
      "1660 {'Id': '1660', 'Type': 'question', 'Title': 'Direct proof of Gelfand-Zetlin identity', 'Tags': ['combinatorics'], 'AcceptedAnswerId': '1719', 'urls': ['http://imrn.oxfordjournals.org/content/vol2010/issue13/images/medium/rnp223ueq3.gif'], 'exp': ['D(a_1,\\\\dots,a_n)', 'i}(a_j-a_i)', 'a_i', 'a_1\\\\le a_2\\\\le\\\\dots\\\\le a_n', 'D(a_1,...,a_n)/D(1,...,n)', '\\\\frac{n(n+1)}2', 'a_i', 'a_1\\\\le b_1&lt;a_2\\\\le b_2&lt;a_3', \"b_1\\\\le b'&lt;b_2\", '\\\\frac{(a_3-a_2)(a_3-a_1)(a_2-a_1)}{3}', 'gl_n', 'n=3', 'a_1\\\\le b_1&lt;a_2\\\\le b_2&lt;a_3', \"a_1\\\\le b'&lt;a_3\", \"b_1\\\\le b'&lt; b_2\", '(b_1,b_2)', \"b'\", '1/2', 'D(a_1,a_2,a_3)/D(1,2,3)'], 'Body': \"Denote by <math_exp> the product <math_exp>. Assuming that <math_exp> are integers s.t. <math_exp>, proove that <math_exp> is the number of Gelfand-Zetlin triangles (that is, triangles consisting of <math_exp> integers, s.t. each number is greater it's lower-left neighbor but not greater than lower-right neighbor) with the base <math_exp>. For example, for n=3 one needs to prove that number of b1, b2, b' s.t. <math_exp>, <math_exp> is exactly <math_exp>. As one can guess from the name &ldquo;Gelfand-Zetlin&rdquo;, this fact is well-known in representation theory (namely, in LHS we count dimension of a <math_exp>-representation by Weyl formula, and in RHS we count elements in Gelfand-Zetlin basis of the same representation). But maybe someone can come with more or less direct proof? (Some kind of bijective proof, maybe.) For simplicity, consider the case <math_exp>: D(a_1,a_2,a_3) counts the number of triangles s.t. <math_exp>, <math_exp> — and we're interested only in G-Z triangles, i.e. in triangles s.t. <math_exp>. Now, mathematical expectation of the length of the interval <math_exp> is exactly one half of the length of the interval from which <math_exp> is chosen. So one may expect that the probability that random triangle is G-Z is <math_exp> — and the answer is indeed <math_exp>. (The main problem with this computation is that we're multiplying probabilities for events that are clearly not independent. And although for n=3 it's not hard to transform this heuristic argument into a formal proof, even for n=4 I failed to do such thing.) \"}\n",
      "1661 {'Id': '1661', 'Type': 'answer', 'ParentId': '1658', 'urls': ['http://www.geatbx.com/docu/fcnindex-01.html#P160_7291', 'https://i.stack.imgur.com/sU4eh.gif', 'http://www.geatbx.com/docu/fcnindex-msh_f8_500-19.gif', 'https://i.stack.imgur.com/NTqKe.gif', 'http://www.geatbx.com/docu/fcnindex-msh_f8_50-20.gif', 'https://i.stack.imgur.com/VVp1x.gif', 'http://www.geatbx.com/docu/fcnindex-msh_f8_8-21.gif'], 'exp': [' f(\\\\mathbf x) = \\\\frac1{4000}\\\\sum_{i=1}^n x_i^2 - \\\\prod_{i=1}^n \\\\cos\\\\left(\\\\frac{x_i}{\\\\sqrt i}\\\\right) + 1 '], 'Body': 'The Griewank function, <math_exp> which is one of the objective functions used in testing optimization algorithms, looks completely different in large scale (dominated by x2) and small scale (dominated by cos x).  (source: geatbx.com)  (source: geatbx.com)  (source: geatbx.com) '}\n",
      "1662 {'Id': '1662', 'Type': 'answer', 'ParentId': '1658', 'urls': [], 'exp': ['f(x)=\\\\frac{n(x)}{d(x)}=q(x)+\\\\frac{r(x)}{d(x)}', '\\\\frac{r(x)}{d(x)}', '\\\\frac{r(x)}{d(x)}', '\\\\frac{r(x)}{d(x)}'], 'Body': 'Many rational functions <math_exp> (where deg(r) &lt; deg(d)) behave very differently in the general vicinity of the zeros of d(x) than for large (positive or negative) values x.  Near the zeros of d(x), the values of <math_exp> dominate the values of q(x) (that is, f(x) behaves like <math_exp>), whereas for large (positive or negative) values of x, the values of q(x) dominate the values of <math_exp> (that is, f(x) behaves like q(x)). '}\n",
      "1663 {'Id': '1663', 'Type': 'answer', 'ParentId': '1659', 'urls': [], 'exp': ['n', '\\\\mathbb{R}^3', 'x', 'x_1', 'x_2', '++', 'x_1', 'x_2', '-+', '2^n', 'x'], 'Body': \"This is just a rephrasing of ShreevastasR's answer; no credit to me.  It does make sense to divide an <math_exp>-sphere into quadrants, as you explain in <math_exp>: partition by two coordinate planes. But then deciding which quadrant is, as ShreevastasR says, simply looking at the signs of the coordinates of <math_exp>.  If <math_exp> and <math_exp> are both positive, you are in the first, <math_exp>, quadrant; if <math_exp> is negative and <math_exp> positive, you are in the second, <math_exp>, quadrant. And so on.  If instead you partition the sphere into <math_exp> orthants, then you consider all the signs of the coordinates of <math_exp>. \"}\n",
      "1664 {'Id': '1664', 'Type': 'answer', 'ParentId': '1658', 'urls': ['http://mathworld.wolfram.com/ChebyshevBias.html'], 'exp': [], 'Body': 'The Chebyshev bias behavior is properly understood only by examining up to very very large numbers. '}\n",
      "1665 {'Id': '1665', 'Type': 'question', 'Title': 'Prime satisfying a given condition', 'Tags': ['number-theory', 'prime-numbers'], 'AcceptedAnswerId': '1669', 'urls': [], 'exp': ['p', '\\\\frac {p-1}{4} ', '\\\\frac {p+1}{2} ', 'p=13'], 'Body': 'Let <math_exp> be a prime. If <math_exp> and <math_exp> are also primes then prove that <math_exp>. '}\n",
      "1666 {'Id': '1666', 'Type': 'answer', 'ParentId': '1658', 'urls': ['http://en.wikipedia.org/wiki/Analytic_function', 'http://books.google.co.uk/books?id=pntQAAAAMAAJ&amp;q=concrete+mathematics+Ronald+L.+Graham,+Donald+Ervin+Knuth,+Oren+Patashnik&amp;dq=concrete+mathematics+Ronald+L.+Graham,+Donald+Ervin+Knuth,+Oren+Patashnik&amp;hl=en&amp;ei=pSBbTJ7nHcj64Aapq-38AQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CC4Q6AEwAA'], 'exp': ['f(x)', 'x', 'f(x)', 'x', 'f(x)', 'x\\\\in[-\\\\epsilon,\\\\epsilon]', '\\\\log n\\\\prec n^{0.0001}', 'n = 10^{100}', '\\\\log n = 100', 'n^{0.0001}', '10^{0.01}\\\\approx 1.0233', 'n = 10^{10^{100}}', '\\\\log n = 10^{100}', 'n^{0.0001} = 10^{10^{96}}'], 'Body': 'The values of a function <math_exp> for relatively small values of its argument <math_exp> is typically a very bad predictor of asymptotic behavior of <math_exp> for large <math_exp>. This is true even when <math_exp> is an analytic function which is uniquely determined by its values on any small interval <math_exp>. Have a look at this excerpt from \"Concrete Mathematics\" for a (not the worst possible) example of how deceptive the \"small argument values\"  intuition could be. It helps to cultivate an expansive attitude when we\\'re doing asymptotic analysis: We should think big, when imagining a variable that approaches infinity. For example, the hierarchy says that <math_exp>; this might seem wrong if we limit our horizons to teeny-tiny numbers like one googol, <math_exp>. For in that case, <math_exp>, while <math_exp> is only <math_exp>. But if we go up to a googolplex, <math_exp>, then <math_exp> pales in comparison with <math_exp>. '}\n",
      "1667 {'Id': '1667', 'Type': 'answer', 'ParentId': '1649', 'urls': [], 'exp': ['f', '[a,b]', 'f', '[a,b]', 'c_1 &lt; c_2&lt;\\\\dots f(c_2)', 'f', '[c_1, c_2]', 'f', 'f', 'f'], 'Body': \"Here's a stab at why the problem is hard for highly non-differentiable but continuous functions. So say <math_exp> is nowhere differentiable on <math_exp>. I claim that there are either infinitely or no local extrema of <math_exp>. (By local extrema, I mean extrema that occur in the interior of the interval <math_exp>.) Indeed, suppose there were finitely many, say <math_exp>. Then we can take the global maximum of <math_exp> on <math_exp>, which occurs at an interior point; it is thus a local extremum of <math_exp>. If <math_exp> is a monotone function, then it is a theorem of Lebesgue that it is differentiable a.e. In particular, the above reasoning shows that the existence of finitely many local extrema implies that <math_exp> is differentiable a.e. \"}\n",
      "1668 {'Id': '1668', 'Type': 'answer', 'ParentId': '1649', 'urls': ['http://www.johndcook.com/blog/2010/01/13/soft-maximum/'], 'exp': [], 'Body': 'The approach you outlined is commonly used in practice.  If your original problem has some nice properties, such as convexity, the approach will work well.  For example, the soft maximum is a common way to construct a series of smooth, convex approximations to the maximum function. '}\n",
      "1669 {'Id': '1669', 'Type': 'answer', 'ParentId': '1665', 'urls': [], 'exp': ['\\\\frac{p-1}{4}', 'p', '8k+5', '\\\\frac{p-1}{4}', '2k+1', '\\\\frac{p+1}{2}', '4k+3', 'k = 0,1,2 (\\\\mod 3)', 'k=0(\\\\mod 3): 2,1,0 (\\\\mod 3)', 'k=1(\\\\mod 3): 1,0,1 (\\\\mod 3)', 'k=2(\\\\mod 3): 0,2,2 (\\\\mod 3)', '3', 'k=0', '5,1,3', 'k=1', '13,3,7', 'k&gt;1', '&gt; 3', '\\\\frac{p-1}{4} = 2', 'p = 9'], 'Body': 'nice one. For <math_exp> to be odd, <math_exp> must be of the form <math_exp>, so <math_exp> is of the form <math_exp> and <math_exp> of the form <math_exp>. If <math_exp> then the three numbers are respectively congruent to <math_exp> <math_exp> <math_exp>  This means that the only way all three of them are prime numbers is that one of them is <math_exp>.  For <math_exp> we have <math_exp> which is ruled out; for <math_exp> we have <math_exp> which satisfies hypotheses; for <math_exp> all numbers are <math_exp>. The only other case to be checked is <math_exp>; in this case however <math_exp>, so this could not be a solution. '}\n",
      "1670 {'Id': '1670', 'Type': 'answer', 'ParentId': '1660', 'urls': [], 'exp': [], 'Body': \"Just to make sure: you know that what you are asking for is actually Weyl's dimension formula for the highest weight representation in the case of SL(n,C) resp. SU(n)? I suppose there is no direct combinatorial proof that does not make use of this fact. \"}\n",
      "1671 {'Id': '1671', 'Type': 'answer', 'ParentId': '1656', 'urls': [], 'exp': [], 'Body': 'Of the projective transformations fixing a conic, there is a unique one sending any given ordered triple of points to any other given ordered triple of points.  So you are free to determine the location of the first 3 vertices of the n-gon, modulo projective equivalence, but any two placements of the remaining (n-3) points are projectively inequivalent. Thus, the space of n-gons up to projective equivalence can be thought of as the space of (n-3) points on a projective line (or conic, it is isomorphic) punctured at three given points.  This has dimension (n-3). '}\n",
      "1672 {'Id': '1672', 'Type': 'answer', 'ParentId': '1601', 'urls': ['http://www.cse.unsw.edu.au/~cs2011/lect/2711_Adversary.pdf', 'http://euler.slu.edu/~goldwasser/class/loyola/comp363/2003_Spring/handouts/selectionproofs.pdf'], 'exp': ['n + \\\\lceil \\\\lg n \\\\rceil - 2', '\\\\lg n', '\\\\log_2 n', 'n', 'n/2', 'n-1', '\\\\lceil \\\\lg n \\\\rceil', '\\\\lceil \\\\lg n \\\\rceil - 1', 'm', 'n + \\\\lceil \\\\lg n\\\\rceil  - 2', 'm \\\\ge \\\\lceil \\\\lg n \\\\rceil', '\\\\lceil \\\\lg n\\\\rceil', '0', 'n', '\\\\lceil \\\\lg n\\\\rceil'], 'Body': 'The (tight) lower bound is <math_exp> (where <math_exp> means <math_exp>). I\\'ll prove tightness first: that this can be achieved (apparently the idea is due to Lewis Carroll!). First find the maximum using a \"tennis tournament\" structure: first compare the <math_exp> elements in pairs, then compare the <math_exp> \"winners\" in pairs, and so on. (Unpaired elements get a bye to the next round.) Since every element except the winner loses exactly once, this takes <math_exp> comparisons. But now note that the second largest element must be one which lost to the winner, as it couldn\\'t have been defeated by any other element. So you need to find the maximum among all the (up to) <math_exp> elements that were defeated by the winner, and finding this maximum can be done in <math_exp>. We can prove this is a lower bound as well. Let the number of elements that lost to the maximum be <math_exp>. This proves the <math_exp> lower bound if we show that <math_exp>, i.e., an adversary can make sure you always have at least <math_exp> elements that lost to the winner. This is proved here or (with a bug pointed out by @noedne below) here; an argument goes as follows: the adversary keeps for each element a “weight”, which is either <math_exp> if the element has ever lost a comparison (and is therefore known not to be the maximum element) or else the number of elements known to be less than or equal to it. Whenever a query\\'s result is already “known”, the adversary gives that answer, else it declares the one which has a larger weight the winner (breaking ties arbitrarily). The maximum is determined when some weight grows to <math_exp>, and with each comparison any weight can only double, so the number of comparisons needs to be at least <math_exp>. '}\n",
      "1673 {'Id': '1673', 'Type': 'answer', 'ParentId': '1648', 'urls': ['http://en.wikipedia.org/wiki/Poisson_distribution', 'http://en.wikipedia.org/wiki/Poisson_distribution#Parameter_estimation'], 'exp': [], 'Body': 'I\\'d vote for a Poisson distribution with a constant offset. A hand-wavy reasoning might be that the round trip time is due to a constant offset being the best-case round-trip time assuming no delays due to router queues (= wave propagation velocity over physical distance, + minimum processing time), with \"rare events\" (see wikipedia page) corresponding to queueing delays in one or more routers that make up the network path(s). As far as parameter estimation goes, I\\'m not familiar with how to do it for samples taken from a (suspected) Poisson distribution, but I\\'m sure you could find something on the Internet. aha, here we go: http://en.wikipedia.org/wiki/Poisson_distribution#Parameter_estimation -- you could use this after subtracting off the minimum of a large number of samples. drat, stupid me, I glossed over the fact that Poisson = discrete probability distribution. '}\n",
      "1674 {'Id': '1674', 'Type': 'question', 'Title': 'Non-completeness of the space of bounded linear operators', 'Tags': ['real-analysis', 'functional-analysis', 'normed-spaces'], 'AcceptedAnswerId': '1679', 'urls': [], 'exp': ['X', 'Y', 'B(X,Y)', 'X', 'Y', 'Y', 'X,Y', 'B(X,Y)'], 'Body': 'If <math_exp> and <math_exp> are normed spaces I know that the space <math_exp> of bounded linear functions from <math_exp> to <math_exp>,  is complete if <math_exp> is complete.  Is there an example of a pair of normed spaces <math_exp> s.t. <math_exp> is not complete? '}\n",
      "1675 {'Id': '1675', 'Type': 'question', 'Title': 'How can I calculate this expected rate?', 'Tags': ['statistics'], 'AcceptedAnswerId': '1690', 'urls': ['http://www.dotastrategy.com/hero-71-MogulKhanAxe.html'], 'exp': ['17\\\\%', '100 / 125 / 150 / 175', '1, 2, 3', '4', '0.7 / 0.65 / 0.6 / 0.55', '5', '1.667', '3', '1 - 0.83^5', '60.61 / 75.76 / 90.91 / 106.07'], 'Body': 'In DotA, there is a character called \"Axe\". Every time he is attacked, he has a chance to spin (<math_exp>) his blade and deal damage based on what level the skill is, <math_exp> damage for levels <math_exp>, and <math_exp>. When the spin activates, it triggers the cooldown of <math_exp> seconds, so that attacking Axe does not generate a chance to spin. I was trying to calculate an average damage per second that this skill generates, given that most of the time you will find Axe taking around <math_exp> attacks per second (average <math_exp> attacks per second from creeps, <math_exp> creeps per camp), so I figured that would mean the probability of him spinning in any second is <math_exp>, so given the damage from earlier, we should expect him to deal <math_exp> damage per second? I got this calculation, but then I realized that I have to factor in the cooldown somewhere, but I have no idea where to start. '}\n",
      "1676 {'Id': '1676', 'Type': 'question', 'Title': 'An example of a scheme in the language of schemes', 'Tags': ['intuition', 'algebraic-geometry'], 'urls': ['https://math.stackexchange.com/questions/1249/motivating-example-for-algebraic-geometry-scheme-theory', 'https://mathoverflow.net/questions/27971/why-is-there-no-cayleys-theorem-for-rings', 'http://en.wikipedia.org/wiki/Scheme_%28mathematics%29', 'http://en.wikipedia.org/wiki/Spectrum_of_a_ring', 'http://en.wikipedia.org/wiki/Locally_ringed_space'], 'exp': ['\\\\mathbb{Z}_n'], 'Body': \"Somewhat related to this question, but almost infinitely more basic. I am, should classification prove essential, a differential geometer and a topologist by inclination and by training: as an undergraduate I shunned any ring that wasn't <math_exp> or a ring of differential operators and held close the differentiable and the non-singular. It did not seem to matter then that these exotic 'schemes' and their exciting projective morphisms were beyond me, and to a certain extent it does not seem to matter now; but increasingly my old uni friends, fellow MOers (and, hey, even math.stack exchangers) are talking about nothing else but schemes. In recent months (after a frighteningly eye-opening MO question) I have found myself becoming more amenable to rings, and am less daunted by my paucity of understanding than previously. In spite of this, though, I remain entirely in the dark about schemes. I am not a complete novice. I completed an undergraduate course in algebraic geometry: illuminating, interesting, but all classical beyond belief. I have read and re-read the wikipedia page on schemes several times- taking in all of the neccessary components: the spectrum of a ring, a locally ringed space et al, but have no idea how these fit together to make the objects I fiddled with over a semester two years ago. I have made numerous guesses about generalised nulstellensatze and structure sheaves, but to explain any would probably be to complicate matters further unneccessarily. I am aware there are probably brilliant texts that do exactly what I am asking for, but I am not currently affiliated to a university and my current library would require ordering in, which for the sort of toe-dipping excercise I intend here would be overkill. So I ask: Can anyone provide me with a canonical example of a scheme, pointing along the way the topology and the spectra associated to each open set. Perhaps deeper, if it pleases: what I am looking for is a sort of 'scheme jargon safari'. I am aware this is silly, and perhaps asking for a verbatim quotation of page 2 of any decent algebraic geometry text, but I would be ever so grateful. Can anyone help? \"}\n",
      "1678 {'Id': '1678', 'Type': 'answer', 'ParentId': '1618', 'urls': [], 'exp': ['\\\\sum_{i=1}^n a_{i} f_i \\\\otimes g_i', 'L^p(X) \\\\otimes L^p(Y)', 'f_i', 'a_1 \\\\ne 0', 'g_1 \\\\ne 0', 'F(x,y) = \\\\sum_{i=1}^n a_{i} f_i(x) g_i(y) = 0', 'g_1 \\\\ne 0', 'B \\\\subset Y', '\\\\int_B g_1 \\\\ne 0', 'x', ' 0 = \\\\int_{B} F(x,y)dy = \\\\sum_{i=1}^n a_i \\\\left(\\\\int_{B} g_i\\\\right) f_i(x). ', 'f_i'], 'Body': \"EDIT: Here is a cleaned-up and corrected version of this answer, based on Pierre-Yves' suggestion (thanks!).  His answer above contains a much more complete version. If <math_exp> is not the zero element of <math_exp>, we may assume without loss of generality that the <math_exp> are linearly independent.  We can also assume that <math_exp> and <math_exp>. Suppose that the corresponding function <math_exp> a.e.  Since <math_exp>, there is a measurable <math_exp> of positive finite measure such that <math_exp> (the integral is finite by H&ouml;lder).    Then by Fubini's theorem, for a.e. <math_exp> we have <math_exp> This contradicts the assumed linear independence of the <math_exp>. \"}\n",
      "1679 {'Id': '1679', 'Type': 'answer', 'ParentId': '1674', 'urls': [], 'exp': ['X = \\\\mathbb{R}', 'Y', 'B(X, Y) \\\\simeq Y'], 'Body': 'Let <math_exp> with the Euclidean norm and let <math_exp> be a normed space which is not complete.  You should find that <math_exp>. '}\n",
      "1680 {'Id': '1680', 'Type': 'answer', 'ParentId': '1676', 'urls': [], 'exp': ['\\\\mathbb C', '\\\\mathbb C', '\\\\mathbb C', 'V', '\\\\mathbb C^n', 'n', '\\\\mathbb C^n', 'V', 'V', '0', '\\\\mathbb C', '\\\\mathbb C', 'n', 'V', 'V', 'P', 'V', 'V', 'V', '\\\\mathbb C', 'V', 'V', 'V', 'V', 'V'], 'Body': 'I am going to consider everything here over the field <math_exp>.  You can replace <math_exp> by any algebraically closed field (or even any field) with essentially no changes, but working over <math_exp> is the natural starting point, and has the advantage that one can connect with the kind of geometry/topololgy with which you are more familiar. Given a classical variety <math_exp>, you can consider all the closed subvarieties.  These satisfy the axioms of a topology, called the Zariski topology.   For definiteness, let\\'s say that our variety is an affine variety, so it is being cut out by polynomials in <math_exp>, for some <math_exp>.  The usual topology on <math_exp> induces a topology on <math_exp>, which has many more open sets than the Zariski topology (unless <math_exp> is <math_exp>-dimensional).  The point is that to be closed in the Zariski topology, you have to really be the zero locus of some polynomial, i.e. another variety, so it is hard to be Zariski closed, and hence similarly hard to be Zariski open.    (Just to be absolutely clear, let\\'s look at an example: the real line is closed in <math_exp>, but is not Zariski closed; there is no polynomial in one variable over <math_exp> that vanishes precisely on the points of the real line; indeed, such a polynomial either vanishes at only finitely many points, or else is identically zero,  and so vanishes everywhere.) You also have the notion of rational function on the variety (just think of the restriction of a ratio of polynomials in <math_exp>-variables to <math_exp>, such that the denominator does not vanish identically on <math_exp>); a rational function is called regular at a point <math_exp> of the variety if it has no singularity at that point.  Being a singularity is a Zariski closed condition (singularities occur where the denominator of the rational function, which is a polynomial, vanish), so being regular at a point is a Zariski open condition.  If we fix a Zariski open set in advance, we can look at the ring of all rational functions that are regular on that open set. These form a sheaf on <math_exp> (with its Zariski topology).  It is much \"smaller\" than the sheaves you are used to, like smooth or continuous functions.  Not only are there many fewer open sets to think about (just the Zariski open ones), but on a given open set, there will be incredibly more continuous or smooth functions than regular functions, just because being the ratio of polynomials is a very restrictive condition on a function. If we look at the global sections of this sheaf, i.e. the functions that are regular on the whole of <math_exp>, we in particular get a ring which is called the affine ring of <math_exp>.  If I just hand you this ring (as a <math_exp>-algebra), it turns out that you can recover <math_exp>, namely <math_exp> is the maximal spectrum of this ring (i.e. point of <math_exp> are in natural bijection with maximal ideals of <math_exp>).  The map one way is easy: given a point, we can look at all regular functions on <math_exp> that vanish at the point; this gives a maximal ideal in the ring of all regular functions.  That this is a bijection is harder, and is essentially equivalent to the Nullstellensatz. To see the role of the entire spectrum of the ring (i.e. the prime ideals as well as the maximal ideals) one has to say and think about more, but this is probably enough for now. To learn more, you should google \"affine ring of a variety\" or similar expressions, and you should find troves of information, at a great range of levels.  Once you understand this  basic connection, it makes sense to look at schemes in more detail. '}\n",
      "1681 {'Id': '1681', 'Type': 'answer', 'ParentId': '1676', 'urls': [], 'exp': ['\\\\text{Spec } \\\\mathbb{C}[x]', '(x - a), a \\\\in \\\\mathbb{C}', '(0)', '\\\\mathbb{C}', '(0)', '(x - a)', '\\\\mathbb{C}(x)', 'a', '(0)', '\\\\mathbb{C}(x)', 'U', '\\\\mathbb{C}(x)', 'a \\\\in U'], 'Body': '<math_exp> is probably as basic as it gets.  As a set, this is the collection of maximal ideals <math_exp> together with the prime ideal <math_exp>.  As a topological space, this is <math_exp> in the cofinite topology (the closed sets are the finite ones) together with a point <math_exp> whose closure is the entire space (and which is in every open set except the empty set).  The local ring at <math_exp> is the subring of <math_exp> of rational functions which are defined at <math_exp>, and the local ring at <math_exp> is all of <math_exp>; more generally, the section of the structure sheaf over a Zariski-open set <math_exp> is the subring of <math_exp> of rational functions which are defined at every <math_exp>. '}\n",
      "1682 {'Id': '1682', 'Type': 'answer', 'ParentId': '1676', 'urls': [], 'exp': [], 'Body': \"You should read David Eisenbud and Joe Harris's The Geometry of Schemes. Really. :) \"}\n",
      "1683 {'Id': '1683', 'Type': 'answer', 'ParentId': '709', 'urls': [], 'exp': [], 'Body': 'Echoing Americo\\'s and Isaac\\'s answers, but without appealing to the quadratic equation: First treat the special case f(x)=ax²+d (i.e. no \"x\" term); by remembering that the line that divides the parabola symmetrically passes through the vertex, and is in fact the y-axis (x=0), we see that 0 is the abscissa of the vertex. To treat the more general f(x)=ax²+bx+c, we try a substitution of the form x=x\\'-h (geometrically, this corresponds to translating your coordinate system horizontally by h units). Expanding this, you will get something like a(x\\')²+(2ah+b)x\\'+(constant term). To go back to the special case we first treated, we find the value of h such that the coefficient of x\\' is 0. Thus you obtain the expression for the vertex\\'s x-coordinate. '}\n",
      "1684 {'Id': '1684', 'Type': 'answer', 'ParentId': '1676', 'urls': [], 'exp': ['P^1', '\\\\mathbb{C}^2', 'S^2 = \\\\mathbb{C} \\\\cup { \\\\infty }', '\\\\mathbb{C}', '\\\\mathbb{C}^* \\\\cup {\\\\infty}', 'S^2', 'S^2', 'P^1', 'P^1 \\\\to \\\\mathbb{C}', '\\\\mathbb{C}^*', 'z ', '1/z', '1/z', 'z', '{\\\\infty}', '\\\\mathbb{C}', 'xy=1', 'A^1', '(x,y) \\\\to x'], 'Body': 'Well, Qiaochu lists the  affine line, so I\\'m going to do the projective line. (Everything here is over the complex numbers.) The projective line <math_exp>, or equivalently the Riemann sphere.  This is the space of all lines through the origin in <math_exp>.  Recall that the Riemann sphere can be obtained by gluing two copies of the complex plane. Namely, if <math_exp>, then one copy is just the subset <math_exp>. The other copy is <math_exp>, which is identified with the complex numbers by taking reciprocals. This is obviously a Riemann surface.  In fact, the maps defining this \"gluing\" can be checked to be algebraic (they\\'re just reciprocals), so it is in fact a scheme, and a toy example which isn\\'t affine. What\\'s a regular function over an open subset of <math_exp>? Well, one defines a holomorphic function over an open subset of <math_exp> by saying that the pull-back to each chart is holomorphic. Now here we say that the pull-back to each chart is regular in the sense of its being a rational function. Interestingly, there are only constant functions which are regular on all of <math_exp>. Indeed, such a function (considered as a map <math_exp>) would be a holomorphic map from a compact Riemann surface, hence constant. Here\\'s an algebraic argument. Consider the open set <math_exp>. On this (affine) open set, it can be checked that the only regular functions are polynomials in <math_exp> and <math_exp> (or otherwise the denominator would blow up).  However, if the function is regular everywhere, then <math_exp> can\\'t occur (that would blow up at the origin) and <math_exp> can\\'t occur (that isn\\'t defined at <math_exp>). The projective line is important because it is compact in the complex topology. The algebraic version of this is that it is proper over <math_exp>. In particular, any map out of it into a complex variety is a emph closed map. However, with affine varieties, you don\\'t have this closed property anymore. For instance, the hyperbola <math_exp> in the affine plane (clearly a closed set in the Zariski topology) projects to the non-open complement of the origin of <math_exp> via <math_exp>. This doesn\\'t happen for the projective line.  (\"Proper\" is an algebraic analog of \"compact,\" just as \"separated\" is an analog of \"Hausdorff.\") '}\n",
      "1685 {'Id': '1685', 'Type': 'question', 'Title': 'Rigorous synthetic geometry without Hilbert axiomatics', 'Tags': ['geometry', 'euclidean-geometry', 'trigonometry'], 'AcceptedAnswerId': '1697', 'urls': [], 'exp': [], 'Body': \"Are there books or article that develop (or sketch the main points) of Euclidean geometry without fudging the hard parts such as angle measure, but might at times use coordinates, calculus or other means so as to maintain rigor or avoid the detail involved in Hilbert-type axiomatizations? I am aware of Hilbert's foundations and the book by Moise.   I was wondering if there is anything more modern that tries to stay (mostly) in the tradition of synthetic geometry. \"}\n",
      "1686 {'Id': '1686', 'Type': 'answer', 'ParentId': '1563', 'urls': [], 'exp': ['(Q,&lt;)', '(R,&lt;)', 'F_p'], 'Body': \"First, I'm glad you are reading my book! :) Let me make a couple of comments on Pete's answer--this is my first time here and I don't see how to leave comments. 2) Any two dense linear orders without endpoints are elementarily equivalent. In particular <math_exp> and <math_exp> are elementarily equivalent.  So there is no first order way of expressing the completeness of the reals. 3) Any two algebraically closed fields of the same characteristic are elementarily equivalent.  So the algebraic numbers is elementarily equivalent to the complex numbers.  This means you can prove first order things about the algebraic numbers using complex analysis or the complex numbers using Galois Theory or countability. 4) Similarly the reals field is elementarily equivalent to the real algebraic numbers or to the field of real Puiseux series. One can for example use the Puiseux series to prove asymptotic properties of semialgebraic functions. Finally, Pete's comment 5) about infinite models of the theory of finite fields being elementarily equivalent isn't quite right.  This is only true if the relative algebraic closure of the prime fields are isomorphic. For example, a) take an  ultrapower of finite fields <math_exp> where the ultrafilter containing {2,4,8,...} then the resulting model has  characteristic 2, while if the ultrafilter contains the set of primes, then the ultraproduct has characteristic 0. b) if the ultrapower contains the set of primes congruent to 1 mod 4, in the ultraproduct -1 is a square, while if the ultraproduct contains the set of primes congruent to 3 mod 4 then in the ultraproduct -1 is not a square.. \"}\n",
      "1687 {'Id': '1687', 'Type': 'question', 'Title': 'Generating bezier handles based on constraints', 'Tags': ['geometry', 'algebra-precalculus', 'bezier-curve'], 'AcceptedAnswerId': '1689', 'urls': ['http://img530.imageshack.us/img530/5756/bezier.png'], 'exp': ['0.6', 'i + 1', 'i', 'i + 2'], 'Body': 'I want to try to emulate what this application can do: alt text http://img530.imageshack.us/img530/5756/bezier.png Given the red round dots (from the mouse) it is able to solve for the bezier handles given that the tension of the curve is set to <math_exp>. How could I solve for the bezier handles? Is there some way I could do this with the slope or perpendicular of the previous and next curve? Thanks *I should also add that each point is equidistant meaning point <math_exp> is the distance to point <math_exp> or point <math_exp>. '}\n",
      "1688 {'Id': '1688', 'Type': 'answer', 'ParentId': '240', 'urls': ['https://mathoverflow.net/q/12342', 'https://mathoverflow.net/a/12400'], 'exp': [], 'Body': \"To this MathOverflow question, I posted the following answer (and there are several other interesting answers there): What is the reason? The reason is the fecundity of the proof, meaning our ability to use the proof to make further mathematical conclusions. When we prove an implication (p implies q) directly, we assume p, and then make some intermediary conclusions r1, r2, before finally deducing q. Thus, our proof not only establishes that p implies q, but also, that p implies r1 and r2 and so on. Our proof has provided us with additional knowledge about the context of p, about what else must hold in any mathematical world where p holds. So we come to a fuller understanding of what is going on in the p worlds. Similarly, when we prove the contrapositive (&not;q implies &not;p) directly, we assume &not;q, make intermediary conclusions  r1, r2, and then finally conclude &not;p. Thus, we have also established not only that &not;q implies &not;p, but also, that it implies r1 and r2 and so on. Thus, the proof tells us about what else must be true in worlds where q fails. Equivalently, since these additional implications can be stated as (&not;r1 implies q), we learn about many different hypotheses that all imply q. These kind of conclusions can increase the value of the proof, since we learn not only that (p implies q), but also we learn an entire context about what it is like in a mathematial situation where p holds (or where q fails, or about diverse situations leading to q). With reductio, in contrast, a proof of (p implies q) by contradiction seems to carry little of this extra value. We assume p and &not;q, and argue  r1, r2, and so on, before arriving at a contradiction. The statements r1 and r2 are all deduced under the contradictory hypothesis that p and &not;q, which ultimately does not hold in any mathematical situation. The proof has provided extra knowledge about a nonexistant, contradictory land. (Useless!) So these intermediary statements do not seem to provide us with any greater knowledge about the p worlds or the q worlds, beyond the brute statement that (p implies q) alone. I believe that this is the reason that sometimes, when a mathematician completes a proof by contradiction, things can still seem unsettled beyond the brute implication, with less context and knowledge about what is going on than would be the case with a direct proof. For an example of a proof where we are led to false expectations in a proof by contradiction, consider Euclid's theorem that there are infinitely many primes. In a common proof by contradiction, one assumes that p1, ..., pn are all the primes. It follows that since none of them divide the product-plus-one p1...pn+1, that this product-plus-one is also prime. This contradicts that the list was exhaustive. Now, many beginner's falsely expect after this argument that whenever p1, ..., pn are prime, then the product-plus-one is also prime. But of course, this isn't true, and this would be a misplaced instance of attempting to extract greater information from the proof, misplaced because this is a proof by contradiction, and that conclusion relied on the assumption that p1, ..., pn were all the primes. If one organizes the proof, however, as a direct argument showing that whenever p1, ..., pn are prime, then there is yet another prime not on the list, then one is led to the true conclusion, that p1...pn+1 has merely a prime divisor not on the original list. (And Michael Hardy mentions that indeed Euclid had made the direct argument.) \"}\n",
      "1689 {'Id': '1689', 'Type': 'answer', 'ParentId': '1687', 'urls': [], 'exp': ['p_k', 'p_{k-1}', 'p_{k+1}'], 'Body': 'From the graphic, it appears that the handles at a given point <math_exp> are parallel to the line through <math_exp> and <math_exp> and that the lengths of the handles at every point are equal.  I suspect that the length of the handles is determined by the \"tension\" that you mention. '}\n",
      "1690 {'Id': '1690', 'Type': 'answer', 'ParentId': '1675', 'urls': [], 'exp': [], 'Body': 'Let r be the chance an attack caused axe to spin and d be the damage it does. Since we know the number of attacks on Axe per second, we just need to find the expected spin damage per attack, call this x. If someone attacks Axe once, we expect the retaliation damage to be rd. However, each attack blocks a certain number of attacks depending on cooldown, call this number b. This reduces our expected value by brx.  So we have: Note that this assumes the combat goes for an infinite amount of attacks. When the combat is shorter, the expected damage will be higher. Given the speed that combat happens in Dota, this effect will often be significant '}\n",
      "1691 {'Id': '1691', 'Type': 'question', 'Title': 'Trying to piece together an integral addition theorem', 'Tags': ['calculus', 'algebraic-geometry', 'conic-sections', 'elliptic-curves'], 'AcceptedAnswerId': '1695', 'urls': [], 'exp': ['C', '\\\\{(x,y) \\\\in \\\\mathbb{R}^2: \\\\, P(x,y) = 0 \\\\}', '\\\\omega=\\\\frac{\\\\mathrm{d}x}{y}', '\\\\int\\\\limits_0^A \\\\omega + \\\\int\\\\limits_0^B \\\\omega = \\\\int\\\\limits_0^{A \\\\oplus B} \\\\omega', '\\\\oplus', 'P(x,y) = x^2 + y^2 - 1', '(3/5,4/5)', 'AB', 'A \\\\oplus B'], 'Body': \"Let <math_exp> be a plane curve given by the set <math_exp> and define <math_exp>. Then, it is <math_exp> (with <math_exp> being addition on a group on the curve) a theorem? I'm quite sure this is what's happening when C is an elliptic curve, but I have failed to make this work out when C is defined by <math_exp> (unit circle) and the group law is defined by firing a ray through <math_exp> (chosen arbitrarily) parallel to <math_exp> and taking it's intersection with the circle as <math_exp>. \"}\n",
      "1692 {'Id': '1692', 'Type': 'answer', 'ParentId': '709', 'urls': [], 'exp': ['\\\\frac{b}{2a}', '\\\\frac{b}{2a}'], 'Body': \"Here is the simplest way I know, using a bit of algebra.  Starting with Isaac's observation that the vertex lies on the axis of symmetry, the line x = k.  Question is, what's k? That's your answer. if f(x) = ax2 + bx + c Then the symmetry around x = k means that, for any h f(k+h) = f(k-h) From that, just multiply it all out and some simple additive cancellation gives -2akh - bh = 2akh + bh h cancels out, and a bit more algebra gives k = -<math_exp> To really complete this, however, we should show that x = k = -<math_exp> is a maximum or minimum point. Again, with more algebra: f(k+h) - f(k) boils down to ah2 So when a > 0, then f(k+h) > f(k) for any h != 0, establishing that x = k is a minimum point And likewise when a &lt; 0, x = k is a maximum. This also shows the symmetry around x = k, and how the coefficient a determines whether the curve is concave upward or downward. \"}\n",
      "1694 {'Id': '1694', 'Type': 'question', 'Title': 'How did the notation \"ln\" for \"log base e\" become so pervasive?', 'Tags': ['notation', 'logarithms', 'math-history'], 'urls': ['http://en.wikipedia.org/wiki/Logarithm'], 'exp': ['x', '\\\\ln(x)', '\\\\log_e(x)', '\\\\log(x)', '\\\\ln', '\\\\ln', '\\\\ln', '\\\\ln', '\\\\ln'], 'Body': 'Wikipedia sez: The natural logarithm of <math_exp> is often written \"<math_exp>\", instead of <math_exp> especially in disciplines where it isn\\'t written \"<math_exp>\". However, some mathematicians disapprove of this notation. In his 1985 autobiography, Paul Halmos criticized what he considered the \"childish <math_exp> notation,\" which he said no mathematician had ever used. In fact, the notation was invented by a mathematician, Irving Stringham, professor of mathematics at University of California, Berkeley, in 1893. Apparently the notation \"<math_exp>\" first appears in Stringham\\'s book Uniplanar algebra: being part I of a propædeutic to the higher mathematical analysis. But this doesn\\'t explain why \"<math_exp>\" has become so pervasive. I\\'m pretty sure that most high schools in the US at least still use the notation \"<math_exp>\" today, since all of the calculus students I come into contact with at Berkeley seem to universally use \"<math_exp>\". How did this happen? '}\n",
      "1695 {'Id': '1695', 'Type': 'answer', 'ParentId': '1691', 'urls': [], 'exp': ['0', '0', 'dx/y', 'dx/y', 'dx/y', 'y^2=f(x)', '\\\\deg(f)=3'], 'Body': '<math_exp>, the lower limit for the integrals, should be whatever point on the curve is <math_exp> for the group law. Then <math_exp> should be rotation-invariant for any circle. It will not be true for a rotated ellipse; there is an invariant differential but it is not <math_exp>. For an elliptic curve, <math_exp> being the invariant differential relies on the curve being written as <math_exp> with <math_exp>. If you move the curve the invariant differential will be a different one. For higher genus the curve does not have an addition law. '}\n",
      "1697 {'Id': '1697', 'Type': 'answer', 'ParentId': '1685', 'urls': ['http://books.google.co.uk/books?id=EJCSL9S6la0C&amp;lpg=PP1&amp;dq=hartshorne&amp;pg=PP1#v=onepage&amp;q&amp;f=false'], 'exp': [], 'Body': \"You might look at Hartshorne's . \"}\n",
      "1698 {'Id': '1698', 'Type': 'answer', 'ParentId': '1694', 'urls': [], 'exp': [], 'Body': 'One simple possible reason It is very difficult to come up with a   notation that is concise, correct and   understandable by at least two people.   - P A M Dirac / Richard Feynmann This one is concise, correct and is understandable at least by you and me! Generally, notations used in popular textbooks spread like virus. Books become popular because of good notation and notation becomes widespread because book is popular. '}\n",
      "1700 {'Id': '1700', 'Type': 'answer', 'ParentId': '1694', 'urls': ['http://en.wikipedia.org/wiki/Reverse_Polish_notation'], 'exp': ['\\\\ln', '\\\\log_e', 'e', '2.71828...', '\\\\log_c x = \\\\frac{\\\\log_a x}{\\\\log_a c}'], 'Body': 'It is probably at least in part an instance of consumer lock-in, the phenomenon where the advantages of the majority of the market using the same or compatible products outweigh sufficiently minor differences of one version of the product versus another.  (When I learned this term, the canonical example was VHS versus Beta.  I am just about old enough to remember that when VCR terminology first came out, vendors would carry both.  Already when the first video stores opened up there was more product available in VHS, so a lot of stores would have just one rack with all the Beta products.  And eventually of course Beta died.  However those who remember and care seem to largely agree that Beta was the superior technology.  Apologies for giving such an old-fogey example.  Can someone suggest something more current?) In particular, when it comes to electronic calculators, having everyone agree what\\'s going to happen when you press a certain button is a good thing.  (In fact, another lock-in phenomenon is that when I was in high school in the early 90\\'s, most students had Texas Instruments calculators of one kind or other.  Among the real geeks it was known that the \"cadillac of calculators\" was actually the Hewlett-Packard, which used reverse polish notation.  Serious CS people appreciate RPN, but the problem is that if you\\'re a high school kid and pick up such a calculator for the first time, it\\'s very hard to figure out what\\'s going on.  I haven\\'t seen an HP calculator for many years.)  The notation <math_exp> is simple and unambiguous: you don\\'t have to like it (and I don\\'t, especially), but you know what it means, and it\\'s easier to fit on a small calculator key than <math_exp>.  I think if you\\'re first learning about logarithms, then base ten is probably the simplest (to have any clue what <math_exp> is other than \"about <math_exp>\" requires some calculus, and is in my experience one of the more subtle concepts of first year calculus), so it\\'s reasonable to have that be the standard base for the logarithm for a general audience. Also, I\\'m sure everyone here knows this but I wish my calculus students had a better appreciation of it: exactly what base you take for the logarithm only matters up to a multiplicative constant anyway, since <math_exp>.  So it\\'s no big deal either way. '}\n",
      "1701 {'Id': '1701', 'Type': 'question', 'Title': 'Yoneda-Lemma as generalization of Cayley`s theorem?', 'Tags': ['abstract-algebra', 'group-theory', 'category-theory', 'yoneda-lemma'], 'AcceptedAnswerId': '1708', 'urls': [], 'exp': [], 'Body': \"I came across the statement that Yoneda-lemma is a generalization of Cayley`s theorem which states, that every group is isomorphic to a group of permutations. How exactly does generalizes Yoneda-lemma Cayley`s theorem? Can Cayley's theorem be deduced from Yoneda lemma, is it a generalization of a particular case of Yoneda, or is this instead, a philosophical statement? To me, it seems that Yoneda embedding is more canonical than Cayley's theorem because in the latter you have to choose whether the group acts from the left or from the right on itself. But maybe this is an optical illusion. \"}\n",
      "1702 {'Id': '1702', 'Type': 'answer', 'ParentId': '1601', 'urls': ['http://compgeom.cs.uiuc.edu/~jeffe/teaching/497/02-selection.pdf'], 'exp': ['k', 'V(n,k)', 'n', ' V(n,k) \\\\ge n + R(n,k) - 2\\\\sqrt{R(n,k)} ', ' R(n,k) = \\\\log \\\\binom{n}{k} - \\\\log (n-k+1) + 3'], 'Body': 'The definitive answer for any <math_exp> (not just the second largest) is discussed in these notes. In short, if <math_exp> is the number of comparisons needed to determine the kth largest element in a set of size <math_exp>, then  <math_exp> where <math_exp> '}\n"
     ]
    }
   ],
   "source": [
    "no_of_posts_loaded = 1500\n",
    "nodeListReduced = {}\n",
    "nodeList = []\n",
    "for r in range(min(no_of_posts_loaded, len(root))):\n",
    "    nodeList.append(root[r].attrib)\n",
    "    \n",
    "    \n",
    "for node in nodeList:\n",
    "    post = {}\n",
    "    post['Id'] = node['Id']\n",
    "    \n",
    "    if (node['PostTypeId'] == '1'): #if question\n",
    "        post['Type'] = 'question'\n",
    "        post['Title'] = node['Title']\n",
    "        post['Tags'] = getTopicTags(node['Tags'])\n",
    "        if 'AcceptedAnswerId' in node:\n",
    "            post['AcceptedAnswerId'] = node['AcceptedAnswerId']\n",
    "            \n",
    "    if (node['PostTypeId'] == '2'): #if answer\n",
    "        post['Type'] = 'answer'\n",
    "        post['ParentId'] = node['ParentId']\n",
    "        \n",
    "    body = node['Body']\n",
    "    body = removeParagraphTags(body)\n",
    "    \n",
    "    anchor_data = processAnchorTags(body)\n",
    "    post['urls'] = anchor_data['urls']\n",
    "    body = anchor_data['para']\n",
    "    \n",
    "    math_data = processMathFormulae(body)\n",
    "    post['exp'] = math_data['exp']\n",
    "    body = math_data['para']\n",
    "    \n",
    "    body = removeAnyOtherTags(body)\n",
    "    \n",
    "    post['Body'] = body\n",
    "    nodeListReduced[node['Id']] = post\n",
    "\n",
    "for postId in nodeListReduced:\n",
    "    print(postId, nodeListReduced[postId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5e3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8e66c7f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6102668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6063934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b4981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e085b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71dd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67545054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27648524",
   "metadata": {},
   "source": [
    "# Text Matching Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb7c4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING DATA FOR SEARCHING     SUBJECT TO CHANGE\n",
    "\n",
    "q_data = []\n",
    "a_data = []\n",
    "\n",
    "# for node in list(nodeListReduced.values()):\n",
    "#     print(node, type(node))\n",
    "\n",
    "for node in list(nodeListReduced.values()):\n",
    "    if(node['Type'] == 'question'):\n",
    "        data = {}\n",
    "        data['id'] = node['Id']\n",
    "        para = node['Title']+\" \"+node['Body']+\" \"+\". \".join(node['Tags'])\n",
    "        data['para'] = para\n",
    "        data['exp'] = node['exp']\n",
    "        q_data.append(data)\n",
    "    if(node['Type'] == 'answer'):\n",
    "        data = {}\n",
    "        data['id'] = node['Id']\n",
    "        data['para'] = node['Body']\n",
    "        data['exp'] = node['exp']\n",
    "        a_data.append(data)\n",
    "\n",
    "# for data in q_data:\n",
    "#     print(data)\n",
    "# for data in a_data:\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29411d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts:  1500\n",
      "Correctly answered:  381\n",
      "Accuracy:  25.4 %\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "paragraphs = [data['para'] for data in q_data]+[data['para'] for data in a_data]\n",
    "\n",
    "# paragraphs = [\n",
    "#     \"Three years later, the coffin was still full of Jello.\",\n",
    "#     \"Three years later, the coffin was still full of Jello.\",\n",
    "#     \"The person box was packed with jelly many dozens of months later.\",\n",
    "#     \"The person box was packed with jelly many dozens of months later.\"\n",
    "# ]\n",
    "\n",
    "para_embeddings = model.encode(paragraphs)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for data in q_data:\n",
    "    data['embedding'] = para_embeddings[idx]\n",
    "    idx+=1\n",
    "    \n",
    "for data in a_data:\n",
    "    data['embedding'] = para_embeddings[idx]\n",
    "    idx+=1\n",
    "\n",
    "table_size = 20\n",
    "    \n",
    "for question in q_data:\n",
    "    score_table = []\n",
    "    for answer in a_data:\n",
    "        score = {}\n",
    "        score['id'] = answer['id']\n",
    "        value = cosine_similarity([question['embedding']], [answer['embedding']])\n",
    "        score['value'] = value[0][0]\n",
    "        score_table.append(score)\n",
    "    score_table.sort(key=lambda i:i['id'])\n",
    "    question['text_score'] = score_table # slice this table using table_size variable if needed\n",
    "\n",
    "count = 0    \n",
    "for data in q_data:\n",
    "    answers = [ans['id'] for ans in data['text_score']]\n",
    "    for ans_id in answers:\n",
    "        parent_question = nodeListReduced[ans_id]['ParentId']\n",
    "        if(data['id'] == parent_question):\n",
    "            count+=1\n",
    "            break\n",
    "\n",
    "print(\"Total posts: \", no_of_posts_loaded)\n",
    "print(\"Correctly answered: \", count)\n",
    "print(\"Accuracy: \", (count*100)/no_of_posts_loaded,\"%\")\n",
    "#     print(data['id'], \"-->\", data['text_score'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bf5bd",
   "metadata": {},
   "source": [
    "# Math Regex Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3821b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEscapeSequence(math_exp):\n",
    "    chars = ['.', '^', '$', '*', '+', '?','(', ')', '\\\\', '[', '{', '|', '-', ']']\n",
    "    new_exp = ''\n",
    "    for ch in math_exp:\n",
    "        if ch in chars:\n",
    "            new_exp += '\\\\' + ch\n",
    "        else:\n",
    "            new_exp += ch\n",
    "    return new_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eb7f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNumbers(math_exp):\n",
    "    new_exp = ''\n",
    "    regex = re.compile(r'(((\\+|-)?([0-9]+)(\\.[0-9]+)?)|((\\+|-)?\\.?[0-9]+))')\n",
    "    new_exp = re.sub(regex, '(((\\+|-)?([0-9]+)(\\.[0-9]+)?)|((\\+|-)?\\.?[0-9]+))', math_exp)\n",
    "    return new_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9975526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceVariables(math_exp):\n",
    "    new_exp = ''\n",
    "    regex = re.compile(r'[a-zA-Z]+')\n",
    "    new_exp = re.sub(regex, '[a-zA-Z]+', math_exp)\n",
    "    return new_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fac1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNonTags(math_exp):\n",
    "    return replaceVariables(replaceNumbers(addEscapeSequence(math_exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98451444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_character_referecnes(math_exp): # checking for characters in the form of \"&(something);\" like \"&amp;\"\n",
    "    new_exp = ''\n",
    "    regex = re.compile(r'&.+?;')\n",
    "    start = 0\n",
    "    end = len(math_exp)\n",
    "    while regex.search(math_exp, start, end) is not None:\n",
    "        x = regex.search(math_exp, start, end)\n",
    "        new_exp += replaceNonTags(math_exp[start:x.start()])+math_exp[x.start():x.end()]\n",
    "        start = x.end()\n",
    "    if len(math_exp[start:end])>0:\n",
    "        new_exp += replaceNonTags(math_exp[start:end])\n",
    "    return new_exp        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fe399a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTagsAndVariables(math_exp):\n",
    "    tag_set = set(['frac', 'tan', 'log_2', 'tanh', 'sqrt'])\n",
    "    new_exp = ''\n",
    "    breaks = [0]\n",
    "    \n",
    "    for idx in range(len(math_exp)):\n",
    "        if(math_exp[idx] == '\\\\'):\n",
    "            breaks.append(idx)\n",
    "            \n",
    "    breaks.append(len(math_exp))\n",
    "    \n",
    "    for idx in range(len(breaks)-1):\n",
    "        start = breaks[idx]\n",
    "        end = breaks[idx+1]\n",
    "        sub_exp = math_exp[start:end]\n",
    "        start = 0\n",
    "        new_sub_exp = ''\n",
    "        if(sub_exp == '\\\\'):\n",
    "            new_exp += sub_exp\n",
    "            continue\n",
    "        elif(len(sub_exp) and sub_exp[0] == '\\\\'):\n",
    "            new_sub_exp += '\\\\'\n",
    "            start = 1\n",
    "        found = False\n",
    "        for pos in reversed(range(start, len(sub_exp))):\n",
    "            if(sub_exp[start:pos+1] in tag_set):\n",
    "                found = True\n",
    "                new_sub_exp += sub_exp[:pos+1]+check_character_referecnes(sub_exp[pos+1:])\n",
    "                break\n",
    "        if(not found):\n",
    "            new_sub_exp = check_character_referecnes(sub_exp)\n",
    "        new_exp += new_sub_exp\n",
    "    \n",
    "    return new_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eedab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_regexes(input_regex, regexes):\n",
    "    stack = []\n",
    "    for i in range(len(input_regex)):\n",
    "        if (input_regex[i:i+2] == '\\\\{'):\n",
    "            stack.append(i+1)\n",
    "        elif (input_regex[i] == '}'):\n",
    "            new_regex = input_regex[0:stack[-1]+1]+'.+?'+input_regex[i:]\n",
    "            if new_regex in regexes:\n",
    "                return\n",
    "            regexes.add(new_regex)\n",
    "            create_all_regexes(new_regex, regexes)\n",
    "            del stack[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a3751",
   "metadata": {},
   "source": [
    "# Math Score Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4150bdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'para': 'What Does it Really Mean to Have Different Kinds of Infinities? Can someone explain to me how there can be different kinds of infinities? I was reading \"The man who loved only numbers\" by Paul Hoffman and came across the concept of countable and uncountable infinities, but they\\'re only words to me. Any help would be appreciated.  elementary-set-theory. intuition. infinity. faq', 'exp': []}\n",
      "{'id': '3', 'para': 'List of interesting math podcasts? mathfactor is one I listen to.  Does anyone else have a recommendation?  soft-question. big-list. online-resources', 'exp': []}\n",
      "{'id': '5', 'para': 'How can you prove that the square root of two is irrational? I have read a few proofs that <math_exp> is irrational. I have never, however, been able to really grasp what they were talking about. Is there a simplified proof that <math_exp> is irrational?  elementary-number-theory. proof-writing. radicals. rationality-testing', 'exp': ['\\\\sqrt{2}', '\\\\sqrt{2}']}\n",
      "{'id': '6', 'para': \"What is your favorite online graphing tool? I'm looking for a nice, quick online graphing tool. The ability to link to, or embed the output would be handy, too.  soft-question. math-software\", 'exp': []}\n",
      "{'id': '8', 'para': \"How are we able to calculate specific numbers in the Fibonacci Sequence? I was reading up on the Fibonacci Sequence,  <math_exp> when I've noticed some were able to calculate specific numbers. So far I've only figured out creating an array and counting to the value, which is incredibly simple, but I reckon I can't find any formula for calculating a Fibonacci number based on it's position. Is there a way to do this? If so, how are we able to apply these formulas to arrays?  linear-algebra. combinatorics. generating-functions. fibonacci-numbers\", 'exp': ['\\\\text {{1,1,2,3,5,8,13,....}}']}\n",
      "{'id': '11', 'para': 'Is it true that <span class=\"math-container\" id=\"360\">0.999999999\\\\dots=1</span>? I\\'m told by smart people that <math_exp> and I believe them, but is there a proof that explains why this is?  real-analysis. algebra-precalculus. real-numbers. decimal-expansion', 'exp': ['0.999999999\\\\dots=1']}\n",
      "{'id': '18', 'para': 'How do you calculate the semi-minor axis of an ellipsoid? Given the semi-major axis and a flattening factor, is it possible to calculate the semi-minor axis?  geometry', 'exp': []}\n",
      "{'id': '20', 'para': \"What is a real number (also rational, decimal, integer, natural, cardinal, ordinal...)? In mathematics, there seem to be a lot of different types of numbers. What exactly are: And as workmad3 points out, some more advanced types of numbers (I'd never heard of) Are there any other types of classifications of a number I missed?  terminology. definition. number-systems\", 'exp': []}\n",
      "{'id': '22', 'para': 'Why is the matrix-defined Cross Product of two 3D vectors always orthogonal? By matrix-defined, I mean <math_exp> ...instead of the definition of the product of the magnitudes multiplied by the sign of their angle, in the direction orthogonal) If I try cross producting two vectors with no <math_exp> component, I get one with only <math_exp>, which is expected. But why? As has been pointed out, I am asking why the algebraic definition lines up with the geometric definition.  linear-algebra. matrices. inner-product-space. orthogonality. cross-product', 'exp': ['\\\\left&lt;a,b,c\\\\right&gt;\\\\times\\\\left&lt;d,e,f\\\\right&gt; = \\\\left|     \\\\begin{array}{ccc}  i &amp; j &amp; k\\\\\\\\  a &amp; b &amp; c\\\\\\\\  d &amp; e &amp; f  \\\\end{array}    \\\\right|', 'k', 'k']}\n",
      "{'id': '29', 'para': \"Can you recommend a decent online or software calculator? I'm looking for an online or software calculator that can show me the history of items I typed in, much like an expensive Ti calculator. Can you recommend any?  soft-question. big-list. math-software. computer-algebra-systems\", 'exp': []}\n",
      "{'id': '51', 'para': 'The cow in the field problem (intersecting circular areas) What length of rope should be used to tie a cow to an exterior fence post of a circular field so that the cow can only graze half of the grass within that field? updated: To be clear: the cow should be tied to a post on the exterior of the field, not a post at the center of the field.  geometry', 'exp': []}\n",
      "{'id': '56', 'para': \"What Is An Inner Product Space? As I've understood it, what I've learned is that the dot product is just one of many possible inner product spaces. Can someone explain this concept? When is it useful to define it as something other than the dot product?  linear-algebra. vector-spaces. inner-product-space\", 'exp': []}\n",
      "{'id': '57', 'para': 'How do the Properties of Relations work? This is simply not clicking for me. I\\'m currently learning math during the summer vacation and I\\'m on the chapter for relations and functions. There are five properties for a relation: Reflexive - <math_exp> Symmetrical - <math_exp> ; <math_exp> Antisymmetrical - <math_exp> &amp;&amp; (<math_exp>|| <math_exp>) Asymmetrical -<math_exp> &amp;&amp; !(<math_exp>|| <math_exp>) Transitive - if <math_exp> &amp;&amp; <math_exp>, then <math_exp> If that\\'s not what you call the properties in English, then please let me know- I have to study it in German, unfortunately, and these are my rough translations. Continuing on, I just don\\'t know what to do with this information practically. The examples of the book are horrible: 1) \"Is the same age as\" is apparently reflexive, symmetrical and transitive.  2) \"Is related to\" is also apparently reflexive, symmetrical and transitive. 3) \"Is older than\" is asymmetric, antisymmetric and transitive. There are more useless examples like this. I have no idea how it comes to these conclusions because we\\'re talking about a literal statement. I was hoping perhaps for some real mathematical examples, but the book falls short on those. I would greatly appreciate it if somebody could explain the above example and perhaps give me a better use for Relations other than... that. Also, how can a relation be a- and antisymmetrical at the same time? Don\\'t they cancel each other out?  elementary-set-theory. relations', 'exp': ['R \\\\rightarrow  R', 'R \\\\rightarrow S', 'S \\\\rightarrow  R', 'R \\\\rightarrow S', 'R \\\\rightarrow  R', 'S \\\\rightarrow  S', 'R \\\\rightarrow S', 'R \\\\rightarrow  R', 'S \\\\rightarrow  S', 'R \\\\rightarrow S', 'S \\\\rightarrow T', 'R \\\\rightarrow T']}\n",
      "{'id': '58', 'para': \"Real life usage of Benford's Law I recently discovered Benford's Law. I find it very fascinating. I'm wondering what are some of the real life uses of Benford's law. Specific examples would be great.  soft-question. big-list. statistics. applications\", 'exp': []}\n",
      "{'id': '59', 'para': 'Calculating an Angle from <span class=\"math-container\" id=\"911\">2</span> points in space Given two points <math_exp> , <math_exp> around the origin <math_exp> in <math_exp> space, how would you calculate the angle from <math_exp> to <math_exp>? How would this change in <math_exp> space?  linear-algebra. geometry', 'exp': ['$p_1$', '$p_2$', '(0,0)', '2D', 'p_1', 'p_2', '3D']}\n",
      "{'id': '67', 'para': \"What is an elliptic curve, and how are they used in cryptography? I hear a lot about Elliptic Curve Cryptography these days, but I'm still not quite sure what they are or how they relate to crypto...  cryptography. elliptic-curves\", 'exp': []}\n",
      "{'id': '71', 'para': \"Real world uses of Quaternions? I've recently started reading about Quaternions, and I keep reading that for example they're used in computer graphics and mechanics calculations to calculate movement and rotation, but without real explanations of the benefits of using them. I'm wondering what exactly can be done with Quaternions that can't be done as easily (or easier) using more tradition approaches, such as with Vectors?  soft-question. big-list. linear-algebra. applications. quaternions\", 'exp': []}\n",
      "{'id': '74', 'para': \"Sum of Gaussian Variables Let's say I know <math_exp> is a Gaussian Variable. Moreover, I know <math_exp> is a Gaussian Variable and <math_exp>. Let's <math_exp> and <math_exp> are Independent. How can I prove <math_exp> is a Gaussian Random Variable if and only if <math_exp> is a Gaussian R.V.? It's easy to show the other way around (<math_exp>, <math_exp> Orthogonal and Normal hence create a Gaussian Vector hence any Linear Combination of the two is a Gaussian Variable). Thanks  probability-theory\", 'exp': ['X', 'Y', 'Y=X+Z', 'X', 'Z', 'Y', 'Z', 'X', 'Z']}\n",
      "{'id': '75', 'para': 'What are the differences between rings, groups, and fields? Rings, groups, and fields all feel similar. What are the differences between them, both in definition and in how they are used?  terminology. abstract-algebra', 'exp': []}\n",
      "{'id': '77', 'para': 'Understanding Dot and Cross Product What purposes do the Dot and Cross products serve? Do you have any clear examples of when you would use them?  linear-algebra. cross-product', 'exp': []}\n",
      "{'id': '81', 'para': \"List of Interesting Math Blogs I have the one or other interesting Math blog in my feedreader that I follow. It would be interesting to compile a list of Math blogs that are interesting to read, and do not require research-level math skills. I'll start with my entries:  soft-question. big-list. online-resources\", 'exp': []}\n",
      "{'id': '83', 'para': \"What are some good ways to get children excited about math? I'm talking in the range of 10-12 years old, but this question isn't limited to only that range. Do you have any advice on cool things to show kids that might spark their interest in spending more time with math? The difficulty for some to learn math can be pretty overwhelming. Do you have any teaching techniques that you find valuable?  big-list. education\", 'exp': []}\n",
      "{'id': '90', 'para': \"Online resources for learning Mathematics Not sure if this is the place for it, but there are similar posts for podcasts and blogs, so I'll post this one. I'd be interested in seeing a list of online resources for mathematics learning. As someone doing a non-maths degree in college I'd be interested in finding some resources for learning more maths online, most resources I know of tend to either assume a working knowledge of maths beyond secondary school level, or only provide a brief summary of the topic at hand. I'll start off by posting MIT Open Courseware, which is a large collection of lecture notes, assignments and multimedia for the MIT mathematics courses, although in many places it's quite incomplete.  reference-request. online-resources\", 'exp': []}\n",
      "{'id': '97', 'para': 'How to accurately calculate the error function <span class=\"math-container\" id=\"954\">\\\\operatorname{erf}(x)</span> with a computer? I am looking for an accurate algorithm to calculate the error function <math_exp> I have tried using the following formula, <img src=\"https://i.stack.imgur.com/MY9Mv.gif\" alt=\"math97 second question example\"> (Handbook of Mathematical Functions, formula 7.1.26), but the results are not accurate enough for the application.  statistics. algorithms. numerical-methods. special-functions', 'exp': ['\\\\operatorname{erf}(x)=\\\\frac{2}{\\\\sqrt{\\\\pi}}\\\\int_0^x e^{-t^2}\\\\ dt']}\n",
      "{'id': '98', 'para': \"Chased By a Lion and other Pursuit Problems I am looking for a reference (book or article) that poses a problem that seems to be a classic, in that I've heard it posed many times, but that I've never seen written anywhere: that of the possibility of a man in a circular pen with a lion, each with some maximum speed, avoiding capture by that lion. References to pursuit problems in general would also be appreciated, and the original source of this problem.  reference-request\", 'exp': []}\n"
     ]
    }
   ],
   "source": [
    "for data in q_data[:25]:\n",
    "    print(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08d89ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "def get_ans_exp_score(q_exp, a_exp):\n",
    "    og = processTagsAndVariables(q_exp)\n",
    "    all_regex = set()\n",
    "    all_regex.add(og)\n",
    "    create_all_regexes(og, all_regex)\n",
    "    matches = 0\n",
    "    for regex in all_regex:\n",
    "        if re.search(regex, a_exp) is not None:\n",
    "            matches += 1\n",
    "    score = matches/len(all_regex)\n",
    "    return score\n",
    "\n",
    "math_exp = 'whole\\\\frac{\\\\tanh{45}}{\\\\log_2{denominator}}'\n",
    "math_exp_2 = 'akkar\\\\frac{\\\\tanh{x}}{\\\\log_2{16}}'\n",
    "\n",
    "print(get_ans_exp_score(math_exp, math_exp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddc8cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_answer_score(q_exp, a_exp_list):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    new_list = [a_exp for a_exp in a_exp_list if len(a_exp)>1]\n",
    "    if len(new_list) == 0:\n",
    "        return 0\n",
    "    for a_exp in new_list:\n",
    "        count += 1\n",
    "        total += get_ans_exp_score(q_exp, a_exp)\n",
    "    score = total/count\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a6fd1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_math_matching_score(q_exp_list, a_exp_list):\n",
    "    max_score = 0\n",
    "    for q_exp in q_exp_list:\n",
    "        if len(q_exp) == 1:\n",
    "            continue\n",
    "        max_score = max(max_score, get_overall_answer_score(q_exp, a_exp_list))\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e778805",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in q_data:\n",
    "    score_table = []\n",
    "    for answer in a_data:\n",
    "        score = {}\n",
    "        score['id'] = answer['id']\n",
    "        value = get_math_matching_score(question['exp'], answer['exp'])\n",
    "        score['value'] = value\n",
    "        score_table.append(score)\n",
    "    score_table.sort(key=lambda i: i['id'])\n",
    "    question['math_score'] = score_table # slice this table using table_size variable if needed\n",
    "    \n",
    "    \n",
    "# count = 0    \n",
    "# for data in q_data:\n",
    "#     answers = [ans['id'] for ans in data['math_score']]\n",
    "#     for ans_id in answers:\n",
    "#         parent_question = nodeListReduced[ans_id]['ParentId']\n",
    "#         if(data['id'] == parent_question):\n",
    "#             count+=1\n",
    "#             break\n",
    "\n",
    "# print(\"Total posts: \", no_of_posts_loaded)\n",
    "# print(\"Correctly answered: \", count)\n",
    "# print(\"Accuracy: \", (count*100)/no_of_posts_loaded,\"%\")\n",
    "# #     print(data['id'], \"-->\", data['text_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aedb4fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\sqrt\\{(((\\+|-)?([0-9]+)(\\.[0-9]+)?)|((\\+|-)?\\.?[0-9]+))} <class 'str'>\n",
      "\\sqrt{2} \t\t\t--> 1.0\n",
      "R=\\sqrt{2}=\\frac{Q}{D} \t\t\t--> 1.0\n",
      "R^2 = 2 = \\frac{Q^2}{D^2} \t\t\t--> 0.0\n",
      "Q^2 \t\t\t--> 0.0\n",
      "Q^2 = 2^1 x \t\t\t--> 0.0\n",
      "Q^2 \t\t\t--> 0.0\n",
      "\\sqrt{2} \t\t\t--> 1.0\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "\n",
    "## 16 {'Id': '16', 'Type': 'answer', 'ParentId': '5', 'urls': [], \n",
    "## 'exp': ['\\\\sqrt{2}', 'R=\\\\sqrt{2}=\\\\frac{Q}{D}', 'Q', 'D', 'R', 'R^2 = 2 = \\\\frac{Q^2}{D^2}', \n",
    "## 'Q', 'D', 'Q^2', '2', 'Q^2 = 2^1 x', 'x', 'Q^2', '\\\\sqrt{2}'],\n",
    "\n",
    "query_exp = nodeListReduced['16']['exp'][0]\n",
    "query_regex = processTagsAndVariables(query_exp)\n",
    "# query_regex = re.compile(r'{}'.format(regex_str))\n",
    "print(query_regex, type(query_regex))\n",
    "regex_set = set()\n",
    "regex_set.add(query_regex)\n",
    "create_all_regexes(query_regex, regex_set)\n",
    "for exp in nodeListReduced['16']['exp']:\n",
    "    if (len(exp) == 1):\n",
    "        continue\n",
    "    match_count = 0\n",
    "    for regex in regex_set:\n",
    "        compiled_regex = re.compile(r'{}'.format(regex))\n",
    "        if re.search(compiled_regex, exp) is not None:\n",
    "            match_count+=1\n",
    "    score = match_count/len(regex_set)\n",
    "    print(exp,\"\\t\\t\\t-->\",score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fadf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
